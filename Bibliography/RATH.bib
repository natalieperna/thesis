
@Misc{AAAA-ref,
  todo = {A-0875, A-0876 abandoned!},
  foldingcheck = {\{  egrep -e '^\}|^@' ~/doc/bib/ref.bib | less},
  key = {AAAA-ref},
  editor = {Wolfram Kahl},
  title = {{\tt ref.bib}},
  note = {($Id: ref.bib,v 1.15 2003/08/23 18:48:02 kahl Exp kahl $)},
  abstract = {\obeylines
     # $Log: ref.bib,v $
     # Revision 1.15  2003/08/23 18:48:02  kahl
     # After first simple normalisation using HBibTeX/Normalise.
     #
     # Revision 1.14  2003/08/23 18:41:25  kahl
     # Before normalisation using HBibTeX.
     #
     # Revision 1.13  1995/01/24  13:52:45  kahl
     # Lots and lots of additions, A-032?
     #
     # Revision 1.12  1994/09/06  15:10:46  kahl
     # A-0266, B-0023
     #
     # Revision 1.11  1994/06/10  10:22:43  kahl
     # Moved M- and N- series into doc.bib
     # MPC92, LPAR94, Uncover
     # A-0244, B-0020
     #
     # Revision 1.10  1994/04/28  07:58:47  kahl
     # ICALP1994, CTRS1994, WG1994 refereed papers, TU Berlin reports, Caspi
     # Manuals WKloc M- and N- started
     # A-0232
     #
     # Revision 1.9  1994/04/07  13:10:57  kahl
     # LPAR93, ALP1990, eindhoven-ftp
     #
     # Revision 1.8  1994/04/05  12:56:27  kahl
     # Before move to demokrit
     #
     # Revision 1.7  1994/02/25  17:10:13  kahl
     # Curry, Church, Schoenfinkel etc., weitere Bereinigungen
     #
     # Revision 1.6  1994/02/25  12:04:51  kahl
     # RTA93 abstracts, found a few lost sons.
     #
     # Revision 1.5  1994/02/23  15:39:28  kahl
     # STACS1993
     #
     # Revision 1.4  1994/02/23  14:56:20  kahl
     # POPL89,93
     #
     # Revision 1.3  1994/02/23  12:37:03  kahl
     # POPL90-92, TLCA93
     #
     # Revision 1.2  1994/02/11  17:12:43  kahl
     # CPM92, IWWERT90, POPL92 first half
     #
     # Revision 1.1  1994/02/11  11:30:10  kahl
     # Initial revision
     #
     .}
}


@InProceedings{AGTIVE2007Sierpinski,
  author =       {Gabriele Taentzer and Enrico Biermann and D{\'e}nes Bisztray and
                  Bernd Bohnet and Iovka Boneva and Artur Boronat and
                  Leif Geiger and Rubino Gei{\ss} and {\'A}kos Horvath and
                  Ole Kniemeyer and Tom Mens and Benjamin Ness and
                  Detlef Plump and Tam{\'a}s Vajk},
  title =        {Generation of {Sierpinski} Triangles: A Case Study for Graph Transformation Tools},
  crossref = {AGTIVE2007},
  OPTDOI =       {10.1007/978-3-540-89020-1_35},
  pages =     {514--539},
  URL =      {http://www.springerlink.com/content/f65r3041v3015222/}
}

@unpublished{Kroll-Mallon-2007,
  author={Moritz Kroll and Christoph H. Mallon},
  title={A finite taste of infinity: A {GrGen.NET} solution of the {Sierpinski} triangle case for the {AGTIVE 2007 Tool Contest}},
  year=2007,
  note = {\url{http://gtcases.cs.utwente.nl/wiki/uploads/sierpinskigrgennet.pdf}, last visited 30 April 2009},
  WKloc = {doc/pap/BIB/Kroll-Mallon-2007_AGTIVE_Sierpinski.pdf}
}

@unpublished{Taentzer-Biermann-2007,
  author={Gabriele Taentzer and Enrico Biermann},
  title={Generating {Sierpinski} Triangles by the {Tiger EMF} Transformation Framework},
  year=2007,
  note = {\url{http://tfs.cs.tu-berlin.de/publikationen/Papers07/TB07.pdf}, last visited 30 April 2009},
  WKloc = {doc/pap/BIB/Taentzer-Biermann-2007_AGTIVE_Sierpinski.pdf}
}

@unpublished{Boronat-Heckel-2007,
  author={Artur Boronat and Reiko Heckel},
  title={{Sierpinski} Triangles in {MOMENT2-GT}},
  year=2007,
  WKloc = {doc/pap/BIB/Boronat-Heckel-2007_AGTIVE_Sierpinski.pdf},
  note = {\url{http://www.cs.le.ac.uk/people/aboronat/tools/moment2-gt/}, last visited 30 April 2009}
}

@Book{ACM-1987,
  editor = {ACM},
  title = {{ACM Turing Award} Lectures, The First Twenty Years: 1966--1985},
  publisher = {ACM Press},
  year = 1987,
  McMaster = {QA 76.24 .A33 1987}
}

@Misc{ASD-SDF-bib-1995,
  author = {The {ASD+SDF group}},
  title = {The {ASD+SDF} Mete-environment: An Annotated Bibliography},
  year = 1995,
  month = DEC,
  WKloc = {A-0546}
}

@Misc{Aarts-Backhouse-Hoogendijk-Voermans-vanderWoude-1992,
  author = {Chritiene Aarts and Roland C. Backhouse and Paul Hoogendijk and Ed Voermans and van der Woude, Jaap},
  title = {A Relational Theory of Datatypes},
  howpublished = {Working document},
  month = DEC,
  year = 1992,
  OPTURL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#book},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS},
  note = {387 pp., available at\newline \textsf{http://www.cs.nott.ac.uk/\~{}rcb/MPC/book.ps.gz}},
  OPTnote = {387 pp., available at\linebreak \url{http://www.cs.nott.ac.uk/~rcb/papers/abstract.html\#book}},
  abstract = {This paper reports ongoing research into a theory of
      datatypes based on the calculus of relations. A fundamental concept
      introduced here is the notion of ``relator'' which is an adaption of
      the categorical notion of functor. Axiomatisations of polynomial
      relators (that is relators built from the unit type and the disjoint
      sum and cartesian product relators) are given, following which the
      general class of initial datatypes is studied. Among the topics
      discussed are natural polymorphism, junctivity and continuity
      properties.

      The current paper is an incomplete draft and will be supplemented at
      later dates.}
}

@InProceedings{Abadi-Burrows-Kaufman-Lampson-1991,
  title = {Authentication and Delegation with Smart-cards},
  author = {M. Abadi and M. Burrows and C. Kaufman and B. Lampson},
  pages = {326--345},
  crossref = {TACS1991},
  abstract = {The authentication of users in distributed systems poses
		  special problems because users lack the ability to
		  encrypt and decrypt.  The same problems arise when
		  users wish to delegate some of their authority to
		  nodes, after mutual authentication.

                  In most systems today, the user is forced to trust
		  the node he wants to use.  In a more satisfactory
		  design, the user carries a smart-card with
		  sufficient computing power to assist him; the card
		  provides encryption and decryption capabilities for
		  authentication and delegation.

                  Authentication is relatively straightforward with a
		  powerful enough smart-card.  However, for practical
		  reasons, protocols that place few demands on
		  smart-cards should be considered.  These protocols
		  are subtle, as they rely on fairly complex trust
		  relations between the principals in the system
		  (users, hosts, services).  In this paper, we discuss
		  a range of public-key smart-card protocols, and
		  analyze their assumptions and the guarantees they offer.}
}

@InProceedings{Abadi-Cardelli-1994,
  title = {A Semantics of Object Types},
  author = {Mart{\'\i}n Abadi and Luca Cardelli},
  pages = {332--341},
  crossref = {LICS9},
  WKloc = {A-0373},
  abstract = {We give a semantics for a typed object calculus, an
		  extension of system~{\bf F} with object subsumption
		  and method override.  We interpret the calculus in a
		  per model, proving the soundness of both typing and
		  equational rules.  This semantics suggests a
		  syntactic translation from our calculus into a
		  simpler calculus with neither subtyping nor objects.}
}

@InProceedings{Abadi-Cardelli-1994a,
  author = {M. Abadi and Luca Cardelli},
  title = {A Theory of Primitive Objects: Second-Order Systems},
  crossref = {ESOP1994},
  pages = {1--25},
  keywords = {OO, self, $\varsigma$-binding, subtyping,
		  inheritance, methods}
}

@InProceedings{Abadi-Cardelli-Curien-1993,
  author = {Mart\'{\i}n Abadi and Luca Cardelli and Pierre-Louis Curien},
  title = {Formal Parametric Polymorphism},
  crossref = {POPL1993},
  pages = {157--170},
  WKloc = {A-0193},
  abstract = {A polymorphic function is parametric if its behavior does not
             depend on the type at which it is instantiated. Starting with
             Reynolds' work, the study of parametricity is typically
             semantic. In this paper, we develop a syntactic approach to
             parametricity, and a system that embodies this approach: system
             {\cal R}. Girard's system F deals with terms and types; {\cal
             R} is an extension of F that deals also with relations between
             types.

             In {\cal R}, it is possible to derive theorems
             about functions from their types, or ``theorems for free'', as
             Wadler calls them. An easy ``theorem for free'' asserts that
             the type $\forall (X) X \rightarrow Bool$ contains only
             constant functions; this is not provable in F. There are many
             harder and more substantial examples. Various metatheorems can
             also obtained, such as a syntactic version of Reynolds'
             abstraction theorem.}
}

@InProceedings{Abadi-Cardelli-Curien-Levy-1990,
  author = {M. Abadi and L. Cardelli and P.-L. Curien and J.-J. L\'evy},
  title = {Explicit Substitutions},
  crossref = {POPL1990},
  pages = {31--46},
  WKloc = {?},
  abstract = {The $\lambda\sigma$-calculus is a refinement of the
		  $\lambda$-calculus where substitutions are
		  manipulated explicitely. The
		  $\lambda\sigma$-calculus provides a setting for
		  studying the theory of substitutions, with pleasant
		  mathematical properties. It is also a useful bridge
		  between the classical $\lambda$-calculus and
		  concrete implementations.},
  annote = {$\lambda\sigma$ rewriting system with second-order
		  de Bruijn implementing $\lambda$-calculus.}
}

@Article{Abadi-Cardelli-Pierce-Plotkin-1991,
  author = {Mart\'{\i}n Abadi and Luca Cardelli and Benjamin Pierce
                   and Gordon Plotkin},
  title = {Dynamic Typing in a Statically Typed Language},
  journal = ACM-TOPLAS,
  year = 1991,
  volume = 13,
  pages = {237--268},
  number = 2,
  month = APR,
  WKloc = {A-0016},
  ACMCAT = {F32 (Logics and Meanings of Programs): Semantics of
		      Programming Languages;
                  F33 (Logics and Meanings of Programs): Studies of
		      Program Constructs --- type structure},
  abstract = {Statically typed programming languages allow earlier
		  error checking, better enforcement of disciplined
		  programming styles, and the generation of more
		  efficient object code than languages where all type
		  consistency checks are performed at run time.
		  however, even in statically typed languages, there
		  is often the need to deal with data whose type
		  cannot be determined at compile time. To handle such
		  situations safely, we propose to add a type {\bf Dynamic}
                  whose values are pairs of a value {\bf v}
		  and a type tag {\bf T}, where {\bf v} has the type
		  denoted by {\bf T}. Instances of {\bf Dynamic} are
		  built with an explicit tagging construct and
		  inspected with a typesafe {\bf typecase} construct.

                  This paper explores the syntax, operational
		  semantics, and denotational semantics of a simple
		  language that includes the type {\bf Dynamic}. We
		  give examples of how dynamically typed values can be
		  used in programming. Then we discuss an operational
		  semantics for our language and obtain a soundness
		  theorem. We present two formulations of the
		  denotational semantics of this language and relate
		  them to the operational semantics. Finally, we
		  consider the implications of polymorphism and some
		  implementation issues.}
}

@InProceedings{Abadi-Lamport-1994,
  author = {M. Abadi and L. Lamport},
  title = {Decomposing Specifications of Concurrent Systems},
  crossref = {PROCOMET94},
  pages = {323--336},
  keywords = {Specifying and Verifying and Reasoning about
		  Programs; Compositionality in Concurrency}
}

@InProceedings{Abadi-Plotkin-1991,
  author = {Martin Abadi and Gordon Plotkin},
  title = {A logical view of composition and refinement},
  editor = {{ACM}},
  booktitle = {{POPL} '91. Proceedings of the eighteenth annual {ACM}
                 symposium on Principles of programming languages,
                 January 21--23, 1991, Orlando, {FL}},
  publisher = {ACM Press},
  address = {New York, NY, USA},
  year = 1991,
  pages = {323--332},
  year = 1991,
  bibdate = {Mon May 3 12:44:16 MDT 1999},
  URL = {http://www.acm.org:80/pubs/citations/proceedings/plan/99583/p323-abadi/},
  acknowledgement = ack-nhfb,
  keywords = {languages; theory},
  subject = {{\bf D.3.3} Software, PROGRAMMING LANGUAGES, Language
                 Constructs and Features, Concurrent programming
                 structures. {\bf F.4.1} Theory of Computation,
                 MATHEMATICAL LOGIC AND FORMAL LANGUAGES, Mathematical
                 Logic. {\bf F.3.1} Theory of Computation, LOGICS AND
                 MEANINGS OF PROGRAMS, Specifying and Verifying and
                 Reasoning about Programs, Logics of programs. {\bf
                 D.2.1} Software, SOFTWARE ENGINEERING,
                 Requirements/Specifications, Languages.},
  bibliographies = {LogRel}
}

@Article{Abadi-Plotkin-1993,
  title = {A logical view of composition},
  author = {Mart{\'\i}n Abadi and Gordon D. Plotkin},
  pages = {3--30},
  journal = TCS,
  year = 1993,
  month = {14~} # JUN,
  volume = 114,
  number = 1,
  WKloc = {A-0942},
  bibliographies = {LogRel}
}

@InProceedings{Abadi-Plotkin-1993a,
  author = {M. Abadi and G. Plotkin},
  title = {A logic for parametric polymorphism},
  booktitle = {International Conference on Typed Lambda Calculi and
                 Applications},
  year = 1993,
  editor = {M. Bezem and J. F. Groote},
  publisher = {Springer-Verlag},
  series = LNCS,
  volume = 664,
  pages = {361--375},
  address = {Utrecht, The Netherlands},
  month = mar,
  note = {TLCA'93},
  annote = {check aouthor's order! \unfinished},
  bibliographies = {LogRel}
}

@InProceedings{Abdeen-Kahl-Maibaum-2007,
  author = {Marwan M. Abdeen and Wolfram Kahl and Tom Maibaum},
  title = {{FDA}: Between Process and Product Evaluation},
  booktitle = {2007 Joint Workshop on High Confidence Medical Devices,
      Software, and Systems and
      Medical Device Plug-and-Play Interoperability
      {(HCMDSS-MDPnP 2007)}},
  year = 2007,
  isbn = {0-7695-3081-8},
  pages = {181--186},
  DOIURL = {http://doi.ieeecomputersociety.org/10.1109/HCMDSS-MDPnP.2007.6},
  DOI = {10.1109/HCMDSS-MDPnP.2007.6},
  publisher = {IEEE Computer Society},
  address = {Los Alamitos, CA, USA},
  editor = 	 {Julian Goldman and Insup Lee},
  abstract = {Several institutions and agencies around the world, from
                  both the private and public sectors, have adopted
                  the practice of software validation and
                  certification to ensure higher levels of confidence
                  in software. The US Food and Drug Administration
                  (FDA) is one of these organisations. The FDA has
                  published several guidance documents concerning the
                  validation of medical device software. In its
                  guidance documents for both the medical software
                  industry and FDA staff, FDA recommends certain
                  activities to be undertaken and certain deliverables
                  to be prepared. This is believed by the FDA to
                  achieve higher confidence in the software quality
                  and, accordingly, better validation. In this paper,
                  we question the clarity and practicability of the
                  FDA approach for medical software validation. We
                  examine the FDA approach by evaluating the FDA
                  guidance documents. We base our analysis on
                  considering the product versus process evaluation
                  criterion. By focusing on these two concepts, we
                  believe that the FDA validation approach leads to
                  confusion in the minds of both software producers
                  and evaluators and it may well lead to lower quality
                  software than desired being certified, at the time
                  the FDA considers its approach as the least
                  burdensome. As a demonstration of our claim, we
                  analyze ``Quality Planning'' as the first activity
                  in the FDA approach. We discuss the corresponding
                  effort for this activity in another product-oriented
                  evaluation approach, the Common Criteria (CC) for
                  Information Technology Security Evaluation. As its
                  main objective, the comparison aims to highlight
                  both the inconsistency and the vagueness of the FDA
                  evaluation function. Reconsideration of these issues
                  by the FDA will lead to a more objective evaluation
                  function and higher confidence in the medical
                  software, which represents the most important
                  concern among the evaluation community and the
                  general public.}
}

@InProceedings{Abdeen-Maibaum-2007,
  author = 	 {Marwan M. Abdeen and Tom Maibaum},
  title = {Contract by Systems Modeling:
           A Case Study on the {FDA} Principles of Software Validation},
  booktitle = 	 {Software Engineering and Applications,
                  {SEA 2007}, 11/19/2007 -- 11/21/2007,
                  {Cambridge, MA, USA}},
  editor = 	 {J. Smith},
  pages = 	 {Track 591-181 \unfinished},
  year = 2007,
  URL = {http://www.actapress.com/Abstract.aspx?paperId=32061},
  inURL = {http://www.actapress.com/Content_of_Proceeding.aspx?proceedingID=465},
  note = {For sale at \url{http://www.actapress.com/Content_of_Proceeding.aspx?proceedingID=465}},
  abstract = 	 {Certification has been a concern amongst the
                  software engineering community for the past few
                  decades and is becoming a major concern
                  today. Several organisations, in charge of
                  certification, have published guidance documents to
                  describe this crucial activity. Indeed, these
                  organisations, through their documents, aim to
                  establish a common understanding between software
                  producers and certifiers (evaluators). These
                  guidance documents use natural language in
                  specifying recommendations, because of the wide
                  audience to which they are addressed and the
                  consequent need for simplicity. However, the
                  specification is not sufficiently explicit and
                  precise to be able to impose a contract (obligation)
                  between the two parties. In this paper, we
                  illustrate this problem as it appears in the
                  guidance documents published by the US Food and Drug
                  Administration (FDA) to validate medical device
                  software. By bearing in mind the clear distinction
                  between products and processes, we use the
                  Product/process (P/p) method to model the ``Quality
                  Planning'' activity of the FDA validation
                  approach. By using P/p modelling, we present a
                  simplified representation for the FDA validation
                  activities. In essence, the P/p methodology takes a
                  general systems approach. It is appropriate to a
                  variety of areas and has proven its applicability in
                  many fields.}
}

@Article{Abdullahi-Ringwood-1998,
  author = {Saleh E. Abdullahi and Graem A. Ringwood},
  title = {Garbage Collecting the {Internet}: A Survey of Distributed Garbage Collection},
  journal = {ACM Computing Surveys},
  year = 1998,
  volume = 30,
  number = 3,
  month = SEP,
  pages = {330--373},
  WKloc = {A-0898}
}

@InProceedings{Abel-ChangBorYuh-Pfenning-2001,
  author =       {Andreas Abel and Bor-Yuh Evan Chang and Frank Pfenning},
  title =        {Human-Readable Machine-Verifiable Proofs for Teaching Constructive Logic},
  booktitle = {Proof Transformation, Proof Presentation and Complexity of Proofs (PTP-01). Workshop Proceedings},
  year =      2001,
  publisher = {Universit\`a degli Studi Siena, Dipartimento di Ingegneria dell'Informazione},
  note =      {Tech.~Report 13/01. (See also \url{http://www2.tcs.ifi.lmu.de/~abel/tutch/}.)},
  WKloc = {doc/pap/BIB},
  bibliographies = {CalcCheck}
}

@Book{Abiteboul-Buneman-Suciu-1999,
  author = {S. (Serge) Abiteboul and Peter Buneman and Dan Suciu},
  title = {Data on the Web: From Relations to Semistructured Data and {XML}},
  publisher = {Morgan Kaufmann Publishers},
  address = {Los Altos, CA 94022, USA},
  pages = {xiii + 257},
  year = 1999,
  ISBN = {1-55860-622-X},
  LCCN = {QA76.9.D3 A258 2000},
  keywords = {Database management.; XML (Document markup language);
                 World Wide Web (Information retrieval system);
                 Databases.; XML (Document markup language)},
  bibliographies = {RelMiCS}
}

@InProceedings{Abiteboul-Papadimitriou-Vianu-1994,
  title = {The Power of Reflective Relational Machines},
  author = {S. Abiteboul and C. H. Papadimitriou and V. Vianu},
  pages = {230--240},
  crossref = {LICS9},
  abstract = {A model of database programming with reflection, called
      ``reflective relational machine'', is introduced and studied. The
      reflection consists here of dynamic generation of queries in a host
      programming language. The main results characterize the power of the
      machine in terms of known complexity classes. In particular, the
      polynomial-time restriction of the machine is shown to express
      PSPACE, and to correspond precisely to uniform circuits of polynomial
      depth and exponential size. This provides an alternative, logic-based
      formulation of the uniform circuit model, more convenient for
      problems naturally formulated in logic terms. Since time in the
      polynomially-bounded machine coincides with time in the uniform
      circuit model, this also shows that reflection allows for more
      ``intense'' parallelism, which is not attainable otherwise (unless P
      = PSPACE). Other results concern the power of the reflective
      relational machine subject to restrictions on the number of variables
      used.},
  bibliographies = {RelMiCS}
}

@TechReport{Abold-Berghammer-Schmidt-1989,
  author = {Hilde Abold-Thalmann and Rudolf Berghammer and
		  Gunther Schmidt},
  title = {Manipulation of Concrete Relations: {The {RELVIEW}-System}},
  WKloc = {TR},
  abstract = {People working in or studying the theory of
		  relations (Boolean matrices) or graph theory often
		  use small examples and manipulate them with pencil
		  and paper in order to prove or disprove some
		  property. (Of course, this is no more feasible with
		  bigger examples.) RELVIEW is an interactive computer
		  system supporting such tasks. This report gives a
		  description of the RELVIEW system (inclusive a
		  user's manual and some implementation details) and
		  informs also about the theoretical background.},
  year = 1989,
  number = 8905,
  pages = 28,
  month = OCT,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  bibliographies = {RelMiCS}
}

@Article{Abott-Altenkirch-Ghani-McBride-2003,
  author =       {M. Abott and T. Altenkirch and N. Ghani and C. McBride},
  title =        {$\diff$ for data},
  journal =      FUNDI,
  year =         2003,
  volume =    65,
  number =    {1,2},
  pages =     {1--28},
  month =     MAR,
  note =      {Special Issue on Typed Lambda Calculi and Applications},
  annote =    {zippers, data type differentiation}
}

@InProceedings{AboulHosn-Kozen-2003,
  author = 	 {Kamal Aboul-Hosn and Dexter Kozen},
  title = 	 {{KAT-ML}: An Interactive Theorem Prover for {Kleene Algebra with Tests}},
  crossref =	 {WIL2003},
  pages =	 {2--12},
  year = 	 2003,
  abstract = {We describe an implementation of an interactive
     theorem prover for Kleene algebra with tests (KAT). The system is
     designed to reflect the natural style of reasoning with KAT that
     one finds in the literature. We illustrate its use with some
     examples.},
  bibliographies = {RelMiCS}
}

@MastersThesis{Abraham-1997,
  author = 	 {Ruth Abraham},
  title = 	 {Evaluating Generalized Tabular Expressions in Software Documentation},
  school = 	 {McMaster University},
  year = 	 {1997},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {SQRL, RelMiCS}
}

@TechReport{Abramsky-1990,
  author = {Samson Abramsky},
  title = {Computational Interpretations of Linear Logic},
  year = 1990,
  month = OCT,
  institution = {Department of Computing, Imperial College London},
  WKloc = {B-0005},
  abstract = {We study Girard's Linear Logic from the point of
		  view of giving a concrete computational
		  interpretation of the logic, based on the
		  Curry-Howard isomorphism. In the case of
		  intuitionistic Linear Logic, this leads to a
		  refinement of the lambda calculus, giving finer
		  control over order of evaluation and storage
		  allocation, while maintaining the logical content of
		  programs as proofs, and computation as
		  cut-elimination. In the classical case, it leads to
		  a concurrent process paradigm with an operational
		  semantics in the style of Berry and Boudol's
		  Chemical Abstract Machine. This opens up a promising
		  new approach to the parallel implementation of
		  functional programming languages; and offers the
		  prospect of typed concurrent programming in which
		  correctness is guaranteed by the typing.}
}

@InCollection{Abramsky-1994,
  author = {Samson Abramsky},
  title = {Interaction Categories and Communicating Sequential Processes},
  crossref = {Roscoe-1994},
  pages = {1--15},
  chapter = 1,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Abramsky-Gay-Nagarajan-1994,
  author = {Samson Abramsky and Simon J. Gay and Rajagopal Nagarajan},
  title = {Interaction Categories and the Foundation of Typed Concurrent Programming},
  crossref = {Marktoberdorf-1994},
  bibliographies = {RelMiCS}
}

@InProceedings{Abramsky-Jensen-1991,
  author = {Samson Abramsky and Thomas P. Jensen},
  title = {A Relational Approach to Strictness Analysis for Higher-Order
          Polymorphic Functions},
  pages = {49--54},
  abstract = {This paper defines the categorical notions of relators and
             transformations and shows that these concepts enable us to give
             a semantics for polymorphic, higher order functional programs.
             We demonstrate the pertinence of this semantics to the analysis
             of polymorphic programs proving that strictness analysis is a
             polymorphic invariant.},
  crossref = {POPL1991},
  WKloc = {A-0172},
  bibliographies = {RelMiCS}
}

@InCollection{Abramsky-Jung-1994,
  author = 	 {S. Abramsky, A. Jung},
  title = 	 {Domain Theory},
  crossref = {HBLCS-III},
  PDF =      {http://www.cs.bham.ac.uk/~axj/pub/papers/handy1.pdf},
  PS =       {http://www.cs.bham.ac.uk/~axj/pub/papers/handy1.ps.gz},
  OPTpages = 	 {},
  OPTpublisher = {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTtype = 	 {},
  OPTchapter = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Abramsky-Sykes-1985,
  author = 	 {S. Abramsky and R. Sykes},
  title = 	 {{Secd-m}: a Virtual Machine for Applicative Programming},
  crossref =	 {FPCA1985},
  pages =	 {81--98},
  WKloc = {A-1633},
  bibliographies = {FP, PMC}
}

@Book{Abrial-1996,
  author = {Jean-Raymond Abrial},
  title = {The {B}-Book: Assigning Programs to Meanings},
  publisher = {Cambridge University Press},
  year = 1996,
  price = {\pounds 40.00},
  ISBN = {0-521-49619-5},
  length = 850,
  annote = {This book is a reference manual for the B-Method
                 developed by Jean-Raymond Abrial, also the originator
                 of the Z notation. B is designed for tool-assisted
                 software development whereas Z is designed mainly for
                 specification. \par Contents: Mathematical reasoning;
                 Set notation; Mathematical objects; Introduction to
                 abstract machines; Formal definition of abstract
                 machines; Theory of abstract machines; Constructing
                 large abstract machines; Example of abstract machines;
                 Sequencing and loop; Programming examples; Refinement;
                 Constructing large software systems; Example of
                 refinement; \par Appendices: Summary of the most
                 current notations; Syntax; Definitions; Visibility
                 rules; Rules and axioms; Proof obligations.}
}

@Article{Abrusci-1991,
  author = {V. Abrusci},
  title = {Phase Semantics and Sequent Calculus for Pure Noncommutative
      Classical Linear Propositional Logic},
  journal = JSYLO,
  year = 1991,
  volume = 56,
  number = 4,
  pages = {1403--1451},
  month = DEC,
  bibliographies = {RelMiCS}
}

@InProceedings{Achten-PeytonJones-2000,
  author = {Peter Achten and Peyton Jones, Simon},
  title = {Porting the {Clean Object I/O} Library to {Haskell}},
  crossref = {IFL2000},
  pages = {194--213},
  WKloc = {A-1224, doc/pap/BIB},
  URL = {http://research.microsoft.com/~simonpj/papers/haskellobjectio.htm},
  abstract = {Pure, functional programming languages offer several
      solutions to construct Graphical User Interfaces. One of these
      approaches is the Clean Object I/O library. It employs an explicit
      environment passing scheme, based on the uniqueness type system of
      Clean. Graphical User Interface elements are defined on a high level
      of abstraction by means of algebraic data types. Composite elements
      are constructed by means of type constructor combinators. The
      behaviour of an interactive element is defined by higher order
      functions. These functions can use global and local state. The Object
      I/O system supports interleaved interactive processes. Interactive
      elements can communicate internally by means of message passing.
      Solutions for Graphical User Interfaces for the programming language
      Haskell are based on monads, using an implicit environment passing
      scheme. In this paper we investigate how the Clean Object I/O library
      can be ported to Haskell. We give an implementation of a small
      fragment of the Object I/O library to show the feasibility. We take
      especial consideration for the relevant design choices.}
}

@Misc{Achten-Plasmeijer-199X,
  author = {Peter Achten and Rinus Plasmeijer},
  title = {Using Type and Constructor Classes to Interpret Object Structures},
  year = {199?},
  WKloc = {A-0798}
}

@Unpublished{Aczel-1978,
  author = {Peter Aczel},
  title = {A general {Church-Rosser} theorem},
  institution = {University of Manchester},
  year = 1978,
  OPTcrossref = {},
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = {},
  note = {unpublished note, summarised for example in \cite{vanRaamsdonk-2003HOR}},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Aczel-1991,
  author = {Peter Aczel},
  title = {Term Declaration Logic and Generalised Composita},
  pages = {22--30},
  crossref = {LICS6}
}

@Misc{Ada95,
  author = {},
  title = {Ada 95 Ratoionale},
  year = {},
  WKloc = {B-0048}
}

@InProceedings{Adachi-Miyadera-Sugita-Tsuchida-Yaku-1998,
  title = {A Visual Programming Environment Based on Graph Grammars
           and Tidy Graph Drawing},
  author = {Yoshihiko Adachi and Youzou Miyadera and Kimio Sugita and
           Kensei Tsuchida and Takeo Yaku},
  authorsAddress = {YA, KT: Toyo U.; YM: Tokyo Gakugei U.;
                   KS: Tokai U.; TY: Nihon U.},
  crossref = {ICSE1998},
  URL = {http://icse98.aist-nara.ac.jp/technical.html}
}

@Book{Adamek-Herrlich-Strecker-1990,
  author = {Ji{\hacek{r}}{\'{\i}} Ad{\'a}mek and Horst Herrlich and George E. Strecker},
  title = {Abstract and Concrete Categories: The Joy of Cats},
  year = 1990,
  WKloc = {doc/pap/BIB},
  publisher = {Wiley Interscience}
}

@InProceedings{AdamsMD-Agacan-2014_LayoutParsing2,
 author = {Adams, Michael D. and A\u{g}acan, \"{O}mer S.},
 title = {Indentation-sensitive Parsing for {Parsec}},
 booktitle = {Proceedings of the 2014 ACM SIGPLAN Symposium on Haskell},
 series = {Haskell '14},
 year = {2014},
 isbn = {978-1-4503-3041-1},
 location = {Gothenburg, Sweden},
 pages = {121--132},
 numpages = {12},
 DOIURL = {http://doi.acm.org/10.1145/2633357.2633369},
 DOI = {10.1145/2633357.2633369},
 WKloc = {doc/pap/BIB},
 acmid = {2633369},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {indentation sensitivity, layout, offside rule, parsec, parsing},
 abstract = {Several popular languages including Haskell and Python
   use the indentation and layout of code as an essential part of their syntax.
   In the past, implementations of these languages used ad hoc techniques
   to implement layout.
   Recent work has shown that a simple extension to context-free grammars
   can replace these ad hoc techniques and provide both formal foundations
   and efficient parsing algorithms for indentation sensitivity.

   However, that previous work is limited to bottom-up, LR($k$) parsing,
   and many combinator-based parsing frameworks including Parsec
   use top-down algorithms that are outside its scope.
   This paper remedies this by showing
   how to add indentation sensitivity to parsing frameworks like Parsec.
   It explores both the formal semantics of and efficient algorithms
   for indentation sensitivity.
   It derives a Parsec-based library for indentation-sensitive parsing
   and presents benchmarks on a real-world language
   that show its efficiency and practicality.}
}

@Booklet{Adelmann-1992,
  year = 1992,
  title = {3{D}-{O}bjektrepr"asentation zur
          {Z}ustandssch"atzung},
  note = {UniBwM ID 10/92},
  month = {Oktober},
  howpublished = {Diplomarbeit},
  author = {Adelmann, Andreas},
  address = {Neubiberg}
}

@Misc{AgdaStdLib-0.7,
  author =    {Nils Anders Danielsson and others},
  title =     {{Agda} Standard Library, Version 0.7},
  month =     JAN,
  year =      2013,
  OPTnote = {\scalebox{0.87}[1.0]{\url{http://wiki.portal.chalmers.se/agda/pmwiki.php?n=Libraries.StandardLibrary}}},
  note = {http://tinyurl.com/AgdaStdlib}
}

@Article{Ager-Danvy-Midtgaard-2004,
  author = {Mads Sig Ager and Olivier Danvy and Jan Midtgaard},
  title = 	 {A functional correspondence between call-by-need evaluators and lazy abstract machines},
  journal = 	 IPLet,
  year = 	 2004,
  volume =	 90,
  number =	 5,
  pages =	 {223--232},
  month =	 JUN,
  WKloc = 	 {A-1548, doc/pap/BIB},
  keywords = {Functional programming; Program derivation; Interpreters;
              Abstract machines; Closure conversion; CPS transformation;
              Defunctionalization},
  abstract = {We bridge the gap between compositional evaluators
     and abstract machines for the lambda-calculus, using closure
     conversion, transformation into continuation-passing style, and
     defunctionalization of continuations. This article is a followup
     of our article at PPDP 2003, where we consider call by name and
     call by value. Here, however, we consider call by need.

     We derive a lazy abstract machine from an ordinary call-by-need
     evaluator that threads a heap of updatable cells. In this
     resulting abstract machine, the continuation fragment for
     updating a heap cell naturally appears as an \emph{update marker},
     an implementation technique that was invented for
     the Three Instruction Machine and subsequently used to construct
     lazy variants of Krivine's abstract machine. Tuning the evaluator
     leads to other implementation techniques such as unboxed
     values. The correctness of the resulting abstract machines is a
     corollary of the correctness of the original evaluators and of
     the program transformations used in the derivation.}
}

@Unpublished{Agerholm-1995,
  author =       {Sten Agerholm},
  title =        {Experiments in Formalizing Basic Category Theory in
        Higher Order Logic and Set Theory},
  note =         {Draft},
  month =     DEC,
  year =      1995,
  WKloc =    {doc/pap/BIB}
}

@Book{Agha-1990,
  author = {G. Agha},
  title = {Actors: A Model of Concurrent Computation},
  publisher = {MIT Press},
  year = 1990
}

@Misc{Agrawal-Kayal-Saxena-2002,
  author = {Manindra Agrawal and Neeraj Kayal and Nitin Saxena},
  title = {PRIMES is in P},
  howpublished = {URL: \textsf{http://www.cse.iitk.ac.in/news/primality.html}},
  month = AUG,
  URL = {http://www.cse.iitk.ac.in/news/primality.html},
  WKloc = {A-1346, doc/pap/BIB},
  abstract = {[The authores] have discovered a polynomial time
      deterministic algorithm to test if an input number is prime or not.
      Lots of people over (literally!) centuries have been looking for a
      polynomial time test for primality, and this result is a major
      breakthrough, likened by some to the P-time solution to Linear
      Programming announced in the 70s.

      One of the main features of this result is that the proof is neither
      too complex nor too long (their preprint paper is only 9 pages
      long!), and relies on very innovative and insightful use of results
      from number theory.}
}

@Article{Aho-Beeri-Ullman-1979,
  author = {A. H. Aho and C. Beeri and J. D. Ullman},
  title = {The Theory of Joins in Relational Databases},
  journal = ACM-TDS,
  volume = 4,
  number = 3,
  year = 1979,
  pages = {297--314},
  bibliographies = {RelMiCS}
}

@Book{Aho-Sethi-Ullman-1986,
  author =	 {Alfred V. Aho and Ravi Sethi and Jeffrey D. Ullman},
  title = 	 {Compilers: Principles, Techniques and Tools},
  publisher = 	 {Addison-Wesley},
  year = 	 1986,
  ISBN = {0-201-10088-6},
  annote = {(red) dragon book}
}

@Misc{Aiken-1995,
  author = {Alex Aiken},
  title = {Constraint-Based Program Analysis},
  year = 1995,
  note = {Invited talk at POPL '95},
  WKloc = {A-0538}
}

@InProceedings{Ahrens-Capriotti-Spadotti-2015,
  author =	{Benedikt Ahrens and Paolo Capriotti and R{\'e}gis Spadotti},
  title =	{{Non-Wellfounded Trees in Homotopy Type Theory}},
  booktitle =	{13th International Conference on Typed Lambda Calculi and Applications (TLCA 2015)},
  pages =	{17--30},
  series =	{Leibniz International Proceedings in Informatics (LIPIcs)},
  ISBN =	{978-3-939897-87-3},
  ISSN =	{1868-8969},
  year =	{2015},
  volume =	{38},
  editor =	{Thorsten Altenkirch},
  publisher =	{Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  address =	{Dagstuhl, Germany},
  URL =		{http://drops.dagstuhl.de/opus/volltexte/2015/5152},
  URN =		{urn:nbn:de:0030-drops-51522},
  DOIURL =      {http://dx.doi.org/10.4230/LIPIcs.TLCA.2015.17},
  DOI =      {10.4230/LIPIcs.TLCA.2015.17},
  annote =	{Keywords: Homotopy Type Theory, coinductive types, computer theorem proving, Agda},
  WKloc = {doc/pap/BIB}
}

@TechReport{Aiken-Williams-Wimmers-1989,
  author = {Alexander Aiken and John H. Williams and Edward L. Wimmers},
  title = {Program Transformation in the Presence of Errors},
  WKloc = {A-0503},
  institution = {IBM Almaden Research Center},
  address = {San Jose, California},
  type = {Research Report},
  number = {RJ 7210 (67689) 12/13/89},
  month = DEC,
  year = 1989,
  descr = {plfun,pttra}
}

@InProceedings{Aiken-Wimmers-Lakshman-1994,
  author = {Alexander Aiken and Edward L. Wimmers and T.K. Lakshman},
  title = {Soft Typing with Conditional Types},
  crossref = {POPL1994},
  pages = {163--173},
  keywords = {control-flow analysis for type inference in
		  dynamically typed programs},
  WKloc = {A-1304, doc/pap/BIB}
}

@Article{AitKaci-1986,
  WKloc = {A-0042},
  year = 1986,
  volume = 45,
  title = {An Algebraic Semantics Approach to the Effective Resolution
	of Type Equations},
  pages = {293--351},
  journal = {Theoretical Computer Science},
  author = {Hassan A{\"\i{}}t-Kaci}
}

@InProceedings{AitKaci-1992,
  nutshell = {discussion of issues and survey, as well our particular way
      (Hassan Ait-Kaci in <1993Sep2.103255.28341@prl.dec.com>)},
  authorsAddress = {DEC Paris Research Laboratory, hak\@prl.dec.com},
  year = {1989    },
  volume = {2      },
  title = {Integrating Logic and Functional Programming},
  pages = {51--89},
  note = {invited lecture, abstract},
  journal = {Lisp and Symbolic Computation},
  crossref = {ALP1992},
  author = {Hassan A{\"\i}t-Kaci and Roger Nasr},
  annote = {SCOOP --- Simple Calculus for Object-Oriented
		  Programming, Inheritance as approximation (realized
		  through a structural endomorphism) that fully
		  commutes with evaluation.}
}

@Article{AitKaci-Nasr-1986,
  WKloc = {A-0060},
  year = 1986,
  volume = 16,
  title = {{LOGIN}: A logic Programming Language with Built-In Inheritance},
  pages = {195--215},
  number = 3,
  journal = {Journal of Logic Programming},
  author = {Hassan A\"{\i}t-Kaci and Andreas Podelski}
}

@TechReport{AitKaci-Podelski-1991,
  WKloc = {A-0104},
  FTP = {gatekeeper.dec.com:.b/DEC/PRL/research-reports/PRL-RR-13.ps.Z},
  filename = {PRL-RR-13.ps.Z},
  DIRECTORY = {~kahl/doc/pap/dec-prl},
  abstract = {LIFE is an experimental programming language proposing to
      integrate logic programming, functional programming, and
      object-oriented programming. It replaces first-order terms with
      $\psi$-terms, data structureswhich allow computing with partial
      information. These are approximation structures denoting sets of
      values. LIFE further enriches the expressiveness of $\psi$-terms with
      functional dependency constraints. We must explain the meaning and
      use of functions in LIFE declaratively as solving partial information
      constraints. These constraints do not attempt to generate their
      solutions but behave as demons filtering out anything else. In this
      manner, LIFE functions act as declarative coroutines. We need to show
      that the $\psi$-term's approximation semantics is congruent with an
      operational semantics viewing functional reduction as an effective
      enforcing of passive constraints.

      In this article, we develop a general framework for entailment and
      disentailment of constraints based on a echnique called relative
      simplification, we study its operational and semantical properties,
      and we use it to account for functional application over $\psi$-terms
      in LIFE},
  year = 1991,
  type = {PRL Research Report},
  title = {Towards a Meaning of {LIFE}},
  number = 13,
  note = {Revised, November 1992},
  month = jun,
  institution = {Digital Equipment Corporation, Paris Research Laboratory},
  author = {Hassan A\"{\i}t-Kaci and Andreas Podelski},
  address = {Rueil-Malmaison, France}
}

@TechReport{AitKaci-Podelski-1992,
  author = {Hassan A\"{\i}t-Kaci and Andreas Podelski},
  title = {Towards a Meaning of {LIFE}},
  year = 1992,
  institution = {digital Paris Research Laboratory},
  number = 11,
  WKloc = {A-0103},
  FTP = {gatekeeper.dec.com:.b/DEC/PRL/research-reports/PRL-RR-11.ps.Z},
  filename = {PRL-RR-11.ps.Z},
  DIRECTORY = {~kahl/doc/pap/dec-prl},
  abstract = {LIFE is an experimental programming language
		  proposing to integrate three orthogonal programming
		  paradigms proven useful for symbolic computation.
		  From the programmer's standpoint, it may be
		  perceived as a language taking after logic
		  programming, functional programming, and
		  object-oriented programming. From a formal
		  perspective, it may be seen as an instance (or
		  rather, as a composition of three instances) of a
		  Constraint Logic Programming scheme due to H\"ohfeld
		  and Smolka refining that of Jaffar and Lassez.

                  We start with an informal overview demonstrating
		  LIFE as a programming language, illustrating how its
		  primitives offer rather unusual, and perhaps
		  (pleasantly) startling, conveniences. The second
		  part is a formal account of LIFE's object
		  unification seen as constraint-solving over specific
		  domains. We build on work by Smolka and Rounds to
		  develop type-theoretic, logical, and algebraic
		  renditions of a calculus of order-sorted feature
		  approximations.}
}

@Article{AitKaci-Podelski-1993,
  FTP = {gatekeeper.dec.com:.b/DEC/PRL/research-reports/PRL-RR-11.ps.Z},
  filename = {PRL-RR-11.ps.Z},
  DIRECTORY = {~kahl/doc/pap/dec-prl},
  abstract = {LIFE is an experimental programming language
		  proposing to integrate three orthogonal programming
		  paradigms proven useful for symbolic
		  computation. From the programmer's standpoint, it
		  may be perceived as a language taking after logic
		  programming, functional programming, and
		  object-oriented programming. From a formal
		  perspective, it may be seen as an instance (or
		  rather, as a composition of three instances) of a
		  Constraint Logic Programming scheme due to H\"ohfeld
		  and Smolka refining that of Jaffar and Lassez.

		  We start with an informal overview demonstrating
		  LIFE as a programming language, illustrating how its
		  primitives offer rather unusual, and perhaps
		  (pleasantly) startling, conveniences. The second
		  part is a formal account of LIFE's object
		  unification seen as constraint-solving over specific
		  domains. We build on work by Smolka and Rounds to
		  develop type-theoretic, logical, and algebraic
		  renditions of a calculus of order-sorted feature
		  approximations.},
  year = 1993,
  volume = 16,
  title = {Towards a Meaning of {LIFE}},
  pages = {195--234},
  number = {3\&4},
  journal = JLOG,
  author = {Hassan A\"{\i}t-Kaci and Andreas Podelski},
  bibliographies = {RelMiCS}
}

@InProceedings{AitKaci-Podelski-1993a,
  author = {Hassan A\"{\i}t-Kaci and Andreas Podelski},
  title = {Entailment and Disentailment of Order-Sorted Feature Constraints},
  crossref = {LPAR93},
  pages = {1--18},
  WKloc = {A-0119},
  abstract = {LIFE uses matching on order-sorted feature structures for
             passing arguments to functions. As opposed to unification which
             amounts to normalizing a conjunction of constraints, solving a
             matching problem consists of deciding whether a constraint
             (guard) or its negation are entailed by the context. We give a
             complete and consistent set of rules for entailment and
             disentailment of order-sorted feature constraints. These rules
             are directly usable for relative simplification, a general
             proof-theoretic method for proving guards in concurrent
             constraint logic languages using guarded rules.}
}

@TechReport{AitKaci-Podelski-Smolka-1992,
  author = {Hassan A{\"\i}t-Kaci and Andreas Podelski and Gert Smolka},
  title = {A Feature-based Constraint System for Logic
		 Programming with Entailment},
  year = 1992,
  type = {Research Report},
  number = {RR-92-17},
  note = {Also appeared in: {\it Proceedings of the International
		 Conference on Fifth Generation Computer Systems
		 1992}, June 1--5, 1992, Tokio, Japan.},
  bibliographies = {RelMiCS},
  month = MAR,
  institution = DFKI,
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  abstract = {This paper presents the constraint system FT, which
		  we feel is an intriguing alternative to Herbrand
		  both theoretically and practically.  As does
		  Herbrand, FT provides a universal data structure
		  based on trees. However, the trees of FT (called
		  feature trees) are more general than the trees of
		  Herbrand (called constructor trees), and the
		  constraints of FT are finer grained and of different
		  expressivity.  The basic notion of FT are functional
		  attributes called features, which provide for
		  record-like descriptions of data avoiding the
		  overspecification intrinsic in Herbrand's
		  constructor-based descriptions.

                  The feature tree structure fixes an algebraic
		  semantics for FT. We will also establish a logical
		  semantics, which is given by three axiom schemes
		  fixing the first-order theory FT.

                  FT is a constraint system for logic programming,
		  providing a test for unsatisfiability, and a test
		  for entailment between constraints, which is needed
		  for advanced control mechanisms.

                  The two major technical contributions of this paper
		  are (1) an incremental entailment simplification
		  system that is proved to be sound and complete, and
		  (2) a proof showing that FT satisfies the so-called
		  ``independence of negative constraints''.}
}

@InProceedings{Aitken-Dickens-Kwiatkowski-deMoor-Richter-Simonyi-1998,
  author = {William Aitken and Brian Dickens and Paul Kwiatkowski and de Moor, Ooge and David Richter and Charles Simonyi},
  title = {Transformation in Intentional Programming},
  pages = {114--123},
  crossref = {ICSR1998}
}

@PhDThesis{AlZoubi-1992,
  keywords = {change analysis, attributed program dependency graphs},
  year = 1992,
  title = {Attributed Graph-Based Representations for Software
		  View Generation and Impact-of-Change Analysis},
  school = {University of Michigan},
  author = {Ratib Houssein Al-Zoubie},
  annote = {QER92-26831}
}

@Book{Alagar-Periyasamy-2011,
  author =    {V.S. Alagar and K. Periyasamy},
  title =        {Specification of Software Systems},
  publisher =    Springer,
  year =         2011,
  series =    {Texts in Computer Science},
  edition =   {2nd},
  ISBN =     {978-0-85729-277-3},
  note =      {\url{http://www.springerlink.com/content/978-0-85729-276-6}}
}

@Misc{Alberich-Rossello-Valiente-1997,
  author = {Ricardo Alberich and Francesc Rossell{'o} and Gabriel Valiente},
  title = {Single-Pushout Hypergraph Rewriting through Free Completions},
  month = FEB,
  year = 1997,
  WKloc = {A-1097}
}

@ARTICLE{Albert-Hanus-Huch-Oliver-Vidal-2005,
  author       = {Albert, E. and Hanus, M. and Huch, F. and Oliver, J. and Vidal, G.},
  title        = {Operational Semantics for Declarative Multi-Paradigm Languages},
  year         = {2005},
  journal      = {Journal of Symbolic Computation},
  number       = {1},
  pages        = {795--829},
  volume       = {40},
  URL = {http://www.informatik.uni-kiel.de/~mh/publications/papers/JSC05.html},
  WKloc = {doc/pap/BIB},
  abstract = {Declarative multi-paradigm languages combine the most
      important features of functional, logic and concurrent
      programming. The computational model of such integrated languages is
      usually based on a combination of two different operational
      principles: narrowing and residuation. This work is motivated by the
      fact that a precise definition of an operational semantics including
      all aspects of modern multi-paradigm languages like laziness, sharing,
      non-determinism, equational constraints, external functions,
      concurrency, etc.\ does not exist.  Therefore, in this article, we
      present the first rigorous operational description covering all the
      aforementioned features in a precise and understandable manner.  We
      develop our operational semantics in several steps. First, we define a
      natural (big-step) semantics covering laziness, sharing and
      non-determinism. We also present an equivalent small-step semantics
      which additionally includes a number of practical features like
      equational constraints and external functions. Then, we introduce a
      deterministic version of the small-step semantics which makes the
      search strategy explicit; this is essential for profiling, tracing,
      debugging, etc.  Finally, the deterministic semantics is extended in
      order to cover the concurrent facilities of modern declarative
      multi-paradigm languages. The developed semantics provides an
      appropriate foundation to model actual declarative multi-paradigm
      languages like Curry.  The complete operational semantics has been
      implemented and used for various programming tools.}
}

@Article{Alchourron-Gaerdenfors-Makinson-1985,
  author = {Alchourr{\'o}n, C.E. and G\"{a}rdenfors, P. and Makinson, D.},
  journal = JSYLO,
  pages = {510--530},
  title = {On the Logic of Theory Change: {Partial} Meet Contraction and
      Revision Functions},
  volume = 50,
  year = 1985,
  bibliographies = {RelMiCS}
}

@InProceedings{Aldrich-Simmons-ShinKey-2008,
  author =       {Jonathan Aldrich and Robert J. Simmons and Key Shin},
  title =        {{SASyLF}: An Educational Proof Assistant for Language Theory},
  OPTcrossref =  {FDPE2008},
  OPTkey =       {},
  booktitle = {Proceedings of the 2008 International Workshop on Functional and Declarative Programming in Education, {FDPE '08}},
  pages =     {31--40},
  year =      {2008},
  editor =    {Frank Huch and Adam Parkin},
  isbn = {978-1-60558-068-5},
  publisher = {ACM},
  DOIURL =      {http://dx.doi.org/10.1145/1411260.1411266},
  DOI =    {10.1145/1411260.1411266},
  abstract = {Teaching and learning formal programming language theory is hard, in part because it's easy to make mistakes and hard to find them. Proof assistants can help check proofs, but their learning curve is too steep to use in most classes, and is a barrier to researchers too.

    In this paper we present SASyLF, an LF-based proof assistant specialized to checking theorems about programming languages and logics. SASyLF has a simple design philosophy: language and logic syntax, semantics, and meta-theory should be written as closely as possible to the way it is done on paper. We describe how we designed the SASyLF syntax to be accessible to students learning type theory, and how students can understand its semantics directly in terms of the theory they are taught in class. SASyLF can express proofs typical of an introductory graduate type theory course. SASyLF proofs are generally very explicit, but its built-in support for variable binding provides substitution properties for free and avoids awkward variable encodings. We describe preliminary experience teaching with SASyLF.}
}

@InProceedings{Alekhnovich-Borodin-BureshOppenheim-Impagliazzo-Magen-Pitassi-2005,
  author = 	 {Michael Alekhnovich and Allan Borodin and Buresh-Oppenheim, Joshua and Russell Impagliazzo and Avner Magen and Toniann Pitassi},
  title = 	 {Toward a Model for Backtracking and Dynamic Programming},
  crossref =  {CCC2005},
  pages = 	 {\unfinished},
  WKloc = {A-1726, doc/pap/BIB}
}

@InProceedings{Alessi-1994,
  author = {Fabio Alessi},
  title = {Type Preorders},
  crossref = {CAAP94},
  pages = {37--51},
  abstract = {Various type structures, called Intersection Type
		  Structures, have been introduced in the literature
		  in order to define models of $\lambda$-calculus and
		  simultaneously to reason in a finitary way about
		  $\lambda$-terms. All these systems are only employed
		  as meet-semilattices generated by preorders built on
		  {\em prime} types. For this reason these structures
		  are linguistically redundant. Starting from this
		  observation we introduce the category of {\em Type
		  Preorders}, which arises naturally when we eliminate
		  redundant types from Intersection Type
		  Structures. We give a Stone-duality type result for
		  Type Preorders, showing that they are equivalent to
		  the category of prime-algebraic complete lattices
		  and Scott continuous functions. Thus we clarify the
		  domain-theoretic description of Intersection Type
		  Structures, which often appears opaque. As an
		  application we give the domain-theoretic reading of
		  the Intersection Union Type Structure.}
}

@InProceedings{Alessi-DezaniCiancaglini-deLiguoro-1994,
  author = {F. Alessi and M. Dezani-Ciancaglini and
		  de'Liguoro, U.},
  title = {May and Must Convergency in Concurrent $\lambda$-calculus},
  crossref = {MFCS94},
  pages = {211--220},
  abstract = {The present paper deals with features of
		  non-determinism and concurrency in the setting of
		  functional languages. In this framework a language
		  which has deserved some interest in the recent
		  literature is the pure $\lambda$-calculus enriched
		  with an internal choice operator `+' and/or a
		  synchronous parallel operator `||'. Following [9] we
		  call this calculus ``concurrent
		  $\lambda$-calculus''.

                  There is a general agreement on interpreting `||' as
		  a constructor which takes the best among the
		  behaviours of its operands; therefore the semantics
		  of `||' is usually done by means of Hoare
		  powerdomain. On the contrary for the
		  non-deterministic choice operator `+' essentially
		  two proposals have been exploited: either use
		  Plotkin powerdomain ([9],[10]) or Smyth powerdomain
		  ([6],[7]). In the present setting we are interested
		  in interpreting `+' using a partial order relation
		  finer than the demonic one, but preserving the
		  mutual distributivity of the operators `+' and
		  `||'. This last condition ensures that the obtained
		  powerdomain remains in the category of
		  lattices. This implies that it is possible to
		  describe our construction using Intersection Type
		  Structures [3] instead of Domain Prelocales [1]. In
		  particular, we require that the preorder we use to
		  define the powerdomain behaves like the Egli-Milner
		  one as far as convergency is concerned, i.e.\null{}
		  if $M$ is convergent and $\Omega$ is divergent, then
		  $$\Omega sqsubs M + \Omega \sqsubs M\,.$$}
}

@PhDThesis{Alexander-1995,
  author = {Geoffrey David Alexander},
  title = {Proving First-Order Equality Theorems with Hyper-Linking},
  school = {University of North Carolina at Chapel Hill},
  year = 1995,
  WKloc = {B-0067}
}

@InProceedings{Alexandre-Bsaies-Finance-Quere-1992,
  author = {Francis Alexandre and Khaled Bsa{\:{\i}}es and
		  Jean-Pierre Finance and Alain Qu\'er\'e},
  title = {Spes: A System for Logic Program Transformation},
  crossref = {LPAR92},
  pages = {445--503},
  WKloc = {A-0401},
  keywords = {program transformation, unfold, fold, strategy},
  abstract = {Spes is an interactive system for transforming logic
		  programs. It is intended as a formal tool for
		  transforming Horn clauses specifications into
		  correct and more efficient programs. The main
		  transformations used in the Spes system are
		  Unfolding and Folding.}
}

@TechReport{Alexiev-1993,
  author = {Vladimir Alexiev},
  title = {Applications of Linear Logic to Computation: An Overview},
  institution = ALBERTA,
  year = 1993,
  type = {Technical Report},
  number = {TR93-18},
  address = {},
  month = dec,
  OPTnote = {Available from ftp.cs.ualberta.ca: pub/TechReports/TR93-18,
      file TR93-18.ps.gz or TR93-18.ps.Z},
  bibliographies = {RelMiCS}
}

@Article{Alexiev-1994,
  author = {Vladimir Alexiev},
  title = {Applications of Linear Logic to Computation: An Overview},
  journal = BIGPL,
  year = 1994,
  volume = 2,
  number = 1,
  pages = {77--107},
  OPTmonth = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Algebraic-Logic-Problems-1991,
  author = {R. J. Thompson and Roger D. Maddux and others},
  title = {Open Problems},
  crossref = {Andreka-Monk-Nemeti-1991},
  pages = {727--746},
  OPTnote = {},
  OPTauthorsaddress = {},
  WKloc = {A-0129},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@MastersThesis{Alhassy-2015,
  author =       {Musa Al-hassy},
  title =        {A Mechanisation of Internal {Galois} Connections
                  In Order Theory Formalised Without Meets},
  school =       {McMaster Unviersity, Dept.\null{} of Computing and Software},
  year =         {2015},
  type =      {M.Sc.~thesis},
  month =     APR,
  url = {http://hdl.handle.net/11375/17276},
  OPTURL = {https://macsphere.mcmaster.ca/handle/11375/17276},
  abstract =    {Using the the dependently-typed programming language Agda,
    we formalise orders, with attention to the theory of Galois Connections,
    and showcase it by formalising a few results of the category of algebraic
    contexts with relational homomorphisms
    presented by \citet{Jipsen-2012,Moshier-2013}.

    We aim to exhibit an internal theory of Galois Connections
    and Closure operators
    where the ambient space need not have a notion of meets (intersections),
    which are the usual medium in presenting antisymmetry of partial orders.
    Instead we consider `symmetric quotients' as being the relational counterpart
    of propositional calculus' primitive connective: equivalence.
    We argue that it as a more natural primitive than meet ---
    especially its connection with certain proof heuristics regarding posets.
    Moreover, not only do we constrain ourselves to an unconventional
    set of primitive operators,
    but in fact we discard the familiar setting of relations and sets
    in favour of the more general setting
    of ordered categories with converse (OCCs) ---
    in fact, a large portion does not require identities
    and so semigroupoids may be used instead.}
}

@InProceedings{Alhassy-Kahl-2015_AContext,
  author = 	 {Musa Al-hassy and Wolfram Kahl},
  title = {Mechanised Relation-Algebraic Order Theory in Ordered Categories without Meets},
  year = 	 {2015},
  month = 	 APR,
  crossref =  {RelMiCS2015},
  pages = 	 {151--168},
  bibliographies = {RelMiCS, RelMiCS15, WK},
  DOI = {10.1007/978-3-319-24704-5_10},
  OPTSpringerURL = {},
  abstract = {In formal concept analysis, complete lattices of “concepts”
    are represented by entity-attribute relations called “contexts”.
    Using the dependently-typed programming language Agda,
    we build on a previous formalisation of the category of contexts
    to obtain a fully verified abstract implementation
    of the duality between contexts and complete lattices
    in the abstract setting of locally ordered categories with converse,
    residuals, symmetric quotients, and direct powers.}
}

@InProceedings{Alikacem-Sghaier-Desharnais-ElOuali-Tchier-1994,
  author = {Abderrahim Alikacem and Ben Mohamed Sghaier, Salah
                  and Jules Desharnais and El Ouali, Meryem and
                  Fairouz Tchier},
  title = {From Demonic Semantics to Loop Construction: A
                  Relation Algebraic Approach},
  booktitle = {{$3^{rd}$} Maghrebian Conf.\null{} on Software
                  Engineering and Artificial Intelligence, Rabat, Marocco},
  year = 1994,
  month = APR,
  pages = {239--248},
  bibliographies = {RelMiCS}
}

@Article{ AllanVH-JonesRB-LeeRandallM-AllanSJ-1995,
  author = {Vicki H. Allan and Reese B. Jones and Randall M. Lee and
Stephen J. Allan},
  title = {Software pipelining},
  journal = {ACM Comput.~Surv.},
  volume = {27},
  number = {3},
  year = {1995},
  issn = {0360-0300},
  pages = {367--432},
  DOI = {10.1145/212094.212131},
  DOIURL = {http://doi.acm.org/10.1145/212094.212131},
  publisher = {ACM Press},
  CiteSeer = {http://citeseer.ist.psu.edu/allan95software.html},
  bibliographies = {Coconut},
  abstract = {Utilizing parallelism at the instruction level is an
      important way to improve performance. Because the time spent in
      loop execution dominates total execution time, a large body of
      optimizations focuses on decreasing the time to execute each
      iteration. Software pipelining is a technique that reforms the
      loop so that a faster execution rate is realized. Iterations are
      executed in overlapped fashion to increase parallelism. Let
      $\{ABC\}^n$ represent a loop containing operations $A$, $B$, $C$
      that is executed $n$ times. Although the operations of a single
      iteration can be parallelized, more parallelism may be achieved
      if the entire loop is considered rather than a single
      iteration. The software pipelining transformation utilizes the
      fact that a loop $\{ABC\}^n$ is equivalent to $A\{BCA\}^{n-1}
      BC$. Although the operations contained in the loop do not
      change, the operations are from different iterations of the
      original loop. Various algorithms for software pipelining
      exist. A comparison of the alternative methods for software
      pipelining is presented. The relationships between the methods
      are explored and possibilities for improvement highlighted.}
}

@InProceedings{Allen-1981,
  author = {James F. Allen},
  title = {An Interval-based Representation of Temporal Knowledge},
  booktitle = {Proc.\null{} of the {$7^{th}$} Internat.\null{} Joint
		Conf.\null{} on Artificial Intelligence, (IJCAI)},
  year = 1981,
  pages = {221--226},
  bibliographies = {RelMiCS}
}

@Article{Allen-1983,
  author = {James F. Allen},
  title = {Maintaining Knowledge About Temporal Intervals},
  journal = CACM,
  volume = 26,
  number = 11,
  month = NOV,
  year = 1983,
  pages = {832--842},
  bibliographies = {RelMiCS}
}

@Article{Allen-1984,
  author = {James F. Allen},
  title = {Towards a General Theory of Action and Time},
  journal = AI,
  volume = 23,
  year = 1984,
  number = 2,
  month = JUL,
  pages = {123--154},
  bibliographies = {RelMiCS}
}

@Book{Allen-Hand-2001,
  author =    {Colin Allen and Michael Hand},
  title =        {Logic Primer},
  publisher =    {MIT Press},
  year =         2001,
  edition =   {second edition},
  ISBN =         {0-262-51126-6},
  URL =      {http://logic.tamu.edu/},
  note =    {See also \textsf{http://logic.tamu.edu/}}
}

@Book{Allen-Kennedy-2002,
  author =	 {Randy Allen and Ken Kennedy},
  title = 	 {Optimizing Compilers for Modern Architectures: A Dependence-Based Approach},
  publisher = 	 {Academic Press},
  year = 	 2002,
  ISBN = 	 {1-55860-286-0},
  McMaster = 	 {QA 76.76 .C65 A45 2001},
  bibliographies = {Coconut}
}

@InProceedings{AllenStuart-1987,
  author = 	 {Stuart Allen},
  title = 	 {A Non-Type-Theoretic Definition of {Martin-L\"of}'s Types},
  crossref =  {LICS2},
  OPTpages = 	 {},
  WKloc = 	 {A-1601, doc/pap/BIB (modified version: TR87-832 Computer Science Department, Cornell University)}
}

@Article{Allen-Cocke-1976,
  author = {F. E. Allen and J. Cocke},
  title = {A Program Data Flow Analysis Procedure},
  journal = CACM,
  volume = 19,
  number = 3,
  pages = {137--147},
  month = mar,
  year = 1976,
  CODEN = {CACMA2},
  ISSN = {0001-0782},
  abstract = {The global data relationships in a program can be
                 exposed and codified by the static analysis methods
                 described in this paper. A procedure is given which
                 determines all the definitions which can possibly
                 ``reach'' each mode of the control flow graph of the
                 program and all the definitions that are ``live'' on
                 each edge of the graph. The procedure uses an
                 ``interval'' ordered edge listing data structure and
                 handles reducible and irreducible graphs
                 indistinguishably.},
  acknowledgement = ack-nhfb,
  classcodes = {C4240 (Programming and algorithm theory)},
  classification = 723,
  corpsource = {IBM Thomas J. Watson Res. Center, Yorktown Heights,
                 NY, USA},
  journalabr = {Commun ACM},
  keywords = {compilers; data flow analysis; data processing; edge
                 listing data; flow graphs; optimisation; program
                 optimisation; programming theory; static analysis;
                 structure},
  treatment = {A Application; T Theoretical or Mathematical}
}

@InProceedings{Allen-Hayes-1985,
  author = {James F. Allen and Patrick J. Hayes},
  title = {A Commonsense Theory of Time},
  booktitle = {Proc.\null{} of the {$9^{th}$} Internat.\null{} Joint
      Conf.\null{} on Artificial Intelligence (IJCAI)},
  address = {Los Angeles, CA},
  publisher = Kauf,
  year = 1985,
  pages = {528--531},
  bibliographies = {RelMiCS}
}

@InProceedings{Allen-Hayes-1987,
  author = {James F. Allen and Patrick J. Hayes},
  title = {Short Time Periods},
  booktitle = {Proc.\null{} of the {$10^{th}$} Internat.\null{} Joint
      Conf. on Artificial Intelligence},
  address = {Milano, Italy},
  publisher = Kauf,
  year = 1987,
  pages = {981--983},
  bibliographies = {RelMiCS}
}

@TechReport{Allen-Hayes-1987a,
  author = {James F. Allen and Patrick J. Hayes},
  title = {Moments and Points in an Interval-based Temporal Logic},
  number = {TR180},
  month = DEC,
  year = 1987,
  institution = ROCH,
  bibliographies = {RelMiCS}
}

@InCollection{Allen-Kautz-1985,
  author = {James F. Allen and H. Kautz},
  title = {A Model of Naive Temporal Reasoning},
  booktitle = {Formal Theories of the Commonsense World},
  publisher = Ablex,
  year = 1985,
  editor = {J.R. Hobbs and R.C. Moore},
  bibliographies = {RelMiCS}
}

@InProceedings{Allen-Koomen-1983,
  author = {James F. Allen and Johannes A. Koomen},
  title = {Planning Using a Temporal World Model},
  booktitle = {Proc.\null{} of the {$8^{th}$} Internat.\null{} Joint
      Conf.\null{} on Artificial Intelligence (IJCAI)},
  address = {Karlsruhe, Germany},
  month = AUG,
  pages = {741--747},
  year = 1983,
  bibliographies = {RelMiCS}
}

@Article{Allison-1992,
  author = {L. Allison},
  title = {Lazy dynamic programming can be eager},
  journal = {Inf. Proc. Lett.},
  year = 1992,
  volume = 43,
  number = 4,
  pages = {207-212},
  month = SEP,
  WKloc = {A-0260},
  keywords = {FP, lazy functional programming, fast, efficient,
		  dynamic programming algorithm, DPA, edit distance,
		  LCS, LCSS, similar string strings sequence, II, alignment},
  annote = {Lazy evaluation in a functional language is
		  exploited to make the simple dynamic programming
		  algorithm for the edit-distance problem run quickly
		  on similar strings:  being lazy can be fast. Runs in
		  O(n*d) time thanks to laziness.}
}

@Misc{Allison-Carrington-Jones-StewartZerba-1996,
  author = {W. Allison and D. Carrington and T. Jones and L. Stewart-Zerba
      and J. Welsh},
  title = {A Tool for Graphical Presentation of Software Documents},
  institution = {The University of Queensland},
  number = {SVRC TR96-9},
  address = {St. Lucia, Queensland 4072},
  month = MAY,
  year = 1996,
  URL = {http://citeseer.nj.nec.com/allison96tool.html}
}

@Book{Almeida-Frade-Pinto-deSousa-2011,
  author =    {Jos{\'e} Bacelar Almeida and Maria Jo{\~a}o Frade and
                  Jorge Sousa Pinto and Melo de Sousa, Sim{\~a}o},
  title =     {Rigorous Software Development ---
               An Introduction to Program Verification},
  publisher = Springer,
  year =      2011,
  series =    {Undergraduate Topics in Computer Science},
  address =   {London},
  DOI =       {10.1007/978-0-85729-018-2},
  SpringerURL = {http://www.springerlink.com/content/978-0-85729-017-5/},
  CompanionURL = {http://www.di.uminho.pt/rsd-book}
}

@Article{Almeida-Pinto-Vilaca-2008,
  author = {Jos{\'e} Bacelar Almeida and Jorge Sousa Pinto and Miguel Vila{\c{c}}a},
  title = "A Tool for Programming with Interaction Nets ",
  journal = ENTCS,
  volume = 219,
  pages = "83--96",
  year = 2008,
  note = {Proc.~Eighth International Workshop on Rule Based Programming (RULE 2007)},
  issn = "1571-0661",
  DOIURL = "http://dx.doi.org/10.1016/j.entcs.2008.10.036",
  DOI = "10.1016/j.entcs.2008.10.036",
  DirectURL = "http://www.sciencedirect.com/science/article/pii/S1571066108004301",
  keywords = "Interaction Nets",
  keywords = "editor/interpreter ",
  abstract = {This paper introduces INblobs, a visual tool developed at Minho for integrated development with Interaction Nets. Most of the existing tools take as input interaction nets and interaction rules represented in a textual format. INblobs is first of all a visual editor that allows users to edit interaction systems (both interaction nets and interaction rules) graphically, and to convert them to textual notation. This can then be used as input to other tools that implement reduction of nets. INblobs also allows the user to reduce nets within the tool, and includes a mechanism that automatically selects the next active pair to be reduced, following one of the given reduction strategies. The paper also describes other features of the tool, such as the creation of rules from pre-defined templates.}
}


@Article{Alonso-1992,
  author = {Alonso, Juan M.},
  title = {Fibrations that are cofibrations. II.},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Proceedings of the American Mathematical Society},
  year = 1989,
  volume = 105,
  number = 2,
  pages = {486--},
  month = FEB,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@TechReport{Alpern-Carle-Rosen-1987,
  author = {B. Alpern and A. Carle and B. Rosen},
  title = {Incremental evaluation of attributed graphs},
  number = {CS 87-29},
  year = 1987,
  institution = {Brown University}
}

@InProceedings{Alpern-Carle-Rosen-Sweeney-Zadeck-1988,
  author = {Bowen Alpern and Alan Carle and Barry Rosen and Peter Sweeney and Kenneth Zadeck},
  title = {Graph attribution as a specification paradigm },
  booktitle = {Proc. ACM SIGSOFT/SIGPLAN Software Engineering
                Symposium on Practical Software Development Environments},
  confdata = {November 28--30, 1988, Boston, MA USA },
  pages = {121--129},
  year = 1988,
  WKloc = {A-1054, doc/pap/BIB},
  URL = {http://www.acm.org/pubs/citations/proceedings/soft/64135/p121-alpern/},
  abstract = {An interactive software development environment can be
      viewed as a structure-based editor, provided that structure is
      broadly interpreted. The user sees and manipulates complex objects at
      various levels of detail. Many of the implications of changes are
      analyzed and made available to the user as frequently as the user
      wishes, without a mode change from editing to compiling or linking.
      To help provide these services in a uniform way that can readily
      respond to changes in the programming language(s) supported or in the
      preferences of individual users, some programming environments use
      attribute grammars.

      The attributed graph specifications (AGS's) defined here are inspired
      by attribute grammars but are free of their restriction to structures
      expressible by parse

      trees generated by context-free grammars. An AGS deals with whatever
      structure is appropriate in a given application. The graph concept
      here is not tied to

      any decision about pictorial representation. Indeed, we do not care
      whether the user sees pictures or text or a combination of the two.
      The AGS formalism is

      a uniform paradigm for specifying the desired relations among many
      and varied chunks of information, some of which are changed by the
      user. The benefits of the original attribute grammar formalism were
      confined to parse trees and severely restricted manipulations of
      parse trees. The AGS paradigm extends those benefits to software
      development on a larger scale.}
}

@Article{Alpuente-Falaschi-Vidal-1998,
  author = {Maria Alpuente and Moreno Falaschi and German Vidal},
  title = {A Unifying View of Functional and Logic Program Specialization},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 9},
  WKloc = {A-0902, 37--40},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Alpuente-Falashi-Ramis-Vidal-1994,
  author = {M. Alpuente and M. Falashi and M.J. Ramis and G. Vidal},
  title = {A Compositional Semantics for Conditional Term
		  Rewriting Systems},
  crossref = {ICCL94},
  pages = {171--182},
  keywords = {partial evaluation, program generation, automatic
		  programming, self-application, interpreters}
}

@Article{Alpuente-Lucas-Vidal-Hanus-2005,
  author = 	 {Maria Alpuente and Salvador Lucas and Germ{\'a}n Vidal and Michael Hanus},
  title = 	 {Specialization of functional logic programs based on needed narrowing},
  journal = 	 TLP,
  year = 	 2005,
  volume = 	 5,
  number = 	 3,
  pages = 	 {273--303},
  WKloc = 	 {A-1688, doc/pap/BIB},
  bibliographies = {PMC, FLP},
  abstract = 	 {Many functional logic languages are based on
                  narrowing, a unification-based goal-solving
                  mechanism which subsumes the reduction mechanism of
                  functional languages and the resolution principle of
                  logic languages. Needed narrowing is an optimal
                  evaluation strategy which constitutes the basis of
                  modern (narrowing-based) lazy functional logic
                  languages. In this work, we present the fundamentals
                  of partial evaluation in such languages. We provide
                  correctness results for partial evaluation based on
                  needed narrowing and show that the nice properties
                  of this strategy are essential for the
                  specialization process. In particular, the structure
                  of the original program is preserved by partial
                  evaluation and, thus, the same evaluation strategy
                  can be applied for the execution of specialized
                  programs. This is in contrast to other partial
                  evaluation schemes for lazy functional logic
                  programs which may change the program structure in a
                  negative way. Recent proposals for the partial
                  evaluation of declarative multi-paradigm programs
                  use (some form of) needed narrowing to perform
                  computations at partial evaluation time. Therefore,
                  our results constitute the basis for the correctness
                  of such partial evaluators.}
}

@InProceedings{Altenkirch-1993,
  author = {Thorsten Altenkirch},
  title = {A Formalization of the Strong Normalization Proof for System {F} in
          {LEGO}},
  abstract = {We describe a complete formalization of a strong normalization
             proof for the Curry style presentation of System F in LEGO. The
             underlying type theory is the Calculus of Constructions
             enriched by inductive type. The proof follows Girard et al
             [GLT89],
             i.e.~we use the notion of {\em candidates of reducibility},
             but we make essential use of general inductive types to
             simplity the presentation. We discuss extension and variations
             of the proof: the extraction of a normalization function, the
             use of saturated sets instead of candidates, and the extension
             to a Church Style presentation. We conclude with some general
             observations about Computer Aided Formal Reasoning.},
  crossref = {TLCA93},
  pages = {13--28},
  WKloc = {A-0177}
}

@InProceedings{Altenkirch-1998,
  author = {Thorsten Altenkirch},
  title = {Logical Relations and Inductive/Coinductive Types},
  abstract = {We investigate a lambda calculus with positive
              inductive and coinductive types,
              which we call $\lambda^{\mu,\nu}$
              using logical relations.
              As an application we show that using the principle of
              monotone inductive definitions on positive type schemes
              no new functions can be defined.},
  URL = {http://www.tcs.informatik.uni-muenchen.de/~alti/abstracts.html#CSL98.abstract,
        ftp://ftp.tcs.informatik.uni-muenchen.de/pub/alti/publ/CSL98.ps.Z},
  booktitle = {Computer Science Logic, {Proc. Annual Conference of the
               European Association for Computer Science Logic 98}},
  pages = {343-354},
  year = 1998,
  series = {LNCS},
  volume = 1584,
  WKloc = {A-0941},
  bibliographies = {LogRel}
}

@InProceedings{Altenkirch-2004,
  author = {Thorsten Altenkirch},
  title = 	 {$\lambda$-Calculus and Types},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {Midland Graduate School / APPSEM Spring School},
  OPTpages = 	 {},
  OPTyear = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1711},
  OPTannote = 	 {}
}

@InProceedings{Altenkirch-McBride-2003,
  author =       {Thorsten Altenkirch and Conor McBride},
  title =        {Generic Programming Within Dependently Typed Programming},
  booktitle =    {Generic Programming},
  year =         {2003},
  note =         {Proceedings of the IFIP TC2 Working Conference on Generic Programming, Schloss Dagstuhl, July 2002},
  publisher = Kluwer,
  pages = {1--20},
  WKloc = {A-1694}
}

@Misc{Altenkirch-McBride-McKinna-2005,
  author =	 {Thorsten Altenkirch and Conor McBride and James McKinna},
  title =	 {Why Dependent Types Matter},
  howpublished = {Manuscript, available online},
  month =	 {April},
  year =	 {2005},
  URL = {http://www.cs.nott.ac.uk/~txa/publ/ydtm.pdf}
}

@InProceedings{Altenkirch-McBride-SwierstraW-2007,
  author = 	 {Thorsten Altenkirch and Conor McBride and Wouter Swierstra},
  title = 	 {Observational Equality, Now!},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle =	 {PLPV 07},
  OPTpages = 	 {},
  year =	 {2007},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = {doc/pap/BIB},
  OPTannote = 	 {}
}

@InProceedings{Althaus-Altmeyer-Naujoks-2011,
 author = {Althaus, Ernst and Altmeyer, Sebastian and Naujoks, Rouven},
 title = {Precise and efficient parametric path analysis},
 booktitle = {Proceedings of the 2011 SIGPLAN/SIGBED conference on Languages, compilers and tools for embedded systems},
 series = {LCTES '11},
 year = {2011},
 isbn = {978-1-4503-0555-6},
 location = {Chicago, IL, USA},
 pages = {141--150},
 numpages = {10},
 DOIURL = {http://doi.acm.org/10.1145/1967677.1967697},
 DOI = {10.1145/1967677.1967697},
 acmid = {1967697},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {graph theory, parametric timing analysis},
 bibliographies = {Coconut, ZhaoYuhang},
 abstract = {Hard real-time systems require tasks to finish in time.
   To guarantee the timeliness of such a system,
   static timing analyses derive
   upper bounds on the worst-case execution time (WCET) of tasks.
   There are two types of timing analyses: numeric and parametric.
   A numeric analysis derives a numeric timing bound and, to this end,
   assumes all information such as loop bounds to be given a priori.
   If these bounds are unknown during analysis time,
   a parametric analysis can compute
   a timing formula parametric in these variables.
   A performance bottleneck of timing analyses,
   numeric and especially parametric, is the so-called path analysis,
   which determines the path in the analyzed task
   with the longest execution time bound.

   In this paper, we present a new approach to path analysis.
   This approach exploits the often rather regular structure
   of software for hard real-time and safety-critical systems.
   As we show in the evaluation of this paper,
   we strongly improve upon former techniques
   in terms of precision and runtime in the parametric case.
   Even in the numeric case,
   the approach competes with state-of-the-art techniques
   and may be an alternative to commercial tools employed for path analysis.}
}

@InProceedings{Alur-Henzinger-1994,
  title = {Finitary Fairness},
  author = {Rajeev Alur and Thomas Henzinger},
  pages = {52--61},
  crossref = {LICS9},
  abstract = {Fairness is a mathematical abstraction: in a
      multiprogramming environment, fairness abstracts the details of
      admissible (``fair'') schedulers; in a distributed environment,
      fairness abstracts the speeds of independent processors. We argue
      that the standard definition of fairness often is unnecessarily weak
      and can be replaced by the stronger, yet still abstract, notion of
      finitary fairness. While standard weak fairness requires that no
      enabled transition is postponed forever, finitary weak fairness
      requires that for every run of a system there is an unknown bound~$k$
      such that no enabled transition is postponed more than $k$
      consecutive times. In general, the finitary restriction $\mathop{\rm
      fin}(F)$ of any given fairness assumption~$F$ is the union of all
      $\omega$-regular safety properties that are contained in~$F$. \par
      The adequacy of the proposed abstraction is demonstrated in two ways.
      Suppose that we prove a program property under the assumption of
      finitary fairness. In a multiprogramming environment, the program
      then satisfies the property for all fair finite-state schedulers. In
      a distributed environment, the program then satisfies the property
      for all choices of lower and upper bounds on the speeds (or timings)
      of processors. \par The benefits of finitary fairness are twofold.
      First, the proof rules for verifying liveness properties of
      concurrent programs are simplified: well-founded induction over the
      natural numbers is adequate for proving termination under finitary
      fairness. Second, the fundamental problem of consensus in a faulty
      asynchronous distributed environment can be solved assuming finitary
      fairness.}
}

@InProceedings{Amadio-Cardelli-1991,
  author = {Roberto M. Amadio and Luca Cardelli},
  title = {Subtyping Recursive Types},
  pages = {104--118},
  abstract = {Subtyping is an inclusion relation between types that is
             present to some degree in many programming languages. Subtyping
             is especially important in object-oriented languages, where it
             is crucial for understanding the much more complex notions of
             inheritance and subclassing. Recursive types are also present
             in most languages; these types are supposed to unfold
             recursively to match other types. In this paper we investigate
             the interaction of unrestricted recursive tyoes with subtyping,
             which we refer to as recursive subtyping, in the context of
             structural type matching. This interaction is present in some
             modern languages such as Modula-3[Cardelli Donahue Jordan
             Kalsow Nelson 89] and Quest [Cardelli 89]. More precisely, we
             relate various of type equivalence ($\alpha=\beta$) and
             subtyping ($\alpha<\beta$) in a simply typed $\lambda$-calculus
             with subtyping and recursive types. These relations are induced
             by: $\ldots$},
  crossref = {POPL1991},
  WKloc = {A-0173}
}

@InProceedings{Ambler-Kwiatkowska-Measor-1993,
  author = {S. Ambler and M. Kwiatkowska and N. Measor},
  title = {On Duality for the Modal $\mu$-calculus},
  crossref = {CSL93},
  pages = {18--32}
}

@InProceedings{Ammari-Mili-2000,
  author = {Habib Ammari and Ali Mili},
  title = {Lattice-Based Distances},
  crossref = {RelMiCS2000-M},
  pages = {1--18},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {While traditionally we think of measures of distance
    as binary functions that take values in the set of real numbers
    and satisfy some prede ned properties, we find that it is possible
    to define distance-like functions that take their values in an
    ordered set that has lattice properties. The ordering properties
    of the set are useful to give meaning to the concept of proximity,
    and the lattice operations are useful to define a subtraction-like
    operation.}
}

@InProceedings{Amtoft-1994,
  author = {T. Amtoft},
  title = {Local Type Reconstruction by Means of Symbolic Fixed
		  Point Iteration},
  crossref = {ESOP1994},
  pages = {43--57},
  abstract = {We convert, via a version that uses constraints, a
		  type inference system for strictness analysis into
		  an algorithm which given an expression finds the set
		  of possible typings. Although this set in general
		  does not possess a minimal element, it can be
		  represented compctly by means of {\em symbolic
		  expressions in normal form} -- such expressions have
		  the property that once values for the constraint
		  cvariables with {\em negative polarity} have been
		  supplied it is straight-forward to compute the
		  minimal values for the constraint variables with
		  {\em positive polarity}. The normalization process
		  works {\em on the fly}, i.e.\null{} by a
		  leaf-to-root traversal of the inference tree.}
}

@TechReport{Anand-Carette-Curtis-Miller-2005a,
  author = 	 {Christopher Kumar Anand and Jacques Carette and Andrew Thomas Curtis and David Miller},
  title = 	 {{Cog-Pets}: Code Generation for Parameter Estimation in Time Series},
  institution =  {Software Quality Research Laboratory, McMaster University},
  year = 	 {2005},
  type = 	 {{SQRL Report}},
  number = 	 {31},
  note = {available from \textsf{http://sqrl.mcmaster.ca/sqrl\_reports.html}},
  bibliographies = {Coconut},
  month = 	 MAY,
  WKloc = {A-1612, doc/pap/BIB}
}

@TechReport{Anand-Carette-Kahl-2004a,
  author = 	 {Christopher Kumar Anand and Jacques Carette and Wolfram Kahl and Cale Gibbard and Ryan Lortie},
  title = 	 {Declarative Assembler},
  institution =  {Software Quality Research Laboratory, McMaster University},
  year = 	 {2004},
  type = 	 {{SQRL Report}},
  number = 	 {20},
  note = {available from \textsf{http://sqrl.mcmaster.ca/sqrl\_reports.html}},
  bibliographies = {RelMiCS, WK, Coconut},
  month = 	 OCT,
  abstract = {As part of a larger project, we have built a
      declarative assembly language.  This language enables us to specify
      multiple code paths to compute particular quantities, giving the
      instruction scheduler more flexibility in balancing execution
      resources for superscalar execution.  The instruction scheduler is
      also innovative in that it includes aggressive pipelining, and
      exhaustive (but lazy) search for optimal instruction schedules.  We
      present some examples where our approach has produced very promising
      results.}
}

@TechReport{Anand-Kahl-2007a,
  author = 	 {Christopher Kumar Anand and Wolfram Kahl},
  title = 	 {A Domain-Specific Language for the Generation
                  of Optimized {SIMD}-Parallel Assembly Code},
  institution =  {Software Quality Research Laboratory, McMaster University},
  year = 	 {2007},
  type = 	 {{SQRL Report}},
  number = 	 {43},
  note = {available from \textsf{http://sqrl.mcmaster.ca/sqrl\_reports.html}},
  URL = {http://www.cas.mcmaster.ca/~kahl/Publications/TR/Anand-Kahl-2007a_DSL/},
  bibliographies = {RelMiCS, WK, Coconut},
  month = 	 MAY,
  abstract = {We present a domain-specific language (DSL) embedded
      into Haskell that allows mathematicians to formulate novel
      high-performance SIMD-parallel algorithms for the evaluation of
      special functions.

      Developing such functions involves explorations both of
      mathematical properties of the functions which lead to effective
      (rational) polynomial approximations, and of specific properties
      of the binary representation of floating point numbers.

      Our framework includes support for estimating the effectiveness
      of different approximation schemes in Maple.  Once a scheme is
      chosen, the Maple-generated component is integrated into the
      code generation setup. Numerical experimentation can then be
      performed interactively, with support functions for running
      standard tests and tabulating results. Once a satisfactory
      formulation is achieved, a code graph representation of the
      algorithm can be passed to other components which produce C
      function bodies, or to a state-of-the-art scheduler which
      produces optimal or near-optimal schedules, currently targeting
      the ``Cell Broadband Engine'' processor.

      Encapsulating a considerable amount of knowledge about specific
      ``tricks'' in DSL constructs allows us to produce algorithm
      specifications that are precise, readable, and compile to
      optimal-quality assembly code, while formulations of the
      equivalent algorithms in C would be almost impossible to
      understand and maintain.}
}

@InProceedings{Anand-Kahl-2007_AGTIVE,
   author = {Christopher Kumar Anand and Wolfram Kahl},
   title = {Code Graph Transformations for Verifiable Generation of {SIMD}-Parallel Assembly Code},
   pages = {217--232},
   crossref = {AGTIVE2007}
}

@InProceedings{Anand-Kahl-2007_AGTIVE_PP,
   author = {Christopher Kumar Anand and Wolfram Kahl},
   title = {Code Graph Transformations for Verifiable Generation of {SIMD}-Parallel Assembly Code},
   pages = {213--228},
   crossref = AGTIVE2007PP,
}

@InProceedings{Anand-Kahl-2007_MultiLoop,
  author = {Christopher Kumar Anand and Wolfram Kahl},
  title = {{MultiLoop}:  Efficient Software Pipelining for Modern Hardware},
  DOIURL = {http://doi.acm.org/10.1145/1321211.1321242},
  DOI = {10.1145/1321211.1321242},
  pages = {260--263},
  OPTbooktitle = {{CASCON '07: Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research}},

  year = 2007,
  month = OCT,
  OPTpublisher = {ACM},
  editor = {Bruce Spencer and Margaret-Anne Storey and Darlene Stewart},
  booktitle = {Proc.\null{} CASCON 2007},
   publisher = {IBM Centre for Advanced Studies},
   address = {Toronto},
  abstract = {This paper is motivated by trends in processor models
    of which the Cell BE is an exemplar, and by the need to reliably apply
    multi-level code optimizations in safety-critical code.

    A MultiLoop is a loop specification construct
    designed to expose in a structured way details of instruction scheduling
    needed for performance-enhancing transformations.
    For a representative collection of examples from scientific computation,
    we have shown that MultiLoops can be used to express software branch prediction,
    completely eliminating branch misses.
    For some examples, transformations so enabled reduce code size by a factor of two,
    and in other examples reduce execution time by a factor of two.}
}

@InCollection{Anand-Kahl-2009a,
  author = 	 {Christopher K. Anand and Wolfram Kahl},
  title = 	 {Synthesizing and Verifying Multicore Parallelism
                  in Categories of Nested Code Graphs},
  DOI = {10.1201/9781420064872.pt1},
  crossref =  {Alexander-Gardner-2009},
  bibliographies = {WK, Coconut, RelMiCS},
  pages = 	 {3--45},
  chapter = 	 {1}
}

@Article { Anand-Kahl-2009b,
  author = { Christopher K. Anand and Wolfram Kahl },
  title = { An Optimized {Cell BE} Special Function Library Generated by {Coconut}},
  journal = {IEEE Transactions on Computers},
  month = AUG,
  year = 2009,
  volume = 58,
  number = 8,
  pages = {1126--1138},
  DOI = {10.1109/TC.2008.223},
  DOIURL = {http://doi.ieeecomputersociety.org/10.1109/TC.2008.223},
  OPTabstract = {}
}

@Article{Anand-Terlaky-WangB-2004,
  author = 	 {C. K. Anand and T. Terlaky and B. Wang},
  title = 	 {Rapid, Embeddable Design Method for Spiral Magnetic Resonance Image Reconstruction Resampling Kernels},
  journal = 	 {Optimization and Engineering},
  year = 	 2004,
  volume =	 5,
  number =	 4,
  pages =	 {485--502},
  bibliographies = {Coconut}
}

@InProceedings{Andersen-Goldsmith-Scattergood-Teitelbaum-1997,
  author = {Paul Andersen and Michael Goldsmith and Bryan Scattergood and Tim Teitelbaum},
  title = {An Environment for Integrating Formal methods Tools},
  booktitle = {User-Interfaces for Theorem Provers 97 {(UITP97), INRIA, Sophia-Antipolis, September 1--2, 1997}},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0478}
}

@InProceedings{Andersen-Stirling-Winskel-1994,
  title = {A Compositional Proof System for the Modal $\mu$-Calculus},
  author = {Henrik Reif Andersen and Colin Stirling and Glynn Winskel},
  pages = {144--153},
  crossref = {LICS9},
  abstract = {We present a proof system for determining satisfaction
      between processes in a fairly general process algebra and assertions
      of the modal $\mu$-calculus. The proof system is compositional in the
      structure of processes. It extends earlier work on compositional
      reasoning within the modal $\mu$-calculus and combines it with
      techniques from work on local model checking. The proof system is
      sound for all processes and complete for a class of finite-state
      processes.}
}

@InProceedings{Andersson-Bjesse-Cook-Hanna-2002,
  author = 	 {Gunnar Andersson and Per Bjesse and Byron Cook and Ziyad Hanna},
  title = 	 {A Proof Engine Approach to Solving Combinational Design Automation Problems},
  booktitle = {DAC 2002},
  year =	 2002,
  URL = 	 {http://www.sigda.org/Archives/ProceedingArchives/Dac/Dac2002/papers/2002/dac02/htmfiles/sun_sgi/dacabs.htm#47_1},
  WKloc = {A-1500, doc/pap/BIB},
  bibliographies = {Anand},
  ACMcats = {B.6.3 [Hardware]: Logic Design | Design Aids
    General Terms:
    Algorithms, Design, Verification},
  abstract = {There are many approaches available for solving
     combinational design automation problems encoded as tautology or
     satisfiability checks. Unfortunately there exists no single
     analysis that gives adequate performance for all problems of
     interest, and it is therefore critical to be able to combine
     approaches. In this paper, we present a proof engine framework
     where individual analyses are viewed as strategies|functions
     between different proof states. By defining our proof engine in
     such a way that we can compose strategies to form new, more
     powerful, strategies we achieve synergistic effects between the
     individual methods. The resulting framework has enabled us to
     develop a small set of powerful composite default strategies. We
     describe several strategies and their interplay; one of the
     strategies, variable instantiation, is new. The strength of our
     approach is demonstrated with experimental results showing that
     our default strategies can achieve up to several magnitudes of
     speed-up compared to BDD-based techniques and search-based
     satisfiability solvers such as ZCHAFF.}
}

@Article{Anderson-1980,
  author = {B. Anderson},
  title = {Type syntax in the language '{C}', an object lesson in
                 syntactic innovation},
  journal = {ACM SIG{\-}PLAN Notices},
  volume = 15,
  number = 3,
  pages = {21--27},
  month = mar,
  year = 1980,
  coden = {SINODQ},
  ISSN = {0362-1340},
  bibdate = {Sat Apr 25 11:46:37 MDT 1998},
  acknowledgement = ack-nhfb,
  classification = {C6140D (High level languages)},
  corpsource = {Man-Machine Lab., Univ. of Essex, Colchester, UK},
  keywords = {C; compilers; formatter; Interdata 8/32; operating
                 systems; PDP 11; procedure oriented languages; RSX 11;
                 RT 11; syntax; syntax analyzers; systems implementation
                 language; type structure; Unix; user errors; VAX},
  treatment = {P Practical}
}

@InProceedings{Anderson-Foster-Guha-Jeannin-Kozen-Schlesinger-Walker-2014_NetKAT,
 author = {Anderson, Carolyn Jane and Foster, Nate and Guha, Arjun and Jeannin, Jean-Baptiste and Kozen, Dexter and Schlesinger, Cole and Walker, David},
 title = {{NetKAT}: Semantic Foundations for Networks},
 booktitle = {Proceedings of the 41st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
 series = {POPL '14},
 year = {2014},
 isbn = {978-1-4503-2544-8},
 location = {San Diego, California, USA},
 pages = {113--126},
 numpages = {14},
 DOIURL = {http://doi.acm.org/10.1145/2535838.2535862},
 DOI = {10.1145/2535838.2535862},
 acmid = {2535862},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {domain-specific languages, frenetic, kleene algebra with tests, netkat, network programming languages, software-defined networking},
}

@InProceedings{Andrei-Fernandez-Kirchner-Melancon-Namet-Pinaud-2011,
  author =       {Oana Andrei and Maribel Fern{\'{a}}ndez and H{\'e}l{\`e}ne Kirchner and Guy Melan{\c{c}}on and Olivier Namet and Bruno Pinaud},
  title =        {{PORGY}: Strategy-driven interactive transformation of graphs},
  crossref =  {TERMGRAPH2011},
  pages =     {54--68},
  OPTmonth =     {},
  OPTaddress =   {},
  OPTorganization = {},
  OPTpublisher = {},
  DOI = {10.4204/EPTCS.48.7},
  DOIURL = {http://dx.doi.org/10.4204/EPTCS.48.7},
  abstract = {This paper investigates the use of graph rewriting systems as a modelling tool, and advocates the embedding of such systems in an interactive environment. One important application domain is the modelling of biochemical systems, where states are represented by port graphs and the dynamics is driven by rules and strategies. A graph rewriting tool's capability to interactively explore the features of the rewriting system provides useful insights into possible behaviours of the model and its properties. We describe PORGY, a visual and interactive tool we have developed to model complex systems using port graphs and port graph rewrite rules guided by strategies, and to navigate in the derivation history. We demonstrate via examples some functionalities provided by PORGY.}
}

@Article{Andreka-1987,
  author = {Hajnal Andr{\'e}ka},
  title = {Boolean Reducts of Relation and Cylindric Algebras and
		the Cube Problem},
  journal = PROAMS,
  volume = 100,
  number = 1,
  year = 1987,
  month = MAY,
  pages = {148--153},
  bibliographies = {RelMiCS}
}

@Article{Andreka-1988,
  author = {Hajnal Andr{\'e}ka},
  title = {On Taking Subalgebras of Relativized Relation Algebras},
  journal = ALGU,
  volume = 25,
  year = 1988,
  pages = {96--100},
  bibliographies = {RelMiCS}
}

@Booklet{Andreka-1988a,
  author = {Hajnal Andr{\'e}ka},
  title = {On the ``Union-relation composition'' Reducts
		of Relation Algebras},
  note = {Preprint, September 1988, pp.\null{} 11},
  bibliographies = {RelMiCS}
}

@Article{Andreka-1991,
  author = {Hajnal Andr{\'e}ka},
  title = {Representations of Lattice-ordered Semigroups with
		Binary Relations},
  journal = ALGU,
  volume = 28,
  year = 1991,
  pages = {12--25},
  bibliographies = {RelMiCS}
}

@Article{Andreka-1994,
  journal = ALGU,
  volume = 32,
  author = {Hajnal Andr{\'e}ka},
  title = {Weakly Representable but not Representable Relation Algebras},
  publisher = BIRKH,
  address = {Basel},
  year = 1994,
  pages = {31--43},
  bibliographies = {RelMiCS}
}

@InProceedings{Andreka-Comer-Nemeti-1985,
  author = {Hajnal Andr\'eka and Stephen D. Comer  and Istv\'an  N{\'e}meti},
  title = {Clones of Operations on Relations},
  booktitle = {Universal Algebra and Lattice Theory},
  note = {Proc.\null{} of the Southeastern Conf.\null{} in Universal
		Algebra and Lattice Theory, July 11-14, 1984},
  series = LNM,
  volume = 1149,
  publisher = Springer,
  year = 1985,
  pages = {17--21},
  bibliographies = {RelMiCS}
}

@Booklet{Andreka-Duentsch-Nemeti-1988,
  author = {Hajnal Andr{\'e}ka and Ivo D{\"u}ntsch and Istv\'an N{\'e}meti},
  title = {A Non-permutational Integral Relation Algebra},
  note = {Preprint, June 19, 1988, pp. 16},
  bibliographies = {RelMiCS}
}

@Article{Andreka-Givant-Nemeti-1994,
  journal = JSYLO,
  volume = 59,
  number = 2,
  author = {Hajnal Andr{\'e}ka and Steven Givant and Istv\'an N{\'e}meti},
  title = {The Lattice of Varieties of Representable Relation Algebras},
  apublisher = {Association for Symbolic Logic},
  year = 1994,
  pages = {631--661},
  bibliographies = {RelMiCS}
}

@InProceedings{Andreka-Jonsson-Nemeti-1990,
  author = {Hajnal Andr\'eka and Bjarni J{\'o}nsson and Istv\'an N{\'e}meti},
  title = {Relatively Free Relation Algebras},
  crossref = {ALUACS1988},
  pages = {1--14},
  bibliographies = {RelMiCS}
}

@Article{Andreka-Jonsson-Nemeti-1991,
  author = {Hajnal Andr\'eka and Bjarni J{\'o}nsson
		and Istv\'an N{\'e}meti},
  title = {Free Algebras in Discriminator Varieties},
  journal = ALGU,
  volume = 28,
  year = 1991,
  pages = {401--447},
  bibliographies = {RelMiCS}
}

@Booklet{Andreka-Maddux-1988,
  author = {Hajnal Andr\'eka and Roger Duncan Maddux},
  title = {Representations for Small Relation Algebras},
  note = {Preprint, March 16, 1988, pp 14},
  bibliographies = {RelMiCS}
}

@Article{Andreka-Maddux-Nemeti-1991,
  author = {Hajnal Andr\'eka and Roger Duncan Maddux
		and Istv\'an N{\'e}meti},
  title = {Splitting in Relation Algebras},
  journal = PROAMS,
  volume = 111,
  number = 4,
  year = 1991,
  month = APR,
  pages = {1085--1094},
  OPTnote = {Zbl 721.03046 (J. Cirulis)},
  bibliographies = {RelMiCS}
}

@Article{Andreka-Mikulas-1994,
  author = {Hajnal Andr{\'e}ka and Szabolcs Mikul\'as},
  journal = JLLI,
  number = 1,
  pages = {1--38},
  title = {Lambek Calculus and its Relational Semantics:
            Completeness and Incompleteness},
  volume = 3,
  year = 1994,
  bibliographies = {RelMiCS}
}

@Booklet{Andreka-Nemeti-1988,
  author = {Hajnal Andr\'eka and Istv\'an N{\'e}meti},
  title = {Relation Algebraic Conditions for the
		Representability of Cylindric and Polyadic Algebras},
  note = {Preprint, 1988, pp 46},
  bibliographies = {RelMiCS}
}

@TechReport{Andreka-Nemeti-Sain-1992,
  author = {Hajnal Andr{\'e}ka and Istv\'an N{\'e}meti and Ildik{\'o} Sain},
  title = {Abstract Model Theoretic Approach to Algebraic Logic
                  (an Overview)},
  institution = {Dept.\null{} of Statistics and Methodology, PSCW,
                  Univ.\null{} Amsterdam},
  year = 1992,
  type = {CCSOM Working Paper},
  number = {92-92},
  bibliographies = {RelMiCS}
}

@TechReport{Andreka-Nemeti-Sain-1993,
  author = {Hajnal Andr{\'e}ka and Istv\'an N{\'e}meti and Ildik{\'o} Sain},
  title = {Algebras of Relations and Algebraic Logic. an Introduction},
  institution = {Dept.\null{} of Statistics and Methodology, PSCW,
                  Univ.\null{} Amsterdam},
  year = 1993,
  type = {CCSOM Working Paper},
  number = {93-91},
  bibliographies = {RelMiCS}
}

@Unpublished{Andreka-Nemeti-Sain-1993a,
  author = {Hajnal Andr{\'e}ka and Istv\'an N{\'e}meti and Ildik{\'o} Sain},
  title = {Methodology of Applying Algebraic Logic to Logic},
  note = {Course Material Version},
  year = 1993,
  month = JUN,
  bibliographies = {RelMiCS}
}

@Unpublished{Andreka-Nemeti-Sain-1997,
  author = {Hajnal Andr{\'e}ka and Istv\'an N{\'e}meti and Ildik{\'o} Sain},
  title = {Algebraic Logic},
  note = {129 pages},
  year = 1997,
  publisher = Kluwer,
  booktitle = {Handbook of Philosophical Logic},
  edition = {second edition},
  editor = {Dov Gabbay},
  bibliographies = {RelMiCS},
  WKloc = {B-0100}
}

@Article{Andreka-Thompson-1988,
  author = {Hajnal Andr\'eka and Richard J. Thompson},
  title = {A {Stone} Type Representation Theorem
		for Algebras of Relations of Higher Rank},
  journal = TRAMS,
  volume = 309,
  number = 2,
  year = 1988,
  month = OCT,
  pages = {671--682},
  bibliographies = {RelMiCS}
}

@Book{Andrews-2002,
  author =    {Peter B. Andrews},
  title =        {An Introduction to Mathematical Logic and Type Theory: To Truth Through Proof},
  publisher =    {Springer Netherlands},
  year =         2002,
  volume =    27,
  series =    {Applied Logic Series},
  edition =   {Second edition},
  DOI =     {10.1007/978-94-015-9934-4},
  ISBN =      {978-90-481-6079-2},
  McMaster = {QA 9 .A638 2002},
  McMasterURL = {http://catalogue.mcmaster.ca/catalogue/Record/1270877},
  SpringerURL = {http://link.springer.com/book/10.1007\%2F978-94-015-9934-4}
}

@Article{Andrews-Brown-Pfenning-Bishop-Issar-XiHongwei-2004_ETPS,
   author = {Andrews, Peter B. and Brown, Chad E. and Pfenning, Frank and Bishop, Matthew and Issar, Sunil and Xi, Hongwei},
   title = {{ETPS}: A System to Help Students Write Formal Proofs},
   journal = {Journal of Automated Reasoning},
   publisher = Springer,
   issn = {0168-7433},
   keyword = {Computer Science},
   pages = {75-92},
   volume = {32},
   issue = {1},
   url = {http://dx.doi.org/10.1023/B:JARS.0000021871.18776.94},
   note = {10.1023/B:JARS.0000021871.18776.94},
   abstract = {ETPS (Educational Theorem Proving System) is a program that logic students can use to write formal proofs in first-order logic or higher-order logic. It enables students to concentrate on the essential logical problems involved in proving theorems, and it automatically checks the proofs.},
   year = {2004}
}

@Book{AndrewsGR-1991,
  author = {Gregory R. Andrews},
  title = {Concurrent Programming: Principles and Practice},
  publisher = {The Benjamin/Cummings Publishing Company, Inc.},
  year = 1991,
  address = {Redwood City, California},
  McMaster = {QA 76.642 .A53 1991},
  contents = {\begin{tabular}{l@{\ }l@{\hskip1em}l}
    1 & Sequential Programming & 7 \\
    2 & Concurrency and Synchronization & 57 \\
    3 & Fine-Grained Synchronization & 97 \\
    4 & Semaphores & 171 \\
    5 & Conditional Critical Regions & 225 \\
    6 & Monitors & 263 \\
    7 & Asynchronous Message Passing & 343 \\
    8 & Synchronous Message Passing & 423 \\
    9 & RPC and Rendezvous & 483 \\
    10 & Language Overviews & 553
    \end{tabular}},
  bibliographies = {SE3B},
  annote = {Includes formal semantics and proofs!}
}

@Article{Andrews-Brown-Pfenning-Bishop-Issar-XiHongwei-2004_ETPS,
   author = {Andrews, Peter B. and Brown, Chad E. and Pfenning, Frank and Bishop, Matthew and Issar, Sunil and Xi, Hongwei},
   title = {{ETPS}: A System to Help Students Write Formal Proofs},
   journal = {Journal of Automated Reasoning},
   publisher = Springer,
   issn = {0168-7433},
   keyword = {Computer Science},
   pages = {75-92},
   volume = {32},
   issue = {1},
   url = {http://dx.doi.org/10.1023/B:JARS.0000021871.18776.94},
   note = {10.1023/B:JARS.0000021871.18776.94},
   abstract = {ETPS (Educational Theorem Proving System) is a program that logic students can use to write formal proofs in first-order logic or higher-order logic. It enables students to concentrate on the essential logical problems involved in proving theorems, and it automatically checks the proofs.},
   year = {2004}
}

@InProceedings{Andries-Engels-1993,
  author = {Marc Andries and Gregor Engels},
  title = {Syntax and Semantics of Hybrid Database Languages},
  crossref = {GTCS93},
  pages = {19--36},
  abstract = {We present the hybrid query language HQL/EER for an
		  extended Entity-Relationship model. As its main
		  characteristic, this language allows a user to
		  freely mix graphical and textual formulation of a
		  query. We show how syntax and semantics of this
		  hybrid language are formally defined by means of a
		  slightly extended version of PROGRES, a
		  specification formalism based on programmed and
		  attributed graph rewriting systems.},
  annote = {The term ``hybrid'' is inspired by hybrid syntax
		  directed editors, where the user can freely choose
		  between a syntax-directed and a free style of
		  editing \cite{Engels-Schaefer-1989}}
}

@Article{Andries-Engels-Habel-Hoffmann-Kreowski-Kuske-Plump-Schuerr-Taentzer-1999,
  author = {Marc Andries and Gregor Engels and Annegret Habel and
            Berthold Hoffmann and Hans J\"org Kreowski and Sabine Kuske and
            Detlef Plump and Andy Sch\"urr and Gabriele Taentzer},
  title = {Graph Transformation for Specification and Programming},
  year = 1999,
  journal = {Science of Computer Programming},
  volume = 34,
  pages = {1--54},
  WKloc = {A-0674},
  annote = {GRACE}
}

@InProceedings{Andronick-Chetali-PaulinMohring-2005,
  author = 	 {June Andronick and Boutheina Chetali and Christine Paulin-Mohring},
  title = {Formal Verification of Security Properties of
           Smart Card Embedded Source Code},
  OPTcrossref =  {FM2005},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2005},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  note = 	 {unpublished draft},
  WKloc = 	 {A-1607, reviewed for FM2005}
}

@InProceedings{Anellis-Houser-1991,
  author = {Irving H. Anellis and Nathan Houser},
  title = {The {$19^{th}$} Century Roots of Universal Algebra and
   Algebraic Logic},
  crossref = {AL1991},
  pages = {1--36},
  WKloc = {A-0126},
  bibliographies = {RelMiCS}
}

@InProceedings{Antimirov-1995,
  author = 	 {Valentin Antimirov},
  title = 	 {Partial derivatives of regular expressions and finite automata constructions},
  crossref =  {STACS1995},
  URL = 	 {http://www.springerlink.com/content/f154277135706604/},
  OPTbooktitle = {},
  pages = 	 {455--466},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {900},
  OPTnumber = 	 {},
  OPTseries = 	 LNCS,
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  bibliographies = {RelMiCS},
  abstract = {We introduce a notion of a partial derivative of a regular
                  expression. It is a generalization to the
                  non-deterministic case of the known notion of a
                  derivative invented by Brzozowski. We give a
                  constructive definition of partial derivatives,
                  study their properties, and employ them to develop a
                  new algorithm for turning regular expressions into
                  relatively small NFA and to provide certain
                  improvements to Brzozowski's algorithm constructing
                  DFA. We report on a prototype implementation of our
                  algorithm constructing NFA and present some
                  examples.}
}

@InProceedings{Antoy-Middeldorp-1994,
  author = {Sergio Antoy and Aart Middeldorp},
  title = {A Sequential Reduction Strategy},
  crossref = {ALP1994},
  pages = {168--185},
  WKloc = {A-0317},
  abstract = {Kennaway proved the remarkable result that every
		  (almost) orthogonal term rewriting system admits a
		  computable sequential normalizing reduction
		  strategy. In this paper we present a computable
		  sequential reduction strategy similar in scope, but
		  simpler and more general. Our strategy can be
		  thought of as an outermost-fair-like strategy that
		  is allowed to be unfair to some redex of a term when
		  contracting the redex is useless for the
		  normalization of the term. Unlike the strategy of
		  Kennaway, our strategy does not rely on syntactic
		  restrictions that imply confluence. On the contrary,
		  it can easily be applied to any term rewriting
		  system, and we show that the class of term rewriting
		  systems for which our strategy is normalizing
		  properly includes all (almost) orthogonal
		  systems. Our strategy is more versatile; in case of
		  (almost) orthogonal term rewriting systems, it can
		  be used to detect certain cases of
		  non-termination. Our normalization proof is more
		  accessible than Kennaway's. We also show that our
		  sequential strategy sometimes succeeds where the
		  parallel-outermost strategy fails.},
  annote = {\cite{Bergstra-Klop-1986,Kennaway-1989}}
}

@Article{Antoy-Echahed-Hanus-2000,
  author = 	 {Sergio Antoy and Rachid Echahed and Michael Hanus},
  title = 	 {A Needed Narrowing Strategy},
  journal = 	 JACM,
  year = 	 {2000},
  OPTkey = 	 {},
  OPTvolume = 	 {47},
  OPTnumber = 	 {4},
  OPTpages = 	 {776--822},
  WKloc = 	 {A-1613, doc/pap/BIB},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@INPROCEEDINGS{Antoy-Hanus-2005,
  author       = {Antoy, S. and Hanus, M.},
  title        = {Declarative Programming with Function Patterns},
  year         = {2005},
  booktitle    = {Proceedings of the International Symposium on Logic-based
                  Program Synthesis and Transformation (LOPSTR'05)},
  pages        = {6-22},
  publisher    = {Springer LNCS 3901},
  URL = {http://www.informatik.uni-kiel.de/~mh/publications/papers/LOPSTR05.html},
  WKloc = {doc/pap/BIB},
  abstract = {We propose an extension of functional logic
      languages that allows the definition of operations with patterns
      containing other defined operation symbols.  Such ``function
      patterns'' have many advantages over traditional constructor
      patterns. They allow a direct representation of specifications as
      declarative programs, provide better abstractions of patterns as
      first-class objects, and support the high-level programming of queries
      and transformation of complex structures.  Moreover, they avoid known
      problems that occur in traditional programs using strict equality.  We
      define their semantics via a transformation into standard functional
      logic programs. Since this transformation might introduce an infinite
      number of rules, we suggest an implementation that can be easily
      integrated with existing functional logic programming systems.}
}

@InProceedings{Antoy-Hanus-2006,
  author       = {Antoy, S. and Hanus, M.},
  title        = {Overlapping Rules and Logic Variables in Functional Logic Programs},
  year         = {2006},
  booktitle    = {Proceedings of the International Conference on Logic Programming
                  (ICLP 2006)},
  pages        = {87-101},
  publisher    = {Springer LNCS 4079},
  URL = {http://www.informatik.uni-kiel.de/~mh/publications/papers/ICLP06.html},
  WKloc = {doc/pap/BIB},
  abstract = {Functional logic languages extend purely functional
      languages with two features: operations defined by overlapping
      rules and logic variables in both defining rules and expressions
      to evaluate.  In this paper, we show that only one of these
      features is sufficient in a core language. On the one hand,
      overlapping rules can be eliminated by introducing logic
      variables in rules. On the other hand, logic variables can be
      eliminated by introducing operations defined by overlapping
      rules. The proposed transformations between different classes of
      programs not only give a better understanding of the features of
      functional logic programs but also may simplify implementations
      of functional logic languages.}
}

@Book{Aoe-1994,
  author = {Jun-ichi Aoe},
  title = {Computer Algorithms, String Pattern Matching Strategies},
  publisher = {IEEE Computer Society Press},
  year = 1994,
  UniBwM = {INF300/Y3342},
  annote = {Nice survey articles starting each chapter},
  contents = {\def\art#1#2#3#4{#1,{\em #2}, from {\sl #3}, #4}
                Preface\\
                Chapter 1: Single keyword matching\\
                 \art{Fast Pattern Matching in Strings}{D.E. Knuth
		  and J.H. Morris and V.R. Pratt}{SIAM Journal of
		  Computing}{June 1977}\\
                 \art{A Fast String Searching Algorithm}{R.S. Boyer
		  and J.S. Moore}{Communications of the ACM}{October
		  1977}\\
                 \art{Algorithms for Pattern Matching}{G. Davies and
		  S. Bowsher}{Software --- Practice and
		  Experience}{June 1986}\\
                Chapter 2: Matching sets of keywords\\
                 \art{Efficient String Matching: An Aid to
		  Bibliographic Search}{A.V. Aho and
		  M.J. Corasick}{Communications of the ACM}{June
		  1975}\\
                 \art{A Method for Improving String Pattern Matching
		  Machines}{J. Aoe and Y. Yamamoto and
		  R. Shimada}{IEEE Trans. on Software
		  Engineering}{January 1984}\\
                 \art{An Efficient Algorithm for Matching Multiple
		  Patterns}{J.-J. Fan and K.-Y. Su}{IEEE Trans. on
		  Knowledge and Data Engineering}{April 1993}\\
                Chapter 3: Approximate String Matching\\
                 \art{Approximate String Matching}{P.V. Hall and
		  G.R. Dowling}{ACM Computing Surveys}{December
		  1980}\\
                 \art{Optimal Correspondence of String
		  Subsequences}{Y.P. Wang and T. Pavlidis}{IEEE
		  Trans. on Pattern Analysis and Machine
		  Intelligence}{November 1990}\\
                 \art{The Noisy Substring Matching
		  Problem}{R.L. Kashyap and B.J. Oommen}{IEEE
		  Trans. on Software Engineering}{May 1983}\\
                Chapter 4: Multidimensional Matching\\
                 \art{Pattern Matching in Trees}{C.M. Hoffmann and
		  M.J. O'Donnell}{Journal of the ACM}{January 1982}\\
                 \art{Code Generation Using Tree Matching and Dynamic
		  Programming}{A.V. Aho and M. Ganapathi and
		  S.W.K. Tjiang}{ACM Trans. on Programming Languages
		  and Systems}{October 1989}\\
                 \art{The Tree-to-Tree Correction
		  Problem}{K.-C. Tai}{Journal of the ACM}{July 1979}\\
                 \art{A Technique for Two-Dimensional Pattern
		  Matching}{R.F. Zhu and T. Takaoka}{Communications of
		  the ACM}{September 1989}\\
                Chapter 5: Hardware matching\\
                 \art{Performance and Architectural Issues for String
		  Matching}{M.E. Isenman and D.E. Shasha}{IEEE
		  Trans. on Computers}{February 1990}\\
                 \art{HYTREM --- A Hybrid Text-Retrieval Machine for
		  Large Databases}{D.L. Lee and F.H. Lochovsky}{IEEE
		  Trans. on Computers}{January 1990}\\
                References\\
                About the Author}
}

@TechReport{Appel-1992,
  author = {Andrew W. Appel},
  title = {A Critique of Standard ML},
  institution = {Princeton University},
  year = 1992,
  number = {CS-TR-364-92},
  month = NOV,
  WKloc = {A-0277},
  abstract = {Standard ML is an excellent language for many kinds
		  of programming. It is safe, efficient, suitably
		  abstract, and concise. There are many aspects of the
		  language that work well.

                  However, nothing is perfect: Standard ML has a few
		  shortcomings. In some cases there are obvious
		  solutions, and in other cases further research is required.}
}

@Book{Appel-1998,
  author =	 {Andrew W. Appel},
  title = 	 {Modern Compiler Implementation in ML},
  publisher = 	 CambridgeUP,
  year = 	 1998,
  ISBN = {0-521-60764-7 (paperback)},
  URL = {http://www.cs.princeton.edu/~appel/modern/ml/}
}

@Misc{Appel-Davidson-Ramsey-1998,
  author = {Andrew Appel and Jack Davidson and Norman Ramsey },
  title = {The {Zephyr} Compiler Infrastructure},
  month = NOV,
  year = 1998,
  URL = {http://www.cs.virginia.edu/zephyr},
  WKloc = {A-1196}
}

@Article{Appel-Felty-2004,
  author = {Andrew W. Appel and Amy P. Felty},
  title = {Dependent Types Ensure Partial Correctness of Theorem Provers},
  journal = 	 JFP,
  year = 	 2004,
  volume =	 14,
  number =	 1,
  pages =	 {3--19},
  month =	 JAN,
  WKloc = 	 {A-1599, doc/pap/BIB},
  bibliographies = {HHOL},
  URL = {http://www.site.uottawa.ca/~afelty/abstracts/jfp04.html},
  abstract = {Static type systems in programming languages allow
     many errors to be detected at compile time that wouldn't be
     detected until runtime otherwise. Dependent types are more
     expressive than the type systems in most programming languages,
     so languages that have them should allow programmers to detect
     more errors earlier. In this paper, using the Twelf system, we
     show that dependent types in the logic programming setting can be
     used to ensure partial correctness of programs which implement
     theorem provers, and thus avoid runtime errors in proof search
     and proof construction. We present two examples: a tactic-style
     interactive theorem prover and a union-find decision procedure.}
}

@Book{Appelt-1988,
  UniBwM = {INF600/R12180},
  ISBN = {3-89319-115-1},
  contents = {1 Textauszeichnung
                  2 Notation
                  3 Fonts
                  4 Blanks
                  5 Makros und Parameter
                  6 Textstrukturen
                  7 Verweise
                  8 Makropakete
                  9 Deutschsprachiger Text
                  Literatur
                  Anhang: \TeX{}-Syntax
                          Index zum Anhang
                  Index},
  year = 1988,
  title = {{\TeX{} f\"ur Fortgeschrittene}},
  publisher = {Addison-Wesley},
  author = {Wolfgang Appelt}
}

@InProceedings{Apt-1985,
  author = {Krysztof R. Apt},
  title = {Proving Correctness of {CSP} Programs, a Tutorial},
  pages = {441--474},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Apt-Bol-1994,
  author = {Krzysztof R. Apt and Roland Bol},
  title = {Negation in Logic Programming},
  crossref = {LPAR94},
  note = {invited tutorial},
  authorsAddress = {apt\@cwi.nl, bol\@win.tue.nl},
  abstract = {The use of negation in logic programming has been
		  thoroughly studied over the last 15 years. One of
		  its interesting features is that it can naturally
		  support non-monotonic reasoning.  The aim of this
		  tutorial is to provide an overview of the main
		  developments in the proof theory and model theory of
		  logic programming with negation, and on the
		  relationship between them. More specifically, we
		  shall discuss the SLDNF- and SLS-resolution
		  procedures and their modification by means of
		  tabulation, and various 2-valued and 3-valued
		  semantics, including Clark's completion, stable
		  model and well-founded semantics.}
}

@Book{Apt-Olderog-1997,
  author = {Krzystof R. Apt and Ernst-R{\"u}diger Olderog},
  title = {Verification of Sequential and Concurrent Programs},
  publisher = Springer,
  year = 1997,
  series = {Graduate Texts in Computer Science},
  edition = {2nd},
  note = {(1st edition 1991)},
  McMaster = {QA 76.76 .V47A67 1997},
  bibliographies = {SE3B}
}

@InProceedings{Apt-Pedreschi-1991,
  title = {Proving Termination of General Prolog Programs},
  author = {Krzysztof R. Apt and Dino Pedreschi},
  pages = {265--289},
  crossref = {TACS1991},
  abstract = {We study here termination of general logic programs with
		  the Prolog selection rule.  To this end we extend
		  the approach of Apt and Pedreschi [AP90] and
		  consider the class of {\em left terminating\/}
		  general programs.  These are general logic programs
		  that terminate with the Prolog selection rule for
		  all ground goals.  We introduce the notion of an
		  {\em acceptable program\/} and prove that all
		  acceptable programs are left terminating.  This
		  provides us with a practical method of proving
		  termination.

                  The converse implication does not hold by we show
		  that under the assumption of non-floundering from
		  ground goals every left terminating program is
		  acceptable.  Finally, we prove that various ways of
		  defining semantics must coincide for acceptable
		  programs.  The method is illustrated by giving
		  simple proofs of termination of a ``game'' program
		  and the transitive closure program for the desired
		  class of goals.}
}

@InProceedings{Apt-Pellegrini-1992,
  keywords = {Martelli-Montanari algorithm, LD-derivation,
		  well-moded, data driven, nicely moded, output
		  driven, strictly moded},
  authorsAddress = {KRA: Amsterdam; AP: Padova},
  abstract = {In most Prolog implementations for efficiency
		  reasons the so-called occur-check is omitted from
		  the unification algorithm. We provide here natural
		  syntactic conditions which allow the occur-check to
		  be safely omitted. The established results apply to
		  most well-known Prolog programs and seem to explain
		  why this omission does not lead in practice to any
		  complications.},
  title = {Why the Occur-check is Not a Problem},
  pages = {69--86},
  crossref = {PLILP1992},
  author = {Krzysztof R. Apt and Alessandro Pellegrini}
}

@InProceedings{Apt-Richier-1985,
  author = {Krysztof R. Apt and Jean-Luc Richier},
  title = {Real Time Clocks versus Virtual Clocks},
  pages = {475--501},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Arbib-Manes-1978,
  author = {Arbib, M.A. and Manes, E.G.},
  title = {Partially-Additive Monoids, Graph-Growing and the
		  Algebraic Semantics of Recursive Calls},
  crossref = {GG1978},
  bibliographies = {RelMiCS},
  abstract = {The way in which PFN (sets and partial functions)
               provides a setting for the semantics of deterministic
               programs [and REL (sets and relations) provides a
               setting for the semantics of nondeterministic programs]
               has led us to an axiomatic notion of a partially-additive
               monoid. We show that programs incorporating procedure
               calls may be represented by graph grammars, with one
	       non-terminal and one production for each distinct procedure
               (including the program itself). Program execution may be
               construed as a process of interpretation of graphs
               obtained by repeated graph substitution. We show that the
               resultant interpretive semantics yields the same result
               ary of the canonical fixpoint for abstract recursion
	       schemas introduced in an earlier paper.}
}

@Misc{Areces-Heguiabehere-,
  author = 	 {Carlos Areces and Juan Heguiabehere},
  title = 	 {Direct Resolution for Modal-like Logics},
  OPThowpublished = {},
  OPTmonth = 	 {},
  year = 	 {???},
  URL = 	 {http://ww.hylo.net/},
  WKloc = {A-1507},
  bibliographies = {HHOL}
}

@InProceedings{Arieli-Avron-1994,
  title = {Logical Bilattices and Inconsistent Data},
  author = {Ofer Arieli and Arnon Avron},
  pages = {468--476},
  crossref = {LICS9},
  abstract = {The notion of a {\em bilattice\/} was first proposed by
        Ginsberg as a general framework for many applications. This notion was
        further investigated and applied for various goals by Fitting. In the
        present paper we develop {\em proof systems}, which correspond to
        bilattices in an essential way. We then show how to use those
        bilattices for efficient inferences from possibly inconsistent data.
        For this we incorporate certain ideas of Kifer and Lozinskii
        concerning inconsistencies, which happen to suit well the framework of
        bilattices. The outcome is a paraconsistent logic with a lot of
        desirable properties.},
  bibliographies = {RelMiCS}
}

@PhDThesis{Ariola-1992,
  author = {Zena Matilde Ariola},
  title = {An Algebraic Approach to the Compilation and
		  Operational Semantics of Functional Languages with
		  {I}-Structures},
  school = {MIT},
  year = 1992,
  month = JAN,
  OPTtype = {},
  note = {appeared as Technical Report MIT/LCS/TR-544},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  abstract = {Modern languages are too complex to be given direct operational semantics. For example, the operational semantics of functional languages has traditionally been given by translating them to the l-calculus extended with constants. Compilers do a similar translation into an intermediate form in the process of generating code for a machine.}
}

@InProceedings{Ariola-1993,
  author = {Zena M. Ariola},
  title = {Relating Graph and Term Rewriting via B\"ohm Models},
  crossref = {RTA93},
  pages = {183--197},
  WKloc = {A-0133},
  abstract = {Dealing properly with sharing is important for expressing some
             of the common compiler optimizations, such as common
             subexpressions elimination, lifting of free expressions and
             removal of invariants from a loop, as source-to-source
             transformations. Graph rewriting is a suitable vehicle to
             accommodate these concerns. In [4] we have presented a term
             model for graph rewriting systems (GRSs) without interfering
             rules, and shown the partial correctness of the aforementioned
             optimizations. In this paper we define a different model for
             GRSs, which allows us to prove total correctness of those
             optimizations. Differently from [4] we will discard sharing
             from our observations and introduce more restrictions on the
             rules. We will introduce the notion of B\"ohm tree for GRSs,
             and show that in a system without interfering and non-left
             linear rules (orthogonal GRSs), B\"ohm tree equivalence
             defines a congruence. Total correctness then follows in a
             straightforward way from showing that if a program M contains
             sharing than a program N , then both M and N have the same
             B\"ohm tree.

             We will also show that orthogonal GRSs are a
             correct implementation of orthogonal TRSs. The basic idea of
             the proof is to show that the behavior of graph can be deduced
             from its finite approximations, that is, graph rewriting is a
             continuous operation. Our approach differs from that of other
             researchers \cite{Farmer-Watro-1989,Kennaway-Klop-Sleep-deVries-1991},
             which is based on infinite rewriting.}
}

@Article{Ariola-Arvind-1995,
  author = {Zena Ariola and Arvind},
  title = {Properties of First-Order functional languages with sharing},
  journal = {Theoretical Computer Science},
  year = 1995,
  volume = 146,
  number = {1--2},
  pages = {69--108},
  month = Jul,
  WKloc = {},
  abstract = {}
}

@Misc{Ariola-Felleisen-Maraist-Odersky-Wadler-199X,
  author = {Zena Ariola and Matthias Felleisen and John Maraist and Martin Odersky and Philip Wadler},
  title = {A Call-by-Need Lambda Calculus},
  crossref = {POPL1995},
  pages = "233--246",
  WKloc = {A-0593, doc/pap/BIB},
  CiteSeer = {http://citeseer.ist.psu.edu/132534.html},
}

@InProceedings{Ariola-Klop-1994,
  title = {Cyclic Lambda Graph Rewriting},
  author = {Zena M. Ariola and Jan Willem Klop},
  pages = {416--425},
  WKloc = {A-0322},
  bibliographies = {Coconut},
  abstract = {This paper is concerned with the study of cyclic
		  $\lambda$-graphs.  The starting point is to treat a
		  $\lambda$-graph as a system of recursion equations
		  involving $\lambda$-terms, and to manipulate such
		  systems in an unrestricted manner, using equational
		  logic, just as is possible for first-order term
		  rewriting. Surprisingly, now the confluence property
		  breaks down in an essential way.

                  Confluence can be restored by introducing a
		  restraining mechanism on the `copying'
		  operation. This leads to a family of $\lambda$-graph
		  calculi, which are inspired by the family of
		  $\lambda\sigma$-calculi ($\lambda$-calculi with
		  explicit substitution).  However, these concern
		  acyclic expressions only.  In this paper we are not
		  concerned with optimality questions for acyclic
		  $\lambda$-reduction.  We also indicate how
		  Wadsworth's interpreter can be simulated in the
		  $\lambda$-graph rewrite rules that we propose.},
  crossref = {LICS9}
}

@InProceedings{Ariola-Klop-1994-x,
  title = {Cyclic Lambda Graph Rewriting},
  author = {Zena M. Ariola and Jan Willem Klop},
  pages = {416--425},
  WKloc = {A-0322},
  abstract = {This paper is concerned with the study of cyclic
		  $\lambda$-graphs.  The starting point is to treat a
		  $\lambda$-graph as a system of recursion equations
		  involving $\lambda$-terms, and to manipulate such
		  systems in an unrestricted manner, using equational
		  logic, just as is possible for first-order term
		  rewriting. Surprisingly, now the confluence property
		  breaks down in an essential way.

                  Confluence can be restored by introducing a
		  restraining mechanism on the `copying'
		  operation. This leads to a family of $\lambda$-graph
		  calculi, which are inspired by the family of
		  $\lambda\sigma$-calculi ($\lambda$-calculi with
		  explicit substitution).  However, these concern
		  acyclic expressions only.  In this paper we are not
		  concerned with optimality questions for acyclic
		  $\lambda$-reduction.  We also indicate how
		  Wadsworth's interpreter can be simulated in the
		  $\lambda$-graph rewrite rules that we propose.},
  booktitle = {Proceedings, Ninth Annual IEEE Symposium on Logic in
		  Computer Science},
  year = 1994,
  month = {4--7 } # jul,
  address = {Paris, France},
  organization = {IEEE Computer Society Press}
}

@InProceedings{Armando-Basin-etal-,
  author = 	 {A. Armando and D. Basin and Y. Boichut and Y. Chevalier and L. COmpagna and J. Cuellar and Hankes Drielsma, P. and P. C. He{\'a}m and O. Kouchnarenko and J. Mantovani and S. M{\"o}dersheim and von Oheimb, D. and M. Rusinowitch and J. Santiago and M. Tutuani and L. Vigan{\`o} and L. Vigneron},
  title = 	 {The {AVISPA} Tool for the Automatic Validation of Internet Security Protocols and Applications},
  crossref =  {CAV2005},
  WKloc = {A-1615, doc/pap/BIB/AVISPA*}
}

@InProceedings{Armbruster-1991,
  title = {Polynomial Recursion Analysis in Pascal Like Programs},
  author = {Dieter Armbruster},
  pages = {447--458},
  crossref = {TACS1991},
  abstract = {Besides being of theoretical interest the knowledge about
		  a procedure's calling behavior is valuable for an
		  optimizing compiler. It is well known, however, that
		  such properties like recursivity or reachability of
		  procedures are unfortunately {\em undecidable} for
		  programs in {\em ALGOL--like} languages and are
		  still worse than {\em P--Space Complete} in the {\em
		  ISO--Pascal} case.

                  We extend this language hierarchy (with respect to
		  parameter restrictions) at the lower end to {\em
		  Wirth's Pascal} and show that there the recursivity
		  problem for procedures is decidable within {\em
		  polynomial time.} In order to establish this (rather
		  unexpected) result we
                  \begin{itemize}
                  \item[1.] reduce recursivity of a procedure to a
		    reachability problem -- with both properties being
		    defined on an infinite tree representation of the program;
                  \item[2.] show the equivalence between reachability
		    in such an {\em infinite tree} on the one hand and
		    reachability in the {\em finite graph}
		    representation on the other hand;
                  \item[3.] solve then the reachability problem in
		    this graph in $O(n s)$ of a program with $n$
		    procedures as vertices and $s$ call statements as edges.
                  \end{itemize}}
}

@Article{Armour-2000,
  author = {Phillip G. Armour},
  title = {The Business of Software: the case for a new business model},
  journal = CACM,
  year = 2000,
  volume = 43,
  number = 8,
  pages = {19--22},
  month = AUG,
  WKloc = {A-1035, doc/pap/BIB/Armour-2000.*},
  bibliographies = {SpecTech}
}

@InProceedings{Armstrong-1974,
  author = {W. W. Armstrong},
  title = {Dependency Structures of Database Relationships},
  booktitle = {1974 IFIP Congress},
  year = 1974,
  publisher = NoHo,
  pages = {580--583},
  bibliographies = {RelMiCS}
}

@Misc{Armstrong-2000,
  author = {Eric Armstrong},
  title = {Encoding Source in {XML}},
  subtitle = {A Strategic Analysis},
  OPThowpublished = {},
  month = JUN,
  year = 2000,
  WKloc = {A-1143},
  bibliographies = {LitProg}
}

@InProceedings{Arnborg-1993,
  author = {Stefan Arnborg},
  title = {Decomposability Helps for Deciding Logics of
		  Knowledge and Belief},
  crossref = {GTCS93},
  pages = {37--50},
  abstract = {We show that decision problems in modal logics
		  (logics of knowledge and belief) are easy for
		  decomposable formulas. Satisfiability of a formula
		  of size $n$ and treewidth $k$ can be decided in time
		  $O(nf(k))$, where $f$ is a double exponential
		  function. This result holds not only for the logics
		  S5 and KD45 with NP-complete decision problems, but
		  also for extensions to multiple agents as in the
		  standard logics \def\LO#1{${\rm #1}_n$}\LO{K},
		  \LO{T}, \LO{S4}, \LO{S5} and \LO{KD45}, whose
		  decision problemsare PSPACE complete for arbitrary
		  formulas. Moreover, the method works for these
		  logics extended with operators for distributed and
		  common knowledge, which otherwise cause a complexity
		  increase to exponential time for the satisfiability
		  problem.},
  annote = {For a selfcontained description of these logics, see
		  \cite{Halpern-Moses-1992}.}
}

@Article{Arnborg-Courcelle-Proskurowski-Seese-1993,
  author = {Stefan Arnborg and Bruno Courcelle and Andrzej
		  Proskurowski and Detlef Seese},
  title = {An Algebraic Theory of Graph Reduction},
  journal = {Journal of the ACM},
  year = 1993,
  volume = 40,
  number = 5,
  pages = {1134--1164},
  month = NOV,
  WKloc = {A-0223},
  abstract = {We show how membership in classes of graphs
		  definable in monadic second-order logic and of
		  bounded treewidth can be decided by finite sets of
		  terminating reduction rules. The method is
		  constructive in the sense that we describe an
		  algorithm that will produce, from a formula in
		  monadic second-order logic and an integer $k$ such
		  that the class defined by the formula is of
		  treewidth $\leq k$, a set of rewrite rules that
		  reduces any member of the class to one of finitely
		  many graphs, in a number of steps bounded by the
		  size of the graph. We illustrate our results with
		  reduction systems that recognize some families of
		  outerplanar and planar graphs.}
}

@InProceedings{Arnold-1994,
  author = {Andr\'e Arnold},
  title = {Hypertransition Systems},
  crossref = {STACS1994},
  pages = {327--338},
  authorsAddress = {Bordeaux I},
  abstract = {Hypertransition systems are extensions of transition
		  systems in the same way as tree automata are
		  extensions of word automata and hypergraphs are
		  extensions of graphs. In this paper we explain why
		  we need such an extension in order to model
		  nondeterminism and refinements in systems of
		  concurrent processes.}
}

@InProceedings{Arques-Michel-1995,
  author = {D.G. Arqu\`es and Ch. J. Michel},
  title = {A Possible Code in the Genetic Code},
  crossref = {STACS1995},
  pages = {640--652},
  note = {invited talk},
  authorsAddress = {Marne-la-Vall\'ee},
  abstract = {In order to analyse the genetic code, the
		  distribution of the 64 trinucleoides $w$ (words of 3
		  letters on the gene alphabet $\{A,C,G,T\}$, $w\in
		  {\cal T} = \{AAA,\ldots,TTT\}$) in the procaryotic
		  protein coding genes (wods of large size) is studied
		  with autocorrelation functions. $\ldots$}
}

@InProceedings{Arts-Dam-Fredlund-Gurov-1998,
  author = {Thomas Arts and Mads Dam and Lars-\o{a}ke Fredlund and Dilian Gurov},
  title = {System Description: Verification of Distributed {Erlang} Programs},
  crossref = {CADE1998},
  pages = {38--41},
  OPTabstract = {},
  WKloc = {A-0601}
}

@InProceedings{ArunKumar-Hennessy-1991,
  title = {An Efficiency Preorder for Processes},
  author = {S. Arun-Kumar and M. Hennessy},
  pages = {152--175},
  crossref = {TACS1991},
  abstract = {A simple efficiency preorder for CCS processes is
		  introduced in which $p\stackrel{<}{\sim}q$ means
		  that $q$ is at least as fast as $p$, or more
		  generaly, $p$ uses at least as much resources as
		  $q$.  It is shown to be preserved by all CCS
		  contexts except summation and it is used to analyse
		  a non-trivial example: two different implementations
		  of a bounded buffer.  Finally we give a sound and
		  complete proof system for finite processes.}
}

@MastersThesis{Arwanitis,
  year = 1991,
  title = {Spezifikation und {Implementierung} einer komfortablen
		  {X-Windows}-gest\"utzten {Benutzeroberfl\"ache}
		  f\"ur {HOPS}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 42/91},
  month = DEC,
  author = {Peter Arwanitis}
}

@Article{Ashman-2000,
  author = {Helen Ashman},
  title = {Electronic Document Addressing: Dealing with Change},
  journal = ACMCS,
  year = 2000,
  volume = 32,
  number = 3,
  pages = {201--212},
  month = SEP,
  WKloc = {A-1249},
  bibliographies = {DigBib},
  abstract = {The management of electronic document collections is
      fundamentally different from the management of paper documents. The
      ephemeral nature of some electronic documents means that the document
      address (i.e., reference details of the document) can become
      incorrect some time after coming into use, resulting in references,
      such as index entries and hypertext links, failing to correctly
      address the document they describe. A classic case of invalidated
      references is on the World Wide Web?links that point to a named
      resource fail when the domain name, file name, or any other aspect of
      the addressed resource is changed, resulting in the well-known Error
      404. Additionally, there are other errors which arise from changes to
      document collections.This paper surveys the strategies used both in
      World Wide Web software and other hypertext systems for managing the
      integrity of references and hence the integrity of links. Some
      strategies are preventative, not permitting errors to occur; others
      are corrective, discovering references errors and sometimes
      attempting to correct them; while the last strategy is adaptive,
      because references are calculated on a just-in-time basis, according
      the current state of the document collection.}
}

@Article{Ashman-Simpson-1999,
  author = {Helen Ashman and Rosemary Michelle Simpson},
  title = {{Computing Surveys}' electronic symposium on hypertext and hypermedia},
  journal = ACMCS,
  year = 1999,
  volume = 31,
  number = 4,
  WKloc = {A-1250},
  bibliographies = {DigBib}
}

@Article{Asperti-1992,
  author = {Andrea Asperti},
  title = {The Categorical Understanding of Environment Machines},
  journal = {Journal of Functional Programming},
  year = 1992,
  volume = 2,
  number = 1,
  month = JAN,
  pages = {23--59},
  WKloc = {A-0619},
  abstract = {$\ldots$},
  annote = {cited in \cite{Lescanne-1994} as categorical analog to
		  $\lambda\upsilon$.}
}

@InProceedings{Asperti-1995,
  author = {Andrea Asperti},
  title = {$\delta \circ ! \epsilon = 1$ --- {Optimizing} Optimal $\lambda$-Calculus Implementations},
  crossref = {RTA95},
  pages = {102--116},
  OPTabstract = {},
  WKloc = {A-0557}
}

@Misc{Asperti-199X,
  author = {Andrea Asperti},
  title = {{P = NP}, up to sharing},
  year = {199?},
  WKloc = {A-0796}
}

@InCollection{Asperti-2012,
  author={Asperti, Andrea},
  title={A Compact Proof of Decidability for Regular Expression Equivalence},
  pages={283--298},
  DOI={10.1007/978-3-642-32347-8_19},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-32347-8_19},
  year={2012},
  isbn={978-3-642-32346-1},
  booktitle={Interactive Theorem Proving},
  volume={7406},
  series=LNCS,
  LNCSbooktitle = {ITP 2012},
  editor={Beringer, Lennart and Felty, Amy},
  publisher={Springer Berlin Heidelberg},
  language={English},
  abstract = {The article describes a compact formalization of the relation between regular expressions and deterministic finite automata, and a formally verified, efficient algorithm for testing regular expression equivalence, both based on the notion of pointed regular expression [8]}
}

@InProceedings{Asperti-Danos-Laneve-Regnier-1994,
  title = {Paths in the lambda-calculus},
  author = {Andrea Asperti and Vincent Danos and Cosimo Laneve and
		  Laurent Regnier},
  pages = {426--436},
  crossref = {LICS9},
  WKloc = {A-0323},
  abstract = {Since the rebirth of $\lambda$-calculus in the late
		  sixties, three major theoretical investigations of
		  $\beta$-reduction were undertaken: 1)~L\'evy's
		  analysis of families of redexes (and the associated
		  concept of labeled reductions); 2)~Lamping's
		  graph-reduction algorithm; 3)~Girard's geometry of
		  interaction.

                  All these three studies happened to make crucial (if
		  not always explicit) use of a notion of a path.
		  Namely and respectively: legal, consistent and
		  regular paths.  We prove they are equivalent.}
}

@TechReport{Asperti-Laneve-1992,
  author = {Andrea Asperti and Cosimo Laneve},
  title = {{Interaction Systems I:} The theory of optimal reductions},
  institution = {INRIA-Rocquencourt},
  number = 1748,
  year = 1992,
  month = SEP
}

@InProceedings{Asperti-Laneve-1993,
  author = {Andrea Aspert and C. Laneve},
  title = {Paths, Computations and Labels in the $\lambda$-Calculus},
  crossref = {RTA93},
  pages = {152--167},
  WKloc = {A-0131},
  abstract = {We provide a new characterization of L\'evy's
		  redex-families in the $\lambda$-calculus [11] as
		  suitable paths in the initial term of the
		  derivation. The idea is that redexes in a same
		  family are created by ``contraction'' of a unique
		  common path in the initial term. This fact gives new
		  evidence about the ``common nature'' of redexes in a
		  same family, and about the possibility of sharing
		  their reduction. From this point of view, our
		  characterization underlies all recent worts on
		  optimal graph reduction techniques for the
		  $\lambda$-calculus
		  \cite{Lamping-1990,Gonthier-Abadi-Levy-1992}[7,1],
		  providing an original and intuitive understanding of
		  optimal implementations.

                  As an easy by-product, we prove that neither overlining
		  nor underlining are required in L\'evy's labelling.}
}

@InProceedings{Asperti-Laneve-1993a,
  author = {Andrea Asperti and Cosimo Laneve},
  title = {{Interaction Systems}},
  WKloc = {A-0315},
  keywords = {CRS, combinatory rewriting systems, optimal reduction},
  pages = {1--19},
  crossref = {HOA1993}
}

@TechReport{Asperti-Laneve-1993b,
  author = {Andrea Asperti and Cosimo Laneve},
  title = {{Interaction Systems II, The Practice of Optimal
		  Reductions}},
  institution = {Laboratory for Computer Science, University of
		  Bologna},
  year = 1993,
  number = {UBLCS-93-12},
  WKloc = {A-0214},
  keywords = {CRS, combinatory rewriting systems, optimal reduction},
  abstract = {Lamping's optimal graph reduction technique for the
		  $\lambda$-calculus is generalized to a new class of
		  higher order rewriting systems, called Interaction
		  Systems. Interaction Systems provide a nice
		  integration of the functional paradigm with a rich
		  class of data structures (all inductive types), and
		  some basic control flow constructs such as
		  conditionals and (primitive or general) recursion.
		  We describe a uniform and optimal implementation, in
		  Lamping's style, for all these features. The paper
		  is the natural continuation of
		  \cite{Asperti-Laneve-1992}, where we focused on the
		  {\em theoretical} aspects of optimal reductions in
		  Interaction Systems (family relation, labeling, extraction)}
}

@InProceedings{Asperti-Laneve-1993c,
  author = {Asperti, Andrea and Laneve, Cosimo},
  title = {Optimal Reductions in Interaction Systems},
  pages = {485--500},
  crossref = {TAPSOFT1993}
}

@InProceedings{Asperti-Laneve-1995,
  author = {Asperti, Andrea and Laneve, Cosimo},
  title = {Comparing $\lambda$-Calculus Translations in Sharing Graphs},
  pages = {1--15},
  crossref = {TLCA95},
  WKloc = {A-0559}
}

@InProceedings{Asperti-Laneve-1997,
  author = {Asperti, Andrea and Laneve, Cosimo},
  title = {On the Dynamics of Sharing Graphs},
  pages = {259--269},
  crossref = {ICALP1997},
  UniBwM = {Z5792-24},
  WKloc = {A-0794}
}

@Book{Asperti-Longo-1991,
  author = {Andrea Asperti and Giuseppe Longo},
  title = {Categories, Types, and Structures: An Introduction
		  to Category Theory for the Working Computer Scientist},
  year = 1991,
  series = {Foundations of Computing},
  publisher = {MIT Press},
  address = {Cambridge, MA},
  UniBwM = {INF400/V14531},
  WKloc = {B-0029},
  bibliographies = {RelMiCS}
}

@InProceedings{Asperti-Martini-1989,
  author = {Asperti, A. and Martini, S.},
  title = {Projections Instead of Variables: A Category Theoretic
		  Interpretation of Logic Programs},
  crossref = {ICLP1989},
  pages = {337--352}
}

@InProceedings{Aspinall-2000,
  author = {David Aspinall},
  title = {{Proof General}: A Generic Tool for Proof Development},
  crossref = {TACAS2000},
  pages = {38--42},
  note = {See also \url{http://www.proofgeneral.org}.}
}

@Article{Astesiano-Bidoit-KirchnerH-KriegBrueckner-Mosses-Sanella-Tarlecki-2002_TCS_CASL,
  title = {{CASL}: the Common Algebraic Specification Language},
  journal = TCS,
  volume = "286",
  number = "2",
  pages = "153--196",
  year = "2002",
  note = "Current trends in Algebraic Development Techniques ",
  issn = "0304-3975",
  DOIURL = "http://dx.doi.org/10.1016/S0304-3975(01)00368-1",
  DOI = "10.1016/S0304-3975(01)00368-1",
  DirectURL = "http://www.sciencedirect.com/science/article/pii/S0304397501003681",
  author = "Egidio Astesiano and Michel Bidoit and Hélène Kirchner and Bernd Krieg-Brückner and Peter D. Mosses and Donald Sannella and Andrzej Tarlecki",
  abstract = {The Common Algebraic Specification Language (CASL) is an expressive language for the formal specification of functional requirements and modular design of software. It has been designed by COFI, the international Common Framework Initiative for algebraic specification and development. It is based on a critical selection of features that have already been explored in various contexts, including subsorts, partial functions, first-order logic, and structured and architectural specifications. CASL should facilitate interoperability of many existing algebraic prototyping and verification tools. This paper gives an overview of the CASL design. The major issues that had to be resolved in the design process are indicated, and all the main concepts and constructs of CASL are briefly explained and illustrated — the reader is referred to the CASL Language Summary for further details. Some familiarity with the fundamental concepts of algebraic specification would be advantageous. }
}

@InProceedings{Astesiano-Cerioli-1993,
  author = {E. Astesiano and M. Cerioli},
  title = {Multiparadigm Specification Languages: A First
		  Attempt at Foundations},
  crossref = {SoSL93},
  pages = {168--185},
  keywords = {simulation, simulation independent metalanguage}
}

@InProceedings{Astesiano-Costa-1979,
  author = {Astesiano and Costa},
  title = {Sharing in Nondeterminism},
  booktitle = {Annual International Colloquium on Automata, Languages
                 and Programming},
  year = 1979
}

@InProceedings{Astesiano-Costa-1980,
  author = {E. Astesiano and G. Costa},
  title = {Languages with Reducing Reflexive Types},
  pages = {38--50},
  annote = {see also \cite{Astesiano-Costa-1983}},
  booktitle = {&th Annual International Colloquium on Automata, Languages
                 and Programming},
  year = 1980,
  series = {LNCS},
  volume = 85
}

@Article{Astesiano-Costa-1983,
  author = {E. Astesiano and G. Costa},
  title = {The Insensitivity Theorem for Nonreducing Reflexive
                 Types},
  journal = {Journal of Computer and System Sciences},
  volume = 27,
  year = 1983
}

@TechReport{Attali-Pascual-Roudet-1997,
  author = {Isabelle Attali and Val{\'e}rie Pascual and Christophe Roudet},
  title = {A language and an integrated envornment for program transformations},
  year = 1997,
  month = DEC,
  institution = {INRIA},
  number = {RR-3313},
  WKloc = {A-0665}
}

@Book{Audretsch-Mainzer-1990,
  UniBwM = {PHY150/W5007},
  year = 1990,
  title = {{Wieviele Leben hat Schr\"odingers Katze? Zur Physik
		    und Philosophie der Quantenmechanik}},
  publisher = BI,
  editor = {J\"urgen Audretsch and Klaus Mainzer},
  bibliographies = {RelMiCS}
}

@InProceedings{Augusteijn-1992,
  author = {Lex Augusteijn},
  title = {An Alternative Derivation of a Binary Heap
		  Construction Function},
  crossref = {MPC1992},
  pages = {368--374},
  WKloc = {A-0243},
  OPTabstract = {In \cite{Schoenmakers-1992} a derivation of a binary
		  heap construction algorithm is presented, makeing
		  use of program inversion techniques. In this paper,
		  we present an alternative derivation, using
		  algebraic program transformation techniques in a
		  purely functional setting. The main technique used
		  is that of recursion elimination.}
}

@Manual{Augusteijn_doggy,
  title = {Doggy: a generator for graph transformers in compilers},
  OPTkey = {},
  author = {Lex Augusteijn},
  OPTorganization = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1422, doc/pap/BIB}
}

@TechReport{Augustejn-,
  author = {Ir. A. Augustejn},
  title = {Definition of the Programming Language {Elegant}},
  institution = {Philips Research, Information and Software Technology},
  year = {199X},
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = {},
  CiteSeer = {http://citeseer.nj.nec.com/423546.html},
  WKloc = {A-1377, doc/pap/BIB},
  abstract = {A language for the specification of an attribute grammar has
      been designed, which is called Elegant. Elegant can also be used as a
      general purpose programming language. Elegant oers the facility to
      specify a compiler by means of an attribute grammar. Any noncyclic
      attribute dependency relation is allowed. Attributes are implemented
      by lazy evaluation. Local as well as global attributes are available
      as user friendly extensions of the attribute grammar formalism.
      Context conditions can be [...]}
}

@InProceedings{Augustsson-1985,
  author = 	 {Lennart Augustsson},
  title = 	 {Compiling Pattern Matching},
  crossref =	 {FPCA1985},
  pages =	 {368--381},
  WKloc = {A-1632},
  bibliographies = {FP, PMC}
}

@Misc{Augustsson-199X,
  author = {Lennart Augustsson},
  title = {Cayenne --- a language with dependent types},
  year = {199?},
  WKloc = {A-0650}
}

@Misc{Augustsson-199Y,
  author = {Lennart Augustsson},
  title = {Cayenne --- Spice up your programming with dependent types},
  year = {199?},
  WKloc = {A-0716}
}


@Unpublished{Augustsson-1999,
  author =       {Lennart Augustsson},
  title =        {Equality proofs in {Cayenne}},
  year =         {1999},
  OPTnote =         {\url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.9415} (accessed 2014-01-03)},
  note =         {\url{http://tinyurl.com/Aug99eqproof} (accessed 2014-01-03)},

  SRCURL = {http://www.augustsson.net/Darcs/Cayenne/doc/eqproof.tex},
  annote =    {Cited by \cite{MuShinCheng-KoHsianShang-Jansson-2009}.}
}

@Misc{Augustsson-2008a,
  author = 	 {Lennart Augustsson},
  title = 	 {Lost and Found},
  howpublished = {Blog entry, \url{http://augustss.blogspot.com/2008/07/lost-and-found-if-i-write-108-in.html}},
  month = 	 JUL # {~3},
  year = 	 2008,
  bibliographies = {Coconut},
  abstract = {If I write $10^8$ in Haskell, how many multiplications will be used to compute the power?}
}

@Misc{Augustsson-2008b,
  author = 	 {Lennart Augustsson},
  title = 	 {\textsf{Debug.Traced}},
  howpublished = {Package \textsf{traced} on \textsf{hackage.haskell.org}.},
  month = 	 JUL,
  year = 	 2008,
  URL = {}
}

@InProceedings{Augustsson-Coquand-Nordstroem-1990,
  author =       {Lennart Augustsson and Thierry Coquand and B. Nordstr{\"o}m},
  title =        {A short description of {Another Logical Framework}},
  OPTcrossref =  {},
  OPTkey =       {},
  booktitle = {Proceedings of the First Workshop on Logical Frameworks, Antibes },
  OPTpages =     {},
  year =      {1990},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@InProceedings{Augustsson-Johnsson-1989,
  author = {Lennart Augustsson and Thomas Johnsson},
  title = {Parallel Graph Reduction with the $\langle \nu, G\rangle$-machine},
  WKloc = {A-0669, doc/pap/BIB},
  crossref = {FPCA-1989},
  pages     = {202--213},
  DOIURL    = {http://doi.acm.org/10.1145/99370.99386}
}

@Article{Ausiello-Manni-Italiano-1990,
  author = {Ausiello, G. and Manni, U. and Italiano, G. F.},
  title = {Dynamic Maintenance Of Directed Hypergraphs},
  journal = {Theoretical Computer Science},
  year = 1990,
  volume = 72,
  number = 2,
  pages = {97--117},
  abstract = {In this paper we are concerned with the online
               maintenance of directed hypergraphs a generalization of
               directed graphs previously introduced in the literature,
               In particular, we show how to maintain efficiently
               information about hyperpaths while new hyperarcs are
               inserted. We present a data structure which allows us to
               check wether there exists a hyperpath between an
               arbitrarily given pair of nodes in constant time and to
               return such a hyperpath in a time which is linear in its
               size. The total time required to maintain that data
               structure during the insertion of new hyperarcs is 0(mn),
               where m is the total size of the description of the
               hyperarcs and n is the number of nodes. This generalizes
               a previous result known for directed graphs and has
               applications in several areas of computer science, such
               as rewriting systems, database schemes, logic
               programming and problem solving. An extension of these
               results to hyperpaths between sets of nodes is also
               presented.}
}

@InCollection{Austen-Janas-Wiehle-1989,
  authorsAddress = {inf31, inf2},
  year = 1989,
  title = {{\"Uber das ISO Norm-Projekt zur verteilten
		  Transaktionsverarbeitung: Stand und technische
		  Alternativen}},
  publisher = {Springer-Verlag},
  pages = {99--114},
  editor = {P.J. K\"uhn},
  booktitle = {{Kommunikation in verteilten Systemen}},
  author = {Austen, M.W. and  Janas, J.M. and  Wiehle, H.R.},
  address = {Berlin}
}

@InProceedings{Autexier-Hutter-Mossakowski-Schairer-2002,
  author = 	 {Serge Autexier and Dieter Hutter and Till Mossakowski and Axel Schairer},
  title = 	 {The Development Graph Manager {MAYA}},
  crossref = 	 {AMAST2002},
  pages =	 {495--501},
  bibliographies = {OPG}
}

@InProceedings{Avenhaus-LoriaSaenz-1994,
  author = {J\"urgen Avenhaus and Carlos Lor\'{\i}a-S\'aenz},
  title = {Higher-Order Conditional Rewriting and Narrowing},
  crossref = {CCL94},
  pages = {269--284},
  keywords = {quasi first order, simple rewrite system}
}

@InProceedings{Avenhaus-LoriaSaenz-1994a,
  author = {J\"urgen Avenhaus and Carlos Lor\'{\i}a-S\'aenz},
  title = {On conditional rewrite systems with extra variables
		  and deterministic logic programs},
  crossref = {LPAR94},
  pages = {215--229},
  authorsAddress = {University of Kaiserslautern},
  abstract = {We study deterministic conditional rewrite systems,
		  i.e.\null{} conditional rewrite systems where the
		  extra variables are not totally free but 'input
		  bounded'. If such a system $R$ is quasi-reductive
		  then $\rightarrow_R$ is decidable and
		  terminating. We develop a critical pair criterion to
		  prove confluence if $R$ is quasi-reductive and
		  strongly deterministic. We apply our results to
		  prove Horn clause programs to be uniquely terminating.}
}

@InProceedings{Avitzur-1990,
  author = {R. Avitzur},
  title = {Suggestions for a Friendly User Interface},
  crossref = {DISCO90},
  pages = {282--283},
  annote = {Something like a spec for MathSpad}
}

@Article{Avron-1988,
  author = {A. Avron},
  title = {The Semantics and Proof Theory of Linear Logic},
  journal = TCS,
  year = 1988,
  volume = 57,
  pages = {161--184},
  bibliographies = {RelMiCS}
}

@Article{Avron-1998a,
  author = {Arnon Avron},
  title = {Two Types of Multiple-Conclusion Systems},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 5,
  pages = {695--717},
  UniBwM = {INF/Z},
  WKloc = {A-0565},
  bibliographies = {RelMiCS}
}

@TechReport{Awodey-Butz-1997,
  author = {Steve Awodey  and Carsten Butz},
  title = {Topological completeness for higher-order logic},
  institution = {BRICS},
  type = {BRICS Report},
  number = {RS-97-21},
  URL = {http://euclid.math.mcgill.ca/butz/publications/abstracts.html\#topcomp},
  month = JUL,
  year = 1997,
  note = {To appear in J. Symbolic Logic},
  abstract = {Using recent results in topos theory, two systems of
      higher-order logic are shown to be complete with respect to sheaf
      models over topological spaces, so-called `topological semantics'.
      The first is classical higher-order logic, with relational
      quantification of finitely high type; the second system is a
      predicative fragment thereof with quantification over functions
      between types, but not over arbitrary relations. The second theorem
      applies to intuitionistic as well as classical logic.},
  WKloc = {A-1070, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Misc{Awodey-Warren-2007,
  author = 	 {Steve Awodey and Michael A. Warren},
  title = 	 {Homotopy theoretic models of identity types},
  pages = 11,
  howpublished = {arXiv:0709.0248v1},
  month = 	 SEP,
  year = 	 2007,
  URL = 	 {http://arxiv.org/abs/0709.0248},
  WKloc = 	 {doc/pap/BIB},
  abstract = {This paper presents a novel connection between
                  homotopical algebra and mathematical logic. It is
                  shown that a form of intensional type theory is
                  valid in any Quillen model category, generalizing
                  the Hofmann-Streicher groupoid model of Martin-Loef
                  type theory.}
}

@Manual{Azero-1999,
  title = {{UU$\_$Pretty} User Manual},
  OPTkey = {},
  OPTauthor = {Pablo R. Azero},
  OPTorganization = {Department of Computer Science, Utrecht University},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = JAN,
  OPTyear = 1999,
  WKloc = {A-1216, doc/pap/BIB},
  URL = {http://www.cs.uu.nl/groups/ST/Software/UU_Pretty/},
  abstract = {This file is a literate Haskell script intended as a short
              User Manual for the UU$\_$Pretty library.
              Knowledge of the language Haskell is assumed.
              It is also assumed that the reader has access
              to a Haskell compiler or (better) an interpreter
              while reading this document.
              Along the text the reader is presented with examples
              that he/she can later on find in the UU$\_$Pretty$\_$ext library.
              Thus it is also an introduction
              to the use of UU$\_$Pretty$\_$ext library.

              The manual is organized as follows.
              The first section treats the single layout combinators,
              almost the same to the ones found in version 1.x of the library
              (the reader familiar with them can skip
              most of the material in this section, but not all).
              The second section explains how to program
              combinators of multiple layouts.
              The Appendix contains the interface of UU$\_$Pretty
              including the type signature
              of all the functions in the module interface.}
}

@Misc{Azero-Swierstra-1998,
  author = {Pablo R. Azero and S. Doaitse Swierstra},
  title = {Optimal Pretty-Printing Combinators},
  year = 1998,
  WKloc = {A-0514},
  note = {submitted to ICFP '98},
  annote = {partially garbled PostScript layout}
}

@InProceedings{Baader-1989,
  author = 	 {Franz Baader},
  title = 	 {Unification properties of commutative theories: A categorical treatment},
  crossref =  {CTCS1989},
  pages = 	 {273--299},
  URL = 	 {http://www.springerlink.com/content/w476326k64512512/},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {A general framework for unification in
                  ``commutative'' theories is investigated, which is
                  based on a categorical reformulation of theory
                  unification. This yields algebraic characterizations
                  of unification type unitary (resp. finitary for
                  unification with constants). We thus obtain the
                  well-known results for abelian groups, abelian
                  monoids and idempotent abelian monoids as well as
                  some new results as corollaries to a general
                  theorem. In addition, it is shown that constant-free
                  unification problems in ``commutative'' theories are
                  either unitary or of unification type zero and we
                  give an example of a ``commutative'' theory of type
                  zero.}
}

@InProceedings{Baader-1990a,
  author = {Franz Baader},
  title = {Unification Theory},
  pages = {151--170},
  crossref = {IWWERT90},
  WKloc = {A-0154},
  abstract = {The purpose of this paper is not to give an overview
		  of the state of art in unification theory. It is
		  intended to be a short introductin into the area of
		  equational unification which should give the reader
		  a feeling for what unification theory might be
		  about. The basic notions such as complete and
		  minimal compete sets of unifiers, and unification
		  types of equational theories are introduced and
		  illustrated by examples. The we shall describe the
		  original motivations for considering unification (in
		  the empty theory) in resolution theorem proving and
		  term rewriting. Starting with Robinson's first
		  unification algorithm it will be sketched how more
		  efficient unification algorithms can be derived.

		  We shall then explain the reasons which lead to the
		  introduction of unification in non-empty theories
		  into the above mentioned areas theorem proving and
		  term rewriting. For theory unification it makes a
		  difference whether single equations or systems of
		  equations are considered. In addition, one has to be
		  careful with regard to the signature over which the
		  terms of the unification problems can be built. This
		  leads to the distinction between elementary
		  unification, unification with constants, and general
		  unification (where arbitrary free function symbols
		  may occur). Going from elementary unification to
		  general unification is an instance of the so-called
		  combination problem for equational theories which
		  can be formulated as follows: Let $E,F$ be
		  equational theories over disjoint signatures. How
		  can unification algorithms for $E,F$ be combined to
		  a unification algorithm for the theory $E\uni F$.},
  bibliographies = {RelMiCS}
}

@InProceedings{Baader-1990b,
  author = {Franz Baader},
  title = {Unification in Varieties of Completely Regular Semigroups},
  pages = {210--230},
  crossref = {IWWERT90},
  abstract = {All varieties of idempotent semigroups have been
		  classified with respect to the unification types of
		  their defining sets of identities. With the
		  exception of eight finitary unifying theories, they
		  are all of unification type zero. This yields
		  countably many examples of theories of this type
		  which are more ``natural'' than the first example
		  constructed by Fages and Huet.

		  The lattice of all varieties of idempotent
		  semigroups is a sublattice of the lattice of all
		  varieties of orthodox bands of groups, and this
		  lattice is a sublattice of the lattice of all
		  varieties of complex regular semigroups. The proof
		  which was used to establish the result for the
		  varieties of idempotent semigroups of type zero
		  can---with some modifications---also be applied to
		  the larger lattice of all varieties of complete
		  regular semigroups. This shows that type zero is not
		  an exception, but rather common for varieties of
		  semigroups.

		  To establish the results for the eight exceptional
		  finitary varieties of idempotent semigroups we have
		  developed a method which under certain conditions
		  allows to deduce the unification type of a join of
		  varieties from the types of the varieties
		  participating in this join. This method can also be
		  employed for varieties of orthodox bands of abelian
		  groups. Any variety of orthodox bands of abelian
		  groups is the join of a variety of idempotent
		  semigroups and a variety of abelian groups. It turns
		  out that the unification type of such a join is just
		  the type of the variety of idempotent semigroups
		  taking part in this join.

		  The emphasis of the paper is on describing the tools
		  necessary for proving all the mentioned results.},
  bibliographies = {RelMiCS}
}

@TechReport{Baader-Buerckert-Nebel-Nutt-Smolka-1991,
  abstract = {Feature logics are the logical basis for so-called
		  unification grammars studied in computational
		  linguistics.  We investigate the expressivity of
		  feature terms with negation and the functional
		  uncertainty construct needed for the description of
		  long-distance dependencies and obtain the following
		  results: satisfiability of feature terms is
		  undecidable, sort equations can be internalized,
		  consistency of sort equations is decidable if there
		  is at least one atom, and consistency of sort
		  equations is undecidable if there is no atom.},
  year = 1991,
  type = {Research Report},
  title = {On the Expressivity of Feature Logics with Negation,
                 Functional Uncertainty, and Sort Equations},
  number = {RR-91-01},
  month = JAN,
  institution = DFKI,
  author = {F. Baader and H.-J. B\"urckert and B. Nebel and W. Nutt and
                   G. Smolka},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Baader-Hollunder-1992,
  abstract = {In a recent paper we have proposed terminological
		  default logic as a formalism which combines both
		  means for structured representation of classes and
		  objects, and for default inheritance of properties.
		  The major drawback which terminological default
		  logic inherits from general default logic is that it
		  does not take precedence of more specific defaults
		  over more general ones into account. This behaviour
		  has already been criticized in the general context
		  of default logic, but it is all the more problematic
		  in the terminological case where the emphasis lies
		  on the hierarchical organization of concepts.

                  The present paper addresses the problem of modifying
		  terminological default logic such that more specific
		  defaults are preferred. It turns out that the
		  existing approaches for expressing priorities
		  between defaults do not seem to be appropriate for
		  this purpose. Therefore we shall consider an
		  alternative approach for dealing with prioritization
		  in the framework of Reiter's default logic. The
		  formalism is presented in the general setting of
		  default logic where priorities are given by an
		  arbitrary partial ordering on the defaults. We shall
		  exhibit some interesting properties of the new
		  formalism, compare it with existing approaches, and
		  describe an algorithm for computing extensions.},
  year = 1992,
  type = {{DFKI} Research Report},
  title = {How to Prefer More Specific Defaults in
                  Terminological Default Logic},
  number = {RR-92-58},
  note = {A short version will be published in the Proc.\null{}
                   of IJCAI'93},
  institution = DFKI,
  author = {F. Baader and B. Hollunder},
  address = {Kaisers\-lautern, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Baader-Hollunder-Nebel-Profitlich-Franconi-1993,
  abstract = {We consider different methods of optimizing the
		  classification process of terminological
		  representation systems, and evaluate their effect on
		  three different types of test data.  Though these
		  techniques can probably be found in many existing
		  systems, until now there has been no coherent
		  description of these techniques and their impact on
		  the performance of a system.  One goal of this paper
		  is to make such a description available for future
		  implementors of terminological systems.  Building
		  the optimizations that came off best into the KRIS
		  system greatly enhanced its efficiency.},
  year = 1993,
  type = {Research Report},
  title = {An Empirical Analysis of Optimization Techniques for
		  Terminological Representation Systems or: Making
		  {KRIS} get a move on},
  number = {RR-93-03},
  note = {A shorter version has been published in {Proc.\null{} KR'92}},
  month = JAN,
  institution = DFKI,
  author = {Franz Baader and Bernhard Hollunder and Bernhard
		  Nebel and Hans-J{\"u}rgen Profitlich and Enrico Franconi},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS}
}

@InProceedings{Baader-Leucker-1998,
  author = "F. Baader and M. Leucker",
  title = "Comparison of two Semantic Approaches to Unification",
  booktitle = "Proceedings of the 12th International Workshop on Unification",
  number = "SI-98/8",
  address = "Universita di Roma, La Sapienza",
  year = "1998",
  WKloc = {doc/pap/BIB},
  url = {http://citeseer.ist.psu.edu/baader98comparison.html}
}

@Book{Baader-Nipkow-1998,
  author = 	 {Baader, Franz and Nipkow, Tobias},
  title = 	 {Term rewriting and all that},
  publisher = 	 {Cambridge University Press},
  year = 	 {1998},
  McMaster = 	 {QA 267 .B314 1998},
  ISBN10 = 	 {0521455200},
  ISBN13 = 	 {9780521455206},
  OPTseries = 	 {http://www.loc.gov/catdir/toc/cam027/97028286.html},
  OPTaddress = 	 {http://www.loc.gov/catdir/description/cam028/97028286.html},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Article{ Baader-Nutt-1996,
    author = "Franz Baader and Werner Nutt",
    title = "Combination Problems for Commutative/Monoidal Theories: {H}ow Algebra Can Help in Equational Reasoning",
    journal = "Journal of Applicable Algebra in Engineering, Communication and Computing",
    volume = "7",
    number = "4",
    pages = "309--337",
    year = "1996",
    WKloc = {doc/pap/BIB},
    url = "citeseer.ist.psu.edu/nutt96combination.html"
}

@TechReport{Baader-Schulz-1991,
  abstract = {Most of the work on the combination of unification
		  algorithms for the union of disjoint equational
		  theories has been restricted to algorithms which
		  compute finite  complete sets of unifiers.  Thus the
		  developed combination methods usually cannot be used
		  to  combine decision procedures, i.e., algorithms
		  which just decide solvability of unification
		  problems without computing unifiers.  In this paper
		  we describe a combination algorithm for decision
		  procedures which works for arbitrary equational
		  theories, provided that solvability of so-called
		  unification problems with constant restrictions---a
		  slight generalization of unification problems with
		  constants---is decidable for these theories.  As a
		  consequence of this new method, we can for example
		  show that  general A-unifiability, i.e., solvability
		  of A-unification problems with free function
		  symbols, is decidable. Here A stands for the
		  equational theory of one associative function
		  symbol.

                  Our method can also be used to combine algorithms
		  which compute finite complete sets of unifiers.
		  Manfred Schmidt-Schauss's combination result, the
		  until now most general result in this direction, can
		  be obtained as a consequence of this fact.  We also
		  get the new result that unification in the union of
		  disjoint  equational theories is finitary, if
		  general unification---i.e., unification of terms
		  with  additional free function symbols---is finitary
		  in the single theories.},
  year = 1991,
  type = {Research Report},
  title = {Unification in the Union of Disjoint Equational Theories:
                  Combining Decision Procedures},
  number = {RR-91-33},
  month = NOV,
  institution = DFKI,
  author = {Baader, Franz and Klaus U. Schulz},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS}
}

@InCollection{Baader-Siekmann-1993,
  author = {F. Baader and J.H. Siekmann},
  title = {Unification Theory},
  booktitle = {Handbook of Logic in Artificial Intelligence
                     and Logic Programming},
  publisher = {Oxford University Press},
  editor = {D.M. Gabbay and C.J. Hogger and J.A. Robinson},
  year = 1993,
  address = {Oxford, UK},
  filename = {UnificationTheory.ps.Z},
  WKloc = {B-0035},
  abstract = {Most knowledge-based systems in artificial
		  intelligence (AI) with a commitment to a symbolic
		  representation support one important operation:
		  matching of descriptions. This operation, called
		  {\em unification} in the field of deduction systems,
		  is the addition and multiplication of many AI
		  systems, and is consequently often supported by
		  special purpose hardware or by a fast instruction
		  set.  Unification theory provides the formal
		  framework for investigations into the properties of
		  this operation, which is in essence the solving of
		  equations in an (equationally defined) free
		  algebra. This paper gives an introduction into
		  unification theory, and treats some of the important
		  research topics in this area in more detail.},
  bibliographies = {RelMiCS}
}

@InProceedings{ Baars-Swierstra-Viera-2009,
  author = {Arthur Baars and S. Doaitse Swierstra and Marcos Viera},
  title  = {Typed Transformations of Typed Abstract Syntax},
  booktitle = {TLDI '09: fourth ACM SIGPLAN Workshop on Types in Language Design and Implementation},
  year = {2009},
  location = {Savannah, Georgia, USA},
  publisher = {ACM},
  address = {New York, NY, USA},
  URL = {http://www.cs.uu.nl/wiki/bin/view/Center/TTTAS},
  WKloc = {doc/pap/BIB},
  abstract = {Advantages of embedded domain-specific languages (EDSLs) are
     that one does not have to implement a separate type system
     nor an abstraction mechanism,
     since these are directly borrowed from the host language.
     Straightforward implementations of embedded domain-specific languages
     map the semantics of the embedded language
     onto a function in the host language.
     The semantic mappings are usually compositional,
     i.e. they directly follow the syntax of the embedded language.

     One of the questions which arises is
     whether conventional compilation techniques,
     such as global analysis and resulting transformations,
     can be applied in the context of EDSLs.
     The approach we take is that,
     instead of mapping the embedded language directly onto a function,
     we first build a representation
     of the abstract syntax tree of the embedded program fragment.
     This syntax tree is subsequently analyzed and transformed,
     and finally mapped onto a function representing its denotational semantics.
     In this way we achieve run-time ``compilation'' of the embedded language.
     Run-time transformations on the embedded language can have
     a huge effect on performance.
     In previous work (Viera et al. 2008) we present a case study
     comparing the Read instances generated by Haskells deriving construct
     with instances on which run-time grammar transformations
     (precedence resolution, left-factorisation and left-corner transformation)
     have been applied.

     In this paper we present the library, which has an arrow like interface,
     which supports in the construction of analyses and transformations,
     and we demonstrate its use in implementing
     a common sub-expression elemination transformation.
     The library uses typed abstract syntax
     to represent fragments of embedded programs
     containing variables and binding structures,
     while preserving the idea
     that the type system of the host language is used
     to emulate the type system of the embedded language.
     The tricky issue is how to keep
     a collection of mutually recursive structures well-typed
     while it is being transformed.

     We finally discuss the typing rules of Haskell,
     its extensions and those as implemented by the GHC
     and show that pure System-F based systems are sufficiently rich
     to express what we want to express,
     albeit at the cost of an increased complexity of the code.}
}

@InProceedings{Baaz-Fermueller-Leitsch-1994,
  title = {A Non-Elementary Speed-Up in Proof Length by Structural Clause
      Form Transformation},
  author = {Matthias Baaz and Christian Ferm{\"u}ller and Alexander
      Leitsch},
  pages = {213--219},
  crossref = {LICS9},
  abstract = {The effects on minimal proof length of different types of
      translations of first-order formulas to clausal form are
      investigated. It is shown that there is a sequence of unsatisfiable
      formulas $\langle F_n \rangle$ such that the length of all
      refutations of non-structural clause forms of $F_n$ is non-elementary
      (in the size of $F_n$), whereas an the other hand there are
      refutations of structural clause forms of $F_n$ that are of
      elementary (at most triple exponential) length.}
}

@Misc{Babel-Olariu-1997,
  author = {Luitpold Babel and Stephan Olariu},
  title = {On the Structure of $P_4$-connected Graphs and the Separable-Homogeneous Decomposition},
  year = 1997,
  WKloc = {A-0441},
  note = {short version in WG '97}
}

@Misc{Babel-Olariu-1997a,
  author = {Luitpold Babel and Stephan Olariu},
  title = {On the Structure of Graphs with Few $P_4$s},
  year = 1997,
  WKloc = {A-0442}
}

@Misc{Babel-Olariu-1997b,
  author = {Luitpold Babel and Stephan Olariu},
  title = {A New Characterization of $P_4$-connected Graphs},
  year = 1997,
  WKloc = {A-0481}
}

@Misc{Baber-2001,
  author = {Robert L. Baber},
  title = {Mathematically Rigorous Software Development},
  howpublished = {McMaster University, Department of Computing and Software, Lecture Notes (draft)},
  year = 2001,
  WKloc = {A-1091}
}

@InProceedings{Bachmair-Ganzinger-1994,
  title = {Rewrite Techniques for Transitive Relations},
  author = {Leo Bachmair and Harald Ganzinger},
  pages = {384--393},
  crossref = {LICS9},
  WKloc = {A-0377},
  bibliographies = {RelMiCS},
  abstract = {We propose inference systems for dealing with transitive
		  relations in the contqext of resolution-type theorem
		  proving.  These inference mechanisms are based on
		  standard techniques from term rewriting and
		  represent a refinement of chaining methods.  We
		  establish their refutational completeness and also
		  prove their compatibility with the usual
		  simplification techniques used in rewrite-based
		  theorem provers.  A key to the practicality of
		  chaining techniques is the extent to which so-called
		  variable chainings can be restricted.  We
		  demonstrate that rewrite techniques considerably
		  restrict variable chaining, though we also show that
		  they cannot be completely avoided for transitive
		  relations in general.  If the given relation
		  satisfies additional properties, such as symmetry,
		  further restrictions are possible.  In particular,
		  we discuss (partial) equivalence relations and
		  congruence relations.}
}

@InProceedings{Bachmair-Ganzinger-1994a,
  author = {L. Bachmair and H. Ganzinger},
  title = {Associative-Commutative Superposition},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Max Planck Institut}
}

@InProceedings{Bachmair-Ganzinger-1998,
  author = {Leo Bachmair and Harald Ganzinger},
  title = {Strict Basic Superposition},
  crossref = {CADE1998},
  pages = {160--174},
  OPTabstract = {},
  WKloc = {A-0610 (with PDF faults)}
}

@InProceedings{Bachmair-Ganzinger-Voronkov-1998,
  author = {Leo Bachmair and Harald Ganzinger and Andrei Voronkov},
  title = {Elimination of Equality via Transformation with Ordering Constraints},
  crossref = {CADE1998},
  pages = {175--190},
  OPTabstract = {},
  WKloc = {A-0611 (with PDF faults)}
}

@Article{Back-1981,
  author = {R. J. R. Back},
  title = {On Correct Refinement of Programs},
  journal = JCOMSYS,
  year = 1981,
  volume = 23,
  number = 1,
  pages = {49--68},
  bibliographies = {RelMiCS}
}

@Misc{Back-199X,
  author = {Adam Back},
  title = {The Quest for the Most Diminutive Munitions Program},
  year = {199?},
  WKloc = {A-0774},
  note = {Web page describing a three-line perl/dc program implementing RSA encryption/decryption}
}

@InProceedings{Back-Sere-1994,
  author = {R.J.R. Back and K. Sere},
  title = {Action Systems with Synchronous Communication},
  crossref = {PROCOMET94},
  pages = {101--123},
  keywords = {Specifying and Verifying and Reasoning about
		  Programs; Concurrent Programming}
}

@Article{Back-vonWright-1992,
  author = {Ralph-Johan R. Back and von Wright, Joakim},
  title = {Combining Angels, Demons and Miracles in Program
                   Specifications},
  journal = TCS,
  year = 1992,
  volume = 100,
  pages = {365--383},
  bibliographies = {RelMiCS}
}

@Book{Back-vonWright-1998,
  author = {Ralph-Johan R. Back and von Wright, Joakim},
  title = 	 {Refinement Calculus, A Systematic Introduction},
  publisher = 	 Springer,
  year = 	 1998,
  series =	 {Graduate Texts in Computer Science},
  McMaster = 	 {QA 76.6 .B28 1998},
  bibliographies = {RelMiCS}
}

@Booklet{Backer-1970,
  author = {Fred Backer},
  title = {Representable Relation Algebras},
  note = {Report for a seminar on relation algebras conducted by
		A.\null{} Tarski, mimeographed, Spring, 1970},
  address = {Berkeley},
  year = 1970,
  bibliographies = {RelMiCS}
}

@PhDThesis{Backhouse-1975,
  author = {Roland C. Backhouse},
  title = {Closure algorithms and the star-height problem of regular languages},
  school = {University of London},
  year = {1975},
  bibliographies = {RelMiCS}
}

@Book{Backhouse-1979,
  author =       "Roland C. Backhouse",
  title =        "Syntax of Programming Languages, Theory and Practice",
  edition =      "1",
  publisher =    "Prentice Hall",
  address =      "Englewood Cliffs",
  year =         "1979",
  ISBN =         "0-13-879999-7",
  descriptor =   "Algol 60, Kontextfreie Grammatik, LL-Verfahren,
                 LR-Verfahren, Pascal, Regulaere Sprache,
                 Syntaxanalyse",
}

@Article{Backhouse-Carre-1982,
  author =       "R. C. Backhouse and B. A. Carr\'{e}",
  title =        "A comparison of {Gaussian} and {Gauss-Jordan}
                 elimination in regular algebra",
  journal =      "International J. of Computer Math.",
  volume =       "10",
  pages =        "311--325",
  year =         "1982",
}

@TechReport{Backhouse-1988,
  year = 1988,
  title = {An Exploration of the {Bird--Meertens} Formalism},
  type = {Computing Science Notes},
  number = {CS 8810},
  institution = GRON_DMC,
  author = {Roland C. Backhouse},
  bibliographies = {RelMiCS}
}

@Unpublished{Backhouse-1991,
  author = {Roland Backhouse},
  title = {Adjunctions in the Spec Calculus},
  note = {ftp://ftp.win.tue.nl/pub/math.prog.construction},
  year = 1991,
  month = AUG,
  WKloc = {A-0222},
  OPTabstract = {This paper records an initial exploration of the
		  notion of an adjunction in the spec calculus. Its
		  main contribution is probably to make much more
		  evident the truly {\em polymorphic} character of the
		  spec calculus as opposed to the {\em parametric}
		  nature of category theory.

                  There are still many gaps in this investigation
		  which hopefully will be filled in the
		  not-too-distant future.}
}

@Unpublished{Backhouse-1992a,
  author = {Backhouse, R.C.},
  title = {Calculating the {W}arshall/{F}loyd path algorithm},
  note = {Eindhoven University of Technology, Department of Computing
		  Science, Computer Science Note No. 92/09},
  month = {May},
  WKloc = {A-0137},
  abstract = {A calculational derivation is given of an all-pairs path
		algorithm two instances of which are Warshall's
		reachability algorithm and Floyd's shortest-path algorithm.
		The derivation provides an elementary example of the
		so-called star-decomposition rule.},
  year = 1992
}

@Unpublished{Backhouse-1992f,
  author = {Backhouse, R.C.},
  title = {Monad decomposition},
  note = {Available via anonymous ftp from
		  \verb+ftp.win.tue.nl+ in directory
		  \verb+pub/math.prog.construction+},
  crossref = {},
  year = 1992,
  month = {November},
  WKloc = {A-0140},
  abstract = {Chapters 7 and 8 of the 1992 STOP proceedings are
		  devoted to the study of closure operators. At the
		  end of chapter 8 a research programme was outlined,
		  a goal of which was to be the derivation of programs
		  on lists as by-products of calculations with the
		  Kleene star. This note makes a start on that
		  research programme by showing how calculations with
		  the closure star (discussed in chapter 7 of the STOP
		  proceedings) can be ``lifted'' to constructive
		  proofs about monads. In particular, the fundamental
		  closure-decomposition rule is ``lifted'' to a monad
		  decomposition theorem.}
}

@Unpublished{Backhouse-1993a,
  author = {Backhouse, Roland C.},
  title = {Constructive Lattice Theory},
  OPTaddress = {Eindhoven University of Technology, Department of Mathematics and Computing Science},
  OPTcrossref = {CRCS93},
  year = {1993},
  month = OCT,
  note = {\url{http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#isos}},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#isos},
  WKloc = {A-0141, doc/pap/BIB},
  abstract = {A notion of simulation of one datatype by another is defined
		as a {\em constructive} preorder. A calculus of datatype
		simulation is developed by formulating constructive versions
		of least fixed point theorems in lattice theory. The
		calculus is applied to the construction of several
		isomorphisms between classes of datatypes. In particular
		canstructive adaptations of theorems in regular algebra
		are shown to yield isomorphisms between list structures.}
}

@Unpublished{Backhouse-1993b,
  author = 	 {Roland C. Backhouse},
  title = 	 {Monad Decomposition},
  note = 	 {Available at \textsf{http://www.cs.nott.ac.uk/\~{}rcb/papers/abstract.html\#monad}},
  month = 	 APR,
  year = 	 1993,
  WKloc =        {doc/pap/BIB},
  abstract = 	 {This paper makes a start on a research programme the
                  goal of which is to reformulate theorems in category
                  theory as constructive theorems in lattice
                  theory. It is argued that monads should be
                  formulated as constructive closure operators. In
                  order to support this thesis it is shown how, by
                  adapting the relevant theorem in lattice theory,
                  every unary relator defines a monad. The minimality
                  of the constructed monad is captured by a monad
                  simulation theorem, which is then used to establish
                  a monad decomposition theorem by adapting proofs of
                  the closure decomposition theorem in lattice
                  theory.}
}

@Article{Backhouse-1998,
  author = {Roland C. Backhouse},
  title = {Pair Algebras and Galois Connections},
  journal = {Information Processing Letters},
  volume = 67,
  number = 4,
  month = AUG,
  year = 1998,
  pages = {169--176},
  WKloc = {A-0880},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/papers.html},
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-2000a,
  author = {Roland Backhouse},
  title = {Fixed Point Calculus},
  booktitle = {Summer School and Workshop on Algebraic and Coalgebraic
      Methods in the Mathematics of Program Construction, {Oxford, April
      11--14, 2000}},
  OPTcrossref = {},
  OPTpages = {},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html},
  year = 2000,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  abstract = {Fixed point calculus is about the solution of recursive
      equations defined by a monotonic endofunction on a partially ordered
      set. This tutorial discusses applications of fixed point calculus in
      the construction of computer programs, beginning with standard
      applications and progressing to recent research. The basic properties
      of least and greatest fixed points are presented. Well-foundedness
      and inductive properties of relations are expressed in terms of fixed
      points. A class of fixed point equations, called ``hylo'' equations,
      is introduced. A methodology of recursive program design based on the
      use of hylo equations is presented. Current research on
      generalisations of well-foundedness and inductive properties of
      relations, making these properties relative to a datatype, is
      introduced.},
  bibliographies = {RelMiCS}
}

@TechReport{Backhouse-Bijsterveld-1994,
  author =       "Roland Backhouse and Marcel Bijsterveld",
  title =        "Category Theory as Coherently Constructive Lattice
                 Theory: An Illustration",
  type =         "Research report",
  number =       "94-43",
  institution =  "Dept.\ of Maths.\ and Computing Science, Eindhoven
                 Univ.\ of Technology",
  month =        nov,
  year =         "1994",
  URL =          "ftp://ftp.win.tue.nl/pub/math.prog.construction/BeautifulTheorem.dvi.Z",
}

@Article{Backhouse-Carre-1975,
  author =       "R. C. Backhouse and B. A. Carr{\'e}",
  title =        "{Regular Algebra} applied to path-finding problems",
  journal =      "Journal of the Institute of Mathematics and its
                 Applications",
  volume =       "15",
  number =       "??",
  pages =        "161--186",
  year =         "1975",
  CODEN =        "JMTAA8",
  ISSN =         "0020-2932",
  MRclass =      "15A30 (68A10)",
  MRnumber =     "55 #372",
  mrreviewer =   "Claude Benzaken",
  bibdate =      "Fri Apr 5 07:16:18 MST 2002",
  acknowledgement = ack-nhfb,
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#Path-findingProblems},
  zmnumber =     "0304.68082",
  abstract = {In an earlier paper, one of the authors presented an
     algebra for formulating and solving extremal path problems. There
     are striking similarities between that algebra and the algebra of
     regular languages, which lead one to consider whether the
     previous results can be generalized---for instance to path
     enumeration problems---and whether the algebra of regular
     languages can itself be profitably used for the general study of
     path-finding problems. This paper gives affirmative answers to
     both these questions.}
}

@Unpublished{Backhouse-Doornbos-1993,
  author = {Backhouse, R.C. and Doornbos, H.},
  title = {Induction and recursion on datatypes},
  note = {Department of Computing Science, Eindhoven
		  University of Technology.
                  Available via anonymous ftp from
		  \verb+ftp.win.tue.nl+ in directory
		  \verb+pub/math.prog.construction+},
  year = 1993,
  WKloc = {A-0143}
}

@TechReport{Backhouse-Doornbos-1994,
  author = {Roland C. Backhouse and H. Doornbos},
  title = {Mathematical Induction Made Calculational},
  institution = {Eindhoven Univ.\null{} of Technology, Dept.\null{} of
                  Mathematics and Computing Science},
  year = 1994,
  type = {Computing Science Notes},
  number = {94/16},
  month = APR,
  bibliographies = {RelMiCS}
}

@Article{Backhouse-ETAL-1995,
  author = {Roland C. Backhouse and others},
  title = {Fixed-point Calculus},
  journal = IPLET,
  year = 1995,
  volume = 53,
  pages = {131--136},
  bibliographies = {RelMiCS}
}

@Unpublished{Backhouse-Fokkinga-2000,
  author = {Roland Backhouse and Maarten Fokkinga},
  title = {The Associativity of Equivalence and the Towers of Hanoi Problem},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#Hanoi},
  keywords = {propositional calculus, program proof},
  OPTmonth = {},
  year = 2000,
  abstract = {Dijsktra and Scholten have argued that greater use
              should be made of the associativity of equivalence.
              This note shows how the property is used
              in specifying the rotation of the disks
              in the well-known Towers of Hanoi problem.},
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-Hoogendijk-1992b,
  author = {Backhouse, R.C. and Hoogendijk, P. },
  title = {Elements of a Relational Theory of Datatypes},
  booktitle = {Formal Program Development. {Proc. IFIP TC2/WG 2.1 State of the Art Seminar, Rio de Janeiro, Jan.~1992}},
  editor = {B. M\"oller and H.A. Partsch and S.A. Schuman},
  volume = 755,
  series = LNCS,
  year = 1992,
  URL = {http://www.springerlink.com/content/p4m49mt88qj2240l/},
  DOI = {10.1007/3-540-57499-9_15},
  pages = {7--42},
  bibliographies = {RelMiCS},
  OPTnote = {To appear.
		Presented at IFIP Working Group 2.1
		state of the art summer school,
		Itacuru\c{c}\'{a} Island, Brazil, January 10-23, 1992},
  WKloc = {A-0145, doc/pap/BIB},
  abstract = {The ``Boom hierarchy'' is a hierarchy of types that
                  begins at the level of trees and includes lists,
                  bags and sets. This hierarchy forms the basis for
                  the calculus of total functions developed by Bird
                  and Meertens, and which has become known as the
                  ``Bird-Meertens formalism''.

                  This paper describes a
                  hierarchy of types that logically precedes the Boom
                  hierarchy. We show how the basic operators of the
                  Bird-Meertens formalism (map, reduce and filter) can
                  be introduced in a logical sequence by beginning
                  with a very simple structure and successively
                  refining that structure.

                  The context of this work
                  is a relational theory of datatypes, rather than a
                  calculus of total functions. Elements of the theory
                  necessary to the later discussion are summarised at
                  the beginning of the paper.}
}

@Article{Backhouse-Hoogendijk-1999,
  author = {Roland Backhouse and Paul Hoogendijk},
  title = {Final Dialgebras: From Categories to Allegories},
  journal = {Theoretical Informatics},
  year = 1999,
  volume = 33,
  number = {4/5},
  pages = {401--426},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html},
  abstract = {The study of inductive and coinductive types (like finite
      lists and streams, respectively) is usually conducted within the
      framework of category theory, which to all intents and purposes is a
      theory of sets and functions between sets. Allegory theory, an
      extension of category theory due to Freyd, is better suited to
      modelling relations between sets as opposed to functions between
      sets. The question thus arises of how to extend the standard
      categorical results on the existence of final objects in categories
      (for example, coalgebras and products) to their existence in
      allegories. The motivation is to streamline current work on generic
      programming, in which the use of a relational theory rather than a
      functional theory has proved to be desirable. In this paper, we
      define the notion of a relational final dialgebra and prove, for an
      important class of dialgebras, that a relational final dialgebra
      exists in an allegory if and only a final dialgebra exists in the
      underlying category of maps. Instances subsumed by the class we
      consider include coalgebras and products. An important lemma
      expresses bisimulations in allegorical terms and proves this
      equivalent to Aczel and Mendler's categorical definition.},
  WKloc = {A-0881},
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-Jansson-Jeuring-Meertens-1998,
  author = {Roland Backhouse and Patrik Jansson and Johan Jeuring and Lambert Meertens},
  title = {Generic Programming. An Introduction},
  crossref = {AFP1998},
  pages = {28--117},
  WKloc = {A-1295, doc/pap/BIB},
  bibliographies = {RelMiCS, RelMiS},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html#AFP-Portugal},
  abstract = {Lecture notes for an advanced course on functional programming focusing on generic (polytypic) programming.},
  contents = {\begin{itemize}
       \item Introduction.
       \item Algebras, Functors and Datatypes, PolyP
       \item Generic Unification
       \item From Functions to Relations
       \item Solutions to Exercises
       \item Bibliography
       \item Glossary of Symbols
       \item Index
       \end{itemize}}
}

@InProceedings{Backhouse-Michaelis-2003,
  author = 	 {Roland Backhouse and Diethard Michaelis},
  title = 	 {Fixed-Point Characterisation of Winning Strategies
              in Impartial Games},
  OPTcrossref =  {RelMiCS2003},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1491, doc/pap/BIB (both draft)},
  abstract = {We use fixed-point calculus to characterise winning
     strategies in impartial, two-person games. A byproduct is the
     fixed-point characterisation of winning, losing and stalemate
     positions. We expect the results to be most useful in teaching
     calculational reasoning about least and greates fixed points.}

}

@InProceedings{Backhouse-Voermans-vanDerWoude-1991,
  year = 1991,
  title = {A Relational Theory of Types},
  OPTnote = {email: {\tt wsinrcb\@win.tue.nl}},
  booktitle = {Proc.\null{} EURICS Workshop on Calculational Theories of
      Program Structures},
  author = {Roland C. Backhouse and Ed Voermans and Jaap van der Woude},
  bibliographies = {RelMiCS}
}

@TechReport{Backhouse-deBruin-Hoogendijk-Malcolm-Voermans-vanDerWoude-1991,
  author = {Roland C. Backhouse and Peter J. de Bruin and Paul Hoogendijk
      and Grant Malcolm and Ed Voermans and Jaap van der Woude},
  title = {Polynomial Relators},
  type = {Computing Science Notes},
  number = {91/10},
  institution = EIND_MC,
  year = 1991,
  month = MAY,
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-deBruin-Hoogendijk-Malcolm-Voermans-vanDerWoude-1991b,
  author = {Backhouse, R.C. and   Peter J. de Bruin and Hoogendijk, P. and Malcolm, G. and Voermans, T.S. and  Woude, J. van der},
  title = {Polynomial Relators},
  crossref = {AMAST1991},
  pages = {303--362},
  bibliographies = {RelMiCS}
}

@Unpublished{Backhouse-deBruin-Malcolm-Voermans-vanDerWoude-1990,
  year = 1990,
  title = {A Relational Theory of Types},
  note = {Dept.\null{} of Computing Science, Rijksuniversiteit Groningen,
	and Dept.\null{} of Math.\null{} and Computing Science,
	Technische Univ.\null{} Eindhoven},
  month = MAY,
  author = {Roland C. Backhouse and Peter de Bruin and Grant Malcolm and
      Ed Voermans and Jaap van der Woude},
  annote = {As it says, a relational theory of types. Built on three
      layers, a complete distributive lattice (sets), a monoid
      (composition) and a `reverse structure' (inverses). Types (called
      `monotypes' here) are a subset of the identity. A thorough
      exploration of which desirable properties (it turns out, most) from
      functions carry over to relations.},
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-deBruin-Malcolm-Voermans-vanDerWoude-1991,
  title = {Relational Catamorphisms},
  pages = {319--371},
  crossref = {IFIP1991},
  author = {Roland C. Backhouse and Peter J. de Bruin and
		  Grant Malcom and Ed Voermans and Jaap van der Woude},
  bibliographies = {RelMiCS}
}

@InProceedings{Backhouse-vanGasteren-1992a,
  author = {Backhouse, Roland and A.J.M. van Gasteren},
  title = {Calculating a path algorithm},
  crossref = {MPC1992},
  pages = {32--44},
  WKloc = {A-0139},
  abstract = {A calculational derivation is given of an abstract path
		algorithm, one instance of the algorithm being Dijkstra's
		shortest-path algorithm, another being
		breadth-first/depth-first search of a directed graph.
		The basis for the derivation is the algebra of regular
		languages.}
}

@InProceedings{Backhouse-vanGasteren-1992a-x,
  author = {Backhouse, Roland and A.J.M. van Gasteren},
  title = {Calculating a path algorithm},
  editor = {Bird, R.S. and Morgan, C.C. and Woodcock, J.C.P.},
  booktitle = {Mathematics of Program Construction.  2nd International Conference, June/July 1992},
  publisher = {Springer Verlag},
  series = {Lecture Notes in Computer Science},
  volume = 669,
  pages = {32--44},
  year = 1993,
  WKloc = {A-0139},
  abstract = {A calculational derivation is given of an abstract path
		algorithm, one instance of the algorithm being Dijkstra's
		shortest-path algorithm, another being
		breadth-first/depth-first search of a directed graph.
		The basis for the derivation is the algebra of regular
		languages.}
}

@Unpublished{Backhouse-vanderWoude-1992d,
  author = {Backhouse, R.C. and Woude, J. van der},
  title = {Demonic operators and Monotype Factors},
  note = {Eindhoven University of Technology, Department of Computing
		  Science, Computing Science Note No. 92/11,
                 (To appear in Mathematical Structures in Computer Science)},
  month = {May},
  year = 1992,
  WKloc = {A-0146},
  abstract = {This paper tackles the problem of constructing a compact,
		point-free proof of the associativity of demonic composition
		of binary relations and its distributivity through demonic
		choice. In order to achieve this goal a definition of
		demonic composition is proposed in which angelic composition
		is restricted by means of a so-called ``monotype factor''.
		Monotype factors are characterised by a Galois connection
		similar to the Galois connection between composition
		and factorisation of binary relations. The identification
		of such a connection is argued to be highly conducive
		to the desired compactness of calculation.}
}

@Article{Backhouse-vanderWoude-1993,
  author = {Roland C. Backhouse and van der Woude, Jaap},
  title = {Demonic Operators and Monotype Factors},
  journal = MSCS,
  volume = 3,
  number = 4,
  month = DEC,
  year = 1993,
  pages = {417--433},
  OPTnote = {Also:  Computing Science Note 92/11, Dept.\null{}of Mathematics and
    Computer Science, Eindhoven Univ.\null{}of Technology, 1992},
  WKloc = {A-0955, doc/pap/BIB},
  URL = {http://www.cs.nott.ac.uk/\~rcb/papers/abstract.html\#demon},
  bibliographies = {RelMiCS}
}

@Unpublished{Backhouse-vanDerWoude-1993a,
  author = {Roland Backhouse and van der Woude, Jaap},
  title = {Domain Operators and Domain Kinds},
  note = {Available at \textsf{http://www.cs.nott.ac.uk/\~rcb/papers/abstract.html\#perorder}},
  month = SEP,
  year = 1993,
  WKloc = {A-0220},
  abstract = {The notions of domain operator and domain kind are
		  introduced. Several examples are presented. In
		  particular, it is shown that the partial equivalence
		  relations form a domain kind. The proof involves the
		  construction of a Galois connection demonstrating
		  that the partial equivalence relations form a
		  complete lattice under the so-called domain ordering,
		  thus providing another illustration of the
		  importance of the early recognition of Galois connections.},
  bibliographies = {RelMiCS}
}

@TechReport{Backofen-Smolka-1992,
  abstract = {Various feature descriptions are being employed in
		  logic programming languages and constrained-based
		  grammar formalisms.  The common notational primitive
		  of these descriptions are functional attributes
		  called features. The descriptions considered in this
		  paper are the possibly quantified first-order
		  formulae obtained from a signature of binary and
		  unary predicates called features and sorts,
		  respectively.  We establish a first-order theory FT
		  by means of three axiom schemes, show its
		  completeness, and construct three elementarily
		  equivalent models.

                  One of the models consists of so-called feature
		  graphs, a data structure common in computational
		  linguistics.  The other two models consist of
		  so-called feature trees, a record-like data
		  structure generalizing the trees corresponding to
		  first-order terms.

                  Our completeness proof exhibits a terminating
		  simplification system deciding validity and
		  satisfiability of possibly quantified feature
		  descriptions.},
  year = 1992,
  type = {Research Report},
  title = {A complete and recursive feature theory},
  number = {RR-92-30},
  month = SEP,
  institution = DFKI,
  author = {Rolf Backofen and Gert Smolka},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  bibliographies = {RelMiCS}
}

@Article{Backus-1978,
  author = {Backus, John},
  title = {{Can Programming Be Liberated from the {von Neumann Style}?  A
		  Functional Style and Its Algebra of Programs}},
  journal = CACM,
  month = AUG,
  number = 8,
  volume = 21,
  year = 1978,
  pages = {613--641},
  WKloc = {A-0264},
  bibliographies = {FP}
}

@Book{Bacon-1992,
  author = {Jean Bacon},
  title = {Concurrent Systems: An Integrated Approach to Operating Systems, Database, and Distributed Systems},
  publisher = {Addison-Wesley},
  year = 1992,
  McMaster = {QA 76.58 .B33 1993},
  bibliographies = {SE3B},
  contents = {\begin{tabular}{l@{\ }l@{\hskip1em}l}
    1 & Introduction: Examples and requirements & 1 \\
    Part I & Background and Fundamentals & 33\\
    2 & System structure and dynamic execution & 35 \\
    3 & Operating Systems: Device and communications management & 53 \\
    4 & Operating Systems: Memory management & 89 \\
    5 & Operating Systems: File management & 112 \\
    6 & Operating Systems: Process management & 136 \\
    7 Language system support for concurrency & 161 \\
    Part II & Single Concurrent Actions 187 \\
    8 & System structure & 191 \\
    9 & Low-level mechanisms for process synchronization & 210 \\
    10 & Language primitives for shared memory & 249 \\
    11 & IPC and system structure & 268 \\
    12 & IPC without shared memory & 281 \\
    13 & Crash resilience and persistent data & 306 \\
    14 & Distributed IPC & 319 \\
    Part III & Concurrent Composite Actions & 349 \\
    15 & Decomposable abstract operations & 351 \\
    16 & Resource allocation and deadlock & 363 \\
    17 & Transactions & 380 \\
    18 & Concurrecy control & 401 \\
    19 & Recovery & 424 \\
    20 & Distributed transactions & 437 \\
    Part IV & Case Studies 457 \\
    21 & UNIX & 459 \\
    22 & Microkernels & 505 \\
    23 & Transaction processing monitors and systems & 530 \\
    24 & Summary and conclusions \\
    Appendix: Two case studies with exercises & 563 \\
    A.1 & $N$-process mutual exclusion for shared memory and distributed systems & 563 \\
    A.2 & Management of a disk block cache & 575 \\
    {} & Bibliography & 583
    \end{tabular}}
}

@Article{Bacon-Graham-Sharp-1994,
  author = {David F. Bacon and Susan L. Graham and Oliver J. Sharp},
  title = {Compiler Transformations for High-Performance Computing},
  journal = {ACM Computing Surveys},
  year = 1994,
  volume = 26,
  number = 4,
  month = DEC,
  pages = {345--420},
  abstract = {$\ldots$ This survey is a comprehensive overview of
		  the important high-level program restructuring
		  techniques for imparative languages such as C and
		  Fortran. $\ldots$}
}

@Book{Badesa-2004,
  author = 	 {Calixto Badesa},
  title = 	 {The Birth of Model Theory},
  publisher = 	 {Princeton University Press},
  year = 	 2004,
  WKloc = 	 {Chapter 1: Algebra of Classes and Propositional Calculus: Historical remarks about Boole and Schr\"oder: A-1693, doc/pap/BIB}
}

@InProceedings{Baelde-Gacek-Miller-Nadathur-Tiu-2006,
  author = 	 {David Baelde and Andrew Gacek and Dale Miller and Gopalan Nadathur and Alwen Tiu},
  title = 	 {A User Guide tp {Bedwyr}},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {2006},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1671},
  abstract = 	 {Some recent theoretical work in proof search has illustrated
      that it is possible to combine the following two computational principles
      into one computational logic:
      \begin{enumerate}
      \item A symmetric tratment of finite success and finite failure.
        This allows capturing both
        aspects of may and must behaviour in operational semantics
        and mixing model checking and logic programming.
      \item Direct support for $\lambda$-tree syntax, as in $\lambda$Prolog,
        via term-level $\lambda$-binders, higher-order pattern unification,
        and the $\Nabla$-quantifier.
      \end{enumerate}
      All these features have a clean proof theory.
      The combination of these features allows, for example,
      specifying rather declarative approaches
      to model checking syntactic expressions containing bindings.
      The Bedwyr system is inteded as an implementation
      of these computational logic principles.}
}

@Article{Baeten-Bergstra-Klop-Weijland-1989,
  TUBIBMUE = 160532,
  abstract = {In this paper we discuss term-rewriting systems with
               rule priorities, which simply is a partial ordering on
               the rules. The procedural meaning of such an ordering
               then is, that the application of a rule of lower priority
               is allowed only if no rule of higher priority is
               applicable. The semantics of such a system is discussed.
               It turns out that the class of all bounded systems
               indeed has such a semantics},
  year = 1989,
  volume = 67,
  title = {Term-Rewriting Systems with Rule Priorities},
  pages = {283--302},
  number = 3,
  journal = TCS,
  author = {Baeten, J.C.M. and Bergstra, J.A. and Klop, J.W. and
		  Weijland, W.P.},
  bibliographies = {RelMiCS}
}

@Book{Baeten-Weijland-1990,
  author = {Baeten, J.C.M. and Weijland, W.P.},
  publisher = CambridgeUP,
  series = {Tracts in Theoretical Computer Science},
  title = {Process Algebra},
  volume = 18,
  year = 1990,
  bibliographies = {RelMiCS}
}

@MastersThesis{Baetz-2002,
  author = {Bradley Baetz},
  title = {Nonpareil -- A Strongly Typed Object Oriented Functional Programming Language},
  school = {University of Sydney},
  year = 2002,
  type = {Honours Thesis},
  note = {supervised by Jeff Kingston},
  keywords = {Lout},
  WKloc = {A-1423}
}

@Misc{Baez-Lauda-2003,
  author =	 {John C. Baez and Aaron D. Lauda},
  title =	 {Higher-Dimensional Algebra {V}: 2-Groups},
  month =	 JUL,
  year =	 2003,
  WKloc = 	 {A-1509}
}

@InProceedings{BaezaYates-1994,
  author = {Ricardo A. Baeza-Yates},
  title = {Analysis of Bounded Disorder},
  crossref = {MFCS94},
  pages = {233--244},
  abstract = {In 1988 Ramakrishna and Mukhopadhyay presented an
		  exact analysis for a data node in the Bounded
		  Disorder (BD) file organization of Litwin and
		  Lomet. Here, we complete their analysis by
		  introducing the B-tree index into the model. Also,
		  we extend the analysis to the case of BD files with
		  two partial expansions as proposed by Lomet. Our
		  main contribution is a detailed analysis of search
		  and insrteion costs, and its comparison with $B^+$-trees.}
}

@Book{BaezaYates-RibeiroNeto-1999,
  author = {Ricardo Baeza-Yates and Berthier Ribeiro-Neto},
  title = {Modern Information Retrieval},
  publisher = Addison,
  year = 1999,
  UniBwM = {KOM120/YF3556},
  series = {acm press}
}

@TechReport{Bagwell-2002,
  author = 	 {Phil Bagwell},
  title = 	 {Fast Functional Lists, Hash-Lists, Deques and Variable Length Arrays},
  institution =  {IC},
  year = 	 2002,
  number =	 200244,
  WKloc = 	 {A-1610, doc/pap/BIB}
}

@Book{Bailey-1990,
  author = {Roger Bailey},
  title = {Functional Programming with {Hope}},
  publisher = {Ellis Horwood},
  year = 1990,
  series = {Ellis Horwood Series in Computers and their Applications},
  address = {Chichester, UK},
  McMaster = {QA 76.72 .B35 1990},
  ISBN = {0-13-338237-0},
  WKloc = {A-1372 (single-sided loose), B-0130},
  bibliographies = {FP, SE3E}
}

@Misc{Bailey-1992,
  WKloc = {A-0031, PDTools},
  abstract = {The Incremental Equational Logic Programming (ielp)
		  system is an incremental compiler for an equational
		  logic language. Ielp eccepts equations entered
		  either interactively or loaded from a file, and
		  lazily computes values of queries, or {\em subject
		  terms}, using the compiled equations.},
  year = 1992,
  title = {Ielp User Guide},
  month = FEB,
  howpublished = {Preliminary System Documentation},
  author = {Stephen W. Bailey}
}

@MastersThesis{Bailey-1993,
  author =       {Anthony Bailey},
  title =        {Representing algebra in {LEGO}},
  school =       {University of Edinburgh},
  year =         1993,
  type =      {Masters dissertation},
  month =     NOV
}

@Article{Bainbridge-1976,
  author = {E. S. Bainbridge},
  title = {Feedback and Generalized Logic},
  journal = {Information and Control},
  year = 1976,
  volume = 31,
  pages = {75--96},
  UniBwM = {INF/Z},
  WKloc = {A-0808},
  bibliographies = {RelMiCS},
  abstract = {Although the distinction between software and hardware is a
      posteriori, there is an a priori distinction that masquerades as the
      software-hardware distinction. This is the distinction between
      procedure interconnection, the semantics of flow chart diagrams,
      which is known to be described by the regular expression calculus;
      and system interconnection, the semantics of network diagrams, which
      is described by a certain logical calculus, dual to a calculus of
      regular expressions. This paper presents a proof of the duality in a
      special case, and gives the interpretation of the logical calculus
      for sequential machine interconnection. A minimal realization theorem
      for feedback systems is proved, which specializes to known open loop
      minimal realization theorems.}
}

@InProceedings{Baker-Ireland-Smaill-1992,
  author = {S. Baker and A. Ireland and A. Smaill},
  title = {On the Use of the Constructive Omega-Rule within
		  Automated Deduction},
  crossref = {LPAR92},
  pages = {214--225},
  WKloc = {A-0255},
  abstract = {In general, cut elimination holds for arithmetical
		  systems with the $\omega$-rule, but not for systems
		  with ordinary induction. Hence in the latter, there
		  is the problem of generalisation, since arbitrary
		  formulae can be cut in. This makes automatic
		  theorem-proving very difficult. An important
		  technique for investigating derivability in formal
		  systems of arithmetic has been to embed such systems
		  into semi-formal systems with the
		  $\omega$-rule. This paper describes the outline of
		  an implementation of such a system. Moreover, an
		  important application is presented in the form of a
		  new method of generalisation by means of ``guiding
		  proofs'' in the stronger system, which sometimes
		  succeeds in producing proofs in the original system
		  when other methods fail.}
}

@Book{Baker-Smith-1996,
  author = {Lou Baker and Bradley J. Smith},
  title = {Parallel Programming},
  publisher = {McGraw-Hill},
  year = 1996,
  McMaster = {QA 76.642 .B35 1996}
}

@PhdThesis{Bakewell-2001,
  author = 	 {Adam Bakewell},
  title = 	 {An Operational Theory of Relative Space Efficiency},
  school = 	 {University of York, Department of Computer Science},
  year = 	 2001,
  month =	 DEC,
  WKloc = 	 {A-1585, doc/pap/BIB}
}

@InProceedings{Balasubramaniam-Pierce-1998,
  author = {S. Balasubramaniam and Benjamin C. Pierce},
  title = {What is a File Synchronizer?},
  booktitle = {MobiCom},
  year = 1998,
  WKloc = {A-1252, doc/pap/BIB},
  URL = {http://www.cis.upenn.edu/~bcpierce/papers/snc-mobicom.ps},
  keywords = {Unison}
}

@Book{Balbes-Dwinger-1974,
  author = {R. Balbes and P. Dwinger},
  title = {Distributive Lattices},
  publisher = {University of Missouri Press},
  year = 1974,
  bibliographies = {RelMiCS}
}

@Article{Baldan-Corradini-Ehrig-Heckel-2005,
  author =       {Baldan, Paolo and Corradini, Andrea and Ehrig, Hartmut and Heckel, Reiko},
  title =        {Compositional semantics for open {Petri} nets based on deterministic processes},
  journal =      MSCS,
  year =         2005,
  volume =    15,
  number =    1,
  pages =     {1--35},
  DOI =    {10.1017/S0960129504004311},
  DOIURL = {http://dx.doi.org/10.1017/S0960129504004311},
  abstract = {In order to model the behaviour of open concurrent systems
    by means of Petri nets, we introduce open Petri nets,
     a generalisation of the ordinary model where some places,
     designated as open,
     represent an interface between the system and the environment.
     Besides generalising the token game to reflect this extension,
     we define a truly concurrent semantics for open nets
     by extending the Goltz–Reisig process semantics of Petri nets.
     We introduce a composition operation over open nets,
     characterised as a pushout in the corresponding category,
     suitable for modelling both interaction through open places
     and synchronisation of transitions.
     The deterministic process semantics is shown to be
     compositional with respect to such a composition operation.
     If a net $Z_3$ results as the composition of two nets $Z_1$ and $Z_2$,
     having a common subnet $Z_0$,
     then any two deterministic processes of $Z_1$ and $Z_2$
     that ‘agree’ on the common part,
     can be ‘amalgamated’ to produce a deterministic process of $Z_3$.
     Conversely, any deterministic process of $Z_3$
     can be decomposed into processes of the component nets.
     The amalgamation and decomposition operations
     are shown to be inverse to each other,
     leading to a bijective correspondence
     between the deterministic processes of $Z_3$
     and the pair of deterministic processes of $Z_1$ and $Z_2$
     that agree on the common subnet $Z_0$.
     Technically, our result is similar to the
     amalgamation theorem for data-types
     in the framework of algebraic specification.
     A possible application field of the proposed constructions and results
     is the modelling of interorganisational workflows,
     recently studied in the literature. This is illustrated by a running example.}
}

@incollection{Baldan-Corradini-Ehrig-Heckel-Koenig-2007,
  author={Baldan, Paolo and Corradini, Andrea and Ehrig, Hartmut and Heckel, Reiko and K{\"o}nig, Barbara},
  title={Bisimilarity and Behaviour-Preserving Reconfigurations of Open {Petri} Nets},
  year={2007},
  isbn={978-3-540-73857-2},
  booktitle={Algebra and Coalgebra in Computer Science},
  volume={4624},
  series=LNCS,
  editor={Mossakowski, Till and Montanari, Ugo and Haveraaen, Magne},
  DOI={10.1007/978-3-540-73859-6_9},
  DOIURL={http://dx.doi.org/10.1007/978-3-540-73859-6_9},
  publisher={Springer Berlin Heidelberg},
  pages={126--142},
  abstract = {We propose a framework for the specification of
                  behaviour-preserving reconfigurations of systems
                  modelled as Petri nets. The framework is based on
                  open nets, a mild generalisation of ordinary Place/
                  Transition nets suited to model open systems which
                  might interact with the surrounding environment and
                  endowed with a colimit-based composition
                  operation. We show that natural notions of (strong
                  and weak) bisimilarity over open nets are
                  congruences with respect to the composition
                  operation. We also provide an up-to technique for
                  facilitating bisimilarity proofs. The theory is used
                  to identify suitable classes of reconfiguration
                  rules (in the double-pushout approach to rewriting)
                  whose application preserves the observational
                  semantics of the net.}
}

@InProceedings{Baldan-Corradini-Montanari-Ribero-2002,
  author = {Paolo Baldan and Andrea Corradini and Ugo Montanari and Leila Ribeiro},
  title = 	 {Coreflective Concurrent Semantics for Single-Pushout Graph Grammars},
  crossref =	 {WADT2002},
  pages =	 {165--184},
  WKloc = 	 {doc/pap/BIB},
  abstract = {The problem of extending to graph grammars the
     unfolding semantics originally developed by Winskel for (safe)
     Petri nets has been faced several times along the years, both for
     the single-pushout and double-pushout approaches, but only
     partial results were obtained. In this paper we fully extend
     Winskels approach to single-pushout grammars providing them with
     a categorical concurrent semantics expressed as a coreflection
     between the category of graph grammars and the category of prime
     algebraic domains.}
}

@article{Baldan-Corradini-Montanari-2005,
  author    = {Paolo Baldan and Andrea Corradini and Ugo Montanari},
  title     = {Relating {SPO} and {DPO} graph rewriting with {Petri} nets having
               read, inhibitor and reset arcs},
  journal   = ENTCS,
  volume    = {127},
  number    = {2},
  year      = {2005},
  pages     = {5--28},
  DOIURL    = {http://dx.doi.org/10.1016/j.entcs.2005.02.003},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@incollection{Baldan-Gadducci-2008,
  author={Baldan, Paolo and Gadducci, Fabio},
  title={Petri Nets Are Dioids},
  year={2008},
  isbn={978-3-540-79979-5},
  booktitle={Algebraic Methodology and Software Technology},
  volume={5140},
  series={Lecture Notes in Computer Science},
  editor={Meseguer, José and Roşu, Grigore},
  DOI={10.1007/978-3-540-79980-1_5},
  DOIURL={http://dx.doi.org/10.1007/978-3-540-79980-1_5},
  publisher={Springer Berlin Heidelberg},
  pages={51--66},
  WKloc = {A-1761, doc/pap/BIB}
}

@InCollection{Baldan-Gadducci-Sobocinski-2011,
  year={2011},
  isbn={978-3-642-22992-3},
  booktitle={Mathematical Foundations of Computer Science 2011},
  volume={6907},
  series=LNCS,
  editor={Murlak, Filip and Sankowski, Piotr},
  doi={10.1007/978-3-642-22993-0_8},
  title={Adhesivity Is Not Enough: Local Church-Rosser Revisited},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-22993-0_8},
  publisher={Springer Berlin Heidelberg},
  keywords={Adhesive and extensive categories; double-pushout rewriting; local Church-Rosser property; parallel and sequential independence},
  author={Baldan, Paolo and Gadducci, Fabio and Soboci{\'n}ski, Pawel},
  pages={48--59},
  abstract = {Adhesive categories provide an abstract setting
    for the double-pushout approach to rewriting,
    generalising classical approaches to graph transformation.
    Fundamental results about parallelism and confluence,
    including the local Church-Rosser theorem,
    can be proven in adhesive categories,
    provided that one restricts to linear rules.
    We identify a class of categories,
    including most adhesive categories used in rewriting,
    where those same results can be proven
    in the presence of rules that are merely left-linear,
    i.e., rules which can merge different parts of a rewritten object.
    Such rules naturally emerge, e.g., when using graphical encodings
    for modelling the operational semantics of process calculi.}
}

@PhDThesis{Ballarin-1999,
  author = {Clemens Ballarin},
  title = {Computer Algebra and Theorem Proving},
  school = {University of Cambridge},
  year = 1999,
  WKloc = {A-1449},
  bibliographies = {MathScheme}
}

@InProceedings{Ballarin-2003,
  author = 	 {Clemens Ballarin},
  title = 	 {Locales and Locale Expressions in {Isabelle/Isar}},
  crossref =  {TYPES2003},
  OPTpages = 	 {},
  WKloc = {A-1534, doc/pap/BIB},
  abstract = {Locales provide a module system for the Isabelle
     proof assistant. Recently, locales have been ported to the new
     Isar format for structured proofs. At the same time, they have
     been extended by locale expressions, a language for composing
     locale specifications, and by structures, which provide syntax
     for algebraic structures. The present paper presents both and is
     suitable as a tutorial to locales in Isar, because it covers both
     basics and recent extensions, and contains many examples.}
}

@TechReport{Ballarin-2007,
  author = 	 {Clemens Ballarin},
  title = 	 {Tutorial to Locales and Locale Interpretation},
  institution =  {Facult\"at f\"ur Informatik, Technische Universit\"at M\"unchen},
  year = 	 2007,
  number =	 {TUM-I0723},
  bibliographies = {RelMiCS},
  WKloc = 	 {A-1696, doc/pap/BIB},
  abstract = {Locales are Isabelle's mechanism to deal with
      parametric theories.  We present typical examples of locale
      specifications, along with inter- pretations between locales to change
      their hierarchic dependencies and interpretations to reuse locales in
      theory contexts and proofs.

      This tutorial is intended for locale novices; familiarity with Isabelle
      and Isar is presumed.}
}

@InProceedings{Balser-Reif-Schellhorn-Stenzel-1998,
  author = {M. Balser and W. Reif and G. Schellhorn and K. Stenzel},
  title = {{KIV 3.0 for Provably Correct Systems}},
  booktitle = {Current Trends in Applied Formal Methods},
  year = 1999,
  series = LNCS,
  publisher = Springer,
  volume = 1641,
  conferenceAddress = {Boppard, Germany},
  bibliographies = {SpecTech},
  WKloc = {A-1063}
}

@Misc{ Balteanu-Fiedorowicz-Schwaenzl-Vogt-,
  author = "C. Balteanu and Z. Fiedorowicz and R. Schw{\"a}nzl and R. Vogt",
  title = "Iterated Monoidal Categories II",
  url = "citeseer.ist.psu.edu/219592.html"
}

@Book{Bamford-,
  author = {James Bamford},
  title = {Body of Secrets: Anatomy of the Ultrasecret National Security Agency, From the Cold War Through the Dawn of a New Century},
  publisher = {},
  year = {},
  keywords = {NSA}
}

@InProceedings{Banach-1989,
  author = {Richard Banach},
  title = {Dataflow Analysis of Term Graph Rewriting Systems},
  OPTcrossref = {PARLE89b},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTpages = {55--73},
  OPTbooktitle = {},
  OPTyear = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InCollection{Banach-1993,
  contents = {1 Introduction
                   2 DACTL abstracted
                   3 Graph rewriting as Grothendieck opfibration
                   4 True DACTL rewriting
                   5 Conclusions
                   References},
  abstract = {Section 7.2 describes the free rewriting core of
		  the original DACTL model. 7.3 describes the
		  categorical construction and how it yields a
		  Grothendieck opfibration. Surprisingly, it turns out
		  that the garbage retention feature of DACTL is the
		  key to the success of the construction. Section 7.4
		  reconsiders true DACTL rewriting and outlines the
		  universal construction that describes it, including
		  the circular example.},
  title = {A Fibration Semantics for Extended Term Graph Rewriting},
  pages = {91--100},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  chapter = 7,
  author = {Richard Banach},
  annote = {{ --- PLGnotes ---}}
}

@InCollection{Banach-1993-x,
  contents = {1 Introduction
                   2 DACTL abstracted
                   3 Graph rewriting as Grothendieck opfibration
                   4 True DACTL rewriting
                   5 Conclusions
                   References},
  abstract = {Section 7.2 describes the free rewriting core of
		  the original DACTL model. 7.3 describes the
		  categorical constructionand how it yields a
		  Grothendieck opfibration. Surprisingly, it turns out
		  that the garbage retention feature of DACTL is the
		  key to the success of the construction. Section 7.4
		  reconsiders true DACTL rewriting and outlines the
		  universal construction that describes it, including
		  the circular example.},
  title = {A Fibration Semantics for Extended Term Graph Rewriting},
  pages = {91--100},
  chapter = 7,
  author = {Richard Banach},
  annote = {{ --- PLGnotes ---}},
  editor = {M.R. Sleep and M.J. Plasmeijer and van Eekelen, M.C.J.D.},
  booktitle = {Term Graph Rewriting: Theory and Practice},
  publisher = {John Wiley},
  year = 1993
}

@Article{Banach-1993a,
  author = {Richard Banach},
  title = {Weak fibrations},
  journal = {Journal of pure and applied algebra},
  year = 1993,
  volume = 86,
  number = 1,
  pages = {7--22},
  month = APR,
  WKloc = {A-0680},
  abstract = {MR 94c:18015 (18F99)\\
                  There is a well-known correspondence between split
		  (op)fibrations $P:{\cal H} \rightarrow {\cal B}$ and
		  the Grothendiek construction for functors $F:{\cal
		  B} \rightarrow {\rm Cat}$. $P$ induces a functor
		  $F:{\cal B} \rightarrow {\rm Cat}$ and when the
		  Grothendiek construction is applied to $F$, it
		  yields a category ${\cal G}({\cal B},F)$ and a split
		  (op)fibration ${\cal G}({\cal B},F) \rightarrow
		  {\cal B}$ such that ${\cal H} \equiv {\cal G}({\cal
		  B},F)$ as fibrations \cite{Barr-Wells-1990}.

                  In this article, the author considers the situation
		  where the image of $P:{\cal H} \rightarrow {\cal B}$
		  is not necessarily a subcategory of ${\cal B}$. This
		  leads to a theory of weak fibrations and the main
		  result that $P:{\cal H} \rightarrow {\cal B}$ is a
		  split weak opfibration iff there exists a category
		  ${\cal M}$, and functors
                  $P_{\cal M}:{\cal H} \rightarrow {\cal M}$,
                  ${\cal K}:{\cal M} \rightarrow {\cal B}$,
                  $F_{\cal M}:{\cal M} \rightarrow {\rm Cat}$ such that (1) $P
		  = {\cal K} \circ P_{\cal M}$; (2) $P_{\cal M}$ is a
		  surjective split opfibtration with the same
		  op-Cartesian assignment as $P$; (3) ${\cal K}$ is a
		  minimal discrete weak opfibration with the same
		  image as $P$; (4) ${\cal H} \equiv {\cal G}({\cal
		  M},F_{\cal M})$.

                  Kimmo I. Rosenthal (1-UNU; Schenectady, NY)}
}

@InCollection{Banach-1993b,
  author = {Richard Banach},
  title = {{MONSTR} Term Graph Rewriting for Parallel Machines},
  chapter = {?},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  pages = {243--252}
}

@Article{Banach-1994,
  author = {Richard Banach},
  title = {Term Graph Rewriting and Garbage Collection using
		  Opfibrations},
  journal = {Theoretical Computer Science},
  year = 1994,
  volume = 131,
  number = 1,
  pages = {29--94},
  month = AUG,
  WKloc = {B-0022},
  abstract = {The categorical semantics of (an abstract version
		  of) the general term graph rewriting language DACTL
		  is investigated. The operational semantics is
		  reformulated in order to revealits universal
		  properties. The technical dissonance between the
		  matchings of left-hand sides of rules to redexes,
		  and the properties of rewrite rules themselves, is
		  taken as the impetus for expressing the core of the
		  model as a Grothendieck opfibration of a category of
		  general rewrites over a base of general rewrite
		  rules. Garbage collection is examined in this
		  framework in order to reconcile the treatment with
		  earlier approaches. It is shown that term rewriting
		  has particularly good garbage-theoretic properties
		  that do not generalise to all cases of graph
		  rewriting and that this has been a stumbling block
		  for aspects of some earlier models for graph rewriting.},
  contents = {1. Introduction
                  2. A brief introduction to DACTL rewriting
                  3. DACTL abstracted
                  3.1. The DACTL rewriting model
                  3.2. Two examples
                  3.3. Review and comparison with other models}
}

@Article{Banach-1994a,
  journal = TCS,
  volume = 129,
  number = 1,
  author = {Banach, Richard},
  title = {Regular Relations and Bicartesian Squares},
  institution = {Computer Science Dept., Univ.\null{} of Manchester},
  year = 1994,
  pages = {187--192},
  DOIURL= "http://dx.doi.org/10.1016/0304-3975(94)90086-8",
  DOI = "10.1016/0304-3975(94)90086-8",
  DirectURL = "//www.sciencedirect.com/science/article/pii/0304397594900868",
  abstract = {It is shown that regular relations,
    which arise in a number of areas of programming theory,
    can be characterised in a variety of ways as pullbacks in Set;
    and up to isomorphism, as bicartesian squares in Set. },
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB}
}

@Article{Banach-1995,
  author = {Richard Banach},
  title = {On Regularity in Software Design},
  journal = {Science of Computer Programming},
  year = 1995,
  volume = 24,
  pages = {221--245},
  WKloc = {A-0933},
  bibliographies = {RelMiCS, RelMiS, SpecTech}
}

@TechReport{Banach-1996a,
  author = {R. Banach},
  title = {A Fibration Semantics for Pi-Calculus modules via Abstract {MONSTR} Rule Systems},
  year = 1996,
  institution = {Department of Computer Science, University of Manchester},
  number = {UMCS-96-7-3},
  WKloc = {A-0805}
}

@TechReport{Banach-1996b,
  author = {R. Banach},
  title = {{DPO} Rewriting and Abstract Semantics via Opfibrations},
  year = 1996,
  institution = {Department of Computer Science, University of Manchester},
  number = {UMCS-96-8-1},
  WKloc = {A-0804}
}

@Misc{Banach-199X,
  author = {R. Banach},
  title = {Transitive Term Graph Rewriting},
  year = {199?},
  WKloc = {A-0550}
}

@TechReport{Banach-Corradini-1996a,
  author = {Richard Banach and Andrea Corradini},
  title = {An Opfibration Account of Typed {DPO} and {DPB} Graph
		  Transformation: General Productions},
  institution = {Department of Computer Science, University of Manchester},
  year = 1996,
  OPTkey = {},
  OPTtype = {},
  number = {UMCS-96-11-2},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  ISSN = {1361-6161},
  WKloc = {B-0077}
}

@Misc{Banach-Corradini-1999,
  author = {Richard Banach and Andrea Corradini},
  title = {Abstract Diagrams and an Opfibration Account of Typed Graph Transformations},
  URL = {http://www..cs.man.ac.uk/\~{}banach/some.pubs/Abs.Diag.Op.Ty.Gr.Tr.ps.gz},
  note = {submitted to TCS},
  year = 1999
}

@InProceedings{Banach-Corradini-2000,
  author = {Richard Banach and Andrea Corradini},
  title = {Graph Transformations via Abstract Diagrams},
  pages = {155--162},
  crossref = {GraTra2000},
  note = {short version of \cite{Banach-Corradini-1999}}
}

@Article{Banach-Papadopoulos-1997,
  author = {Banach, Richard and Papadopoulos, George A.},
  OPTauthor = {R. Banach and G. A. Papadopoulos},
  title = {A Study of Two Graph Rewriting Formalisms: Interaction Nets and {MONSTR}},
  journal = {Journal of Programming Languages},
  year = 1997,
  volume = 5,
  pages = {210--231},
  publisher = {Chapman \& Hall},
  WKloc = {A-0549, doc/pap/BIB},
  Tabstract = {Two superficially similar graph rewriting formalisms,
    Interaction Nets and MONSTR, are studied.
    Interaction Nets come from multiplicative Linear Logic
    and feature undirected graph edges,
    while MONSTR arose from the desire
    to implement generalized term graph rewriting efficiently
    on a distributed architecture
    and utilizes directed graph arcs.
    Both formalisms feature rules with small left-hand sides
    consisting of two main graph nodes.
    A translation of Interaction Nets into MONSTR is described
    for both typed and untyped nets,
    while the impossibility of the opposite translation
    rests on the fact that net rewriting is always Church–Rosser
    while MONSTR rewriting is not.
    Some extensions to the net formalism
    suggested by the relationship with MONSTR are discussed,
    as well as some related implementation issues.},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Banach-Papadopoulos-1997a,
  author = {Banach, Richard and Papadopoulos, George A.},
  title = {Implementing Interaction Nets in {MONSTR}},
  booktitle = {Proceedings of the 1997 ACM Symposium on Applied Computing, {SAC '97}},
  year = {1997},
  isbn = {0-89791-850-9},
  location = {San Jose, California, USA},
  pages = {509--514},
  numpages = {6},
  DOIURL = {http://doi.acm.org.libaccess.lib.mcmaster.ca/10.1145/331697.332340},
  doi = {10.1145/331697.332340},
  acmid = {332340},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {MONSTR, distributed systems, interaction nets, term graph rewriting systems},
  WKloc = {A-1750, doc/pap/BIB}
}

@Misc{Banach-Poppleton-199X,
  author = {R. Banach and M. Poppleton},
  title = {Retrenchment: An Engineering Variation on Refinement},
  year = {199?},
  WKloc = {A-0548},
  bibliographies = {SpecTech}
}

@Article{Banach-Poppleton-2000,
  author = {R. Banach and M. Poppleton},
  title = {Sharp Retrenchment, Modulated Refinement, and Simulation},
  journal = {Form. Asp. Comp.},
  year = 2000,
  WKloc = {A-0934},
  URL = {http://www.cs.man.ac.uk/~banach/some.pubs/Sharp.Retrench.ps.gz},
  bibliographies = {SpecTech}
}

@Book{Bandemer-Gottwald-1995,
  author =	 {Hans Bandemer and Siegfried Gottwald},
  title = 	 {Fuzzy Sets, Fuzzy Logic, Fuzzy Methods with Applications},
  publisher = 	 {Wiley},
  year = 	 1995,
  ISBN = 	 0471956368,
  annote =	 {Translation of: Einf\"uhrung in die Fuzzy Methoden.
Includes bibliographical references (p. [217]-234) and index.},
  McMaster = {Innis QA 248.5 .B3613 1995}
}

@InProceedings{Bandur-Kahl-Wassyng-2012,
  author =       {Victor Bandur and Wolfram Kahl and Alan Wassyng},
  title =        {Microcontroller Assembly Synthesis from Timed Automaton Task Specifications},
  pages =     {63--77},
  crossref = {FMICS2012},
  DOI =          {10.1007/978-3-642-32469-7_5},
  SpringerURL =  {http://www.springerlink.com/content/r453820p28nl1w42/},
  abstract =     {A method for the automatic refinement of single-task
     timed automaton specifications into microcontroller assembly code is proposed.
     The outputs of the refinement are an assembly implementation
     and a timed automaton describing its exact behaviour.
     Implementation is only possible
     when all specified timing behaviours can be met by the target microcontroller.
     Crucially, the implementation does not make the simplifying synchrony assumption,
     yet correctness with respect to timing is guaranteed.
     Currently this method copes with parallel inputs and outputs,
     but is restricted to timed automaton specifications
     with only one clock variable that is reset after each transition.
     Further generalization is possible.
     A tool illustrates the method on a simple example.},
  bibliographies = {WK}
}

@PhDThesis{Banerjee-1995,
  author = {Anindya Banerjee},
  title = {The Semantics and Implementation of Bindings in Higher-Order programming Languages},
  school = {Kansas State University},
  year = 1995,
  WKloc = {B-0098}
}

@TechReport{Banerjee-SchmidtDA-1992,
  author = {Anindya Banerjee and David Schmidt},
  title = {A Categorical Interpretation of Landin's
		  Correspondence Principle},
  institution = {Kansas State University},
  year = 1992,
  URL = {ftp://ftp.cis.ksu.edu/pub/CIS/Schmidt/corr.ps},
  number = {TR-CS-92-39},
  file = {doc/pap/UCSTRI/Schmidt_corr.ps.gz},
  WKloc = {A-0227},
  abstract = {The correspondence principle, which is used as a
		  benchmark for proper programming language design, is
		  characterized and validated here as the
		  exponentiation property of category theory. The
		  framework in which the validation occurs is itself
		  of interest for it defines a hierarchy of
		  call-by-value $\lambda$-calculi, of which
		  call-by-name is the weakest form of call-by-value
		  calculus. Applications to compiling are given.}
}

@Misc{Banerjee-SchmidtDA-1994,
  author = {Anindya Banerjee and David Schmidt},
  title = {Stackability in the Simply-Typed Call-By-Value Lambda Calculus},
  year = {1994?},
  WKloc = {A-0597}
}

@Article{BanerjeeU-2011,
  author = {Banerjee, Utpal},
  title = {Mathematical Foundation of Trace Scheduling},
  journal = TOPLAS,
  issue_date = {April 2011},
  volume = {33},
  number = {3},
  month = may,
  year = {2011},
  issn = {0164-0925},
  pages = {10:1--10:24},
  articleno = {10},
  numpages = {24},
  DOIURL = {http://doi.acm.org/10.1145/1961204.1961206},
  DOI = {10.1145/1961204.1961206},
  acmid = {1961206},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {Coconut},
  WKloc = {TOPLAS, doc/pap/BIB},
  keywords = {Trace scheduling, compensation code, trace replacement},
  abstract = {Since its introduction by Joseph A. Fisher in 1979,
    trace scheduling has influenced much of the work
    on compile-time ILP (Instruction Level Parallelism) transformations.
    Initially developed for use in microcode compaction,
    it quickly became the main technique for machine-level compile-time parallelism exploitation.
    Although it has been used since the 1980s
    in many state-of-the-art compilers (e.g., Intel, Fujitsu, HP),
    a rigorous theory of trace scheduling is still lacking in the existing literature.
    This is reflected in the ad hoc way compensation code is inserted after a trace compaction,
    in the total absence of any attempts to measure the size of that compensation code, and so on.

    The aim of this article is to create a mathematical theory of the foundation of trace scheduling.
    We give a clear algorithm showing how to insert compensation code
    after a trace is replaced with its schedule,
    and then prove that the resulting program is indeed equivalent to the original program.
    We derive an upper bound on the size of that compensation code,
    and show that this bound can be actually attained.
    We also give a very simple proof that the trace scheduling algorithm always terminates.}
}

@InProceedings{Barbanera-DezaniCiancaglini-1991,
  title = {Intersection and Union Types},
  author = {Franco Barbanera and Mariangiola Dezani-Ciancaglini},
  pages = {651--674},
  crossref = {TACS1991},
  abstract = {A type assignment with {\em union\/} and {\em
		  intersection\/} types is introduced.  Relevant
		  syntactical and semantical properties of this system
		  are proved.}
}

@InProceedings{Barbanera-Fernandez-1993,
  author = {Franco Barbanera and Maribel Fern/'andez},
  title = {Combining First and Higher Order Rewrite Systems with Type
          Assignment Systems},
  pages = {60--74},
  abstract = {General computational models obtained by integrating different
             specific models have recently become a stimulating and
             promising research subject for the computer science community.
             In particular, combinations of algebraic term rewriting systems
             and various $\lambda$-calculi, either typed or untyped, have
             been deeply investigated. In the present paper this subject is
             addressed from the point of view of {\em type
		  assignment}. A powerful
             type assignment system, the one based on intersection types, is
             combined with term rewriting system, first and higher order,
             and relevant properties of the resulting system, like strong
             normalization and confluence, are proved.},
  crossref = {TLCA93},
  WKloc = {A-0179}
}

@InProceedings{Barbanera-Fernandez-Geuvers-1994,
  title = {Modularity of Strong Normalization and Confluence in the
      algebraic-$\lambda$-Cube},
  author = {Franco Barbanera and Maribel Fern{\'a}ndez and Herman Geuvers},
  pages = {406--415},
  crossref = {LICS9},
  WKloc = {A-0363},
  abstract = {In this paper we present the algebraic-$\lambda$-cube, an
		  extension of Barendregt's $\lambda$-cube with first-
		  and higher-order algebraic rewriting.  We show that
		  strong normalization is a modular property of all
		  systems in the algebraic-$\lambda$-cube, provided
		  that the first-order rewrite rules are
		  non-duplicating and the higher-order rules satisfy
		  the general schema of Jouannaud and Okada.  This
		  result is proven for the algebraic extension of the
		  Calculus of Cons\-tructions, which contains all the
		  systems of the algebraic-$\lambda$-cube.

                  We also prove that local confluence is a modular
		  property of all the systems in the
		  algebraic-$\lambda$-cube, provided that the
		  higher-order rules do not introduce critical pairs.
		  This property and the strong normalization result
		  imply the modularity of confluence.}
}

@InCollection{Bardohl-Minas-Schuerr-Taentzer-1999,
  author = {R. Bardohl and M. Minas and A. Schuerr and G. Taentzer},
  title = {Application of Graph Transformation to Visual Languages},
  crossref = {HBGraTraII},
  pages = {105--180},
  chapter = 3,
  keywords = {PROGRES, AGG, GenGed, DiaGen}
}

@Misc{Barendregt-1965,
  author = {Henk Barendregt},
  title = {Neighbours},
  year = 1965,
  note = {translated 1997},
  WKloc = {A-0543}
}

@PhDThesis{Barendregt-1971,
  author = {Henk P. Barendregt},
  title = {Some Extensional Term Models for Combinatory Logics and $\lambda$-Calculi},
  school = {University of Utrecht},
  year = 1971
}

@Book{Barendregt-1984,
  author = {Hendrik P. Barendregt},
  publisher = {North-Holland},
  series = {Studies in Logic and the Foundations of Mathematics},
  title = {The Lambda Calculus. Its Syntax and Semantics},
  volume = 103,
  year = 1984,
  UniBwM = {MAT006/M14701; Mag/K6186},
  McMaster = {Mills QA 9.5 .B36},
  bibliographies = {FP}
}

@InProceedings{Barendregt-1988,
  author = {Henk Barendregt},
  title = {Buddhist Phenomenology},
  booktitle = {Proc. Conference on Topics and Perspectives of Contemporary Logic and Philosophy of Science},
  pages = {37--55},
  OPTabstract = {},
  WKloc = {A-0544}
}

@InCollection{Barendregt-1992,
  author = {Hendrik P. Barendregt},
  title = {Lambda Calculi with Types},
  pages = {117--309},
  crossref = {HBLCS-II},
  WKloc = {B-0006},
  bibliographies = {FP}
}

@Misc{Barendregt-1996,
  author = {Henk Barendregt},
  title = {The Quest for Correctness},
  year = 1996,
  month = APR,
  WKloc = {A-0542}
}

@Misc{Barendregt-1996a,
  author = {Henk Barendregt},
  title = {Buddhist Phenomenology II},
  year = 1996,
  WKloc = {A-0545}
}

@Misc{Barendregt-1997,
  author = {Henk Barendregt},
  title = {The Impact of the Lambda Calculus},
  year = 1997,
  WKloc = {A-0539}
}

@Misc{Barendregt-1998a,
  author = {Henk Barendregt},
  title = {Problems in Type Theory},
  year = 1998,
  month = FEB,
  WKloc = {A-0537}
}

@Misc{Barendregt-Barendsen-199X,
  author = {Henk Barendregt and Erik Barendsen},
  title = {Autarkic Computations in Formal Proofs},
  year = {199?},
  WKloc = {A-0540}
}

@Misc{Barendregt-Barendsen-2000,
  author = {Henk Barendregt and Erik Barendsen},
  title = {Introduction to {Lambda Calculus}},
  year = {2000},
  month = MAR,
  note = {Revised edition; first date: December 1998},
  WKloc = {A-1717}
}

@Article{Barendregt-Coppo-DezaniCiancaglini-1983,
  author = {Barendregt, Henk P. and Coppo, M. and Dezani-Ciancaglini,
          Mariangiola},
  title = {A Filter Lambda Model and the Completeness of Type-Assignment},
  journal = {J. Symbolic Logic},
  year = 1983,
  volume = 48,
  number = "4",
  pages = {931--940},
  keywords = {intersection types},
  annote = {Steffen van Bakel <svb@doc.ic.ac.uk>, 28 Nov 2005:
    Re: [TYPES] Semantics of Intersection Type
    ``The paper to start with''}
}

@Misc{Barendregt-Ghilezan-1998,
  author = {Henk Barendregt and Silvia Ghilezan},
  title = {Lambda Terms for Natural Deduction, Sequent Calculus and Cut Elimination},
  year = 1998,
  month = MAY,
  WKloc = {A-0541}
}

@InProceedings{Barendregt-Hemerik-1990,
  author = {Hendrik Barendregt and K. Hemerik},
  title = {Types in Lambda Calculi and Programming Languages},
  crossref = {ESOP1990},
  pages = {1--35},
  note = {invited lecture},
  WKloc = {A-0248},
  keywords = {explicit typing (Church), implicit typing (Curry),
		  lambda-cube}
}

@Article{Barendregt-Kennaway-Klop-Sleep-1987,
  author = {H. P. Barendregt and J. R. Kennaway and J. W. Klop
		  and M. R. Sleep},
  title = {Needed Reduction and Spine Strategies for the Lambda
		  Calculus},
  journal = INFCOMP,
  year = 1987,
  volume = 75,
  pages = {191--231},
  month = DEC,
  number = 3,
  abstract = {A redex $R$ in a lambda-term $M$ is called {\em
		  needed} if in every reduction of $M$ to normal form
		  (some residual of) $R$ is contracted. Among others
		  the following results are proved: 1. $R$ is needed
		  in $M$ off $R$ is contracted in the leftmost
		  reduction path of $M$. 2. Let ${\cal R}: M_0 \tfun
		  M_1 \tfun M_2 \tfun \ldots$ reduce redexes $R_i: M_i
		  \tfun M_{i+1}$, and have the property that $\all
		  i.\exist k. R_j$ is nneded in $M_j$. Then $\cal R$
		  is normalising, i.e., if $M_0$ has a normal form,
		  then $\cal R$ is finite and terminates at that
		  normal form. 3. Neededness is an undecidable
		  property, but has several efficiently decidable
		  approximations, various versions of the so-called
		  {\em spine} redexes.},
  note = {somewhere cited as: KennawaySBK87}
}

@InCollection{Barendregt-Koymans-1980,
  author = {Henk P. Barendregt and K. Koymans},
  title = {Comparing Some Class of Lambda-Calculus Models},
  crossref = {Seldin-Hindley-1980},
  pages = {287--301}
}

@InCollection{Barendregt-Longo-1980,
  author = {Henk P. Barendregt and G. Longo},
  title = {Equality of $\lambda$-Terms in the Model $T^{\omega}$},
  crossref = {Seldin-Hindley-1980},
  pages = {303--337}
}

@InProceedings{Barendregt-vanEekelen-Glauert-Kennaway-Plasmeijer-Sleep-1987,
  author = {H. P. Barendregt and van Eekelen, M. C. J. D. and J.
		  R. W. Glauert and J. R. Kennaway and M. J.
		  Plasmeijer and M. R. Sleep},
  title = {Term Graph Rewriting},
  crossref = {PARLE87b},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  pages = {141--158},
  OPTbooktitle = {},
  OPTyear = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Barendregt-vanEekelen-Glauert-Kennaway-Plasmeijer-Sleep-1987-x,
  author = {H. P. Barendregt and van Eekelen, M. C. J. D. and J.
		  R. W. Glauert and J. R. Kennaway and M. J.
		  Plasmeijer and M. R. Sleep},
  title = {Term Graph Rewriting},
  pages = {141--158},
  booktitle = {Proc. PARLE '87 Conference, vol. II},
  year = 1987,
  key = {PARLE '87 II},
  editor = {de Bakker, J. W. and A. J. Nijman and P. C. Treleaven},
  volume = 259,
  series = {LNCS},
  publisher = {Springer Verlag}
}

@Article{Barendregt-vanEekelen-Plasmeijer-1989,
  author = {Barendregt, H. P. and Van Eekelen, M. C. J. D. and Plasmeijer, M. J.},
  title = {LEAN: An intermediate language based on graph rewriting},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Parallel computing},
  year = 1989,
  volume = 9,
  number = 2,
  pages = {163--},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@TechReport{Barendregt-vanLeeuwen-1986,
  author = {Henk P. Barendregt and M. van Leeuwen},
  title = {Functional programming and the language TALE},
  institution = {University of Utrecht Dept. of Mathematics},
  year = 1986,
  number = 412
}

@TechReport{Barendsen-Smetsers-1992,
  author = {Erik Barendsen and Sjaak Smetsers},
  title = {Graph Rewriting and Copying},
  institution = {Department of Computer Science, University of Nijmegen},
  year = 1992,
  number = {92-20},
  annote = {reference from \cite{Barendsen-Smetsers-1993}; seems
		  to have something similar to the identification lattice.},
  WKloc = {B-0072}
}

@Misc{Barendsen-Smetsers-1992a,
  author = {Erik Barendsen and Sjaak Smetsers},
  title = {Type Systems for Graph Rewriting},
  year = {1992?},
  WKloc = {B-0072}
}

@InProceedings{Barendsen-Smetsers-1993,
  author = {E. Barendsen and S. Smetsers},
  title = {Extending Graph Rewriting with Copying},
  crossref = {GTCS93},
  pages = {51--70},
  abstract = {The notion of term graph rewrite system (TGRS) is
		  extended with a {\em lazy copying} mechanism. By
		  analyzing this mechanism, a confluence result is
		  obtained for these so-called {\em copy term graph
		  rewrite systems} (C-TGRS). Some ideas on the use of
		  lazy copying in practice are presented.},
  WKloc = {A-0286},
  annote = {\cite{Barendsen-Smetsers-1992} seems to have something
		  resembling the identification lattice\\
                  p.55, Def.11(iii): a rule left-linear iff the left-hand
		                     side is a tree\\
                        Def.13: segment of $g$ has a root-preserving
		                injective morphism with zero-ary
		                variables into $g$.\\
                  Repeated references to the $\lamda$-calculus really
		  only refer to TRSs.\\
                  p.58, Def.21(ii) obviously allows nontrivial sharing
		                   between lhs and rhs.\\
                  p.59, Def.26: regular: pairwise disjoint redexes
                                orthogonal: left-linear and regular\\
                  p.61, Def.35: Copy reduction could be defined by the
		                following set og ATGRV rules:
                    $C(V_1^s(V_0^a)) \rightarrow V_1^s(C(V_0^a))$,
                    $C(V_2^s(V_0^a,V_0^b)) \rightarrow
                     V_2^s(C(V_0^a),C(V_0^b))$, $\ldots$\\
                  p.63, Def.40: $e_{D,g}(n)$ corresponds to {\sl BlockExit}.\\
                  p.65: sharing-conservative application of above
		        rules yields
                           $(C(A(x,I(x))) {\rm where} x=B)
		            \rightarrow (A(x,C(I(x))) {\rm where} x=B)$
                        and therewith confluence at least in this example.}
}

@InCollection{Barendsen-Smetsers-1999,
  author = {E. Barendsen and S. Smetsers},
  title = {Graph Rewriting Aspects of Functional Programming},
  crossref = {HBGraTraII},
  pages = {63--102},
  chapter = 2,
  keywords = {Clean}
}

@Article{Barnett-2002,
  author = {Michael P. Barnett},
  title = {Computer algebra in the life sciences},
  journal = {ACM SIGSAM Bulletin},
  volume = 36,
  number = 4,
  year = 2002,
  ISSN = {0163-5824},
  pages = {5--32},
  WKloc = {SIGSAM},
  bibliographies = {Anand},
  DOIURL = {http://doi.acm.org/10.1145/641239.641242},
  DOI = {10.1145/641239.641242},
  publisher = {ACM Press}
}

@Book{Barr-1979,
  author = {M. Barr},
  title = {$*$-Autonomous Categories},
  publisher = Springer,
  year = 1979,
  volume = 752,
  series = LNM,
  pages = 140,
  UniBwM = {MAT200/G17512},
  bibliographies = {RelMiCS}
}

@Article{Barr-1991,
  author = {Michael Barr},
  title = {$\star$-autonomous categories and linear logic},
  journal = {Mathematical Structures in Computer Science},
  year = 1991,
  volume = 1,
  number = 2,
  pages = {159-178},
  month = JUL,
  bibliographies = {RelMiCS}
}

@Book{Barr-Wells-1984,
  author = {Michael Barr and Charles Wells},
  title = {Toposes, Triples and Theories},
  series = Grundlehr,
  volume = 278,
  publisher = Springer,
  address = {Berlin},
  year = 1984,
  UniBwM = {MAT200/N2026},
  keywords = {monad, Kleisli, Eilenberg-Moore},
  bibliographies = {RelMiCS},
  note = {updated version available at
    \textsf{URL: http://www.cwru.edu/artsci/math/wells/pub/ttt.html}
    and \textsf{URL: ftp://ftp.math.mcgill.ca/pub/barr}}
}

@Book{Barr-Wells-1990,
  author = {Michael Barr and Charles Wells},
  title = {Category Theory for Computing Science},
  publisher = {Prentice Hall},
  year = 1990,
  series = {Prentice Hall International Series in Computer Science},
  errata = {file://triples.math.mcgill.ca/pub/barr/ctcsupdate.dvi},
  UniBwM = {MAT200/T14581},
  bibliographies = {RelMiCS}
}

@Book{Barr-Wells-1999,
  author = {Michael Barr and Charles Wells},
  title = {Category Theory for Computing Science},
  publisher = {Centre de recherches math\'ematiques (CRM), Universit\'e de Montr\'eal},
  year = 1999,
  edition = {3$^{\rm rd}$},
  URL = {http://crm.umontreal.ca/pub/Ventes/desc/PM023.html},
  bibliographies = {RelMiCS}
}

@Article{Barricelli-Hansen-1986,
  author = {Nils Aall Barricelli and Bent Billing Hansen},
  title = {The Direct Symbolic Treatment
		of {B}-Mathematical Relation Algebra},
  journal = THEPAP,
  year = 1986,
  volume = 4,
  pages = {39--98},
  bibliographies = {RelMiCS}
}

@article{Barthe-Capretta-Pons-2003,
 author = {Barthe, Gilles and Capretta, Venanzio and Pons, Olivier},
 title = {Setoids in type theory},
 journal = {J. Funct. Program.},
 volume = {13},
 number = {2},
 year = {2003},
 issn = {0956-7968},
 pages = {261--293},
 DOIURL = {http://dx.doi.org/10.1017/S0956796802004501},
 DOI = {10.1017/S0956796802004501},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
 abstract = {Formalising mathematics in dependent type theory often requires to represent sets as setoids, i.e. types with an explicit equality relation. This paper surveys some possible definitions of setoids and assesses their suitability as a basis for developing mathematics. According to whether the equality relation is required to be reflexive or not we have total or partial setoid, respectively. There is only one definition of total setoid, but four different definitions of partial setoid, depending on four different notions of setoid function. We prove that one approach to partial setoids in unsuitable, and that the other approaches can be divided in two classes of equivalence. One class contains definitions of partial setoids that are equivalent to total setoids; the other class contains an inherently different definition, that has been useful in the modeling of type systems. We also provide some elements of discussion on the merits of each approach from the viewpoint of formalizing mathematics. In particular, we exhibit a difficulty with the common definition of subsetoids in the partial setoid approach.}
}

@InProceedings{Barthe-CirsteaH-KirchnerC-Liquori-2003,
  author = {Gilles Barthe and Horatiu Cirstea and Claude Kirchner and Luigi Liquori},
  title = {Pure Patterns Type Systems},
  crossref = {POPL2003},
  DVI = {data/ppts-popl2003.dvi.gz},
  PS = {data/ppts-popl2003.ps.gz},
  PDF = {data/ppts-popl2003.pdf},
  abstract = { We introduce a new framework of algebraic pure
	type systems in which we consider rewrite rules as lambda
	terms with patterns and rewrite rule application as
	abstraction application with built-in matching facilities.
	This framework, that we call \textit{``Pure Pattern Type
	Systems''}, is particularly well-suited for the foundations of
	programming (meta)languages and proof assistants since it
	provides in a fully unified setting higher-order capabilities
	and pattern matching ability together with powerful type
	systems.  We prove some standard properties like confluence
	and subject reduction for the case of a syntactic theory and
	under a syntactical restriction over the shape of patterns.
	We also conjecture the strong normalization of typable terms.
	This work should be seen as a contribution to a formal
	connection between logics and rewriting, and a step towards
	new proof engines based on the Curry-Howard isomorphism.  },
  keywords = {Rewriting calculus, lambda-calculus, type system,
	pattern matching, pure type systems, Curry-deBruijn-Howard,
	Logics},
  bibliographies = {PMC},
  WKloc = {A-1404, doc/pap/BIB}
}

@Misc{Barthe-vanRaamsdonk-199X,
  author = {Gilles Barthe and van Raamsdonk, Femke},
  title = {Termination of algebraic type systems: the syntactic approach},
  year = {199?},
  WKloc = {A-0427}
}

@InProceedings{Barthelmann-Schied-1993,
  author = {Klaus Barthelmann and Georg Schied},
  title = {Graph-Grammar Semantics of a Higher-Order
		  Programming Language for Distributed Systems},
  crossref = {GTCS93},
  pages = {71--85},
  abstract = {We will consider a new tiny, yet powerful,
		  programming language for distributed systems, called
		  DHOP, which has its operational semantics given as
		  algebraic rewrite rules in a certain category of
		  labeled graphs. Our approach allows to separate
		  actions which affect several processes from local
		  changes such as variable bindings. We also sketch
		  how to derive an implementation from this specification.},
  WKloc = {A-0287},
  annote = {``The idea of a labelling category apparently did
		  not emerge earlier than \cite{Schneider-1991}'' ---
		  compare with \cite{Parisi-Ehrig-Montanari-1986}!}
}

@Article{Basin-1998,
  author = {David Basin},
  title = {Logical Framework Based Program Development},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 1},
  WKloc = {A-0902, 1--4},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Basin-1998a,
  author = {David Basin},
  title = {Softwaretechnik},
  howpublished = {Skript und Folien zur Vorlesung an der Universit\"at Freiburg},
  month = APR,
  year = 1998,
  WKloc = {B-0057},
  bibliographies = {SpecTech}
}

@Misc{ Basin-Clavel-Meseguer-1999,
  author = "D. Basin and M. Clavel and J. Meseguer",
  title = "Reflective metalogical frameworks",
  text = "David Basin, Manuel Clavel, and Jos'e Meseguer. Reflective metalogical
    frameworks. In Proceedings of LFM'99: Workshop on Logical Frameworks and
    Meta-languages, Paris, France, September 28, 1999. http://www.cs. bell-labs.com/~felty/LFM99/.",
  year = "1999",
  CiteSeer = "citeseer.ist.psu.edu/basin99reflective.html",
  WKloc = {doc/pap/BIB}
}

@InProceedings{Basin-Howe-1991,
  title = {Some Normalization Properties of {Martin-L\"of's} Type
		  Theory, and Applications},
  author = {David A. Basin and Douglas J. Howe},
  pages = {475--494},
  crossref = {TACS1991},
  abstract = {For certain kinds of applications of type theories, the
		  faithfulness of formalization in the theory depends
		  on intensional, or structural, properties of objects
		  constructed in the theory.  For type theories such
		  as LF, such properties can be established via an
		  analysis of normal forms and types.  In type
		  theories such as Nuprl or Martin-L\"of's polymorphic
		  type theory, which are much more expressive than LF,
		  the underlying programming language is essentially
		  untyped, and terms proved to be in types do not
		  necessarily have normal forms. Nevertheless, it is
		  possible to show that for Martin-L\"of's type
		  theory, and a large class of extensions of it, a
		  sufficient kind of normalization property does in
		  fact hold in certain well-behaved subtheories.
		  Applications of our results include the use of the
		  type theory as a logical framework in the manner of
		  LF, and an extension of the {\em proofs-as-programs}
		  paradigm to the synthesis of verified computer
		  hardware.  For the latter application we point out
		  some advantages to be gained by working in a more
		  expressive type theory.}
}

@InProceedings{Basin-KriegBrueckner-199X,
  author = {David Basin abd Bernd Krieg-Br{\"u}ckner},
  title = {Formalization of the Development Process},
  booktitle = {Handbook of Software Technology???},
  pages = {??},
  OPTabstract = {},
  WKloc = {A-0651}
}

@Article{Basin-Walsh-1996,
  author = 	 {David A. Basin and Toby Walsh},
  title = 	 {A Calculus For and Termination Of Rippling},
  journal = 	 {Journal of Financial Services Research},
  year = 	 1996,
  volume =	 16,
  number =	 {1--2},
  pages =	 {147--180},
  month =	 MAR,
  doi = 	 {10.1007/BF00244462},
  URL = {http://www.springerlink.com/content/t3n8551961523406/},
  bibliographies = {HHOL},
  abstract = {Rippling is a type of rewriting developed for
      inductive theorem proving that uses annotations to direct
      search. Rippling has many desirable properties: for example, it
      is highly goal directed, usually involves little search, and
      always terminates. In this paper we give a new and more general
      formalization of rippling. We introduce a simple calculus for
      rewriting annotated terms, close in spirit to first-order
      rewriting, and prove that is has the formal properties desired
      of rippling. Next we develop criteria for proving the
      termination of such annotated rewriting, and introduce orders on
      annotated terms that lead to termination. In addition, we show
      how to make rippling more flexible by adapting the termination
      orders to the problem domain. Our work has practical as well as
      theoretical advantages: it has led to a very simple
      implementation of rippling that has been integrated in the
      Edinburgh CLAM system.}
}

@Book{Bass-Clements-Kazman-1998,
  author = {L. Bass and P. Clements and R. Kazman},
  title = {Software Architecture in Practice},
  publisher = {Addison Wesley Longman},
  year = 1998,
  keywords = {Software Architecture Analysis Method (SAAM), scenario},
  annote = {Cited by \cite{Pasetti-2002} as reference for scenarios:
      \begin{quote} A SAAM scenario describes a hypothetical change in the
      application specification and considers the ease with which the
      application design and implementation can be modified to meet the new
      specifications.

      \end{quote}}
}

@InProceedings{Basten-Bosnacki-2001,
  author = {Twan Basten and Dragan Bo{\hacek{s}}na{\hacek{c}}ki},
  title = {Enhancing Partial-Order Reduction via Process Clustering},
  crossref = {ASE2001},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  abstract = {Partial-order reduction is a well-known technique
    to cope with the state-space-explosion problem
    in the verification of concurrent systems.
    Using the hierarchical structure of concurrent systems, we present
    an enhancement of the partial-order-reduction scheme of [12, 19].
    A prototype of the new algorithm has been implemented
    on top of the verification tool SPIN.
    The first experimental results are encouraging.}
}

@InProceedings{Batarekh-1990,
  author = {A\"{\i}da Batarekh},
  title = {Fixpoint Techniques for Non-Monotone Maps},
  crossref = {ALP1990},
  pages = {132--143},
  abstract = {Using a technique of successive approximations,
		  conditions were derived under which such
		  approximations converge to a fixpoint. In such
		  cases, the limit of a sequence of approximations
		  provides a mathematically computable fixpoint for
		  non-monotone maps. In this paper, we define a class
		  of logic programs for which such a technique
		  provides a way of constructing a {\em minimal}
		  fixpoint with special properties---stable model,
		  preferable to any other fixpoint. Unlike stratified
		  programs, the class of programs we present has the
		  advantage of allowing negation through recursion. We
		  also proesent some results on unique fixpoints in
		  non-monotone maps.}
}

@InProceedings{Batory-Cardone-Smaragdakis-2000,
  author = {D. Batory and R. Cardone and Y. Smaragdakis},
  title = {Object-Oriented Frameworks and Product Lines},
  booktitle = {Software Product Lines --- Experience and Research Directions},
  pages = {227--248},
  year = 2000,
  editor = {Donohoe, P.},
  publisher = {Kluwer},
  annote = {\cite{Pasetti-2002}:
    The example framework mentioned in \cite{Batory-Cardone-Smaragdakis-2000},
    for instance, targets graph traversal applications.}
}

@InProceedings{Bauderon-1995,
  author = {Michel Bauderon},
  title = {A Uniform Approach to Graph Rewriting: The Pullback
		  Approach},
  crossref = {WG1995},
  pages = {101--115},
  WKloc = {A-1076, doc/pap/BIB}
}

@Article{Bauderon-1995a,
  author = {Michel Bauderon},
  title = {Parallel Rewriting through the Pullback Approach},
  journal = ENTCS,
  volume = 2,
  note = {SEGRAGRA '95},
  year = 1995,
  WKloc = {A-1077, doc/pap/BIB}
}

@InProceedings{Bauderon-1996,
  author = {Bauderon, Michel},
  title = {A Category-Theoretical Approach to Vertex Replacement:
		  {The} Generation of Infinite Graphs},
  editor = {J. Cuny and H. Ehrig and G. Engels and G. Rozenberg},
  booktitle = {Proc. Fifth Intl. Workshop on Graph Grammars and Their
		  Application to Comp. Sci.},
  series = {LNCS},
  volume = 1073,
  pages = {27--37},
  year = 1996,
  publisher = {Springer}
}

@InProceedings{Bauderon-ChenRui-Ly-2009,
  author =       {Michel Bauderon, Rui Chen and Olivier Ly},
  title =        {Context-Free Categorical Grammars },
  crossref =  {CAI2009},
  pages =     {160--171},
  DOI =      {10.1007/978-3-642-03564-7_10},
  SpringerURL =    {http://springerlink3.metapress.com/content/f713478vg91m6753/},
  abstract =    {We define generic categorical notions of rewriting and grammar, using two basic operations, pullback and pushout, and show that these categorical grammars are intrinsically context-free in the sense of Courcelle. We then specialise to various settings, including classical word grammars, hyperedge replacement grammars or node-replacement grammars. We show that some languages which are classical counter-example to context-freeness become context-free within this new framework.}
}

@TechReport{Bauderon-Jacquet-1996a,
  author = {Michel Bauderon and H{\'e}l{\`e}ne Jacquet},
  title = {Categorical Product as a Generic Graph Rewriting Mechanism},
  year = 1996,
  number = {1166--97},
  institution = {LaBRI, University of Bordeaux},
  WKloc = {A-1075, doc/pap/BIB},
  note = {see also \cite{Bauderon-Jacquet-2001}}
}

@Misc{Bauderon-Jacquet-1996b,
  author = {Michel Bauderon and H{\'e}l{\`e}ne Jacquet},
  title = {Node Rewriting in Graphs and Hypergraphs: {A} Categorical Framework},
  year = 1996,
  note = {submitted to TCS, special issue WG '95 \unfinished},
  WKloc = {A-0492, doc/pap/BIB}
}

@InProceedings{Bauderon-Jacquet-1996c,
  author = {Michel Bauderon and H{\'e}l{\`e}ne Jacquet},
  title = {Node Rewriting in Hypergraphs},
  crossref = {WG1996},
  pages = {31--43},
  OPTnote = {},
  OPTannote = {}
}

@Article{Bauderon-Jacquet-2001,
  author = {Michel Bauderon and H{\'e}l{\`e}ne Jacquet},
  title = {Pullback as a Generic Graph Rewriting Mechanism},
  year = 2001,
  journal = {Applied Categorical Structures},
  volume = 9,
  number = 1,
  pages = {65--82},
  abstract = {Rewriting usually relies on a notion of substitution
    which can be understood as the succession of three basic operations:
    deletion of the part to be rewritten to provide a context,
    union of this context with the right-hand side of a rule,
    liaison of those two parts, most often by identification
    of some corresponding items.
    In the field of graph rewriting, this has led to the elegant,
    productive and therefore popular method known as the
    double pushout approach to graph rewriting.
    Yet this method has met its descriptive limits
    when trying to deal with the various notions of node replacement.
    In this paper we show how --- when set in a proper framework ---
    products (or pullbacks) can provide
    a very generic and uniform rewriting mechanism
    which extends uniformly to arbitrary complicated graph-like structures.},
  URL = {http://www.wkap.nl/oasis.htm/196903},
  WKloc = {A-1081}
}

@InProceedings{Bauer-1991,
  title = {{Informatik und Algebra}},
  author = {Friedrich L. Bauer},
  crossref = {Broy-1991},
  pages = {28--40},
  bibliographies = {RelMiCS}
}

@TechReport{Bauer-1994,
  author = {Friedrich L. Bauer},
  title = {{Die ALGOL-Verschw\"orung}},
  institution = {Institut f\"ur Informatik unf Praktische
		  Mathematik,Christian-Albrechts-Universit\"at Kiel},
  year = 1994,
  type = {Bericht},
  number = {Nr. 9409}
}

@Article{Bauer-Moeller-Partsch-Pepper-1989,
  author = {F.L. Bauer and B. M\"oller and H. Partsch and P.Pepper},
  title = {Formal Program Construction By Transformations ---
		  Computer-Aided, Intuition-Guided Programming},
  journal = {IEEE  Transactions on Software Engineering},
  year = 1989,
  volume = 15,
  number = 2,
  pages = {165--180},
  WKloc = {A-0271},
  keywords = {CIP}
}

@InProceedings{Bauer-Wenzel-2001,
  author = {Gertrud Bauer and Markus Wenzel},
  title = {Calculational reasoning revisited, an {Isabelle/Isar} experience},
  crossref = {TPHOL2001},
  pages = {75--90},
  WKloc = {A-1381, doc/pap/BIB},
  abstract = {We discuss the general concept of calculational reasoning
      within Isabelle/Isar, which provides a framework for high-level
      natural deduction proofs that may be written in a human-readable
      fashion. Set- ting out from a few basic logical concepts of the
      underlying meta-logical framework of Isabelle, such as higher-order
      unification and resolution, calculational commands are added to the
      basic Isar proof language in a flexible and non-intrusive manner.
      Thus calculational proof style may be combined with the remaining
      natural deduction proof language in a liberal manner, resulting in
      many useful proof patterns. A case-study on formalizing Computational
      Tree Logic (CTL) in simply-typed set-theory demonstrates common
      calculational idioms in practice.},
  bibliographies = {HHOL}
}

@Book{Bauer-Woessner-1984,
  year = 1984,
  title = {Algorithmische {Sprache} und {Programmentwicklung}},
  publisher = {Springer-Verlag},
  edition = {2.},
  author = {Friedrich L. Bauer and Hans W{\"o}ssner}
}

@InProceedings{Baum-Frias-Haeberer-MartinezLopez-1995,
  author = {Baum, Gabriel A. and Frias, Marcelo F. and Haeberer, Armando Mart\'{\i}n and
		  Mart\'{\i}nez L\'{o}pez, P.E.},
  title = {From Specifications to Programs: A Fork--algebraic
		  Approach to Bridge the Gap},
  booktitle = {Proceedings of Mathematical Foundations of Computer Science 1996
        {(MFCS '96),Cracow, Poland}},
  series = LNCS,
  volume = 1113,
  publisher = Springer,
  pages = {180--191},
  year = 1996,
  bibliographies = {RelMiCS}
}

@Article{Baum-Haeberer-Veloso-1992,
  year = 1992,
  volume = 1,
  title = {On the Representability of the Abstract Relational Algebra},
  number = 3,
  OPTnote = {European Foundation for Logic, Language and
		  Information Interest Group on Programming Logic},
  month = SEP,
  journal = IGPL,
  author = {Baum, Gabriel A. and Haeberer, Armando Mart\'{\i}n and Paulo A.S. Veloso},
  bibliographies = {RelMiCS}
}

@Misc{Baxter-1998,
  author = {Ira D. Baxter},
  title = {Transformation Technology Bibliography, 1998},
  year = 1998,
  WKloc = {A-1131}
}

@Misc{Baxter-1998a,
  author = {Ira D. Baxter},
  title = {Transformation Systems: Domain-Oriented Component and Implementation Knowledge Reuse},
  year = {1998?},
  WKloc = {A-1133}
}

@Misc{Baxter-1999,
  author = {Ira D. Baxter},
  title = {Keynote: {DMS} (Transformational Software Maintenance by Reuse): A Production Research System?},
  year = {1999?},
  WKloc = {A-1134}
}

@InProceedings{Baxter-Mehlich-1997,
  author = {Ira D. Baxter and Michael Mehlich},
  title = {Reverse Engineering is Reverse Forward Engineering},
  booktitle = {Proc.\null{} Fourth Working Conference on Reverse Engineering, October 6--8, Amsterdam},
  OPTpages = {},
  year = 1997,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = {IEEE},
  OPTnote = {},
  WKloc = {A-1014}
}

@InProceedings{Baxter-Pidgeon-1997,
  author = {Ira D. Baxter and Christopher W. Pidgeon},
  title = {Software Change Through Design Maintenance},
  booktitle = {Proc.\null{} {ICSM '97, Sept.~28 -- Oct.~2, 1997 in Bari, Italy}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1997,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  organization = {IEEE},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1150}
}

@InProceedings{Baxter-Yahin-Moura-SantAnna-Bier-1998,
  author = {Ira D. Baxter and Andrew Yahin and Leonardo Moura and Marcelo Sant{'}Anna and Lorraine Bier},
  title = {Clone Detection Using Abstract Syntax Trees},
  booktitle = {Proc.\null{} {ICSM '98}},
  year = 1998,
  organization = {IEEE},
  WKloc = {A-1130}
}

@MastersThesis{Bayer-1987,
  author = {Ludwig J. Bayer},
  title = {{Entwicklung von komplexen Werkzeugen zur
		  interaktiven Auswertung von DAGs im
		  Programmiersystem HOPS}},
  school = {TU M\"unchen},
  year = 1987,
  type = {Diplomarbeit}
}

@TechReport{Bayer-Berghammer-Kempf-1990,
  year = 1990,
  title = {Programmieren mit h\"oheren {Objekten}: {Bausteine} und {Regeln} im {HOPS-System}},
  number = 9003,
  type = {Technischer Bericht},
  month = MAY,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Ludwig J. Bayer and Rudolf Berghammer and Peter Kempf}
}

@InProceedings{Bayer-Derichsweiler-1997,
  author = {Arne Bayer and Frank Derichsweiler},
  title = {{{\sc HOPS} as a Link Between Functional and Data-Flow Oriented Programming}},
  crossref = {Fehmarn-1997},
  pages = {209--217},
  abstract = {We introduce the integrated development environment
     {\sc HOPS}, which is a platform for constructing and transforming
     programs in a high level language, which is independent of any
     target language. We then demonstrate how {\sc HOPS} supports
     different views of a single program source, namely a term DAG view
     and a data flow view.
     Finally we develop an example program in {\sc HOPS}.}
}

@TechReport{Bayer-Grobauer-Kahl-Kempf-Schmalhofer-Schmidt-Winter-1996,
  author = {Arne Bayer and
                  Bernd Grobauer and
                  Wolfram Kahl and
                  Peter Kempf and
                  Franz Schmalhofer and
                  Gunther Schmidt and
                  Michael Winter},
  title = {{The Higher Object Programming System {\sf HOPS}}},
  institution = {Inst.\null{} f\"ur Informatik der Univ.\null{}
                  der Bundeswehr M\"unchen},
  note = {Internal Report.~206~p.},
  year = 1996,
  bibliographies = {RelMiCS}
}

@InProceedings{Bayer-Kahl-1994,
  author = {Arne Bayer and Wolfram Kahl},
  title = {{The {\bf H}igher-{\bf O}bject {\bf P}rogramming
		  {\bf S}ystem ``{\sf HOPS}''}},
  crossref = {KielTool94},
  pages = {154--171},
  filename = {wk/kiel-tool-paper.shopsfw},
  WKloc = {A-1447},
  abstract = {Herein, a graphical tool (\HOPS) for the manipulation
		  of directed acyclic term graphs (DAGs) is
		  described. Programming in a truly functional style
		  is facilitated by the provision of immediate user
		  interaction with functional program DAGs, a novel
		  approach to generic data type construction and a
		  powerful and flexible transformation
		  system. Conceptual decisions in the design of
		  language and system and the way they influence the
		  programmer are outlined.},
  OLDURL = {http://ist.unibw-muenchen.de/kahl/PAPERS/Bayer-Kahl-1994.ps.gz}
}

@InProceedings{Bayer-Kahl-1994-x,
  author = {Arne Bayer and Wolfram Kahl},
  title = {{The {\bf H}igher-{\bf O}bject {\bf P}rogramming
		  {\bf S}ystem ``{\sf HOPS}''}},
  year = 1994,
  series = {Bericht Nr. 9416},
  publisher = {Universit\"at Kiel},
  pages = {154--171},
  editor = {Bettina Buth and Rudolf Berghammer},
  booktitle = {Systems for Computer-Aided Specification,
		  Development and Verification},
  conference = {{Programmsysteme f\"ur rechnergest\"utzte
		  Programmentwicklung und -verifikation, Workshop
		  4. bis 5. Juli 1994, Kiel}},
  filename = {wk/kiel-tool-paper.shopsfw},
  abstract = {Herein, a graphical tool (\HOPS) for the manipulation
		  of directed acyclic term graphs (DAGs) is
		  described. Programming in a truly functional style
		  is facilitated by the provision of immediate user
		  interaction with functional program DAGs, a novel
		  approach to generic data type construction and a
		  powerful and flexible transformation
		  system. Conceptual decisions in the design of
		  language and system and the way they influence the
		  programmer are outlined.},
  OLDURL = {URL: {\sf http://ist.unibw-muenchen.de/kahl/PAPERS/Bayer-Kahl-1994.ps.gz}}
}

@Article{Beatty-1969,
  author = {R. Beatty},
  title = {Peirce's Development of Quantifiers and
		of Predicate Logic},
  journal = NOTRE,
  year = 1969,
  volume = 10,
  pages = {64--76},
  bibliographies = {RelMiCS}
}

@Article{Beaven-Stansifer-1993,
  author = {Mike Beaven and Ryan Stansifer},
  title = {Explaining type errors in polymorphic languages},
  WKloc = {A-0525},
  journal = {ACM Letters on Programming Languages and Systems},
  volume = 2,
  number = 4,
  pages = {17--30},
  month = mar,
  year = 1993,
  coden = {ALPSE8},
  ISSN = {1057-4514},
  bibdate = {Thu May 30 15:54:54 MDT 1996},
  URL = {http://www.acm.org/pubs/toc/Abstracts/1057-4514/176460.html},
  abstract = {Strongly-typed languages present programmers with
                 compile-time feedback about the type correctness of
                 programs. Errors during polymorphic type checking take
                 the form of a unification failure for two types.
                 Finding the source of the type error in the code is
                 often difficult because the error may occur far from
                 the spot where the inconsistency is detected. As
                 functional languages use more and more complex type
                 systems, the difficulty of interpreting and locating
                 these errors will increase. To locate the source of
                 type errors the programmer must unravel the long chain
                 of deductions and type instantiations made during type
                 reconstruction. This paper describes an approach that
                 maintains the deductive steps of type inference and the
                 reasons for type instantiations. The approach could be
                 used in an interactive system to guide the programmer
                 to the source of a type error or to explain why the
                 compiler assigned a particular type to an expression.},
  acknowledgement = ack-nhfb,
  keywords = {LPLS, PLS, data, type, types, check, checking, error,
                 diagnose, diagnosis, languages},
  subject = {{\bf F.3.3}: Theory of Computation, LOGICS AND
                 MEANINGS OF PROGRAMS, Studies of Program Constructs,
                 Type structure. {\bf D.2.6}: Software, SOFTWARE
                 ENGINEERING, Programming Environments. {\bf D.3.3}:
                 Software, PROGRAMMING LANGUAGES, Language Constructs
                 and Features, Data types and structures.}
}

@PhDThesis{Beaver-1995,
  author = {Beaver, D.},
  school = {Univ.\null{}Edinburgh},
  title = {Presupposition and Assertion in Dynamic Semantics},
  year = 1995,
  bibliographies = {RelMiCS}
}

@InProceedings{Beck-1969,
  author = 	 {Jon Beck},
  title = 	 {Distributive laws},
  crossref =  {Triples1969},
  pages = 	 {119--140},
  annote = {monad composition conditions}
}

@InProceedings{Beck-1998,
  author = {Kent Beck},
  title = {Extreme Programming: A Humanistic Discipline of Software Development},
  booktitle = {FASE '98},
  series = {LNCS},
  volume = 1382,
  UniBwM = {INF460-YD4007},
  pages = {1--6},
  WKloc = {A-0659}
}

@Misc{Becker-2001,
  author = {J\"org D. Becker},
  title = {{Kann man die Quantenphysik verstehen?}},
  howpublished = {{Festschrift zur Ausstellung der Bibliothek der Universit\"at der Bundeswehr M\"unchen anl\"a\ss{}lich des 100.~Geburtstags von Werner Heisenberg}},
  month = DEC,
  year = 2001,
  WKloc = {C-0009}
}

@Misc{Becker-Hereth-2001,
  author = {Peter Becker and Joachim Hereth},
  title = {A Simplified Data Model for {CGs}},
  OPThowpublished = {},
  month = SEP,
  year = 2001,
  keywords = {CG, conceptual graph},
  URL = {http://tockit.sourceforge.net/papers/cgDataModel.pdf},
  WKloc = {A-1225, doc/pap/BIB},
  annote = {Date: Wed, 26 Sep 2001 18:42:45 +1000\\ From: Peter Becker
      (pbecker@meganesia.int.gu.edu.au)\\ To: CG List (cg@cs.uah.edu)\\
      Subject: CG: Proposal for a CG Data Model

      \bigskip \noindent Joachim Hereth and I discussed our view on CGs and
      came up with this data model:

      \begin{center}
      \textsf{http://tockit.sourceforge.net/papers/cgDataModel.pdf}
      \end{center} % This is based on the "Abstract Syntax" chapter in the
      former CG standard and on a number of papers from the ICCS
      proceedings of the last years (most noticeably the section on formal
      semantics in last years proceedings). \par \medskip

      It splits CGs into a very simple core model and a number of different
      extensions so subsets of the possible features can be selected for
      different applications. It should be formal enough to model the
      syntactical operations on CGs and formal semantics to construct a
      well-defined algebra for CGs (or multiple algebras for different
      purposes, e.g. with or without negation) as well as syntaxes. \par
      \medskip

      This is a first version and we would be happy to get any kind of
      feedback on it, esp. since we changed some things in a quite
      aggressive manner. Here are some of the most significant changes:

      \begin{itemize}

      \item catalog/support: our model starts with a catalog (a.k.a.
      support) with the hierarchies and the individuals (we call them
      instances). This is a notion already existent in the standard
      (module/knowledge base) but has to be explicit in the data model and
      in any algebra while a syntax can allow to define things implicitely,
      e.g. by assuming that [Person: \#John] should create the instance
      \#John if it does not yet exist in the catalog.

      \item no coreference links/sets: coreference links are useful for DF
      since they are more readable than drawing lines directly from one
      context to the outside, but they introduce problems with the
      semantics if they don't use a copy of the outer node or [$\top$: *]
      for the inner node. Even if one wants to have the option to combine
      two types, this should not be a side effect of the reference so we
      dropped the whole notion of coreference sets. The links in DF can be
      created using our data model by introducing additional nodes in the
      display.

      \item only one type of designator: the difference between the three
      types of locators seems to be only syntactical or unclear (see the
      discussion on the semantics of names) so we dropped this distinction.
      Literals are useful as representions for instances but hard to handle
      in the reasoning processes, so we dropped them completely, modelling
      representations as an additional mapping from the instances to
      encodings. A specific syntax can allow creating these representations
      on the fly, e.g. [Person: "John"] can create a new instance "\#84576"
      with a mapping to the string "John".

      \item relation signatures, relation hierarchies: the signatures on
      the relations are defined on the relation types (in terms of the CG
      standard), there is one hierarchy for each valence having a toplevel
      signature of ($\top$, ..., $\top$) and restricting the signature on
      the subrelation relation. \end{itemize}

      In addition to this we changed some names: \begin{itemize}

      \item ``individual'' $\rightarrow$ ``instance'': maybe it is only
      since we are Germans but ``individual of type mat'' sounds silly in
      German since individual has a connotation of sentient being

      \item ``relation'' $\rightarrow$ ``link'', ``relation type''
      $\rightarrow$ ``relation'': a relation in the mathematical sense is a
      set created of all ``relations'' as used in CGs. The type models the
      intensional aspects while all ``relations'' in the graphs together
      are the extensional part. To match this mathematical view we changed
      the terms to ``relation'' for the former relation types and ``link''
      for the parts of the graphs, the latter should be consistent with
      semantic networks

      \item ``concept'' $\rightarrow$ ``node'': this one is Jos fault :-)
      He convinced me that ``Person'' is more of a concept than ``[Person:
      \#John]'' which refers to one specific instance in the world. The
      term ``node'' matches again some of the work in the semantic networks
      area \end{itemize}

      \noindent We are aware that some of these changes are rather drastic,
      esp. for newcomers like us. Maybe we are wrong in a number of cases
      but at least it might start an interesting discussion ;-) We plan to
      update the paper later to accommodate to any input we get, hoping to
      create some anchor for discussions about a common data model. \par
      \medskip One last note: Jo is currently on holidays -- we originally
      decided to do things more slowly but the time to restart a discussion
      like this seemed to be appropriate now. Blame me and not Jo if you
      disagree with the timing. \par \medskip \bigskip Looking forward to
      your commments, \par \medskip

      Peter}
}

@InCollection{Beckert-1998,
  author	= {Bernhard Beckert},
  title		= {Rigid {$E$}-Unification},
  part  	= {2: Special Calculi and Refinements},
  volume        = {{I}: Foundations},
  pages         = {265--289},
  booktitle	= {Automated Deduction --- A Basis for Applications},
  editor	= {Wolfgang Bibel and Peter H. Schmitt},
  publisher	= {Kluwer, Dordrecht},
  year		= {1998},
  abstract = {By replacing syntactical unification with rigid
     E-unification, equality handling can be added to rigid variable
     calculi for first-order logic, including free variable tableau,
     the mating method, the connection method, and model elimination.

     Ground E-unification (i.e., E-unification with variable-free
     equalities) has long been known to be decidable. and classical
     universal E-unification has long been known to be undecidable.
     Rigid E-unification is in between: It is decidable in the simple,
     non-simultaneous case, but it is undecidable whether there is a
     simultaneous solution for several rigid E-unification problems,
     which is unfortunate as simultaneous rigid E-unification is of
     great importance for handling equality in automated theorem
     proving.

     In this chapter, we describe the basic idea of rigid
     E-unification and its importance for adding equality to rigid
     variable calculi and introduce syntax and semantics of
     first-order logic with equality.  We formally define
     (non-simultaneous) rigid E-unification and the notion of
     (minimal) complete sets of unifiers; and we briefly sketch proofs
     for the decidability of ground E-unification and --- based on
     this --- for rigid E-unification; methods for solving rigid
     E-unification problems are compared.  The problem of finding a
     simultaneous solution for several rigid E-unification problems is
     discussed; and mixed E-unification is introduced, that is a
     combination of classical and rigid E-unification. Using the
     example of free variable semantic tableaux, we show how rigid
     E-unification can be used to handle equality in a free variable
     calculus. Finally, we briefly summarize the properties of the
     different types of E-unification.}
}

@Article{Bednarek-Ulam-1976,
  author = {A. R. Bednarek  and Stanislaw M. Ulam},
  title = {Generators for Algebras of Relations},
  journal = BUAMS,
  volume = 82,
  year = 1976,
  pages = {781--782},
  bibliographies = {RelMiCS}
}

@InProceedings{Bednarek-Ulam-1977,
  author = {A. R. Bednarek  and Stanislaw M. Ulam},
  title = {Some Remarks on Relational Composition in Computational
		Theory and Practice},
  booktitle = {Fundamentals of Computational Theory, Proc.\null{}
		  of the Internat.\null{} FCT-Conf., 19-23 Sept.\null{}1977},
  series = LNCS,
  volume = 56,
  editor = {M. Karpinski},
  pages = {22--32},
  publisher = Springer,
  address = {Poznan-Konik},
  year = 1977,
  bibliographies = {RelMiCS}
}

@Article{Bednarek-Ulam-1978,
  author = {A. R. Bednarek  and Stanislaw M. Ulam},
  title = {Projective Algebras and the Calculus of Relations},
  journal = JSYLO,
  volume = 43,
  year = 1978,
  pages = {56--64},
  bibliographies = {RelMiCS}
}

@Booklet{Bednarek-Ulam-xxxx,
  author = {A. R. Bednarek  and Stanislaw M. Ulam},
  title = {On the Theory of Relational Structures and Schemata for
		Parallel Computation},
  note = {Informal Report},
  publisher = ALAMOS,
  bibliographies = {RelMiCS}
}

@Article{Beeri-Bernstein-1979,
  author = {C. Beeri and P.A. Bernstein},
  title = {Computational Problems Related to the
         Design of Normal Form Relational Schemes},
  journal = ACM-TDS,
  volume = 4,
  number = 1,
  year = 1979,
  pages = {30--59},
  bibliographies = {RelMiCS}
}

@InProceedings{Beeri-Mendelzon-Sagiv-Ullmann-1979,
  author = {A. O. Beeri and A. O. Mendelzon and Y. Sagiv and J.D. Ullman},
  title = {Equivalence of Relational Database Schemes},
  booktitle = {{$11^{th}$} Annual ACM Sympos.\null{} on Theory of Computing},
  year = 1979,
  month = may,
  address = {},
  pages = {319--329},
  bibliographies = {RelMiCS}
}

@Book{Beeson-1985,
  author = 	 {Michael J. Beeson},
  title = 	 {Foundations of Constructive Mathematics},
  publisher = 	 Springer,
  year = 	 1985,
  ISBN = {978-3-642-68954-3 (Print) 978-3-642-68952-9 (Online)},
  SpringerURL =  {http://link.springer.com/book/10.1007\%2F978-3-642-68952-9},
  volume = 	 6,
  series = 	 {Ergebnisse der Mathematik und ihrer Grenzgebiete 3.\null{} Folge:
                 A Series of Modern Surveys in Mathematics},
  McMaster = 	 {QA 9.56 .B44 1984},
  annote = 	 {Martin L\"of},
  bibliographies = {DepTyp}
}

@InProceedings{Beeson-1998,
  author = {Michael Beeson},
  title = {Unification on lambda-Calculi with if-then-else},
  crossref = {CADE1998},
  pages = {103--118},
  OPTabstract = {},
  WKloc = {A-0605}
}

@TechReport{Begwani-1982,
  author = {V. S. Begwani},
  title = {New Approach for Attribute Evaluation and Error
                 Correction in Compilers},
  institution = {Comp. Sc. Department, University of
                 Wisconsin-Madison},
  type = {Technical Report},
  number = {\#483},
  year = 1982,
  keywords = {eval}
}

@Book{Behnke-Bachmann-Fladt-Suess-1966,
  UniBwM = {I-LB490-1 (Grun)},
  keywords = {lattice lattices},
  contents = {A: H. Hermes, W. Markwald: Grundlagen der Mathematik
		B: Arithmetik und Algebra
		W. Gr\"obner: Einleitung
		 1 G. Pickert, L. G\"orke: Aufbau des Systems der
		  reellen Zahlen
		 Anhang zu 1: D. Kurepa, A. Aymanns: Ordinalzahlen
		 2 W. Gasch\"utz, H.Noack: Gruppen
		 3 H. Gericke, H. W\"asche: Lineare Algebra
		 4 G. Pickert, W.R\"uckert: Polynome
		 5 W. Gr\"obner, P. Lesky: Ringe und Ideale
		 6 H.-H. Ostmann, H. Liermann: Zahlentheorie
		 7 O. Haupt, P. Sengenhorst: Algebraische
		  K\"orpererweiterung
		 8 G. Pickert, H.-G. Steiner: Komplexe Zahlen und
		  Quaternionen
		 9 H. Gericke, H. Martens: Verb\"ande
		10 H. Gericke, H. Martens: Einige Grundbegriffe der
		  Strukturtheorie
		11 F. Bachmann, H. Wolff, H. Noack: Zornsches Lemma
		  und Hochkettenprinzip
		Literaturverzeichnis zu Teil B, Kap. 1 bis 10
		Stichwortverzeichnis
		Zeittafel},
  year = 1966,
  volume = {I},
  title = {{Grundlagen der Mathematik, Arithmetik und Algebra}},
  series = Grundz,
  publisher = Vanden,
  editor = {H. Behnke and F. Bachmann and K. Fladt and W. S{\"u}ss},
  address = {G\"ottingen},
  bibliographies = {RelMiCS}
}

@TechReport{Behnke-Berghammer-Schneider-1997,
  author = {Ralf Behnke and Rudolf Berghammer and Peter Schneider},
  title = {Machine Support of Relational Computations: {The} {Kiel} {RELVIEW} System},
  institution = {Institut f\"ur Informatik und Praktische Mathematik, Christian-Albrechts-Universit\"at Kiel},
  year = 1997,
  month = JUN,
  number = 9711,
  bibliographies = {RelMiCS}
}

@InProceedings{Behringer-vonHolt-Dickmanns-1992,
  year = 1992,
  title = {Road and relative ego--state recognition},
  month = {July},
  booktitle = {Proc. Conf. on Intelligent Vehicles, Detroit},
  author = {Behringer, Reinhold and von Holt, Volker
             and Dickmanns, Dirk}
}

@InProceedings{Belkhir-Nemouche-1994,
  author = {Abdelkader Belkhir and Namick Nemouche},
  title = {Towards Integrating Functional and Logic styles
		  using Relations},
  crossref = {PLILP1994},
  pages = {463--464},
  WKloc = {A-0303},
  bibliographies = {RelMiCS}
}

@Article{Belkhiter-Bourhfir-Gammoudi-Jaoua-leThanh-Reguig-1994,
  author = {N. Belkhiter and C. Bourhfir and M. M. Gammoudi and Ali Jaoua
      and le Thanh, N. and M. Reguig},
  title = {D{\'e}composition Rectangulaire Optimale d'une Relation
      Binaire: Application aux Bases de Donn{\'e}es Documentaires},
  journal = INFOSCIOPRES,
  volume = 32,
  year = 1994,
  pages = {34--54},
  bibliographies = {RelMiCS}
}

@InProceedings{Belkhiter-Desharnais-Jaoua-Moukam-1993,
  author = {N. Belkhiter and J. Desharnais and Ali Jaoua and
                  T. Moukam},
  title = {Providing Relevant Additional Information to Users
                  Asking Queries Using a {G}alois Lattice Structure},
  booktitle = {{$8^{th}$} {IEEE} Internat.\null{} Sympos.\null{} on
                  Computer and Information Sciences {(ISCIS-8), Istanbul}},
  year = 1993,
  month = NOV,
  pages = {594--604},
  bibliographies = {RelMiCS}
}

@InProceedings{Belkhiter-Jaoua-Desharnais-Ennis-Ounalli-Gammoudi-1994,
  author = {Nadir Belkhiter and Ali Jaoua and Jules Desharnais
                  and Guy Ennis and Habib Ounalli and Mohamed Moshen
                  Gammoudi.},
  title = {Formal Properties of Rectangular Relations},
  booktitle = {{$9^{th}$ Internat.\null{} Sympos.\null{} on Computer
                  and Information Sciences, Antalya}},
  year = 1994,
  pages = {310--318},
  address = {Antalya},
  month = NOV,
  bibliographies = {RelMiCS}
}

@Booklet{Bell-1987,
  author = {C. E. Bell},
  title = {Representing and Reasoning with Disjunctive
		Temporal Constraints in a Point-based Model},
  note = {Preprint, Univ.\null{} of Iowa,
		Dept.\null{} of Management Sciences, 1987},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Book{Bell-Harari-1999,
  author = {Chip Bell and Oren Harari},
  title = {{BEEP! BEEP! Competing in the Age of the Road Runner}},
  publisher = {Warner Books},
  year = {1999 or 2000?}
}

@Book{Bell-Newell-1971,
  author = {C. Gordon Bell and Allen Newell},
  title = {Computer Structures: Readings and Examples},
  publisher = {McGraw-Hill},
  year = 1971,
  LOC = {75-0109245},
  McMaster = {TK 7888.3 .B37},
  annote = {Chapter 33: The IBM 1800, p.~399--420},
  bibliographies = {SQRL}
}

@InProceedings{Belle-Jay-Moggi-1996,
  author = {G. Bell{\'e} and C. B. Jay and Eugenio Moggi},
  title = {Functorial {ML}},
  crossref = {PLILP1996},
  pages = {32--46},
  OPTabstract = {},
  WKloc = {A-0443}
}

@InProceedings{Bellegarde-1993,
  author = {F. Bellegarde},
  title = {A transformation system combining partial
		  evaluation with term rewriting},
  pages = {40--55},
  crossref = {HOA1993},
  WKloc = {A-0116},
  abstract = {This paper presents a new approach to optimizing
		  functional programs based on partial evaluation and
		  rewriting. Programs are composed of higher-order
		  primitives. Partial evaluation is used to eliminate
		  higher-order functions. First-order rewriting is
		  used to process the transformation. Laws about the
		  higher-order primitives that are relevant for the
		  optimizations are automatically extracted from a
		  library and transformed into first-order terms using
		  partial evaluation. Such a combination of a partial
		  evaluation system and an intrinsically first-order
		  rewriting tool allows a form of higher-order
		  rewriting at a first-order level. This way, it is
		  possible to automate deforestation of higher-order programs.}
}

@InProceedings{Bellegarde-1995,
  author = {F. Bellegarde},
  title = {{ASTRE}: {Towards} a Fully Automated Program Transformation System},
  crossref = {RTA95},
  pages = {403--407},
  OPTabstract = {},
  WKloc = {A-0561}
}

@InProceedings{Bellegarde-Lescanne-1987,
  author = {F. Bellegarde and P. Lescanne},
  title = {Transformation Ordering},
  crossref = {TAPSOFT1987Vol1},
  pages = {69-80},
  abstract = {We define an ordering called transformation ordering which is useful for proving termination of rewriting systems. $\ldots$},
  UniBwM = {INF460/Z7405-2,1},
  WKloc = {A-0598 (first page only)}
}

@InProceedings{BellensP-PerezJM-BadiaRM-LabartaJ-CellBEArchitecture_2006,
  author = {Bellens, Pieter and Perez, Josep M. and Badia, Rosa M. and Labarta, Jesus},
  booktitle = {SC 2006},
  conflocation = {Tampa, Florida, USA},
  published = {IEEE},
  month = NOV,
  ISBN = {0-7695-2700-0/06},
  title = {{CellSs}: A Programming Model for the {Cell BE} Architecture},
  year = {2006},
  WKloc = {A-1698},
  bibliographies = {Coconut}
}

@Article{Bellia-Levi-1986,
  WKloc = {A-0061},
  abstract = {The paper considers different methods of integrating
		  the functional and logic programming paradigms,
		  starting with the identification of their semantic
		  differences. The main method to extend functional
		  programs with logic features (i.e.~unification) are
		  then considered. These iclude narrowing, completion,
		  SLD-resolution of equational formulas, and set
		  abstraction. The different techniques are analyzed
		  from several viewpoints, including the ability to
		  support both paradigms, lazy evaluation, and concurrency.},
  year = 1986,
  volume = 3,
  title = {The Relation Between Logic and Functional Languages: A Survey},
  pages = {217--236},
  journal = JLOG,
  author = {Marco Bellia and Giorgio Levi},
  bibliographies = {RelMiCS}
}

@Article{Bellin-Scott-1994,
  author = {Gianluigi Bellin and Philip J. Scott},
  title = {On the $\pi$-Calculus and Linear Logic},
  journal = {Theoretical Computer Science},
  year = 1994,
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  note = {Proc.\null{} MFPS8 1994},
  WKloc = {A-0923},
  abstract = {We detail Abramsky's ``proofs-as-processes'' paradigm for
      interpreting classical linear logic (CLL) [13] into a ``synchronous''
      version of the $\pi$-calculus recently proposed by Milner [28][27].
      The translation is given at the abstract level of proof structures.
      We give a detailed treatment of information flow in proof-nets and
      show how to mirror various evaluation strategies for proof
      normalization. We also give Soundness and Completeness results for
      the process-calculus translations of various fragments of CLL. The
      paper also gives a self-contained introduction to some of the deeper
      proof-theory of CLL, and its process interpretation.},
  URL = {http://www.site.uottawa.ca/~phil/extra/papers/PiCalcpaper/PiCalcpaper.html},
  OPTannote = {}
}

@InProceedings{Bellot-Jay-1987,
  author = {P. Bellot and V. Jay},
  title = {A Theory for Natural Modelisation and Implementation
		  of Functions with Variable Arity},
  crossref = {FPCA-1987},
  pages = {212--233},
  abstract = {The aim of this article is to provide a new
		  theoretical framework based on combinators for the
		  study and implementation of applicative programming
		  languages. This formal theory can be viewed as a
		  Computability theory where functions are defined in
		  a natural and usable way because Curryfication is
		  abolished.},
  keywords = {combinatory logic tg tge}
}

@InProceedings{Bellot-Jay-1987-x,
  author = {P. Bellot and V. Jay},
  title = {A Theory for Natural Modelisation and Implementation
		  of Functions with Variable Arity},
  volume = 274,
  series = {LNCS},
  pages = {212--233},
  editor = {G. Kahn},
  booktitle = {Functional Programming Languages and Computer
		  Architecture; Proceedings of Conference held at
		  {Portland, OR.}},
  publisher = {Springer-Verlag},
  abstract = {The aim of this article is to provide a new
		  theoretical framework based on combinators for the
		  study and implementation of applicative programming
		  languages. This formal theory can be viewed as a
		  Computability theory where functions are defined in
		  a natural and usable way because Curryfication is
		  abolished.},
  keywords = {combinatory logic tg tge}
}

@Book{BenAri-1990,
  author = {Ben-Ari, M.},
  address = {New York},
  publisher = Prentice,
  title = {Principles of Concurrent and Distributed Programming},
  year = 1990,
  bibliographies = {RelMiCS}
}

@InProceedings{BenDavid-BenEliyahu-1994,
  title = {a modal logic for subjective default reasoning},
  author = {Shai Ben-David and Rachel Ben-Eliyahu},
  pages = {477--486},
  crossref = {LICS9},
  abstract = {We introduce a logic endowed with a two-place modal
      connective that has the intended meaning of ``if~$\alpha$'', then
      normally~$\beta$''. On top of providing a well-defined tool for
      analyzing common default reasoning, such a logic allows nesting of
      the default operator. We present a semantic framework in which many
      of the known default proof systems can be naturally characterized,
      and prove soundness and completeness theorems for several such proof
      systems. \par Our semantics is a ``neighborhood modal semantics,''
      and it allows for {\em subjective\/} defaults, that is, defaults may
      vary within different worlds that belong to the same model. The
      semantics has an appealing intuitive interpretation and may be viewed
      as a set-theoretic generalization of the probabilistic
      interpretations of default reasoning. \par We show that our semantics
      is general in the sense that any modal semantics that is sound for
      some basic axioms for default reasoning is a special case of our
      semantics. Such a generality result may serve to provide a semantical
      analysis of the relative strength of different proof systems and to
      show the nonexistence of semantics with certain properties.}
}

@Article{BenYahia-Ounalli-Jaoua-1999,
  author = {S. Ben Yahia and H. Ounalli and Ali Jaoua},
  title = {An extension of classical functional dependency:
                  dynamic fuzzy functional dependency},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {219--234},
  bibliographies = {RelMiCS},
  abstract = {Relational data model has constituted an incontestable
      success in database history. In this context, a lot of attention has
      been paid to functional dependencies due to their paramount
      importance in the design of relational database. For about fifteen
      years, several attempts to formalize (soft) real world constraints
      imposed on the data has been made, leading to the emergence of the
      concept of fuzzy functional dependency. In this paper, an overview of
      the different proposals of the fuzzy functional dependency is
      presented. A new extension of classical functional dependency based
      on the Lukasiewicz implication is presented and called dynamic fuzzy
      functional dependency. The associated axiomatic system is introduced
      and proved to be sound.},
  keywords = {Fuzzy relational data model; Fuzzy functional dependency; Lukasiewicz implication},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/31/abstract.html},
  WKloc = {A-0655}
}

@Article{Benabou-1963,
  author = {Jean B{\'e}nabou},
  title = {Cat{\'e}gories avec multiplication},
  journal = {C.~R.~Acad.~Sci.~Paris},
  volume = 256,
  pages = {1887--1890},
  year = 1963,
  annote = {Together with \cite{MacLane-1963b}
            origin of monoidal categories.}
}

@PhDThesis{Benabou-1966,
  author = {Jean B{\'e}nabou},
  title = {Structures alg{\'e}briques dans les cat{\'e}gories},
  school = {Universit{\'e} de Paris},
  month = mar,
  year = 1966
}

@InProceedings{Benabou-1967,
  author =       {Jean B{\'e}nabou},
  title =        {Introduction to Bicategories},
  crossref =  {MidwestCatSem1967},
  pages =     {1--77},
  DOI = {10.1007/BFb0074298},
  URL = {http://www.springerlink.com/content/c584758685011767/},
  WKloc =      {doc/pap/BIB}
}

@Article{Benabou-1968,
  author = {Jean B{\'e}nabou},
  title = {Structures Algebriques dans les Cat\'egories},
  journal = {Cahiers de Topologie et G\'eo\-m\'etrie Diff\'erentielle},
  year = 1968,
  volume = 10,
  number = 1,
  pages = {1--126}
}

@Article{Benabou-1985,
  author = {Jean B\'{e}nabou},
  title = {Fibered Categories and the Foundations of Naive
                 Category Theory},
  journal = {Journal of Symbolic Logic},
  volume = 50,
  number = 1,
  pages = {10--37},
  checked = {Yes},
  month = mar,
  year = 1985
}

@TechReport{Benaissa-Lescanne-Rose-1996,
  author = {Zine-El-Abidine Benaissa and Pierre Lescanne and
		  Kristoffer H. Rose},
  title = {Modeling Sharing and Recursion for Weak Reduction
		  Strategies using Explicit Substitution},
  WKloc = {A-0490},
  institution = {BRICS},
  year = 1996,
  type = {Research Report},
  number = {RS-1996-56},
  month = DEC,
  abstract = {We present the $\lambda\sigma$-calculus, a formal
		  synthesis of the concepts of sharing and explicit
		  substitution for weak reduction. We show how
		  $\lambda\sigma$ can be used as a foundation of
		  implementations of functional programming languages
		  by modeling the essential ingredients of such
		  implementations, namely weak reduction strategies,
		  recursion, space leaks, recursive data structures,
		  and parallel evaluation, in a uniform way.

		  First, we give a precise account of the major
		  reduction strategies used in functional programming
		  and the consequences of choosing
		  $\lambda$-graph-reduction vs. environment-based
		  evaluation. Second, we show how to add constructors
		  and explicit recursion to give a precise account of
		  recursive functions and data structures even with
		  respect to space complexity. Third, we formalize the
		  notion of space leaks in $\lambda\sigma$ and use
		  this to define a space leak free calculus; this
		  suggests optimisations for call-by-need reduction
		  that prevent space leaking and enables us to prove
		  that the ``trimming'' performed by the STG machine
		  does not leak space.

		  In summary we give a formal account of several
		  implementation techniques used by state of the art
		  implementations of functional programming
		  languages},
  note = {also INRIA RR.~3092, Jan.~1997}
}

@InProceedings{Benaissa-Lescanne-Rose-1996a,
  author = {Zine-El-Abidine Benaissa and Pierre Lescanne and
		  Kristoffer H. Rose},
  title = {Modeling Sharing and Recursion for Weak Reduction
		  Strategies using Explicit Substitution},
  crossref = {PLILP1996},
  pages = {363--407},
  OPTabstract = {},
  WKloc = {A-0447}
}

@Misc{Benaissa-Moggi-Taha-Sheard-199X,
  author = {Zine-El-Abidine Benaissa and Eugenio Moggi and Walid Taha and Tim Sheard},
  title = {A Categorical Analysis of Multi-Level Languages (Extended Abstract)},
  year = {199?},
  WKloc = {A-0634}
}

@Article{Benanav-Kapur-Narendran-1987,
  year = 1987,
  volume = 3,
  title = {Complexity of Matching Problems},
  pages = {203--216},
  number = {1\&2},
  month = {February/April},
  journal = JSYCO,
  author = {D. Benanav and D. Kapur and P. Narendran},
  bibliographies = {RelMiCS}
}

@Book{Bennighofen-Kemmerich-Richter-1987,
  UniBwM = {INF700/R1420},
  contents = {1 General Concepts from Universal Algebra
	1.1 Algebras, Terms and Substitutions
	1.2 Some Concepts of the Theory of Formal Languages and Automata
	1.3 Decidability
	2 Finite Sets of Reductions
	2.1 First Concepts
	2.2 The Completion Algorithm
	2.3 The Ground Case
	2.4 First Analysis of the Completion Algorithm
	2.5 The Special Word Problem for Groups and Small Cancellation Theory
	2.5.1 Superpos-Deduction-Chains and Criteria for the Solvability
		of the Word Problem
	2.5.2 The Small Cancellation Conditions and the Condition K
	2.6 Relations between the Completion Procedure and the
		Todd-Coxeter Algorithm
	3 Infinite Sets of Reductions
	3.1 Regular Systems
	3.1.1 Regular Systems as Special Infinite Systems
	3.1.2 Application of Regular Reduction Systems to Sets of Words
	3.1.3 The Undecidability of the Church-Rosser Property
	3.1.4 A Possible Church-Rosser Test
	3.2 Forward-Backward Systems
	3.3 The Church-Rosser Property of Forward-Backward Systems
	4 Automata and Reductions
	4.1 General Aspects
	4.2 The Complexity of Reduction Algorithms
	4.3 The Cycle Structure and the Growth Function
	4.4 Effective Aspects of Gromov's Theorem
	4.5 A Relation between the Growth Function and the Completion Algorithm
	5 Deciding Algebraic Properties of Finitely Presented Monoids (by
		Friedrich Otto)
	5.1 Monoid Presentations and Tietze Transformations
	5.2 Markov Properties of Finitely Presented Monoids
	5.3 Automata for Reduction Systems
	5.4 Deciding Algebraic Properties of Monoids Presented by Finite
		Complete Reduction Systems
	5.5 Deciding Algebraic Properties of Monoids Presented by Finite
		Monadic Reduction Systems
	References
	Subject Index
	List of Symbols and Abbreviations},
  year = 1987,
  volume = 277,
  title = {Systems of Reductions},
  series = LNCS,
  publisher = Springer,
  author = {B. Benninghofen and S. Kemmerich and M. M. Richter},
  bibliographies = {RelMiCS}
}

@Book{Bentley-2000,
  author =	 {Jon Bentley},
  title = 	 {Programming Pearls},
  publisher = 	 {Addison-Wesley},
  year = 	 2000,
  edition =	 {Second Edition},
  ISBN = 	 {0-201-65788-0},
  URL = 	 {http://netlib.bell-labs.com/cm/cs/pearls/},
  bibliographies = {SE2S}
}

@InProceedings{Benton-2004,
  author = 	 {Nick Benton},
  title = 	 {Simple Relational Correctness Proofs for Static Analyses and Program Transformations.},
  crossref =  {POPL2004},
  OPTpages = 	 {},
  WKloc = 	 {A-1605, doc/pap/BIB},
  bibliographies = {RelMiCS, OPG},
  abstract = {We show how some classical static analyses for
     imperative programs, and the optimizing transformations which
     they enable, may be expressed and proved correct using elementary
     logical and denotational techniques. The key ingredients are an
     interpretation of program properties as relations, rather than
     predicates, and a realization that although many program analyses
     are traditionally formulated in very intensional terms, the
     associated transformations are actually enabled by more liberal
     extensional properties.

     We illustrate our approach with formal systems for analysing and
     transforming while-programs. The first is a simple type system
     which tracks constancy and dependency information and can be used
     to perform dead-code elimination, constant propagation and
     program slicing as well as capturing a form of secure information
     flow. The second is a relational version of Hoare logic, which
     significantly generalizes our first type system and can also
     justify optimizations including hoisting loop invariants. Finally
     we show how a simple available expression analysis and redundancy
     elimination transformation may be justified by translation into
     relational Hoare logic.}
}

@Article{Benton-2005,
  author = 	 {Nick Benton},
  title = 	 {Embedded Interpreters},
  journal = 	 JFP,
  year = 	 {2005},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  URL = 	 {http://research.microsoft.com/~nick/publications.htm},
  WKloc = {A-1593, doc/pap/BIB}
}

@InProceedings{Benton-Bierman-dePaiva-Hyland-1993,
  author = {Nick Benton and Gavin Bierman and Valeria de Paiva and Martin
           Hyland},
  title = {A Term Calculus for Intuitionistic Linear Logic},
  pages = {75--90},
  abstract = {In this paper we consider the problem of deriving a term
             assignment system for Girard's Intuitionistic Linear Logic for
             both the sequent calculus and natural deduction proof systems.
             Our system differs from previous calculi (e.g. that of Abramsky
             [1]) and has two important properties which they lack. These
             are the {\em substitution property} (the set of valid deductions is
             closed under substitution) and {\em subject reduction}
             (reduction on
             terms is well-typed). We also consider term reduction arising
             from cut-elimination in the sequent calculus and normalisation
             in natural deduction. We explore the relationship between these
             and consider their computational content.},
  crossref = {TLCA93},
  WKloc = {A-0180}
}

@Article{Benton-Hyland-2003,
  author = 	 {Nick Benton and John Hyland},
  title = 	 {Traced Premonoidal Categories},
  journal = 	 {Theoretical Informatics and Applications},
  year = 	 {2003},
  OPTkey = 	 {},
  OPTvolume = 	 {37},
  OPTnumber = 	 {4},
  OPTpages = 	 {273-299},
  WKloc = 	 {A-1604, doc/pap/BIB},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@TechReport{Benveniste-1990,
  year = 1990,
  type = {Rapport de recherche},
  title = {Operational Semantics of a Distributed Object-Oriented Language and its {Z} Formal Specification},
  number = 1230,
  month = MAY,
  institution = INRIA,
  author = {Marc Benveniste},
  address = {France},
  bibliographies = {RelMiCS}
}

@InProceedings{Benzmueller-Kohlhase-1998,
  author = {Christoph Benzm{\"u}ller and Michael Kohlhase},
  title = {Extensional Higher-Order Resolution},
  crossref = {CADE1998},
  pages = {56--71},
  OPTabstract = {},
  WKloc = {A-0602}
}

@InProceedings{Benzmueller-Kohlhase-1998a,
  author = {Christoph Benzm{\"u}ller and Michael Kohlhase},
  title = {System Description: {\sc LEO} --- A Higher-Order Theorem Prover},
  crossref = {CADE1998},
  pages = {139--143},
  OPTabstract = {},
  WKloc = {A-0609}
}

@Book{Berard-Bidoit-Finkel-Laroussinie-Petit-Petrucci-Schnoebelen-1999,
  author =	 {B. B{\'e}rard and M. Bidoit and A. Finkel and F. Laroussinie
              and A. Petit and L. Petrucci and P. Schnoebelen},
  title = 	 {Systems and Software Verification, Model-Checking and Tools},
  publisher = 	 Springer,
  year = 	 1999,
  ISBN = 	 {3-540-41523-8},
  WKloc = 	 {owned, \lent{Ji Xiaoheng}}
}

@InProceedings{Bercoff-1995,
  author = {Ch. Bercoff},
  title = {A Family of Tag Systems for Paperfolding Sequences},
  crossref = {STACS1995},
  pages = {303--312},
  keywords = {automata and formal languages, combinatorics on
		  words, dragon curves},
  abstract = {If one folds in two parts a strip of paper several
		  times on itself (all folds being parallel) one
		  pbtains after unfolding a sequence of ``valley'' and
		  ``ridge'' folds. If one codes these folds over a
		  two-letter alphabet, one obtains a paperfolding word
		  associated to the sequence of folding
		  instructions. A paperfolding sequence is an infinite
		  paperfolding word.

                  This paper is devoted to the effective construction
		  of 2-uniform tg systems which generate every
		  paperfolding sequence associated to ultimately
		  periodic sequences of (un)folding instructions.}
}

@InProceedings{Bercovici-1985,
  WKloc = {A-0077},
  abstract = {We consider the finitely typed lambda calculus with
		  ``fixed-point'' combinators $Y_\sigma$ of each type
		  $(\sigma \tfun \sigma) \tfun \sigma$ satisfying the
		  equation $$ Y = \lambda f:\sigma \tfun \sigma . f(Y f).$$
                  This formal system models computations with
		  recursively defined equations. The decision problem
		  for equality of terms of this calculus is open. We
		  present a procedure for deciding when a
		  $\lambda-Y$-term is ``unsolvable''; this implies
		  decidability of equations between $\lambda-Y$-terms
		  and $\lambda$-terms without Y's. We also give tight
		  characterizations of unsolvable terms under certain
		  syntactic constraints.},
  title = {Unsolvable Terms in Typed lambda Calculus with
		  Fix-Point Operators: Extended Abstract},
  pages = {16--22},
  crossref = {LOP1985},
  author = {Irina Bercovici}
}

@Book{Berge-1973,
  author = {Claude Berge},
  title = {Graphs and Hypergraphs},
  publisher = NoHo,
  year = 1973,
  UniBwM = {MAT880/D13495-(2)},
  contents = {Part I: Graphs\\
               1. Basic Concepts\\
               2. Cyclomatic Number\\
               3. Trees and Arborescences\\
               4. Paths, Centres and Diameters\\
               5. Flow Problems\\
               6. Degrees and Demi-Degrees\\
               7. Matchings\\
               8. {\bf c}-Matchings\\
               9. Connectivity\\
               10. Hamiltonian Cycles\\
               11. Covering Edges with Chains\\
               12. Chromatic Index\\
               13. Stability Number\\
               14. Kernels and Grundy Functions\\
               15. Chromatic Number\\
               16. Perfect Graphs\\
               Part II: Hypergraphs \\
               17. Hypergraphs and Their Duals\\
               18. Transversals\\
               19. Chromatic Number of a Hypergraphs\\
               20. Balanced Hypergraphs and Unimodular Hypergraphs\\
               21. Matroids},
  bibliographies = {RelMiCS}
}

@Book{Berge-1989,
  author = {Claude Berge},
  title = {Hypergraphs, Combinatorics of Finite Sets},
  publisher = NoHo,
  year = 1989,
  UniBwM = {MAT880/T5040},
  contents = {1. General concepts\\
               2. Transversal sets and matchings\\
               3. Fractional transversals\\
               4. Colourings\\
               5. Hypergraphs generalising bipartite graphs\\
               5.1. Hypergraphs without odd cycles\\
               5.2. Unimodular hypergraphs\\
               5.3. Balanced hypergraphs\\
               5.4. Arboreal hypergraphs\\
               5.5. Normal hypergraphs\\
               5.6. Mengerian hypergraphs\\
               5.7. Paranormal hypergraphs\\
               Appendix: Matchings and colourings in matroids},
  bibliographies = {RelMiCS}
}

@TechReport{Berger-1992,
  author = {Emery Berger},
  title = {FP + OOP = Haskell},
  year = 1992,
  month = MAR,
  institution = {University of Texas at Austin},
  WKloc = {A-0268},
  file = {~kahl/doc/pap/BIB},
  keywords = {functional object oriented},
  abstract = {Haskell adds O-O functionality (using a concept
		  known as type classes) to a pure functional
		  programming framework. This paper describes and
		  analyses accomplishments and problems.}
}

@Misc{Berger-McKinley-Blumofe-Wilson-2000,
  author = {Emery D. Berger and Kathryn S. McKinley and Robert D. Blumofe and Paul R. Wilson},
  title = {Hoard: A Scalable Memory Allocator for Multithreaded Applications},
  year = 2000,
  WKloc = {A-1226, doc/pap/BIB},
  URL = {http://www.cs.utexas.edu/users/oops/HoardASPLOS.ps}
}

@InProceedings{Bergeron-1995,
  author = {M. Bergeron and W. S. Hatcher},
  title = {Models of Linear Logic},
  booktitle = {Zapiski Nauchnykh Seminarov Peterburg. Otdel. Mat. Inst.
     Steklov (POMI) (Proc.\null{} of the Steklov Inst.\null{} of Mathematics, St.\null{}
     Petersburg Branch)},
  year = 1995,
  volume = 220,
  pages = {23--35},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-1989,
  year = 1989,
  title = {A Mathematical Basis for Nondeterministic Unfold/Fold},
  series = {Bericht Nr. 10/89},
  publisher = {Fakult\"at f\"ur Informatik, Universit\"at Karlsruhe},
  pages = {46--48},
  month = {apr},
  editor = {U. Furbach and M. Heisel and W. Reif and W. Stephan},
  booktitle = {{Proceedings Workshop ``Verification, Konstruktion
		  und Synthese von Programmen''}},
  author = {Rudolf Berghammer},
  address = {Karlsruhe},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-1989a,
  year = 1989,
  title = {{Eine \"Ubertragung des Park'schen Lemmas auf die
		  Abk\"ommlingsrelation}},
  publisher = {Institut f\"ur Mathematik, Universit\"at Augsburg},
  note = {Report Nr. 214},
  pages = {62--65},
  editor = {W. Dosch},
  booktitle = {Proc.\null{} of {Arbeitstreffen ``Logische und funktionale
		  Programmierung --- Sprachen, Methoden, Implementierungen''}},
  author = {Rudolf Berghammer},
  address = {Hirschegg/Kleinwalsertal},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-1990,
  year = 1990,
  title = {{Zur Beschreibung der ganzen Zahlen als
		  termerzeugtes Modell einer Theorie der
		  Pr\"adikatenlogik erster Stufe}},
  number = 9012,
  institution = {{Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen}},
  author = {Rudolf Berghammer},
  bibliographies = {RelMiCS}
}

@PhDThesis{Berghammer-1990-Habil,
  year = 1990,
  type = {Habilitationsschrift},
  title = {Transformational Programming with Non-deterministic
		  and Higher-order Constructs},
  school = {{Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen}},
  note = {auch als Bericht Nr.\null{} 9012},
  author = {Rudolf Berghammer},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-1991,
  WKloc = {TR},
  CONTENTS = {1 Introduction
		2 Relation Algebraic Preliminaries
		3 Relational Specifications
		4 A Sampler of Simple Relational Specifications
		5 Generic Constructions on Direct Products
		6 Transformation of Relational Specifications
		7 Combining Angelic and Demonic Non-determinism
		8 Concluding Remarks
		9 References},
  abstract = {Abstract relation algebra is proposed as a practical
		  means for specification of data types and programs.
		  We define the concept of a relational specification
		  by transferring some fundamental notions of the
		  algebraic specification approach to the relational
		  case. Furthermore, we demonstrate the usefulness of
		  the relational approach and give an impression of
		  relational calculations in the field of
		  specifications by means of some examples. We treat
		  generic constructions on direct products, the
		  transformation of specifications, and
		  non-determinism in more detail and show e.g., that
		  relational specifications easily can deal with
		  angelic and demonic non-determinism within a single
		  context.},
  year = 1991,
  type = {Tech.\null{} Report},
  title = {Relational Specification of Data Types and Programs},
  number = 9109,
  month = SEP,
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen},
  author = {Rudolf Berghammer},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-1991a,
  title = {Codifying the Differencing Technique into Formal
		  Transformation Rules over {CIP-L}},
  author = {Rudolf Berghammer},
  crossref = {Broy-1991},
  pages = {406--418},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-1991a-x,
  year = 1991,
  title = {Codifying the differencing technique into formal
		  transformation rules over {CIP-L}},
  publisher = {Springer},
  editor = {M. Broy},
  booktitle = {Informatik und Mathematik. Proc. Kolloquium
		  ``Informatik im Kreuzungspunkt von Numerischer
		  Mathematik, Rechnerentwurf, Programmierung, Algebra
		  und Logik'' 12.--14.6.~1989},
  author = {Rudolf Berghammer},
  address = {M\"unchen}
}

@InProceedings{Berghammer-1992,
  year = 1992,
  title = {On the characterization of the integers: The hidden
		  function problem revisited},
  series = {Bericht 7/92-I},
  publisher = {Institut f\"ur Angewandte Mathematik und Informatik,
		  Universit\"at M\"unster},
  pages = {82--92},
  month = JAN,
  editor = {Wolfram-M. Lippe and Gudrun Stroot},
  booktitle = {{Proc.\null{} Workshop ``Programmiersprachen --- Methoden,
		  Semantik, Implementierungen''}},
  author = {Rudolf Berghammer},
  address = {Landhaus Rothenberge, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-1992cut,
  abstract = {The RELVIEW-system is a totally interactive and
		  completely video-oriented computer system for the
		  manipulation of concrete relations. This paper
		  contains a brief description of the present version
		  2.0 of the RELVIEW-system and describes an
		  application, viz.~the computation of the cut
		  completion of a partially ordered set. Also the main
		  topics of future work on RELVIEW are sketched.},
  year = 1992,
  title = {Computing the Cut Completion of a Partially Ordered
		  Set --- An Example for the Use of the {RELVIEW}-System},
  number = 9205,
  month = JUL,
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen},
  author = {Rudolf Berghammer},
  bibliographies = {RelMiCS}
}

@Article{Berghammer-1996,
  author = {Rudolf Berghammer},
  title = {Wiederverwendbare {A}lgorithmenschemata in {ML} am
                 {B}eispiel von {G}raphdurchlauf-{P}roblemen},
  journal = {Informatik, Forschung und Ent\-wick\-lung},
  year = 1996,
  volume = 11,
  number = 4,
  pages = {179--190},
  month = NOV
}

@Article{Berghammer-1999,
  author = {Rudolf Berghammer},
  title = {Combining relational calculus and the Dijkstra-Gries method for deriving relational programs},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {155--171},
  abstract = {We show how to derive imperative programs for relation-based
      discrete structures by combining relational calculus and the
      Dijkstra-Gries method. Three examples are given, viz. Warshall's
      algorithm for transitive closures, a breadth-first-search
      reachability algorithm, and an algorithm for spanning trees.},
  keywords = {Program derivation; Relational algebra; Relational programs; Graph theory},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/27/abstract.html},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-200?,
  author = 	 {Rudolf Berghammer},
  title = 	 {Computing and Visualizing Lattices of Subgroups using Relation Algebra and \textsc{RelView}},
  OPTcrossref =  {},
  WKloc = 	 {A-1700},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Berghammer-2011,
  author = 	 {Rudolf Berghammer},
  title = {A Functional, Successor List Based Version of {Warshall}'s Algorithm with Applications},
  crossref =  {RelMiCS2011},
  pages = 	 {109--124},
  bibliographies = {RelMiCS, RelMiCS12},
  DOI = {10.1007/978-3-642-21070-9_10},
  DOIURL = {http://dx.doi.org/10.1007/978-3-642-21070-9_10},
  SpringerURL = {http://www.springerlink.com/content/91477w7134j60u65/},
  abstract = {We show how formally and systematically to develop a purely functional version of Warshall's algorithm for computing transitive closures by combining the unfold-fold technique, relation-algebra and data refinement. It is based on an implementation of relations by lists of successor lists. The final version can immediately be implemented in Haskell. This resulting Haskell program has the same runtime complexity as the traditional imperative array-based implementation of Warshall's algorithm. We also demonstrate how it can be re-used as component in other functional algorithms. }
}

@Book{Berghammer-2012,
  author =    {Rudolf Berghammer},
  title =        {{Ordnungen, Verb\"ande und Relationen mit Anwendungen}},
  publisher =    {Springer Vieweg},
  year =         2012,
  edition =   {2. Auflage},
  DOI =     {10.1007/978-3-658-00619-8},
  SpringerURL =  {http://www.springerlink.com/content/978-3-658-00618-1/},
  ISBN =    {978-3-658-00618-1}
}

@InCollection{Berghammer-Ehler-1989,
  year = 1989,
  title = {On the Use of Elements of Functional Programming in
		  Program Development by Transformation},
  series = {Bericht MIP-8915},
  publisher = {Fakult\"at f\"ur Mathematik und Informatik,
		  Universit\"at Passau},
  pages = {53--75},
  note = {auch in: Broy, M., Wirsing, M.\null{} (eds.): Methods of
		  Programming, Selected Papers of the CIP-project.
		  LNCS 544, Springer 1991, 193--216},
  editor = {M. Broy and M. Wirsing},
  booktitle = {{Methodik des Programmierens, eine Festschrift zu
		  Ehren von F.L.\null{} Bauer}},
  author = {Rudolf Berghammer and Herbert Ehler}
}

@InProceedings{Berghammer-Ehler-Moeller-1990,
  title = {On the Refinement of Non-Deterministic Recursive Routines
		  by Transformations},
  pages = {53--71},
  crossref = {IFIP1990},
  author = {Rudolf Berghammer and Herbert Ehler and Bernhard M{\"o}ller},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Ehler-Moeller-1990x,
  year = 1990,
  title = {On the Refinement of Non-Deterministic Recursive Routines
		  by Transformations},
  publisher = {North-Holland},
  pages = {53--71},
  editor = {Manfred Broy and C. B. Jones},
  booktitle = {Programming Concepts and Methods,
		Proceedings of the {IFIP} Working Group 2.2/2.3
		Working Conference on Programming Concepts and Methods},
  author = {Rudolf Berghammer and Herbert Ehler and Bernhard M{\"o}ller}
}

@TechReport{Berghammer-Ehler-Zierer-1987,
  year = 1987,
  title = {Towards an Algebraic Specification of Code Generation},
  number = {TUM-I8707},
  institution = {Institut f\"ur Informatik, Technische Universit\"at M\"unchen},
  author = {Rudolf Berghammer and Herbert Ehler and Hans Zierer},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Ehler-Zierer-1987a,
  year = 1987,
  title = {Towards an Algebraic Specification of Code Generation},
  series = {Bericht Nr. 8711},
  publisher = {Institut f\"ur Informatik und praktische Mathematik,
		  Universit\"at Kiel},
  note = {gek\"urzte Version von \cite{Berghammer-Ehler-Zierer-1987}},
  editor = {F. Simon},
  booktitle = {Proceedings Kolloquium ``Programmspezifikation''},
  author = {Rudolf Berghammer and Herbert Ehler and Hans Zierer},
  address = {Midlum, F\"ohr},
  bibliographies = {RelMiCS}
}

@Article{Berghammer-Ehler-Zierer-1988,
  author = {Rudolf Berghammer and Herbert Ehler and Hans Zierer},
  title = {Towards an Algebraic Specification of Code Generation},
  pages = {45--63},
  journal = SCICOP,
  year = 1988,
  volume = 11,
  note = {gek\"urzte Version von
		  \cite{Berghammer-Ehler-Zierer-1987}},
  WKloc = {A-0093},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Ehler-Zierer-1988a,
  author = {Rudolf Berghammer and Herbert Ehler and Hans Zierer},
  title={Development of Several Reachability Algorithms for Directed Graphs},

  title = {Development of Graph Algorithms by Program Transformation},
  series = LNCS,
  publisher = Springer,
  pages = {206--218},
  DOI = {10.1007/3-540-19422-3_16},
  DOIURL = {http://dx.doi.org/10.1007/3-540-19422-3_16},
  booktitle = {Proc.\null{} 13th International Workshop on Graph-Theoretic
		  Concepts in Computer Science},
  editor = {H. G\"ottler and H.J. Schneider},
  year = 1987,
  volume = 314,
  ISBN={978-3-540-19422-4},
  confaddress = {Kloster Banz/Staffelstein},
  bibliographies = {RelMiCS},
  conclusion = {In the development above only the control structure
    of the algorithms was subject to transformations.
    The final versions (6) and (7)
    which are due to the different generalizations in Section 3.1
    are both operational provided implementations
    of the underlying data structures for vertices, graphs, and sets are given.

    If these implementations are too inefficient or are not available at all,
    transformations of the data structures would have to follow.
    For example, an implementation of graphs by successor lists
    and of sets by boolean vectors allows to achieve
    a complexity O(NM) for algorithm (6),
    where M denotes the number of arcs of the graph.
    However, using two different set implementations
    (namely boolean vectors for the argument t and stacks or queues for s)
    and successor lists for graphs again,
    we could even obtain an O(M) version of (7).
    This shows that different strategies and techniques
    may lead to algorithms of different complexity.
    But on the other hand, the example also demonstrates
    that different sequences of design decisions can yield the same result.

    Since the derivation of algorithms is a complex and creative task,
    there is no a priori recipe to determine
    which strategy or technique is best at a specific stage.
    Hence, the transformational approach is user-controlled.
    It offers the programmer various routes to follow
    and due to the correctness of each individual rule
    the final program
    is correct with respect to the initial specification by construction.}
}

@TechReport{Berghammer-Elbl-Schmerl-1992,
  year = 1992,
  title = {Proving Correctness of Programs in Weak Second-Order Logic},
  number = 9206,
  institution = {{Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen}},
  author = {Rudolf Berghammer and Birgit Elbl and Ulf Schmerl},
  bibliographies = {RelMiCS}
}

@Manual{Berghammer-Ernst-Milanese-Tiedt-2001,
  title = {Kiel {Interactive Evaluation Laboratory} (revised version)},
  author = {Rudolf Berghammer and Helge Ernst and Ulf Milanese and Markus Tiedt},
  organization = {Institut f\"ur Informatik und Paraktische Mathematik, Christian-Albrechts-Universit\"at Kiel},
  month = JUN,
  year = 2001,
  WKloc = {A-1426}
}

@Article{Berghammer-Gritzner-1996,
  author = {Thomas F. Gritzner and Rudolf Berghammer},
  title = {A Relation Algebraic Model of Robust Correctness},
  journal = TCS,
  year = 1996,
  volume = 159,
  number = 2,
  month = JUN,
  pages = {245--270},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Gritzner-Schmidt-1993,
  author = {Rudolf Berghammer and Thomas F. Gritzner and Gunther Schmidt},
  title = {Prototyping relational specifications using higher-order objects},
  pages = {56--75},
  note = {also as Tech. Report. 9304, UniBwM, 1993, 33 pages},
  crossref = {HOA1993},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-Gritzner-Schmidt-1993a,
  year = 1993,
  title = {Prototyping Relational Specifications Using Higher-order
      Objects},
  pages = 33,
  number = 9304,
  institution = {{Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen}},
  author = {Rudolf Berghammer and Thomas F. Gritzner and Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Haeberer-Schmidt-Veloso-1993,
  author = {Rudolf Berghammer and Armando M. Haeberer and Gunther Schmidt
            and Paulo A. S. Veloso},
  title = {Comparing Two Different Approaches to Products in
           Abstract Relation Algebra},
  pages = {167--176},
  crossref = {AMAST1993},
  WKloc = {A-0107},
  filename = {Berghammer-Haeberer-Schmidt-Veloso-1993},
  DIRECTORY = {/home/tools/papers},
  bibliographies = {RelMiCS}
}

@Misc{Berghammer-Haeberer-Schmidt-Veloso-1995,
  year = 1995,
  title = {A new Class of Partially Evaluable Fork Algebras:
      Axiomatization and Models},
  OPTnote = {16~p.},
  howpublished = {unpublished},
  author = {Rudolf Berghammer and Haeberer, Armando Mart\'{\i}n and
      Gunther Schmidt and Paulo A.S. Veloso},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Hattensperger-1994,
  author = {Rudolf Berghammer and Claudia Hattensperger},
  title = {Computer-Aided Manipulation of Relational
		  Expressions and Formulae using {RALF}},
  crossref = {KielTool94},
  pages = {62--78},
  bibliographies = {RelMiCS}
}

@Article{Berghammer-Hoffmann-2000,
  author = {Rudolf Berghammer and Thorsten Hoffmann},
  title = {Deriving Relational Programs for Computing Kernels by Reconstructing a Proof of {Richardson}'s Theorem},
  journal = SCICOP,
  year = 2000,
  volume = 38,
  pages = {1--25},
  WKloc = {A-1214},
  abstract = {We combine relational algebra and program derivation methodology
    and reconstruct a proof of Richardson's theorem that
    every finite directed graph without circuits of odd length has a kernel
    as a relational program.
    Also a generalization of the approach is presented.},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-Hoffmann-Leoniuk-1999,
  author = {Rudolf Berghammer and Thorsten Hoffmann and Barbara Leoniuk},
  title = {{Rechnergest\"utzte Erstellung von Prototypen f\"ur Programme auf relationalen Strukturen}},
  institution = {Institut f\"ur Informatik und praktische Mathematik, Christian-Albrechts-Universit\"at Kiel},
  year = 1999,
  number = 9905,
  month = JUL,
  WKloc = {A-1158},
  bibliographies = {RelMiCS},
  note = {\newline\textsf{http://www.informatik.uni-kiel.de/\~{}progsys/relview/}}
}

@Article{Berghammer-Hoffmann-Leoniuk-Milanese-2002,
  author = {Rudolf Berghammer and Thorsten Hoffmann and Barbara Leoniuk and Ulf Milanese},
  title = {Prototyping and Programming with Relations},
  journal = ENTCS,
  year = 2003,
  volume = 44,
  number = 3,
  pages = {3.1--3.24},
  WKloc = {A-1435, doc/RelMiCS/RelMiS2001},
  bibliographies = {RelMiCS, RelMiS, RelMiS2001}
}

@InProceedings{Berghammer-Hoffmann-Leoniuk-Milanese-2002_p,
  author = {Rudolf Berghammer and Thorsten Hoffmann and Barbara Leoniuk and Ulf Milanese},
  title = {Prototyping and Programming with Relations},
  pages = {3.1--3.24},
  crossref =  {RelMiS2001_p},
  WKloc = {A-1435, doc/RelMiCS/RelMiS2001}
}

@Article{Berghammer-Karger-1996,
  author = {Rudolf Berghammer and Burghard von Karger},
  title = {Towards a Design Calculus for {CSP}},
  journal = SCICOP,
  year = 1996,
  volume = 26,
  pages = {99--115},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-Kempf-1993,
  year = 1993,
  title = {On Programming Languages with Infinite Output},
  number = 9206,
  institution = {{Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen}},
  author = {Rudolf Berghammer and Peter Kempf},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Kempf-Schmidt-Stroehlein-1991,
  title = {Relational Algebra and Logic of Programs},
  pages = {37--58},
  author = {Rudolf Berghammer and Peter Kempf and Gunther
		  Schmidt and Thomas Str\"ohlein},
  crossref = {Andreka-Monk-Nemeti-1991},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Leoniuk-Milanese-2000,
  author = 	 {Rudolf Berghammer and Barbara Leoniuk and Ulf Milanese},
  title = 	 {Implementation of Relations Using {BDDs}},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  keywords = 	 {RelView},
  WKloc = 	 {A-1642, doc/pap/BIB}
}

@InProceedings{Berghammer-Milanese-2005,
  author = 	 {Rudolf Berghammer and Ulf Milanese},
  title = 	 {Relational Approach to {Boolean} Logic Problems},
  crossref =     {RelMiCS2005},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  WKloc = 	 {A-1635},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Berghammer-Neumann-2005,
  author = {Rudolf Berghammer and Frank Neumann},
  title = {\textsc{RelView} --- An {OBDD}-Based Computer Algebra System
           for Relations},
  crossref =     {CASC2005},
  pages =	 {40--51},
  WKloc = 	 {A-1635, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {We present an OBDD-based Computer Algebra system for
     relational algebra, called RelView. After a short introduction to
     the OBDD-implementation of relations and the system, we exhibit
     its application by presenting two typical examples.}
}

@Article{Berghammer-Schmidt-1982,
  year = 1982,
  volume = 43,
  title = {Discrete Ordering Relations},
  pages = {1--7},
  journal = DISCR,
  author = {Rudolf Berghammer and Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Schmidt-1982a,
  year = 1982,
  title = {A Relational View on Gotos and Dynamic Logic},
  publisher = {Hanser},
  pages = {13--24},
  editor = {Schneider, H. J. and G\"ottler, H.},
  booktitle = {Proceedings of the 8th Conference on Graphtheoretic
		  Concepts in Computer Science,   {WG 82},
		  {Neunkirchen am Brand}},
  author = {Rudolf Berghammer and Gunther Schmidt},
  address = {M\"unchen},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Schmidt-1991,
  abstract = {People working in or studying the theory of
		  relations or graph theory very often use more or
		  less small examples of ``concrete'' relations and
		  manipulate them with pencil and paper in order to
		  prove or disprove some property. The RELVIEW system
		  is a totally interactive and completely
		  video-oriented computer system supporting such tasks.},
  title = {The {RELVIEW}-System},
  pages = {535--536},
  crossref = {STACS1991},
  author = {Rudolf Berghammer and Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Schmidt-1991x,
  authorsAddress = {inf2},
  abstract = {People working in or studying the theory of
		  relations or graph theory very often use more or
		  less small examples of ``concrete'' relations and
		  manipulate them with pencil and paper in order to
		  prove or disprove some property. The RELVIEW system
		  is a totally interactive and completely
		  video-oriented computer system supporting such tasks.},
  year = 1991,
  volume = 480,
  title = {{The {RELVIEW}-System}},
  series = {LNCS},
  publisher = {Springer-Verlag},
  pages = {535--536},
  month = FEB,
  editor = {C. Choffrut and M. Jantzen},
  booktitle = {STACS 91, 8th Annual Symposium on Theoretical Aspects of Computer Science},
  author = {Rudolf Berghammer and Gunther Schmidt},
  address = {Hamburg, Germany}
}

@InProceedings{Berghammer-Schmidt-1993,
  author = {Rudolf Berghammer and Gunther Schmidt},
  title = {Relational Specifications},
  year = 1993,
  volume = 28,
  series = {Banach Center Publications},
  publisher = {Institute of Mathematics, Polish Academy of
		  Sciences},
  pages = {167--190},
  editor = {C. Rauszer},
  booktitle = {Proc. {XXXVIII} Banach Center Semester on Algebraic
		  Methods in Logic and their Computer Science Applications},
  address = {Warszawa},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-Schmidt-1993a,
  author = {Rudolf Berghammer and Gunther Schmidt},
  title = {{RELVIEW} --- A computer system for the manipulation of
      relations. Notes to a system demonstration},
  crossref = {AMAST1993},
  pages = {403--404},
  bibliographies = {RelMiCS}
}

@Proceedings{Berghammer-Schmidt-1993b,
  title = {{Programmiersprachen und
                      Grundlagen der Programmierung,
                   Kolloquium auf der Barbarah\"utte am Kreuzeck,
                      Sep 15--17, 1993}},
  booktitle = {{Programmiersprachen und
                      Grundlagen der Programmierung}},
  editor = {Rudolf Berghammer and Gunther Schmidt},
  publisher = {Fakult\"at f\"ur Informatik, Univ.\null{} der Bundeswehr
      M\"unchen},
  series = {Tech.~Rep.},
  volume = {93/09},
  OPTnote = {270 p.},
  year = 1993,
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-Schmidt-Zierer-1986,
  year = 1986,
  title = {Symmetric Quotients},
  number = {TUM-INFO 8620},
  note = {18~p.},
  OPTinstitution = {Technische Universit\"at M\"unchen, Fakult\"at f\"ur Informatik},
  institution = {TU M\"unchen, Fak.\null{} Informatik},
  author = {Rudolf Berghammer and Gunther Schmidt and Hans Zierer},
  bibliographies = {RelMiCS}
}

@Article{Berghammer-Schmidt-Zierer-1989,
  year = 1989,
  volume = 33,
  title = {Symmetric Quotients and Domain Constructions},
  pages = {163--168},
  number = 3,
  journal = IPLET,
  DOIURL = {http://dx.doi.org/10.1016/0020-0190(89)90197-X},
  DOI = {10.1016/0020-0190(89)90197-X},
  author = {Rudolf Berghammer and Gunther Schmidt and Hans Zierer},
  bibliographies = {RelMiCS}
}

@TechReport{Berghammer-Zierer-1985,
  author = {Rudolf Berghammer and Hans Zierer},
  title = {Relational Algebraic Semantics of Functional Programs},
  number = {TUM-INFO 8501},
  institution = U_TUMI,
  year = 1985,
  bibliographies = {RelMiCS}
}

@Article{Berghammer-Zierer-1986,
  year = 1986,
  volume = 43,
  title = {Relational Algebraic Semantics of Deterministic and
		  Nondeterministic Programs},
  pages = {123--147},
  journal = TCS,
  author = {Rudolf Berghammer and Hans Zierer},
  bibliographies = {RelMiCS}
}

@InCollection{Berghammer-vonKarger-1997a,
  author = {Rudolf Berghammer and Burghard von Karger},
  title = {Relational Semantics of Functional Programs},
  chapter = 8,
  pages = {115--130},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@InCollection{Berghammer-vonKarger-1997b,
  author = {Rudolf Berghammer and Burghard von Karger},
  title = {Algorithms from Relational Specifications},
  chapter = 9,
  pages = {131--149},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@InProceedings{Berghammer-vonKarger-Wolf-1998,
  author = {Rudolf Berghammer and von Karger, Burghard and Andreas Wolf},
  title = {Relation-Algebraic Derivation of Spanning Tree Algorithms},
  crossref = {MPC1998},
  pages = {23--43},
  WKloc = {A-1095},
  bibliographies = {RelMiCS}
}

@Book{Bergstra-Heering-Klint-1989,
  editor = {J. A. Bergstra and J. Heering and P. Klint},
  title = {Algebraic Specification},
  publisher = {Addison-Wesley},
  year = 1989,
  series = {ACM Press Frontier Series},
  UniBwM = {INF700/S5105}
}

@Article{Bergstra-Klop-1986,
  author = {Bergstra, J.A. and Klop, Jan Willem},
  title = {Conditional Rewrite Rules: Confluence and Termination},
  journal = JCOMSYS,
  year = 1986,
  volume = 32,
  number = 3,
  pages = {323--363},
  TUBIBMUE = 120625,
  abstract = {Algebraic specifications of abstract data types can
		  often be viewed as systems of rewrite rules. Here we
		  consider rewrite rules with conditions, such as they
		  arise, e.g., from algebraic specifications with
		  positive conditional equations. The conditional term
		  rewriting systems thus obtained which we will study,
		  are based upon the well-known class of left-linear
		  nonambiguous TRSS. A large part of the theory for
		  such TRSS can be generalized to the conditional
		  case. Our approach is nonhierarchical the conditions
		  are to be evaluated in the same rewriting system. We
		  prove confluence results and termination results for
		  some well-known reduction strategies},
  bibliographies = {RelMiCS}
}

@Misc{Bergstra-Middelburg-Stefanescu-1995,
  author = {J. A. Bergstra and C. A. Middelburg and Gh. {\c{S}}tef{\u{a}}nescu},
  title = {Network Algebra for Synchronous and Asyynchronous Dataflow},
  year = 1995,
  WKloc = {A-0886},
  filename = {P9508}
}

@InProceedings{Bergstra-Stefanescu-1993,
  author = {J. A. Bergstra and Gh. {\c{S}}tef{\u{a}}nescu},
  title = {Translations Between Flowchart Schemes and Process Graphs},
  OPTcrossref = {FCT1993},
  year = 1993,
  booktitle = {FCT},
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0887}
}

@Misc{Bergstra-Stefanescu-1995,
  author = {J. A. Bergstra and Gh. {\c{S}}tef{\u{a}}nescu},
  title = {Network Algebra with Demonic Relation Operators},
  year = 1995,
  WKloc = {A-0893},
  annote = {P9509},
  bibliographies = {RelMiCS}
}

@Article{Berline-1992,
  author = {C. Berline},
  title = {R\'etractions et interpr\'etation interne du
		  polymorphisme: Le probl\`eme de la r\'etraction universelle},
  journal = RAIRO-I,
  year = 1992,
  volume = 26,
  pages = {59--91},
  number = 1,
  WKloc = {A-0276},
  keywords = {Retraction, internal interpretation},
  abstract = {Le but de cet expos\'e est de synth\'etiser en un
		  seul article tous les r\'esultats connus d'existence
		  de ``r\'etractions universelles'' ($=$ r.u.) dans
		  des sous-classes int\'erressantes de l'ensemble $R$
		  des r\'etractions d;un domaine de Scott. Ce
		  probl\`eme, purement alg\'ebraique, est li\'e \`a la
		  mod\'elisation du polymorphisme. La solution
		  donn\'ee par Berardi fournit comme sous-produit des
		  mod\'eles non triviaux d'un $\lambda$-calcul
		  \'etendu qui n'est pas Church-Rosser. L'inter\^et
		  s\'est port\'e sur des sous-classes de $R$ apr\`es
		  qu'Ershov e\^ut montr\'e, en 1975, que $R$
		  elle-m\^eme n'avait pas d'objet universel dans $D =
		  P_\omega$.

		  We give a condensed and uniform presentation of all
		  known results (some of them unpublished) concerning
		  the existence of ``universal'' retractions in
		  interesting subclasses of the set of retractions of
		  a Scott domain. This purely algebraic problem is
		  linked to the modelisation of polymorphism.
		  Berardi's solution provides, as a by-product, non
		  trivial models to a non Church-Rosser extension of
		  $\lambda$-calculus.},
  bibliographies = {RelMiCS}
}

@Article{Bernardinello-Ferigato-Pomello-2002,
  author = {Luca Bernardinello and Carlo Ferigato and Lucia Pomello},
  title = {An Algebraic Model of Observable Properties in Distributed Systems},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {to appear},
  bibliographies = {concurrency},
  WKloc = {doc/pap/BIB},
  abstract = {We propose orthomodular posets, algebraic models of quantum
      logic, as a formal tool in concurrency theory. We discuss their
      characteristics and study mutual relations with two other models of
      distributed systems: condition event net systems, a basic class of
      Petri nets, and the transition systems modelling CE net system
      behaviour. Central results are an adjointness situation among the
      three models and a strict relationship between fundamental notions in
      the different considered frameworks such as the relations of
      incompatibility and concurrency. Furthermore, substructures of
      orthomodular posets, like Boolean subalgebras or centres are
      interpreted, respectively, as state machine components of CE net
      systems or synchronization structures.},
  OPTannote = {}
}

@InProceedings{Bernays-1959,
  author = {Paul Bernays},
  title = {{\"Uber eine nat\"urliche Erweiterung des
		Relationenkalk\"uls}},
  booktitle = {Constructivity in Mathematics,
		Proc.\null{} of the Colloq., 1957},
  editor = {A. Heyting},
  publisher = NoHo,
  address = {Amsterdam},
  year = 1959,
  pages = {1--14},
  bibliographies = {RelMiCS}
}

@Article{Bernays-Schoenfinkel-1928,
  author = {Paul Bernays and Moses Sch{\"o}nfinkel},
  title = {{Zum Entscheidungsproblem der mathematischen Logik}},
  journal = {Mathematische Annalen},
  volume = {99},
  year = {1928},
  pages = {342--372},
  DOI = {10.1007/BF01459101},
  McMaster = {THODE Periodicals},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB}
}

@Article{Bernkopf-1968,
  author = {G. Bernkopf},
  title = {A History of Infinite Matrices},
  journal = ARHIST,
  year = 1968,
  volume = 4,
  pages = {308--358},
  bibliographies = {RelMiCS}
}

@Book{Bernstein-Lewis-1993,
  author = {Arthur J. Bernstein and Philip M. Lewis},
  title = {Concurrency in Programming and Database Systems},
  publisher = {Jones and Bartlett Publishers},
  year = 1993,
  address = {Boston, London},
  McMaster = {QA 76.642 .B47 1993},
  bibliographies = {SE3B},
  annote = {Includes formal proofs!}
}

@InProceedings{Berry-1994,
  author = {Gerard Berry},
  title = {Synchronous Languages for Reactive Systems: Styles,
		  Semantics, Implementations},
  crossref = {POPL1994},
  note = {tutorial without paper}
}

@Misc{Berry-1997a,
  author = {Jonathan Berry},
  title = {{LINK}: A Software System for Experimentation with Graphs and Hypergraphs},
  month = MAR,
  year = 1997,
  WKloc = {B-0095}
}

@Manual{Berry-1997b,
  author = {Jonathan Berry},
  title = {{LINK} 1.2},
  month = AUG,
  year = 1997,
  WKloc = {B-0095}
}

@Article{Berry-Dean-Goldberg-Shannon-Skiena-2000,
  author = {J. Berry and Nathaniel Dean and Mark K. Goldberg and
                 Gregory E. Shannon and Steven Skiena},
  title = {{{\em LINK\/}}: a system for graph computation},
  journal = {Software \emdash Practice and Experience},
  volume = 30,
  number = 11,
  pages = {1285--1302},
  month = sep,
  year = 2000,
  coden = {SPEXBL},
  ISSN = {0038-0644},
  URL = {http://www3.interscience.wiley.com/cgi-bin/fulltext?ID=72516934&;PLACEBO=IE.pdf;
                 http://www3.interscience.wiley.com/cgi-bin/abstract/72516934/START}
}

@Book{Bertin-1974,
  author = {J. Bertin},
  title = {Graphische Semiologie: Diagramme, Netze, Karten},
  year = 1974,
  publisher = {de Gruyter},
  address = {Berlin, New-York},
  note = {German translation according to the second French
		  edition by ~G.~Jensch, D.~Schade and S.~Scharfe,
		  also available as ``Semiology of graphics'',
		  The University of Wisconsin Press, Madison, WI, 1983}
}

@InProceedings{Bertolazzi-DiBattista-Mannino-Tamassia-1993,
  author = {P. Bertolazzi and Di Battista, G. and C. Mannino and
		  R. Tamassia},
  title = {Optimal Upward Planarity Testing of Single-Source Digraphs},
  crossref = {ESA93},
  pages = {37--48},
  abstract = {sequential $O(n)$ after previous best $O(n^2)$ by
		  Hutton and Lubiw.}
}

@InProceedings{Bertot-1992,
  author = {Yves Bertot},
  title = {Origin Functions in $\lambda$-Calculus and Term
		  Rewriting Systems},
  pages = {49--65},
  crossref = {CAAP92},
  annote = {--- PLGnotes ---
		  APPLY as '\@' in graph representation},
  abstract = {We use {\em Origin functions} to describe the
		  notions of descendance and residuals in reduction
		  systems such as the $\lambda$-calculus and linear
		  term rewriting systems. We compare the origin
		  functions for the $\lambda$-calculus and for term
		  rewriting systems that implement this calculus,
		  $\lambda\sigma$ and $\lambda Env$. We show that the
		  notions of origin do not correspond exactly, but we
		  describe an extension of the notion of origin that
		  permits the correct computation of
		  $\lambda$-calculus origins for derivations of
		  $\lambda Env$. We show that this extension is not
		  sufficient to give the same result for
		  $\lambda\sigma$ and we give another extension for
		  that system. This work is interesting as it provides
		  a distinction between the two term rewriting
		  systems. This work has applications in the debugging
		  of languages based on the $\lambda$-calculus or
		  environments.}
}

@Book{Bertot-Casteran-2004,
  author = 	 {Yves Bertot and Pierre Cast{\'e}ran},
  title = 	 {Interactive Theorem Proving and Program Development. {Coq'Art: The Calculus of Inductive Constructions}},
  publisher = 	 Springer,
  year = 	 2004,
  series = 	 {Texts in Theoretical Computer Science},
  ISBN = 	 {978-3-540-20854-9}
}

@InCollection{Best-1983,
  author = {Eike Best},
  title = {Relational semantics of concurrent programs (with some
      applications)},
  booktitle = {Formal Description of Programming Concept II},
  publisher = NoHo,
  year = 1983,
  editor = {D. Bj{\o}rner},
  pages = {431--452},
  bibliographies = {RelMiCS}
}

@InProceedings{Bestougeff-Ligozat-1985,
  author = {H. Bestougeff and G. Ligozat},
  title = {Parameterized Abstract Objects
		for Linguistic Information Processing},
  booktitle = {Proc.\null{} of the European Chapter of the
		Association for Computational Linguistics},
  address = {Geneva},
  year = 1985,
  pages = {107--115},
  bibliographies = {RelMiCS}
}

@Book{Bestougeff-Ligozat-1989,
  author = {H. Bestougeff and G. Ligozat},
  title = {Outils Logiques pour le Traitement du Temps:
		de la Linguistique \`a l'Intelligence Artificielle},
  publisher = Masson,
  address = {Paris},
  year = 1989,
  bibliographies = {RelMiCS}
}

@Article{Betti-Carboni-Street-Walters-1983,
  author = {R. Betti and A. Carboni and R.H. Street and R.F.C. Walters},
  title = {Variation through enrichment},
  journal = JPAA,
  year = 1983,
  pages = {109--127},
  volume = 29
}

@Article{Betti-Walters-1982,
  author = {R. Betti and R.F.C. Walters},
  title = {The symmetry of the cauchy-completion of a category},
  journal = LNM,
  year = 1982,
  pages = {8--12},
  volume = 962
}

@Article{Betti-Walters-1987,
  author = {R. Betti and R.F.C. Walters},
  title = {Completeness of locally-internal categories},
  journal = JPAA,
  year = 1987,
  pages = {105-117},
  volume = 47
}

@Article{Betti-Walters-1989,
  author = {R. Betti and R.F.C. Walters},
  title = {The calculus of ends over a base topos},
  journal = JPAA,
  year = 1989,
  pages = {211-220},
  volume = 56
}

@InProceedings{Beyer-2006a,
  author = {Dirk Beyer},
  title = {Relational Programming with {CrocoPat}},
  booktitle = {Proceedings of the 28th International Conference on Software Engineering (ICSE~2006, Shanghai, May 20-28)},
  publisher = {ACM Press},
  pages = {807--810},
  year = {2006},
  isbn = {1-59593-085-X},
  keyword = {Structural Analysis and Comprehension},
  pdf = {http://mtc.epfl.ch/~beyer/Publications/2006-ICSE.Relational_Programming_with_CrocoPat.pdf},
  url = {http://mtc.epfl.ch/~beyer/CrocoPat/},
  annote = {Download: \url{http://directory.fsf.org/CrocoPat.html}},
  WKloc = {A-1654, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Many structural analyses of software systems are
      naturally formalized as relational queries, for example, the
      detection of design patterns, patterns of problematic design,
      code clones, dead code, and differences between the as-built and
      the as-designed architecture. This paper describes CrocoPat, an
      application-independent tool for relational programming. Through
      its efficiency and its expressive language, CrocoPat enables
      practically important analyses of real-world software systems
      that are not possible with other graph analysis tools, in
      particular analyses that involve transitive closures and the
      detection of patterns in graphs. The language is easy to use,
      because it is based on the well-known first-order predicate
      logic. The tool is easy to integrate into other software
      systems, because it is a small command-line tool that uses a
      simple text format for input and output of relations.}
}

@Article{Beyer-Noack-Lewerentz-2005,
  author = {Dirk Beyer and Andreas Noack and Claus Lewerentz},
  title = {Efficient Relational Calculation for Software Analysis},
  journal = {IEEE Transactions on Software Engineering},
  volume = {31},
  number = {2},
  year = {2005},
  note = {Special issue of selected papers from WCRE 2003},
  pages = {137--149},
  keyword = {Structural Analysis and Comprehension},
  pdf = {http://mtc.epfl.ch/~beyer/Publications/2005-TSE.Efficient_Relational_Calculation_for_Software_Analysis.pdf},
  url = {http://repositories.cdlib.org/postprints/687},

  abstract = { Calculating with graphs and relations has many
      applications in the analysis of software systems, for example,
      the detection of design patterns or patterns of problematic
      design and the computation of design metrics. These applications
      require an expressive query language, in particular, for the
      detection of graph patterns, and an efficient evaluation of the
      queries even for large graphs. In this paper, we introduce RML,
      a simple language for querying and manipulating relations based
      on predicate calculus, and CrocoPat, an interpreter for RML
      programs. RML is general because it enables the manipulation not
      only of graphs (i.e., binary relations), but of relations of
      arbitrary arity. CrocoPat executes RML programs efficiently
      because it internally represents relations as binary decision
      diagrams, a data structure that is well-known as a compact
      representation of large relations in computer-aided
      verification. We evaluate RML by giving example programs for
      several software analyses and CrocoPat by comparing its
      performance with calculators for binary relations, a Prolog
      system, and a relational database management system.},
  DOIURL = {http://dx.doi.org/10.1109/TSE.2005.23},
  DOI = {10.1109/TSE.2005.23},
  annote = {Also available as postprint at the
    eScholarship Repository, University of California:
    \url{http://repositories.cdlib.org/postprints/687}
   CrocoPat is available at:  \url{http://mtc.epfl.ch/~beyer/CrocoPat}}
}


@InProceedings{Beylin-Dybjer-,
  author =       {Ilya Beylin and Peter Dybjer},
  title =        {Extracting a Proof of Coherence for Monoidal Categories from a Formal Proof of Normalization for Monoids},
  year =         {\unfinished},
  OPTcrossref =  {},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  WKloc =    {A-1742, doc/pap/BIB}
}

@Proceedings{Beylin-Dybjer-,
  title =        {Extracting a Proof of Coherence for Monoidal Categories from a Formal Proof of Normalization for Monoids},
  year =         {\unfinished},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@InProceedings{Bharat-Cardelli-1997,
  author = {Krishna Bharat and Luca Cardelli},
  title = {Migratory Applications},
  booktitle = {Mobile Object Systems --- Towards the Programmable Internet},
  pages = {131-148},
  year = 1997,
  editor = {J. Vitek and C. Tschudin Editors},
  volume = 1222,
  series = LNCS,
  publisher = Springer,
  WKloc = {doc/pap/BIB}
}

@InProceedings{Bherer-Desharnais-Frappier-StDenis-2004,
  author = 	 {Hans Bherer and Jules Desharnais and Marc Frappier and Richard St-Denis},
  title = 	 {Investigating Discrete Controllability with {Kleene} Algebra},
  crossref =	 {RelMiCS2003},
  pages =	 {74--85},
  WKloc = 	 {A-1667}
}

@Article{Biacino-Gerla-1991,
  title = {Connection Structures},
  journal = NOTRE,
  author = {L. Biacino and G. Gerla},
  year = 1991,
  pages = {242--247},
  volume = 32
}

@InProceedings{Bibel-Hoelldobler-Wuertz-1992,
  author = {W. Bibel and S. H{\"o}lldobler and J. W{\"u}rtz},
  title = {Cycle Unification},
  pages = {94--108},
  filename = {CADE92-Wuertz.dvi},
  booktitle = {Proceedings of the Conference on Automated Deduction},
  year = 1992,
  month = {June},
  editor = {D. Kapur},
  abstract = {Two-literal clauses of the form $L\!\leftarrow\!R\/$
		  occur quite frequently in logic programs, deductive
		  databases, and -- disguised as an equation -- in
		  term rewriting systems. These clauses define a cycle
		  if the atoms $L\/$ and $R\/$ are weakly unifiable,
		  ie.\ if $L\/$ unifies with a new variant of $R\/$.
		  The obvious problem with cycles is to control the
		  number of iterations through the cycle. In this
		  paper we consider the  cycle unification problem of
		  unifying two literals $\,G\,$ and $\,F\,$ modulo a
		  cycle. We review the state of the art of cycle
		  unification and give some new results for a special
		  type of cycles called matching cycles, ie.\ cycles
		  $\,L\!\leftarrow\!R\,$ for which there exists a
		  substitution $\,\sigma\,$ such that $\,\sigma L =
		  R\,$ or $\,L = \sigma R\,$. Altogether, these
		  results  show  how the deductive process can be
		  efficiently controlled for special classes of cycles
		  without losing completeness.}
}

@Book{Bidoit-Kreowski-Lescanne-Orejas-Sanella-1991,
  editor = {M. Bidoit and H.-J. Kreowski and P. Lescanne and F. Orejas and D. Sanella},
  title = {Algebraic System Specification and Devalopment --- A Survey and Annotated Bibliography},
  publisher = Springer,
  year = 1991,
  volume = 501,
  series = LNCS,
  UniBwM = {INF700/V7978}
}

@Book{Bidoit-Mosses-2004,
  author =       {Michel Bidoit and Peter D. Mosses},
  title =        {\textsc{Casl} User Manual},
  year =         2004,
  publisher =    Springer,
  series =       LNCS # { (IFIP Series)},
  volume =       {2900},
  URL = {http://link.springer.de/link/service/series/0558/tocs/t2900.htm},
  note =         {With chapters by T. Mossakowski, D. Sannella,
                 and A. Tarlecki},
  keywords = {CASL, algebraic specification}
}

@InProceedings{Bidoit-Sanella-Tarlecki-1998,
  author = {Michel Bidoit and Donald Sanella and Andrzej Tarlecki},
  title = {Archiectural Specifications in {\sc Casl}},
  crossref = {AMAST1998},
  pages = {341--357},
  abstract = {One of the novel features of {\sc Casl}, the Common
      Algebraic Specification Language, is the provision of so-called
      architectural specifications for describing the modular structure of
      software systems. A discussion of refinement of {\sc Casl}
      specifications provides the setting for a presentation of the
      rationale behind architectural specifications. This is followed by
      details of the features provided in {\sc Casl} for architectural
      specifications, hints concerning
      their semantics, and simple results justifying their usefulness in
      the development process.},
  WKloc = {A-1048, doc/pap/BIB}
}

@TechReport{Biersack-Raschke-Simons-1993,
  author = {Maya Biersack and Robert Raschke and Martin Simons},
  title = {The {{\sf DevaWEB}} System: Introduction, Tutorial, User Manual,
          and Implementation},
  year = 1993,
  number = {93-39},
  institution = {Technische Universit\"at Berlin},
  month = DEC,
  type = {Forschungsbericht des Fachbereichs Informatik},
  abstract = {We focus on the literate and structured presentation of formal
             developments. We present an approach that is influenced by
             ideas from Leslie Lamport an how to write proofs, by Donald
             Knuth's paradigm of literate programming, and by the ideas of
             the ``Dutch school'' on formal reasoning. We demonstrate this
             approach by presenting the proofs of two mathematical theorems
             --- the Knaster-Tasrski fixpoint theorem and the
             Schr\"oder-Bernstein theorem --- formalized in Deva. We discuss
             to what degree our aims have been achieved and what is left to
             be done.

             The report includes a tutorial on the use of the
             {\sf DevaWEB} system and a user manual for that tool. Some
             remarks on the implementation which we hope are of general
             interest to the designer of language dependent {\sf WEB}
             systems are also included.

             The report has been prepared
             with the assistance of the {\sf DevaWEB} system and the Deva
             formalization has been checked by an implementation of Deva.}
}

@Article{Bimbo-Dunn-1998,
  author = {Katalin Bimb{\'o} and J. Michael Dunn},
  title = {Two Extensions of the Structurally Free Logic $LC^*$},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 3,
  OPTmonth = {},
  pages = {403--424},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0569},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Bird-1984,
  year = 1984,
  volume = 21,
  title = {Using Circular Programs to Eliminate Multiple
		  Traversals of Data},
  OPTpages = {?},
  journal = ACTIN,
  author = {Richard S. Bird},
  bibliographies = {RelMiCS}
}

@Article{Bird-1986,
  year = 1986,
  volume = 6,
  title = {Transformational Programming and the Paragraph Problem},
  pages = {159--189},
  journal = SCICOP,
  author = {Richard S. Bird},
  bibliographies = {RelMiCS}
}

@InProceedings{Bird-1987,
  author = {Richard S. Bird},
  title = {An Introduction to the Theory of Lists},
  pages = {3--42},
  booktitle = {Logics of Programming and Calculi},
  year = 1987,
  volume = {F36},
  series = {NATO ASI Series},
  publisher = {Springer-Verlag},
  editor = {M. Broy},
  bibliographies = {RelMiCS}
}

@Article{Bird-1987a,
  author = {Richard S. Bird},
  title = {A Formal Development of an Efficient
		  Supercombinator Compiler},
  journal = SCICOP,
  year = 1987,
  volume = 8,
  pages = {113--137},
  bibliographies = {RelMiCS}
}

@InCollection{Bird-1989,
  author = {Richard S. Bird},
  title = {Lectures on Constructive Functional Programming},
  pages = {151--216},
  crossref = {Marktoberdorf-1988},
  annote = {Horner's rule and maximum segment sum, longest
		  all-p-filter segment, $\alpha$-$\beta$ algorithm,
		  arrays and largest filled rectangle, the hip view of
		  trees and heap construction.},
  annote = {Also available as Technical Monograph PRG-69, from
		  the Programming Research Group, Oxford University}
}

@Article{Bird-1989a,
  author = {Richard S. Bird},
  title = {Algebraic Identities for Program Calculation},
  pages = {122--126},
  year = 1989,
  volume = 32,
  number = 2,
  month = APR,
  journal = CJ,
  annote = {\begin{quotation}
	There is a style of functional programming
	which focuses attention on a small collection of
	powerful higher-order functions that capture
	common patterns of computation. The idea is to
	try and express programs in terms of the
	composition of particular instances of these
	useful functions. Explicit use of recursion is
	therefore avoided, except as a last resort.

	A similar, indeed dual, style can be advocated
	when it comes to program proof. The idea here is
	to try and conduct proofs through equational reasoning
	that exploits the algebraic properties of the
	collection of higher-order functions. Explicit use
	of induction is therefore avoided.
	\end{quotation}

	Left zeroes; $\alpha$-$\beta$ algorithm, maximum segment sum.},
  bibliographies = {RelMiCS}
}

@InCollection{Bird-1990,
  author = {Richard S. Bird},
  title = {A Calculus of Functions for Program Derivation},
  pages = {287--308},
  chapter = 11,
  year = 1990,
  series = UTYear,
  publisher = Addison,
  annote = {Also available as Technical Monograph PRG-64, from the Programming Research Group, Oxford University},
  editor = {David A. Turner},
  booktitle = {Research Topics in Functional Programming},
  abstract = {[Run-length encoding; Greedy and Leery Theorems.]},
  bibliographies = {RelMiCS}
}

@Article{Bird-1992,
  author = {Richard S. Bird},
  title = {The Smallest Upravel},
  year = 1992,
  volume = 18,
  pages = {281--292},
  journal = SCICOP,
  WKloc = {A-0028},
  bibliographies = {RelMiCS},
  abstract = {An {\em unravel} of a sequence $x$ is a bag of
		  nonempty subsequences of $x$ that when shuffled
		  together can give back $x$. For example, the
		  sequence ``accompany'' can be unravelled into three
		  lists ``acm'', ``an'', and ``copy''. The order of
		  these lists is not important but duplications do
		  matter; for example, ``peptet'' can be unravelled
		  into two copies of ``pet''. Thus, an unravel is
		  essentially a {\em bag} of sequences and not a list
		  or set.

		  An unravel is called an {\em upravel} if all its
		  component sequences are ascending. Since each of
		  ``acm'', ``an'', and ``copy'' are ascending, they
		  give an upravel of ``accompany''. Each nonempty
		  sequence has at least one upravel, namely the
		  upravel consisting of just singleton sequences.
		  However, of all possible upravels we want to
		  determine one with the least number of elements.

		  The problem of the smallest upravel is one of the
		  most instructive and challenging problems I have
		  ever come across. $\ldots$}
}

@InProceedings{Bird-1995,
  author = {Richard Bird},
  title = {Functional Algorithm Design},
  crossref = {MPC-1995},
  pages = {2--17},
  OPTabstract = {},
  WKloc = {A-0488}
}

@Book{Bird-2000,
  author = {Richard Bird},
  title = {Introduction to Functional Programming using {Haskell}},
  year = 2000,
  publisher = {Prentice-Hall},
  bibliographies = {FP},
  WKloc = {owned, \lent{}}
}

@Article{Bird-Gibbons-Jones-1989,
  year = 1989,
  volume = 12,
  title = {Formal Derivation of a Pattern Matching Algorithm},
  pages = {93--104},
  journal = SCICOP,
  author = {Richard S. Bird and J. Gibbons and G. Jones},
  bibliographies = {RelMiCS}
}

@InProceedings{Bird-Gibbons-Mu-2002,
  author = {Richard Bird and Jeremy Gibbons and Shin-Cheng Mu},
  title = {Algebraic Methods for Optimization Problems},
  crossref = {ACMMPC2002},
  pages = {281--307},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#optimization},
  WKloc = {A-1458, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {We argue for the benefits of relations over functions for
      modelling programs, and even more so for modelling specifications. To
      support this argument, we present an extended case study for a class
      of optimization problems, deriving efficient functional programs from
      concise relational specifications.}
}

@Misc{Bird-Hughes-1984,
  author = {Richard Bird and John Hughes},
  title = {The {KRC} Users Guide},
  year = 1984,
  month = SEP,
  WKloc = {A-0582}
}

@Misc{Bird-Hughes-1985,
  author = {??? and Richard Bird and John Hughes},
  title = {Implementing Functional Languages Using Graph Reduction},
  year = 1985,
  month = SEP,
  WKloc = {A-0583},
  note = {Chapters 6 -- 8, material for Pascal Project in Oxford}
}

@Misc{Bird-Hughes-1985a,
  author = {??? and Richard Bird and John Hughes},
  title = {Pascal Project --- Functional Program Interpreter},
  year = 1985,
  month = NOV,
  WKloc = {A-0584},
  note = {Handwritten task description for Pascal Project in Oxford}
}

@Article{Bird-Hughes-1987,
  author = {Richard S. Bird and John Hughes},
  title = {The Alpha-Beta Algorithm: An Exercise in Program Transformation},
  journal = ACTIN,
  year = 1987,
  volume = 24,
  pages = {53--57},
  WKloc = {A-1312}
}

@Misc{Bird-Paterson-199X,
  author = {Richard Bird and Ross Paterson},
  title = {{de Bruijn} notation as a nested datatype},
  year = {199?},
  WKloc = {A-0909}
}

@Misc{Bird-Paterson-199Y,
  author = {Richard Bird and Ross Paterson},
  title = {Generalised Folds for Nested Datatypes},
  year = {199?},
  WKloc = {A-0910}
}

@Book{Bird-Wadler-1988,
  year = 1988,
  title = {Introduction to Functional Programming},
  publisher = {Prentice-Hall},
  author = {Richard Bird and Phil Wadler},
  bibliographies = {FP}
}

@InProceedings{Bird-deMoor-1992,
  author = {Bird, Richard S. and Oege de Moor},
  title = {Solving Optimisation Problems with Catamorphisms},
  crossref = {MPC1992},
  pages = {45--66},
  WKloc = {A-0102},
  abstract = {Efficient algorithms for solving optimization
		  problems can often be expressed as homomorphisms on
		  initial data types. Such homomorphisms, which
		  correspond to the familiar {\sl fold} operators in
		  functional programming, are called {\em
		  catamorphisms}. In this paper, we report on an
		  attempt to characterize those optimization problems
		  whose efficient solution can be expressed as a
		  catamorphism. Our results are a natural
		  generalization of earlier work by Jeuring
		  [6],\cite{Jeuring-1990a}, who considered the same
		  problem in a slightly less abstract setting. The
		  main result of this paper is to show how seemingly
		  disparate results about subsequences, permutations,
		  sequence partitions and subtrees can be stated as a
		  single theorem.},
  bibliographies = {RelMiCS}
}

@InProceedings{Bird-deMoor-1992a,
  author = {Bird, Richard S. and de Moor, Oege},
  title = {From Dynamic Programming to Greedy Algorithms},
  series = LNCS,
  publisher = Springer,
  pages = {43--61},
  editor = {M\"oller, B. and Partsch, H. and Schuman, S.},
  volume = 755,
  booktitle = {{Formal Program Development: Proc.\ of an IFIP TG2/WG
		  2.1 State of the Art Seminar, Rio de Janeiro, Jan.\ 1992}},
  year = 1992,
  bibliographies = {RelMiCS}
}

@InCollection{Bird-deMoor-1994,
  author = {Richard Bird and de Moor, Oege},
  title = {Relational Program Derivation and Context-free Language Recognition},
  crossref = {Roscoe-1994},
  pages = {17--35},
  chapter = 2,
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Book{Bird-deMoor-1997,
  author = {Bird, Richard S. and de Moor, Oege},
  title = {Algebra of Programming},
  publisher = {Prentice Hall},
  year = 1997,
  volume = 100,
  series = {International Series in Computer Science},
  UniBwM = {MAG/YC12209},
  DirectURL = {http://www.comlab.ox.ac.uk/oucl/publications/books/algebra/},
  bibliographies = {RelMiCS, FP},
  WKloc = {[B-0054] --- Jan Scheffczyk}
}

@Unpublished{Bird-deMoor-Hoogendijk-1993,
  author = {Richard Bird and de Moor, Oege and Paul Hoogendijk},
  title = {Generic programming with relations and functors},
  note = {unpublished draft},
  year = 1993,
  month = NOV,
  filename = {tue/gen.dvi},
  WKloc = {A-0265},
  abstract = {This paper explores the idea of generic programming
		  in which programs are parameterised by data
		  types. Part of the constructive theory of lists,
		  specically the part dealing with properties of
		  segments, is generalised in two ways: from lists to
		  arbitrary inductive data types, and from functions
		  to relations. The new theory is used to solve a
		  generic problem about segments.}
}

@Article{Bird-deMoor-Hoogendijk-1996,
  author = 	 {Richard Bird and de Moor, Oege and Paul Hoogendijk},
  title = 	 {Generic Functional Programming with Types and Relations},
  journal = 	 JFP,
  year = 	 1996,
  volume =	 6,
  number =	 1,
  pages =	 {1--28},
  bibliographies = {RelMiCS}
}

@Misc{Birkedal-Carboni-Rosolini-Scott-199X,
  author = {L. Birkedal and A. Carboni and G. Rosolini and D.S. Scott},
  title = {Type Theory via EExact Categories --- Extended abstract},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  WKloc = {A-1261},
  abstract = {Partial equivalence relations (and categories of these) are
      a standard tool in semantics of type theories and programing
      languages, since they often provide a cartesian closed category with
      extended definability. Using the theory of exact categories, we give
      a category-theoretic explanation of why the construction of a
      category of partial equivalence relations often produces a cartesian
      closed category. We show how several familiar examples of categories
      of partial equivalence relations fit into the general framework.}
}

@InProceedings{Birkedal-Welinder-1994,
  author = {L. Birkedal and M. Welinder},
  title = {Handwriting Program Generator Generators},
  crossref = {PLILP1994},
  pages = {198--214},
  WKloc = {A-0307}
}

@Article{Birkhoff-1942,
  author = {Garrett Birkhoff},
  title = {Lattice-ordered Groups},
  journal = ANMA,
  volume = 43,
  year = 1942,
  pages = {298--331},
  bibliographies = {RelMiCS}
}

@Book{Birkhoff-1948,
  author = {Birkhoff, Garrett},
  title = {Lattice Theory},
  series = Colloq,
  publisher = AMS,
  year = 1948,
  volume = {XXV},
  address = {Providence, R.\null{} I.},
  bibliographies = {RelMiCS}
}

@Book{Birkhoff-1967,
  year = 1967,
  volume = {XXV},
  title = {Lattice Theory},
  series = Colloq,
  publisher = AMS,
  edition = {3rd},
  author = {Garrett Birkhoff},
  address = {Providence, R.\null{} I.},
  bibliographies = {RelMiCS}
}

@Article{Birman-vanRenesse-199?,
  author = {Kenneth P. Birman and van Renesse, Robbert},
  title = {Software for Reliable Networks},
  journal = {Scientific American},
  year = {199?},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0782},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Biro-1987,
  author = {Bal\'azs Bir\'o},
  title = {Non-finite-axiomatizability Results in Algebraic Logic},
  note = {Preprint, 1987, pp.\null{} 19},
  journal = JSYLO,
  year = 1987,
  bibliographies = {RelMiCS}
}

@Article{Biro-1987a,
  author = {Bal\'azs Bir\'o},
  title = {Isomorphic but not Base-isomorphic
		Base-minimal Cylindric Set Algebras},
  journal = ALGU,
  volume = 24,
  pages = {292--300},
  year = 1987,
  annote = {refers to thesis and preprint, ``Isomorphic but not
		base-isomorphic base-minimal relation algebras''},
  bibliographies = {RelMiCS}
}

@Booklet{Biro-Sereny-1985,
  author = {Bal\'azs Bir\'o  and G. Ser\'eny},
  title = {An Explicit Characterization of
		some Non-representable Cylindric Algebras},
  note = {Preprint no. 9, 1985, pp.\null{} 9},
  bibliographies = {RelMiCS}
}

@Article{Biro-Shelah-1988,
  author = {Bal\'azs Bir\'o  and Saharon Shelah},
  title = {Isomorphic but not Lower Base-isomorphic
		Cylindric Set Algebras},
  journal = JSYLO,
  volume = 53,
  year = 1988,
  pages = {846--853},
  note = {Preprint no. 36, 1985, pp.\null{} 20},
  bibliographies = {RelMiCS}
}

@TechReport{Birrell-1989,
  author = {Andrew D. Birrell},
  title = {An Introduction to Programming with Threads},
  institution = {DEC},
  year = 1989,
  type = {SRC Research Report},
  number = 35,
  abstract = {This paper provides an introduction to writing concurrent
      programs with "threads". A threads facility allows you to write
      programs with multiple simultaneous points of execution,
      synchronizing through shared memory. The paper describes the basic
      thread and synchronization primitives, then for each primitive
      provides a tutorial on how to use it. The tutorial sections provide
      advice on the best ways to use the primitives, give warnings about
      what can go wrong and offer hints about how to avoid these pitfalls.
      The paper is aimed at experienced programmers who want to acquire
      practical expertise in writing concurrent programs.},
  pages = 35,
  month = JAN,
  URL = {http://gatekeeper.dec.com/pub/DEC/SRC/research-reports/abstracts/src-rr-035.html},
  WKloc = {doc/pap/BIB}
}

@Book{Bishop-1967,
  author =    {Errett Bishop},
  title =        {Foundations of Constructive Analysis},
  publisher =    {McGraw-Hill},
  year =         1967,
  bibliographies = {AgdaTG}
}

@Article{Biskup-1978,
  author = {J. Biskup},
  title = {On the Complementation Rule for Multivalued Dependencies in
      Database Relations},
  journal = ACTIN,
  volume = 10,
  number = 3,
  year = 1978,
  pages = {297--305},
  bibliographies = {RelMiCS}
}

@Article{Biskup-1980,
  author = {J. Biskup},
  title = {Inference of Multivalued Dependencies in Fixed and Undetermined
      Universes},
  journal = TCS,
  volume = 10,
  year = 1980,
  pages = {93--106},
  bibliographies = {RelMiCS}
}

@PhdThesis{Bjesse-2001,
  author = 	 {Per Bjesse},
  title = 	 {Gate Level Description of Synchronous Hardware and Automatic Verification Based on Theorem Proving},
  school = 	 {Department of Computing Science,
     Chalmers University of Technology and G\"oteborg University},
  year = 	 2001,
  month =	 MAY,
  WKloc = 	 {A-1501, doc/pap/BIB},
  abstract = { Today's hardware development industry faces
       enormous problems. The primary reason for this is that the
       complexity of state-of-the-art hardware devices is growing
       faster than the capacity of the tools that are used to check
       that they are correct. This problematic situation is further
       aggravated by an increasing pressure to make the development
       time as short as possible. As a consequence, components under
       design are more likely to contain errors, while less time can
       be spent making sure that finished products are correct.

       In this thesis, we contribute to improved hardware design
       methods in two ways.

       First, we present Lava, a hardware description and verification
       platform that is embedded in the functional language
       Haskell. Lava uses the capabilities of the host language to
       express synchronous circuits in a mathematically precise way,
       and allows easy connection to external verification tools. Lava
       also uses the capabilities of Haskell to allow the designer to
       devise interconnection patterns, and to write parametrised
       circuit descriptions. We illustrate the power of Lava by
       describing and verifying hardware components for computing the
       Fast Fourier Transform (FFT).

       Second, we present a number of techniques and case studies that
       demonstrate how automatic theorem proving can be used to prove
       correctness and find bugs in synchronous hardware. We show how
       verification can be done both at the level of complex
       arithmetic, and at the boolean level. In the case of the
       verification at the arithmetic level, we use Lava to construct
       special purpose proof strategies that interface with a first
       order logic theorem prover. In the case of the verification at
       the boolean level, we convert a number of standard finite state
       verification methods to use propositional logic theorem
       provers. The resulting converted methods are shown to give
       order of magnitude speedups compared to current state-of-the
       art verification techniques.},
  keywords = {hardware description, hardware verification, model checking,
    reachability analysis, symbolic trajectory evaluation, theorem
    proving, satisfiability, i nduction, signal processing, functional
    languages.}
}

@Misc{Bjesse-Claessen-Sheeran-Singh-199X,
  author = {Per Bjesse and Koen Claessen and Mary Sheeran and Satnam Singh},
  title = {Lava: Hardware Design in {Haskell}},
  year = {199?},
  WKloc = {A-0673}
}

@InCollection{Bjorner-1994,
  author = {Dines Bj{\o}rner},
  title = {Formal Model of Robots: Geometry and kinematics},
  crossref = {Roscoe-1994},
  pages = {37--58},
  chapter = 3,
  OPTnote = {},
  OPTannote = {}
}

@book{BjornerJones82,
  title = {Formal Specification and Software Development},
  editor = {Dines Bj{\o}rner and Cliff B. Jones},
  isbn = {0-13-329003-4},
  length = {501 pages},
  publisher = {Prentice Hall International},
  url = {http://homepages.cs.ncl.ac.uk/cliff.jones/ftp-stuff/BjornerJones1982},
  year = {1982}
}

@TechReport{Blackburn-Venema-1993,
  author = {Blackburn, Patrick and Yde Venema},
  title = {Dynamic squares},
  institution = {Dept.\null{} of Philosophy, Utrecht Univ.},
  type = {Logic Group Preprint Series},
  number = 92,
  year = 1993,
  note = {To appear in J.\null{} Philos.\null{} Logic},
  bibliographies = {RelMiCS}
}

@TechReport{Blackburn-deRijke-Venema-1994,
  author = {Blackburn, Patrick and Maarten de Rijke and Yde Venema},
  title = {The algebra of modal logic},
  institution = {CWI Amsterdam},
  year = 1994,
  type = {CWI Report},
  number = {CS-R9463},
  bibliographies = {RelMiCS}
}

@InCollection{Blackburn-deRijke-Venema-1997,
  author = {Patrick Blackburn and Maarten de Rijke and Yde Venema},
  title = {Logic, Language, and Information},
  chapter = 14,
  pages = {211--225},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Misc{Blanchet-Mackie-1995,
  author = {Bruno Blanchet and Ian Mackie},
  title = {Reversible versus Irreversible Machines: Experimental Results},
  year = 1995,
  month = OCT,
  WKloc = {A-0642}
}

@InProceedings{Blanchet-Cousot-Cousot-Feret-Mauborgne-Mine-Monniaux-Rival-2002,
  author = 	 {Bruno Blanchet and Patrick Cousot and Radhia Cousot and J{\'e}r{\^o}me Feret and Laurent Mauborgne and Antoine Min{\'e} and David Monniaux and Xavier Rival},
  title = 	 {Design and Implementation of a Special-Purpose Static Program Analyzer for Safety-Critical Real-Time Embedded Software},
  crossref =	 {NeilDJones2002},
  pages =	 {85--108},
  WKloc = 	 {A-1492, doc/pap/BIB},
  bibliographies = {OPG},
  abstract = {We report on a successful preliminary experience in
      the design and implementation of a special-purpose Abstract
      Interpretation based static program analyzer for the
      verification of safety critical embedded real-time software. The
      analyzer is both precise (zero false alarm in the considered
      experiment) and efficient (less than one minute of analysis for
      10,000 lines of code). Even if it is based on a simple interval
      analysis, many features have been added to obtain the desired
      precision: expansion of small arrays, widening with several
      thresholds, loop unrolling, trace partitioning, relations
      between loop counters and other variables. The efficiency of the
      tool mainly comes from a clever representation of abstract
      environments based on balanced binary search trees.}
}

@InProceedings{Blanchette-Hoelzl-Lochbihler-Panny-Popescu-Traytel-2014,
  author={Blanchette, JasminChristian and Hölzl, Johannes and Lochbihler, Andreas and Panny, Lorenz and Popescu, Andrei and Traytel, Dmitriy},
  title={Truly Modular (Co)datatypes for Isabelle/HOL},
  pages={93--110},
  crossref = {ITP2014},
  DOI={10.1007/978-3-319-08970-6_7},
  DOIURL={http://dx.doi.org/10.1007/978-3-319-08970-6_7},
  language={English},
  abstract = {We extended Isabelle/HOL with a pair of definitional commands
    for datatypes and codatatypes.
    They support mutual and nested (co)recursion
    through well-behaved type constructors,
    including mixed recursion–corecursion,
    and are complemented by syntaxes for introducing
    primitively (co)recursive functions
    and by a general proof method for reasoning coinductively.
    As a case study,
    we ported Isabelle’s Coinductive library to use the new commands,
    eliminating the need for tedious ad hoc constructions.}
}

@InProceedings{Blanqui-2000,
  author = {Frederic Blanqui},
  title = {Termination and Confluence of Higher-Order Rewrite Systems},
  crossref = {RTA2000},
  pages = {47--61},
  CiteSeer = {http://citeseer.nj.nec.com/484221.html},
  abstract = {In the last twenty years, several approaches to higher-order
      rewriting have been proposed, among which Klop's Combinatory Rewrite
      Systems (CRSs), Nipkow's Higher-order Rewrite Systems (HRSs) and
      Jouannaud and Okada's higher-order algebraic specification languages,
      of which only the last one considers typed terms. The later approach
      has been extended by Jouannaud, Okada and the present author into
      Inductive Data Type Systems (IDTSs). In this paper, we extend IDTSs
      with the CRS higher-order...}
}

@Article{Blazy-Facon-1998,
  author = {Sandrine Blazy and Philippe Facon},
  title = {Partial Evaluation for Program Comprehension},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 17},
  WKloc = {A-0902, 69--72},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@MastersThesis{Bliesener-1992,
  year = 1992,
  title = {{Vererbung und Synthese in gerichteten Graphen:
                   \"Uber die funktionale Programmierung von Graphdurchlaufalgorithmen
                   mittels eines allgemeinen Durchlaufschemas }},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 6/92},
  month = JUL,
  author = {Dirk Bliesener},
  type = {Diplomarbeit}
}

@Misc{Bloem-Engelfriet-1998,
  author = {Roderick Bloem and Joost Engelfriet},
  title = {A Comparison of Tree Transductions defined by Monadic Second Order Logic and by Attribute Grammars},
  OPThowpublished = {},
  OPTmonth = {},
  year = 1998,
  OPTnote = {},
  WKloc = {A-1222, doc/pap/BIB}
}

@PhdThesis{Blom-2001,
  author = 	 {Stefan Blom},
  title = 	 {Term Graph Rewriting, syntax and semantics},
  school = 	 {Vrije Universiteit te Amsterdam},
  year = 	 2001,
  note =	 {IPA Dissertation Series, no.~2001-05},
  WKloc = 	 {A-1653, doc/pap/BIB}
}

@article{Blom-Esic-Stefanescu-1995,
  year={1995},
  issn={0002-5240},
  journal={algebra universalis},
  volume={33},
  number={1},
  DOI={10.1007/BF01190768},
  title={Notes on equational theories of relations},
  DOIURL={http://dx.doi.org/10.1007/BF01190768},
  publisher={Birkhäuser-Verlag},
  author={Bloom, S.L. and Ésik, Z. and Stefanescu, Gh.},
  pages={98-126},
  language={English},
  abstract = {We describe explicitly the free algebras in the equational class
    generated by all algebras of binary relations with operations of union,
    composition, converse and reflexive transitive closure
    and neutral elements 0 (empty relation) and 1 (identity relation).
    We show the corresponding equational theory is decidable
    by reducing the problem to a question about regular sets.
    Similar results are given for two related equational theories.}
}

@InProceedings{Bloo-Rose-1996,
  author = {Roel Bloo and Kristoffer H{\/{o}}gsbro Rose},
  title = {Combinatory Reduction Systems with Explicit
		  Substitution that Preserve Strong Normalisation},
  crossref = {RTA96},
  WKloc = {A-0465},
  pages = {169--184},
  abstract = {In this paper, we generalise the notion of explicit
		  substitution from the $\lambda$-calculus to {\em
		  higher order rewriting}, realised by combinatory
		  reduction systems (CRSs). For every confluent CRS,
		  $R$, we construct an explicit substitution variant,
		  $Rx$, which we prove confluent.

		  We identify a large subset of the CRSs, the
		  structure-preserving CRSs, and show for any
		  structure-preserving CRS $R$ that $Rx$ preserves
		  strong normalisation of $R$.

		  We believe that this is a significant first step
		  towards providing a methodology for reasoning about
		  the operational properties of higher-order rewriting
		  in general, and higher-order program transformations
		  in particular, since confluence ensures correctness
		  of such transformations and preservation of strong
		  normalisation ensures that the transformations are
		  always safe, in both cases independently of the used
		  reduction strategy.}
}

@InProceedings{Bloom-1994,
  author = {Baard Bloom},
  title = {{CHOCOLATE: Calculi of Higher Order COmmunication
		  and LAmbda TErms}},
  crossref = {POPL1994},
  pages = {339--348},
  authorsAddress = {Cornell},
  abstract = {We propose a general definition of higher-order
		  process calculi, generalizing CHOCS [Tho89] and
		  related calculi, and investigate its basic
		  properties. We give sufficient conditions under
		  which a calculus is finitely-branching and
		  effective. We show that a suitable notion of
		  higher-order bisimulation is a congruence for a
		  subclass of higher-order calculi. We illustrate our
		  definitions with a sample calculus strictly stronger
		  than CHOCS.}
}

@Misc{Bloom-2003,
  author =	 {Stephen L. Bloom},
  title =	 {On Permutations Which Are Squares, Cubes, and All That},
  month =	 JAN,
  year =	 2003,
  WKloc = 	 {A-1505}
}

@Book{Bloom-Esik-1993,
  author = {Stephen L. Bloom and Zolt\'an {\'E}sik},
  title = {Iteration Theories: The Equational Logic of
		  Iterative Processes},
  publisher = {Springer-Verlag},
  year = 1993,
  series = {EATCS Monographs on Theoretical Computer Science},
  McMaster = {QA 76.9 .M35 I56 1993}
}

@InProceedings{Bloom-Esik-1994,
  author = {Stephen L. Bloom and Zolt\'an {\'E}sik},
  title = {Solving Polynomial Fixed Point Equations},
  crossref = {MFCS94},
  pages = {52--67},
  abstract = {The aim of this paper is to introduce {\em iteration
		  algebras} in a familiar formalism, namely that of
		  the $\mu$-calculus. In an iteration algebra, it is
		  possible to solve systems of polynomial equations in
		  a canonical way.},
  annote = {Reference to \cite{Bloom-Esik-1993}}
}

@Article{Bloom-Esik-1997,
  author = {Stephen L. Bloom and Zolt\'an {\'E}sik},
  title = 	 {The Equational Logic of Fixed Points},
  journal = 	 TCS,
  year = 	 1997,
  volume =	 179,
  number =	 {1--2},
  pages =	 {1--60},
  month =	 JUN
}

@TechReport{Bloom-Esik-Stefanescu-1992,
  title = {Notes on Equational Theories of Relations},
  author = {S. L. Bloom and Z. {\'E}sik and {Gh}eorghe {\c S}tef{\u a}nescu},
  OPTnote = {ISSN 0250 3638},
  year = 1992,
  series = {Preprint Series},
  institution = {Academiei Romane, Inst.\null{} de Matematica},
  bibliographies = {RelMiCS}
}

@Misc{Bloom-Sabadini-Walters-1994,
  author = {Stephen L. Bloom and N. Sabadini and R.F.C. Walters},
  title = {Matrices, Machines and Behaviors},
  year = 1994,
  month = MAY,
  WKloc = {A-0970}
}

@InProceedings{Bloss-Hudak-1987,
  author = {A. Bloss and Paul Hudak},
  title = {Path Semantics},
  pages = {476--489},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  keywords = {functional lazy evaluation},
  abstract = {Knowledge of order of evaluation of expressions is
		  useful for compile-time optimizations for lazy
		  sequential functional programs. The authors present
		  path semantics, a non-standard semantics that
		  describes order of evaluation for a first-order
		  functional language with lazy evaluation. They also
		  provide an effective abstraction of path semantics
		  that provides compile-time information. They show
		  how path semantics may be used in strictness
		  analysis, process scheduling in parallel systems,
		  and optimized self-modifying thunks.},
  WKloc = {A-0065}
}

@Article{Blute-Cockett-Seely,
  author =       {R.F. Blute and J.R.B. Cockett and R.A.G. Seely},
  title =        {Categories for Computation in Context and Unified Logic: The ``Intuitionist'' Case},
  journal =      {},
  year =         {},
  WKloc =       {A-1739, doc/pap/BIB},
  bibliographies = {DepObj},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Unpublished{Blute-Cockett-Seely-Trimble-1992,
  author = {R.F. Blute and J.R.B. Cockett and R.A.G. Seely and
		  T.H. Trimble},
  title = {Natural Deduction and Coherence for Weakly
		  Distributive Categories},
  note = {To appear in Journal of Pure and Applied Algebra???},
  annote = {available by ftp from triples},
  WKloc = {A-0018, doc/pap/nets.dvi},
  abstract = {We define a two sided notion of proof nets, suitable
		  for categories, like weakly distributove categories,
		  which have the two-tensor structure (TIMES/PAR) of
		  linear logic, but lack a NEGATION operator. These
		  proof nets have a structure more closely parallel to
		  that of traditional natural deduction than Girard's
		  one-sided nets do. In particular, there is no cut,
		  and cut elimination is replaced by normalization. We
		  prove a sequentialization theorem for these nets and
		  the corresponding sequent calculus, and deduce the
		  coherence theorem for weakly distributive
		  categories. We also extend these techniques to cover
		  the case of non-symmetric (``planar'') tensors.

                  We further extend the treatment of coherence to
		  include the units for the tensors, giving a
		  characterization of the Lambek equivalence relation
		  on deductions ({\em i.e.~equality of morphisms}) in
		  terms of the notion of empire.

                  Finally, we derive a conservative extension result
		  for the passage from weakly distributive categories
		  to *-autonomous categories.},
  bibliographies = {RelMiCS}
}

@Misc{Blute-Scott-1995,
  author = {R.F. Blute and P. J. Scott},
  title = {Linear {L\"auchli} Semantics},
  year = 1995,
  month = JUN,
  OPTurl = {},
  WKloc = {A-0925},
  bibliographies = {LogRel}
}

@Misc{Blute-Scott-1996,
  author = {R.F. Blute and P. J. Scott},
  title = {The Shuffle {Hopf} Algebra and noncommutative Full Completeness},
  year = 1996,
  month = AUG,
  OPTurl = {},
  WKloc = {A-0924}
}

@Article{Blyth-1964,
  author = {T. S. Blyth},
  title = {Matrices Over Ordered Algebraic Structures},
  journal = JLON,
  volume = 39,
  year = 1964,
  pages = {427--432},
  bibliographies = {RelMiCS}
}

@Book{Bochenski-1970,
  author = {I. M Boch\'enski},
  title = {History of Formal Logic},
  OPTnote = {translated and edited by Ivo Thomas, Confusing publishing
      history. Check other bibliographies to see how this book is listed.},
  publisher = Chelsea,
  address = {New York},
  year = 1970,
  bibliographies = {RelMiCS}
}

@InProceedings{Bockmayr-1990a,
  author = {Alexander Bockmayr},
  title = {Algebraic and Logical Aspects of Unification},
  pages = {171--180},
  crossref = {IWWERT90},
  WKloc = {A-0155},
  abstract = {During the last years unification theory has become
		  an important subfield of automated reasoning and
		  logic programming. The aim of the present paper is
		  to relate unification theory to classical work on
		  equation solving in algebra and mathematical logic.
		  We show that many problems in unification theory
		  have their counterpart in classical mathematics and
		  illustrate by various examples how classical results
		  can be used to answer unification-theoretic questions.},
  bibliographies = {RelMiCS}
}

@InProceedings{Bockmayr-1990b,
  author = {Alexander Bockmayr},
  title = {Model-Theoretic Aspects of Unification},
  pages = {181--196},
  crossref = {IWWERT90},
  abstract = {Unification is a fundamental operation in various
		  areas of computer science, in particular in
		  automated theorem proving and logic programming. In
		  this paper we establih a relation between
		  unification theory and classical model theory. We
		  show how model-theoretic methods can be used to
		  investigate a generalized form of unification,
		  namely the problem whether, given an equational
		  theory $E$ and a system of equations $S$, there is
		  an extension of the free algebra in $E$ in which $S$
		  is solvable.},
  bibliographies = {RelMiCS}
}

@InProceedings{Bodenhofer-Bogdanowicz-Lanzerstorfer-Kueng-2005,
  author = 	 {Ulrich Bodenhofer and Peter Bogdanowicz and Gerhard Lanzerstorfer and Josef K{\"u}ng},
  title = 	 {Distance-Based Fuzzy Relations in Flexible Query Answering Systems: Overview and Experiences},
  crossref =	 {RelMiCS2005-PP},
  pages =	 {15--22},
  bibliographies = {RelMiCS}
}

@Article{Bodenhofer-DeCock-Kerre-2003,
  author = 	 {Ulrich Bodenhofer and De Cock, Martine and Etienne E. Kerre},
  title = {Openings and Closures of Fuzzy Preorderings:
           Theoretical Basics and Applications to Fuzzy Rule-Based Systems},
  journal = 	 {International Journal of General Systems},
  year = 	 2003,
  volume =	 32,
  number =	 4,
  pages =	 {343--360},
  month =	 AUG,
  WKloc = 	 {A-1617, doc/pap/BIB},
  abstract = {The purpose of this paper is two-fold. Firstly, a
     general concept of closedness of fuzzy sets under fuzzy
     preorderings is proposed and investigated along with the
     corresponding opening and closure operators. Secondly, the
     practical impact of this notion is demonstrated by applying it to
     the analysis of ordering-based modifiers.}
}

@Article{Bodenhofer-Kueng-2004,
  author = 	 {Ulrich Bodenhofer and Josef K{\"u}ng},
  title = 	 {Fuzzy orderings in flexible query answering systems},
  journal = 	 {Soft Computing},
  year = 	 2004,
  volume =	 8,
  number =	 7,
  pages =	 {512­-522},
  bibliographies = {RelMiCS},
  WKloc = 	 {A-1683, doc/pap/BIB},
  abstract = {Abstract This paper describes the benefits of using fuzzy
       orderings in flexible query answering systems. We provide
       a brief overview of those results from the theory of fuzzy
       orderings that are necessary to couple fuzzy orderings with
       flexible querying in a meaningful synergistic way. As one
       case study, we discuss a simple and pragmatic variant of a
       flexible query answering system ­ the so-called Vague
       query system (VQS). The integration of fuzzy orderings
       into that system is provided in full detail along with
       examples.},
  keywords = {Flexible query answering systems, Fuzzy orderings,
            Relational databases, Vague query system}
}

@TechReport{Bodin-Seznec-1994,
  author = {Fran{\c{c}}ois Bodin, Andr{\'e} Seznec},
  title = {Cache organization influence on loop blocking},
  institution = {IRISA},
  year = 1994,
  pages = 23,
  number = {PI-803},
  OPTmonth = {},
  URL = {ftp://ftp.irisa.fr/techreports/1994/PI-803.ps.gz},
  WKloc = {doc/pap/BIB},
  bibliographies = {PowerRel},
  abstract = {Performance tuning on today`s computers has become very
      complex. One of the factor of this complexity is the use of memory
      hierarchies, and particularly of cache memories. Code transformations
      such as loop blocking are used for improving temporal locality in
      numerical codes. Unfortunately, the behavior of direct-mapped caches
      and set-associative caches are very sensitive to parameters such as
      the respective placement of arrays determined by the leading sizes of
      arrays. This leads sometimes to unpredictable and catastrophic
      performance even on blocked numerical kernels. Most users are not
      expert in cache organizations and cannot be aware of such phenomena.
      In this paper, we show that the recently proposed 4-way skewed
      associative cache is quite insensitive to array placements in memory,
      and then provides to the user a quite stable and predictable behavior
      on the basic algorithms as well as blocked algorithms. The average
      behavior of the 4-way skewed-associative cache is also better than
      the average behavior of the 4-way set-associative cache on all
      algorithm versions. When using the 4-way skewed associative cache,
      copying is never necessary forgetting predictable performance while
      it is generally the only mean to get such predictable performance on
      set-associative and direct-mapped caches. Furthermore, on blocked
      algorithms, a large fraction of the cache space in a 4-way skewed
      associative cache may be used for blocking the loop, thus leading to
      a limited overhead due to blocking.}
}

@InProceedings{Boehm-Fonio-Habel-1985,
  author = {Paul Boehm and Harald-Reto Fonio and Annegret Habel},
  title = {Amalgamation of Graph Transformations with Applications to
		  Synchronization},
  editor = {H. Ehrig and C. Floyd and M. Nivat and J. Thatcher},
  booktitle = {Mathematical Foundations of Software Development},
  series = LNCS,
  volume = 185,
  pages = {267--283},
  year = 1985,
  publisher = Springer,
  added = {1996-03-13-10-09-20}
}

@InProceedings{Boehm-Piperno-Guerrini-1994,
  author = {Corrado B{\"o}hm and Adolfo Piperno and Stefano Guerrini},
  title = {$\lambda$-definition of function(al)s by normal forms},
  crossref = {ESOP1994},
  pages = {135--149},
  WKloc = {A-0331},
  abstract = {Lambda-calculus is extended in order to represent a
		  rather large class of recursive equation systems,
		  implicitly characterizing function(al)s or mappings
		  of some algebraic domain into arbitrary
		  sets. Algebraic equality will then be represented by
		  $\lambda\beta\delta$-convertibility (or even
		  reducibility). It is then proved, under very weak
		  assumptions on the structure of the equations, that
		  there always exist solutions in normal form
		  (Interpretation theorem). Some features of the
		  solutions, like the use of parametric
		  representations of the algebraic constructors,
		  higher-order solutions by curryfication,
		  definability of functions on unions of algebras,
		  etc., have been easily checked by a forst
		  implementation of the mentioned theorem, the CuCh machine.}
}

@InProceedings{Boer-1994,
  author = {de Boer, F.S.},
  title = {Compositionality in the Inductive Assertion Method
		  for Concurrent Systems},
  crossref = {PROCOMET94},
  pages = {282--302},
  keywords = {Compositionality in Concurrency}
}

@TechReport{Boerger-1991,
  year = 1991,
  type = {Seminarbericht},
  title = {How to Make a Path Injective},
  number = {91-41},
  institution = HAGEN,
  author = {R. Boerger and others},
  annote = {tubibmue}
}

@Article{Boerger-2002,
  author =       {Egon B{\"o}rger},
  title =        {The Origins and the Development of the {ASM} Method for High Level System Design and Analysis},
  journal =      JUCS,
  year =         2002,
  volume =    8,
  number =    1,
  pages =     {2--74},
  WKloc =     {doc/pap/BIB},
  DOIURL =    {http://dx.doi.org/10.3217/jucs-008-01-0002},
  annote =    {Evolving Algebras},
  abstract =  {The research belonging to the Abstract State Machines approach
               to system design and analysis is surveyed
               and documented in an annotated ASM bibliography.
               The survey covers the period from 1984,
               when the idea for the concept of ASMs
               (under the name dynamic or evolving algebras or structures)
               appears for the first time in a foundational context,
               to the year 2001 where a mathematically well-founded,
               practical system development method based upon the notion of ASMs
               is in place and ready to be industrially deployed.
               Some lessons for the future of ASMs are drawn.}
}

@InProceedings{Boerger-Durdanovic-Rosenzweig-1994,
  author = {E. B{\"o}rger and I. Durdanovic and D. Rosenzweig},
  title = {Occam: Specification and Compiler Correctness {P}art
		  {I}: The Primary Model},
  crossref = {PROCOMET94},
  pages = {481--500},
  keywords = {Processor Architectures; General Distributed
		  Systems; Software, Formal Definitions and Theory}
}

@InProceedings{Boerio-1994,
  author = {Luca Boerio},
  title = {Extending Pruning Techniques to the Second Order
		  $\lambda$-Calculus},
  crossref = {ESOP1994},
  pages = {120--134},
  abstract = {Type theory and constructive logics allow us, from a
		  proof of a formula, to extract a program that
		  satisfies the specification expressed by this
		  formula. Normally, programs extracted in this way
		  are inefficient. Optimization algorithms for these
		  programs have been developed. In this paper we show
		  an algorithm to optimize programs represented by
		  second-order typed $\lambda$-terms. We prove also
		  that the simplified programs are observational
		  equivalent to the original ones.}
}

@Article{Boettinger-1990,
  year = 1990,
  volume = 70,
  title = {On {Scott}'s Thesis for Domains of Information and Well-Quasi-Orderings},
  pages = {151--158},
  journal = TCS,
  author = {Claudia B{\"o}ttinger},
  bibliographies = {RelMiCS}
}

@Article{Boettner-1992a,
  author = {Michael B{\"o}ttner},
  title = {State transition semantics},
  journal = Theoretical_Linguistics,
  pages = {239--286},
  volume = 18,
  year = 1992,
  bibliographies = {RelMiCS}
}

@Article{Boettner-1992b,
  author = {Michael B{\"o}ttner},
  title = {Variable-free semantics for anaphora},
  journal = JPHIL,
  volume = 21,
  pages = {375--390},
  year = 1992,
  bibliographies = {RelMiCS}
}

@InCollection{Boettner-1994,
  author = {Michael B{\"o}ttner},
  title = {Open Problems in Relational Grammar},
  booktitle = {Patrick Suppes: Scientific Philosopher},
  publisher = Kluwer,
  year = 1994,
  editor = {P. Humphreys},
  volume = 3,
  address = {Dordrecht},
  pages = {19--39},
  bibliographies = {RelMiCS}
}

@Article{Boettner-1995,
  author = {Michael B{\"o}ttner},
  title = {A collective extension of relational grammar},
  journal = Journal_of_the_Interest_Group_in_Pure_and_Applied_Logics,
  year = 1996,
  OPTpages = {},
  OPTvolume = {},
  note = {to appear},
  bibliographies = {RelMiCS}
}

@InCollection{Boettner-1997,
  author = {Michael B{\"o}ttner},
  title = {Natural Language},
  chapter = 15,
  pages = {226--246},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Article{Boettner-1998,
  title = {A Collective Extension of Relational Grammar},
  author = {M.  B{\"o}ttner},
  pages = {175--193},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {Relational grammar was proposed in Suppes (1976) as a
      semantical grammar for natural language. Fragments considered so far
      are restricted to distributive notions. In this article, relational
      grammar is extended to collective notions.},
  bibliographies = {RelMiCS}
}

@InProceedings{Boettner-1998a,
  author = {Michael B{\"o}ttner},
  title = {Visiting Some Relatives of Peirce's},
  pages = {71--83},
  abstract = {The notion of a relational grammar is extended to ternary relations
                   and illustrated by a fragment of English.
                   Some of Peirce's terms for ternary relations are shown to be
                   incorrect and corrected.},
  editor = {Ali Jaoua and Peter Kempf and Gunther Schmidt},
  booktitle = {Using Relational Methods in Computer Science},
  year = 1998,
  month = JUL,
  series = {Technical Report Nr.\null{} 1998-03},
  publisher = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  bibliographies = {RelMiCS}
}

@Book{Boettner-1999,
  author = {Michael B{\"o}ttner},
  title = {Relationale Grammatik},
  publisher = {Niemeyer},
  year = 1999,
  address = {T{\"u}bingen},
  ISBN = {3-484-304020-2},
  bibliographies = {RelMiCS}
}

@InCollection{Boettner-2000a,
  author = {Michael B{\"o}ttner},
  title = {Meanings as State Transitions},
  crossref = {Boettner-Thuemmel-2000},
  pages = {182--198},
  bibliographies = {RelMiCS}
}

@Article{Boettner-2001,
  author =       {Michael B{\"o}ttner},
  title =        {Peirce Grammar},
  journal =      {Grammars},
  year =         2001,
  DOI =       {10.1023/A:1011403527615},
  URL = {http://www.springerlink.com/content/w31470k63l31g861/},
  bibliographies = {RelMiCS},
  volume =    4,
  number =    1,
  pages =     {1--19},
  month =     APR,
  abstract =    {Peirce grammar is a context-free grammar
    with a Peirce algebra as its semantics.
    A Peirce algebra is a two-sorted algebra
    that presupposes a Boolean algebra and a relation algebra.
    Since Peirce algebra has an equational theory
    many natural language inferences
    can be captured in terms of equational computation.
    The notions of a Peirce algebra and a Peirce grammar
    are applied to natural language.
    It is shown that the meaning of anaphoric pronouns, in particular
    possessive, relative, reciprocal, identity, and diversity pronouns
    can be constructed without use of variables or generalized quantifiers.}
}

@InProceedings{Bohn-1993,
  WKloc = {?},
  keywords = {?},
  contents = {?},
  abstract = {?},
  title = {Formalizing the transformational design of communicating
                  systems in the theorem prover LAMBDA},
  pages = {?},
  crossref = {HOA1993-PP},
  author = {J. Bohn}
}

@InProceedings{Bojanowski-Iglewski-Madey-Obaid-1994,
  author = {J. Bojanowski and M. Iglewski and Jan Madey and A. Obaid},
  title = {Functional approach to Protocols Specification},
  booktitle = {{Proc.\null{} of the {$14^{th}$} Internat.\null{} IFIP
      Sympos.\null{} on Protocol Specification, Testing and Verification,
      PSTV'94, Vancouver, B.C., 7-10 June 1994}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1994,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  pages = {371--378},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Bol-Apt-Klop-1990,
  author = {Roland Bol and Krzysztof R. Apt and Jan Willem Klop},
  title = {On the Power of Subsumption and Context Checks},
  crossref = {DISCO90},
  pages = {131--140},
  abstract = {Loop checking is a mechanism used to prune infinite
		  SLD-derivations. $\ldots$}
}

@TechReport{Boley-1983a,
  UniBwM = {INF560/M6409},
  year = 1983,
  title = {{FIT-PROLOG}: A Functional/Relational Language Comparison},
  month = DEC,
  institution = {Univ.\null{} Kaiserslautern},
  author = {Harold Boley},
  bibliographies = {RelMiCS}
}

@TechReport{Boley-1983adapter,
  UniBwM = {KYB800/L13971},
  year = 1983,
  title = {From Pattern-Directed to Adapter-Driven Computation via Function-Applying Matching},
  institution = {Universit\"at Kaiserslautern},
  author = {Harold Boley}
}

@TechReport{Boley-1986relfun,
  UniBwM = {INF560/P6399},
  year = 1986,
  title = {{RELFUN}: A Functional/Relational Integration with Valued Clauses},
  month = JAN,
  institution = {Universit\"at Kaiserslautern},
  author = {Harold Boley},
  bibliographies = {RelMiCS}
}

@Misc{Bombieri-,
  author = 	 {Enrico Bombieri},
  title = 	 {The {Riemann} Hypothesis},
  howpublished = {(Open problems web site)},
  WKloc = 	 {A-1690}
}

@Book{Bondoni-2007,
  author = 	 {Davide Bondoni},
  title = 	 {La teoria delle relazioni
                  nell'algebra della logica schroederiana},
  publisher = 	 {LED --- Edizioni Universitarie di Lettere Economia Diritto},
  year = 	 2007,
  OPTschool = {Universit{\`a} degli Studi di Milano ?},
  series = 	 {Spazio Tesi},
  month = 	 MAR,
  ISBN = {978-88-7916-349-1},
  URL = 	 {http://www.ledonline.it/spaziotesi/bondonischroeder.html},
  bibliography = {RelMiCS},
  WKloc = {doc/pap/BIB}
}

@Book{Boole-1847,
  author = {George Boole},
  title = {The Mathematical Analysis of Logic, Being an Essay
                Toward a Calculus of Deductive Reasoning},
  address = {Cambridge},
  publisher = Macmillan,
  year = 1847,
  pages = 82,
  bibliographies = {RelMiCS}
}

@TechReport{Booth-Jones-1994,
  author = {Simon P. Booth and Simon B. Jones},
  title = {A Screen Editor Written in the {Miranda}
           Functional Programming Language},
  year = 1994,
  month = FEB,
  institution = {Department of Computing Science and Mathematics, University of Stirling},
  number = {TR-116},
  URL = {ftp://ftp.cs.stir.ac.uk/pub/tr/cs/1994/TR116.ps.Z},
  WKloc = {A-0754}
}

@PhDThesis{Boquist-1999,
  author = {Urban Boquist},
  title = {Code Optimisation Techniques for Lazy Functional Languages},
  school = {Chalmers University of Technology, Gothenburg},
  month = APR,
  year = 1999,
  WKloc = {B-0052, doc/pap/BIB/Boquist-1999.ps.gz, doc/pap/BIB/Boquist-1999_benchmarks.tar.gz},
  URL = {http://www.cs.chalmers.se/~boquist/phd/index.html},
  abstract = {This thesis describes a complete compiler back-end for lazy
      functional languages, which uses various interprocedural
      optimisations to produce highly optimised code. The most important
      contributions of this work are the following.

      A novel intermediate language, called GRIN (Graph Reduction
      Intermediate Notation), around which the first part of the back-end
      is built. The GRIN language has a very ``functional flavour'', making
      it well suited for analysis and program transformation, but at the
      same time provides the ``low level'' machinery needed to express many
      concrete implementation details.

      We apply a program-wide control flow analysis, also called a heap
      points-to analysis, to the GRIN program. The result of the analysis
      is used to eliminate unknown control flow in the program, i.e.,
      function calls where the call target is unknown at compile time (due
      to forcing of closures).

      We present a large number of GRIN source-to-source program
      transformations that are applied to the program. Most transformations
      are very simple but when taken together, and applied repeatedly,
      produce greatly simplified and optimised code. Several non-standard
      transformations are made possible by the low level control offered by
      the GRIN language (when compared to a more high level intermediate
      language, e.g., the STG language).

      Eventually, the GRIN code is translated into RISC machine code. We
      develop an interprocedural register allocation algorithm, with a main
      purpose of decreasing the function call and return overhead. The
      register allocation algorithm includes a new technique to optimise
      the placement of register save and restore instructions, related to
      Chow's shrink-wrapping, and extends traditional Chaitin-style graph
      colouring with interprocedural coalescing and a restricted form of
      live range splitting. The algorithm produces a tailor-made calling
      convention for each function (the registers used to pass function
      arguments and results).

      A combined compile time and runtime approach is used to support
      garbage collection in the presence of aggressive optimisations (most
      notably our register allocation), without imposing any mutator
      overhead. The method includes a runtime call stack traversal and
      interpretation of registers and stack frames using pre-computed
      descriptor tables.

      Experiments sofar have been very promising. For a set of small to
      medium-sized Haskell programs taken from the \textsf{nofib} benchmark
      suite, the code produced by our back-end executes several times
      faster than the code produced by some other compilers (ghc and hbc).},
  keywords = {Lazy functional languages, compiler back-end, intermediate
      code, graph reduction, code optimisation, program transformation,
      interprocedural register allocation, graph colouring, garbage
      collection.}
}

@InCollection{Borak-Zalewska-2007,
   author = {Borak, Ewa and Zalewska, Anna},
   affiliation = {University of Bialystok, Institute of Computer Science, Białystok Poland},
   title = {Mizar Course in Logic and Set Theory},
   booktitle = {Towards Mechanized Mathematical Assistants},
   series = {Lecture Notes in Computer Science},
   editor = {Kauers, Manuel and Kerber, Manfred and Miner, Robert and Windsteiger, Wolfgang},
   publisher = Springer,
   pages = {191--204},
   volume = {4573},
   DOIURL = {http://dx.doi.org/10.1007/978-3-540-73086-6_17},
   DOI = {10.1007/978-3-540-73086-6_17},
   abstract = {From the very beginning of the development of the Mizar system experiments with using Mizar as a tool for teaching mathematics have been conducted. Numerous organized courses were based on different versions of the system: starting from the first implementation of its processor, through Mizar-MSE, Mizar–4 and PC–Mizar up till its present version. Now Mizar with its mathematical library gives us quite new didactic possibilities.  The purpose of this paper is to present a certain course on logic and set theory offered by our Institute for freshman students. The course employs Mizar as the main tool of instruction. In the paper we discuss the organization of this course and describe some examples of students’ tasks. Finally, some conclusions and remarks are given.},
   year = {2007}
}

@InProceedings{Boreale-Sangiori-1995,
  author = {M. Boreale and D. Sangiori},
  title = {A Fully Abstract Semantics for Causality in the
		  $\pi$-Calculus},
  crossref = {STACS1995},
  pages = {243--255},
  authorsAddress = {MB: La Sapienza; DS: Edinburgh},
  abstract = {We examine the meaning of causality in calculi for
		  mobile processes like the $\pi$-calculus, and we
		  investigate the relationship between interleaving
		  and causal semantics for such calculi.

                  We separate two forms of causal dependencies on
		  actions of $\pi$-calculus processes, called {\em
		  subject} and {\em object} dependencies: The former
		  originate from the nesting of prefixes and are
		  propagated through interactionsamong processes; the
		  latter originate from the binding mehanisms on
		  names. We develop a notion of causal bisimulation
		  which distinguishes processes which differ for the
		  subject or for the object dependencies. We show that
		  this {\em causal} equivalence can be reconducted to,
		  or implemented into, the ordinary {\em interleaving}
		  observation equivalence. This allows us to exploit
		  the simpler theory of the interleaving semantics to
		  reason about the causal one.}
}

@Book{Borghoff-Roedig-Scheffczyk-Schmitz-2003,
  author =  {U. Borghoff and P. R{\"o}dig and J. Scheffczyk and
L. Schmitz},
  title =        {{Langzeitarchivierung: Methoden zur Erhaltung
digitaler Dokumente}},
  publisher =    "dpunkt.Verlag, Heidelberg, Germany",
  year =         2003,
  isbn =         "3-89864-245-3",
}

@InProceedings{Boring-Altendorfer-Broll-Hilliges-Butz-2007,
  author = {Sebastian Boring and Manuela Altendorfer and Gregor Broll and Otmar Hilliges and Andreas Butz},
  title  = {{Shoot \& Copy}: Using Mobile Phones for Accessing Information on Large Displays},
  booktitle = {Extended Abstracts of the Ninth International Conference on Ubiquitous Computing {(Ubicomp)}},
  month = SEP,
  year =   {2007},
  URL = {http://www.medien.ifi.lmu.de/forschung/publikationen/detail?pub=boring2007ubicompdemo},
  WKloc = {doc/pap/BIB},
  bibliographies = {LabCore}
}

@InProceedings{Boring-Hilliges-Butz-2007,
  author = {Sebastian Boring and Otmar Hilliges and Andreas Butz},
  title  = {A Wall-sized Focus plus Context Display},
  booktitle = {Proceedings of the {Fifth Annual IEEE Conference on Pervasive Computing and Communications (PerCom)}},
  month = MAR,
  year =   {2007},
  URL = {http://www.medien.ifi.lmu.de/forschung/publikationen/detail?pub=boring2007percomDW},
  WKloc = {doc/pap/BIB},
  bibliographies = {LabCore}
}

@InProceedings{Borovansky-Kirchner-Kirchner-Moreau-Ringeissen-1998,
  author = {Peter Borovansk{\'y} and Claude Kirchner and
                 H{\'e}l{\`e}ne Kirchner and Pierre-Etienne Moreau and
                 Christophe Ringeissen},
  title = {An Overview of {ELAN}},
  editor = {Claude Kirchner and H{\'e}l{\`e}ne Kirchner},
  volume = 15,
  series = {Electronic Notes in Theoretical Computer Science},
  booktitle = {Proceedings of the International Workshop on Rewriting
                 Logic and its Applications},
  year = 1998,
  publisher = {Elsevier Science},
  address = {Pont-{\`a}-Mousson, France},
  month = SEP,
  URL = {ftp://ftp.loria.fr:21/pub/loria/protheo/COMMUNICATIONS_1998/BorovanskyKKMR-WRLA98.ps.gz}
}

@Article{Bose-Mesner-1959,
  author = {R. C. Bose and D. M. Mesner},
  title = {On Linear Associative Algebras Corresponding to
		Association Schemes of Partially Balanced Designs},
  journal = ANMAST,
  volume = 36,
  year = 1959,
  pages = {21--38},
  bibliographies = {RelMiCS}
}

@Article{Bossut-Dauchet-1995,
  author = 	 {Francis Bossut and Max Dauchet},
  title = 	 {A {Kleene} Theorem for a Class of Planar Acyclic Graphs},
  journal = 	 IandC,
  year = 	 1995,
  volume =	 117,
  pages =	 {251--265},
  WKloc = 	 {A-1582, doc/pap/BIB},
  keywors = 	 {series-parallel graphs}
}

@InProceedings{Boudet-Contejean-1994,
  author = {Alexandre Boudet and Evelyne Contejean},
  title = {``Syntactic'' {AC}-Unification},
  crossref = {CCL94},
  pages = {136--151},
  abstract = {$\ldots$ This provides a new {\sl AC}-unification
		  algorithm which does not make an explicit use of the
		  solving of Diophantine equations.}
}

@InProceedings{Boudet-Contejean-1998,
  author = {Alexandre Boudet and Evelyne Contejean},
  title = {About the Confluence of Equational Pattern Rewrite Systems},
  crossref = {CADE1998},
  pages = {88-102},
  OPTabstract = {},
  WKloc = {A-0604 (with PDF faults)}
}

@Article{Boudriga-Elloumi-Mili-1992,
  author = {N. Boudriga and F. Elloumi and A. Mili},
  title = {On the Lattice of Specifications: Applications to a
      Specification Methodology},
  journal = FACOMP,
  volume = 4,
  year = 1992,
  pages = {544--571},
  bibliographies = {RelMiCS}
}

@InProceedings{Bouhala-Rusinowitch-1995,
  author = {Adel Bouhala and Micha{\"e}l Rusinowitch},
  title = {{SPIKE}: A System for Automatic Inductive Proofs},
  crossref = {AMAST1995},
  pages = {576--577},
  OPTabstract = {},
  WKloc = {A-0626}
}

@article{Bouhoula-Jouannaud-Meseguer-2000,
  title = "Specification and Proof in Membership Equational Logic",
  journal = TCS,
  volume = "236",
  number = "1--2",
  pages = "35--132",
  year = "2000",
  issn = "0304-3975",
  DOI = "http://dx.doi.org/10.1016/S0304-3975(99)00206-6",
  DOIURL = "10.1016/S0304-3975(99)00206-6",
  url = "http://www.sciencedirect.com/science/article/pii/S0304397599002066",
  author = "Adel Bouhoula and Jean-Pierre Jouannaud and Jos{\'e} Meseguer",
  keywords = "Executable algebraic specifications",
  keywords = "Parameterized modules",
  keywords = "Inductive proofs",
  abstract = {This paper is part of a long-term effort to increase expressiveness of algebraic specification languages while at the same time having a simple semantic foundation on which efficient execution by rewriting and powerful theorem-proving tools can be based. In particular, our rewriting techniques provide semantic foundations for Maude's functional sublanguage, where they have been efficiently implemented. This effort started in the late 1970s, led by the \{ADJ\} group, who promoted equational logic and universal algebra as the semantic basis of program specification languages. An important later milestone was the work around order-sorted algebras and the \{OBJ\} family of languages developed at SRI-International in the 1980s. This effort has been substantially advanced in the mid-1990s with the development of Maude, a language based on membership equational logic. Membership equational logic is quite simple, and yet quite powerful. Its atomic formulae are equations and sort membership assertions, and its sentences are Horn clauses. It extends in a conservative way both (a version of) order-sorted equational logic and partial algebra approaches, while Horn logic with equality can be very easily encoded. After introducing the basic concepts of the logic, we give conditions and proof rules with which efficient equational deduction by rewriting can be achieved. We also give completion techniques to transform a specification into one meeting these conditions. We address the important issue of proving that a specification protects a subspecification, a property generalizing the usual notion of sufficient completeness. Using tree-automata techniques, we develop a test-set-based approach for proving inductive theorems about a parameterized specification. We briefly discuss a number of extensions of our techniques, including rewriting modulo axioms such as associativity and commutativity, having extra variables in conditions, and solving goals by narrowing. Finally, we discuss the generality of our approach and how it extends several previous approaches.}
}

@InProceedings{Boulme-Hardin-Rioboo-1999,
  author = {S. Boulm{\'e} and T. Hardin and R. Rioboo},
  title = {Modules, objets et calcul formel},
  booktitle = {Actes des Journées Francophones des Langages Applicatifs, 1999},
  year = 1999,
  URL = {http://calfor.lip6.fr/~foc/jfla99.ps},
  annote = {computer algebra system FOC},
  WKloc = {doc/pap/BIB}
}

@TechReport{Boulme-Hardin-Rioboo-2000,
  author = {S. Boulm{\'e} and T. Hardin and R. Rioboo},
  title = {Polymorphic Data Types, Objects, Modules and Functors: is it too much?},
  institution = {Laboratoire d'Informatique de Paris 6},
  year = 2000,
  keywords = {classes, modules, functors, objects, types, polymorphism,
      functionnal programming, Computer Algebra, specification, library,
      FOC},
  type = {Rapport de Recherche},
  number = {LIP6 2000/014},
  month = MAY,
  abstract = {Abstraction is a powerful tool for developers and it is
      offered by numerous features such as polymorphism, classes, modules
      and functors, ... A working programmer may be confused with this
      abundance. We develop a computer algebra library which is being
      certified. Reporting this experience made with a language (Ocaml)
      offering all these features, we argue that they are all needed
      together. We compare several ways of using classes to represent
      algebraic concept, trying to

      follow as close as possible mathematical specification. Then we show
      how to combine classes and modules to produce code having very strong
      properties. Currently, this library is made of one hundred units of
      functional codes and behaves faster than analogous ones such as
      Axiom.},
  URL = {http://www.lip6.fr/reports/lip6.2000.014.html},
  WKloc = {A-1050, doc/pap/BIB}
}

@Misc{Boulton-1996,
  author = {Richard J. Boulton},
  title = {{Syn}: A Single Language for Specifying Abstract Syntax Trees, Lexical Analysis, Parsing and Pretty-Printing},
  year = 1996,
  OPTurl = {http://www.cl.cam.ac.uk/},
  WKloc = {A-0920}
}

@Book{Bourbaki-1,
  author = {Nicolas Bourbaki},
  title = {{\'El\'ements de Math\'ematique, Livre 1: Th\'eorie
		  des Ensembles}},
  publisher = {Hermann},
  year = 1970,
  UniBwM = {MAT001/B5649-1}
}

@InCollection{Bourdoncle-1990,
  author = {Francois Bourdoncle},
  title = {Interprocedural abstract interpretation of block
                 structured languages with nested procedures, aliasing
                 and recursivity},
  pages = {307--323},
  crossref = {PLILP1990}
}

@Article{Boute-1992,
  author = {Raymond Boute},
  title = {The Euclidean definition of the functions \texttt{div} and \texttt{mod}},
  journal = ACM-TOPLAS,
  year = 1992,
  volume =	 14,
  number =	 2,
  pages =	 {127--144},
  month =	 APR,
  bibliographies = {FP},
  URL = {http://doi.acm.org/10.1145/128861.128862},
  abstract = {The definitions of the functions div and mod in the
      computer science literature and in programming languages are
      either similar to the Algol of Pascal definition (which is shown
      to be an unfortunate choice) or based on division by truncation
      (T-definition) or division by flooring as defined by Knuth
      (F-definition). The differences between various definitions that
      are in common usage are discussed, and an additional one is
      proposed, which is based on Euclid's theorem and therefore is
      called the Euclidean definition (E-definition). Its
      distinguishing feature is that $0 \leq D \mod d < d$
      irrespective of the signs of $D$ and $d$. It is argued that the
      E- and F-definitions are superior to all other ones in
      regularity and useful mathematical properties and hence deserve
      serious consideration as the standard convention at the
      applications and language level. It is also shown that these
      definitions are the most suitable ones for describing number
      representation systems and the realization of arithmetic
      operations at the architecture and hardware level. }
}

@InProceedings{Boute-2006,
  author = {Raymond Boute},
  title = {Using Domain-Independent Problems for Introducing Formal Methods},
  OPTcrossref =  {FM2006},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {316--331},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  WKloc = {A-1660, doc/pap/BIB},
  OPTpublisher = {},
  abstract = {The key to the integration of formal methods into
      engineering practice is education. In teaching,
      domain-independent problems --- i.e., not requiring prior
      engineering background --- offer many advantages.

      Such problems are widely available, but this paper adds two
      dimensions that are lacking in typical solutions yet are crucial
      to formal methods: (i) the translation of informal statements
      into formal expressions; (ii) the role of formal calculation
      (including proofs) in exposing risks or misunderstandings and in
      discovering pathways to solutions.

      A few example problems illustrate this: (a) a small logical one
      showing the importance of fully capturing informal statements;
      (b) a combinatorial one showing how, in going from
      ``real-world'' formulations to mathematical ones, formal methods
      can cover more aspects than classical mathematics, and a
      half-page formal program semantics suitable for beginners is
      presented as a support; (c) a larger one showing how a single
      problem can contain enough elements to serve as a Leitmotiv for
      all notational and reasoning issues in a complete introductory
      course.

      An important final observation is that, in teaching formal
      methods, no approach can be a substitute for an open mind, as
      extreme mathphobia appears resistant to any motivation.},
  keywords = {Domain-independent problems, Formal methods, Functional Predicate Calculus, Funmath, Generic functionals, Teaching, Specification, Word problems.},
  OPTannote = 	 {}
}

@PhdThesis{Bove-2002,
  author = 	 {Ana Bove},
  title = 	 {General Recursion in Type Theory},
  school = 	 {Department of Computer Science,
    Chalmers University of Technology and G\"oteborg University},
  year = 	 2002,
  WKloc = 	 {A-1641, doc/pap/BIB}
}

@Book{Bowen-1994,
  editor = {J.P. Bowen},
  title = {Towards Verified Systems},
  publisher = {Elsevier Science Publishers},
  series = {Real-Time Safety Critical Systems Series},
  year = 1994
}

@Book{Bowen-1996,
  author = {Jonathan Bowen},
  title = {Formal Specification and Documentation using {Z}:
                  A Case Study Approach},
  publisher = {International Thomson Computer Press (ITCP), Thomson Publishing},
  year = 1996,
  ISBN = {1-85032-230-9},
  annote = {Paperbound. xvii+302 pages, Price: \$44.95 US, £24.95 UK},
  bibliographies = {SpecTech}
}

@TechReport{Bowen-Breuer-1990,
  author = {Jonathan Bowen and Peter Breuer},
  title = {An Aliens' Guide to {Oxford}},
  year = 1990,
  number = {PRG-TR-12-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0086},
  abstract = {This guide gives some information about Oxford,
		  particularly for visitors to the Programming
		  Research Group. It was originally written for two
		  collaborative ESPRIT projects, so some parts are
		  targeted in a European and research project context.
		  However it should be of general interest,
		  particularly to overseas visitors who are attempting
		  to unravel the mystique of Oxford (although the
		  authors are still trying to do likewise!).}
}

@TechReport{Bowen-Breuer-Lano-1991,
  author = {Jonathan P. Bowen and Peter T. Breuer, Kevin C. Lano},
  title = {The {REDO} Project: Final Report},
  number = {PRG-TR-23-91},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0089},
  keywords = {ZED, ZEDplusplus},
  abstract = {This report gives an overview of the work performed
		  by the Programming Research Group as part of the
		  European collaborative ESPRIT II ``REDO'' project
		  (no.~2487). This work covered the areas of reverse
		  engineering: redocumentation and re-engineering;
		  validation: post-hoc verification and generation of
		  correct code from specifications; maintenance: new
		  languages and methods to support maintenance.
		  Research in areas of concurrent programming and
		  decompilation were also performed.},
  year = 1991,
  bibliographies = {RelMiCS}
}

@InProceedings{BoydelaTour-Kreitz,
  author = {Boy de la Tour, Thierry and Christoph Kreitz},
  title = {Building Proofs by Analogy via the Curry-Howard Isomorphism},
  crossref = {LPAR92},
  pages = {202--213},
  WKloc = {A-0254},
  OPTabstract = {We present a formalmethod for building proofs by
		  analogy and its implementation as a proof tactic for
		  the NuPRL proof development system. The Curry-Howard
		  Isomorphism is used to represent proof constructions
		  in a term-functional language and to specify
		  analogies by transformation rules on these
		  terms. The method has the advantage to admit {\em
		  complete} formalization and to make use of
		  well-known techniques like higher-order unification.}
}

@InProceedings{Boyland-Graham-1994,
  author = {John Boyland and Susan Graham},
  title = {Composing Tree Attributions},
  crossref = {POPL1994},
  pages = {375--388},
  WKloc = {A-0398},
  abstract = {Using the {\em simple tree attributions} described
		  in this paper, attribute values can themselves be
		  trees, enabling attribution to be used for tree
		  transformations. Unlike higher-order attribute
		  grammars, simple tree attributions have the property
		  of {\em descriptional composition}, which allows a
		  complex transformation to be built up from simpler
		  ones, yet be executed efficiently. In contrast to
		  other formalisms that admit descriptional
		  composition, notably composable attribute grammars,
		  simple tree attributions have the expressive power
		  to handle remote referenes and recursive syntactic
		  (tree-generating) functions, providing significantly
		  more general forms of attribution and transformation.}
}

@InProceedings{Boyle-1994,
  author = {J.M. Boyle},
  title = {Automatic, Self-adaptive Control of Unfold-Fold
		  Transformations},
  crossref = {PROCOMET94},
  pages = {80--100},
  keywords = {Support of Program Derivation}
}

@InProceedings{Bracha-Odersky-Stoutamire-Wadler-1998,
  author = {Gilad Bracha and Martin Odersky and David Stoutamire
                 and Philip Wadler},
  title = {Making the Future Safe for the Past: Adding Genericity
                 to the {Java{\raisebox{1ex}{TM}}} Programming
                 Language},
  pages = {183--200},
  ISSN = {0362-1340},
  booktitle = {Proceedings of the 13th Conference on Object-Oriented
                 Programming, Systems, Languages, and Applications
                 ({OOPSLA}-98)},
  month = oct # {~18--22},
  series = {ACM SIGPLAN Notices},
  volume = {33, 10},
  publisher = {ACM Press},
  address = {New York},
  year = 1998,
  WKloc = {B-0089}
}

@Unpublished{Bracha-Odersky-Stoutamire-Wadler-1998a,
  author = {{Gilad Bracha, Martin Odersky, David Stoutamire} and
                 Philip Wadler},
  title = {{GJ}: Extending the Java Programming Language with
                 type parameters},
  note = {draft paper},
  year = 1998,
  month = aug,
  WKloc = {B-0089}
}

@Unpublished{Bracha-Odersky-Stoutamire-Wadler-1998b,
  author = {Gilad Bracha and Martin Odersky and David Stoutamire and
                 Philip Wadler},
  title = {{GJ} Specification},
  note = {draft paper},
  year = 1998,
  month = may,
  WKloc = {B-0089}
}

@InProceedings{Brady-2011,
 author = {Brady, Edwin C.},
 title = {{IDRIS} --- Systems Programming Meets Full Dependent Types},
 booktitle = {Proceedings of the 5th ACM workshop on Programming languages meets program verification},
 series = {PLPV '11},
 year = {2011},
 isbn = {978-1-4503-0487-0},
 location = {Austin, Texas, USA},
 pages = {43--54},
 numpages = {12},
 url = {http://doi.acm.org/10.1145/1929529.1929536},
 doi = {http://doi.acm.org/10.1145/1929529.1929536},
 acmid = {1929536},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {data description, dependent types},
 abstract = {Dependent types have emerged in recent years as a promising approach to ensuring program correctness. However, existing dependently typed languages such as Agda and Coq work at a very high level of abstraction, making it difficult to map verified programs to suitably efficient executable code. This is particularly problematic for programs which work with bit level data, e.g. network packet processing, binary file formats or operating system services. Such programs, being fundamental to the operation of computers in general, may stand to benefit significantly from program verification techniques. This paper describes the use of a dependently typed programming language, Idris, for specifying and verifying properties of low-level systems programs, taking network packet processing as an extended example. We give an overview of the distinctive features of Idris which allow it to interact with external systems code, with precise types. Furthermore, we show how to integrate tactic scripts and plugin decision procedures to reduce the burden of proof on application developers. The ideas we present are readily adaptable to languages with related type systems.}
}

@InProceedings{Brady-McBride-McKinna-2003,
  author =       {Brady, E. and McBride, C. and McKinna, J.},
  title =        {Inductive families need not store their indices},
  crossref =  {TYPES2003},
  pages =     {115–-129},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@InProceedings{Bradley-1987,
  author = {Bradley, Laurette},
  title = {A Treatment of Languages With Stages of Evaluation},
  pages = {425--443},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  WKloc = {A-0063},
  abstract = {The notion of languages that inherently have
		  multiple stages of evaluation is introduced.
		  Typically, evaluation is done in stages so that
		  evaluation in some one stage is able to be done very
		  efficiently, even at the expense of prior stages of
		  evaluation. A key novel feature in such languages is
		  that constructs may have aprropriate times of
		  meaning as well as appropriate meanings. While it is
		  possible to give semantics to such languages without
		  regard to times of meaning, it is shown that certain
		  semantic related concepts, such as translation,
		  cannot be adequately described without reference to
		  times of evaluation. The major contribution provided
		  here to the study of complex languages with multiple
		  stages of evaluation is the development of a method
		  of describing such languages so that semantics
		  reflecting various times of evaluation can be
		  derived from the descriptions.}
}

@Misc{Bradley-1997,
  author =	 {Jeremy Bradley},
  title =	 {Binary Decision Diagrams --- A Functional Implementation},
  howpublished = {\textsf{http://www.cs.bris.ac.uk/\~{}bradley/publish/bdd/}},
  year =	 1997,
  URL = 	 {http://www.cs.bris.ac.uk/~bradley/publish/bdd/}
}

@InProceedings{Braeuner-1994,
  author = {T. Br\"auner},
  title = {A Model of Intuitionistic Affine Logic from Stable Domain Theory},
  pages = {340--351},
  crossref = {ICALP1994},
  authorsAddress = {Aarhus University},
  abstract = {Girard worked with the category of coherence spaces and
		  continuous stable maps and observed that the functor
		  that forgets the linearity of linear stable maps has
		  a left adjoint. This fundamental observation gave
		  rise to the discovery of Linear Logic. Since then,
		  the category of coherence spaces and linear stable
		  maps, with the comonad induced by the adjunction,
		  has been considered a canonical model of Linear
		  Logic. Now, the same phenomenon is present if we
		  consider the category of pre dI domains and
		  continuous stable maps, and the category of dI
		  domains and linear stable maps: the functor that
		  forgets the linearity has a left adjoint. This gives
		  an alternative model of Intuitionistic Linear
		  Logic. It turns out that this adjunction can be
		  factored in two adjunctions yielding a model of
		  Intuitionistic Affine Logic: the category of pre dI
		  domains and affine stable functions. It is the goal
		  of this paper to show that this category is actually
		  a model of Intuitionistic Affine Logic, and to show
		  that this category moreover has the properties which
		  make it possible to use it to model
		  convergence/divergence behaviour and recursion.}
}

@InProceedings{Braibant-Pous-2010,
  author =       {Thomas Braibant and Damien Pous},
  title =        {An Efficient {Coq} Tactic for Deciding Kleene Algebras},
  crossref =  {ITP2010},
  pages =     {163--178},
  DOI =   {10.1007/978-3-642-14052-5_13},
  SpringerURL =  {http://www.springerlink.com/content/w0342v526838n1q6/?MUD=MP},
  PrePDFURL = {http://hal.archives-ouvertes.fr/docs/00/49/16/64/PDF/dka.pdf},
  DOIURL = {http://dx.doi.org/10.1007/978-3-642-14052-5_13},
  bibliographies = {RATH, RelMiCS},
  abstract =    {We present a reflexive tactic for deciding the equational theory of Kleene algebras in the Coq proof assistant. This tactic relies on a careful implementation of efficient finite automata algorithms, so that it solves casual equations almost instantaneously. The corresponding decision procedure was proved correct and complete; correctness is established w.r.t. any model (including binary relations), by formalising Kozen's initiality theorem.}
}

@Article{Braibant-Pous-2012,
  author =       {Thomas Braibant and Damien Pous},
  title =        {Deciding Kleene Algebras in {Coq}},
  journal =      {Logical Methods in Computer Science},
  year =         {2012},
  month =       MAR,
  volume =    {8},
  number =    {1},
  pages =     {16},
  bibliographies = {RATH, RelMiCS},
  URL =     {http://www.lmcs-online.org/ojs/viewarticle.php?id=934},
  abstract =    {We present a reflexive tactic for deciding the equational theory of Kleene algebras in the Coq proof assistant. This tactic relies on a careful implementation of efficient finite automata algorithms, so that it solves casual equations instantaneously and properly scales to larger expressions. The decision procedure is proved correct and complete: correctness is established w.r.t. any model by formalising Kozen's initiality theorem; a counter-example is returned when the given equation does not hold. The correctness proof is challenging: it involves both a precise analysis of the underlying automata algorithms and a lot of algebraic reasoning. In particular, we have to formalise the theory of matrices over a Kleene algebra. We build on the recent addition of firstorder typeclasses in Coq in order to work efficiently with the involved algebraic structures.}
}

@InCollection{Brandes-2001,
  author = {Ulrik Brandes},
  title = {Drawing on Physical Analogies},
  crossref = {Kaufmann-Wagner-2001},
  pages = {71--86},
  chapter = 4,
  contents = {4.1 The Springs
       4.2 Force-Directed Placement
       4.3 Energy-Based Placement
       4.4 Modeling with Forces and Energies},
  URL = {http://link.springer.de/link/service/series/0558/bibs/2025/20250071.htm},
  WKloc = {A-1217 (incomplete), doc/pap/BIB}
}

@Book{Brassard-Bratley-1993,
  author = {Gilles Brassard and Paul Bratley},
  title = {{Algorithmik, Theorie und Praxis}},
  publisher = {Wolfram's Verlag},
  year = 1993,
  address = {Attenkirchen},
  UniBwM = {INF700/YA3326}
}

@INPROCEEDINGS{Brassel-Hanus-Huch-2004,
  author       = {Bra{\ss}el, B. and Hanus, M. and Huch, F.},
  title        = {Encapsulating Non-Determinism in Functional Logic Computations},
  year         = {2004},
  keywords     = {functional programming, logic programming, semantics},
  journal      = {Journal of Functional and Logic Programming},
  number       = {6},
  publisher    = {EAPLS},
  volume       = {2004},
  URL = {http://www.informatik.uni-kiel.de/~mh/publications/papers/JFLP04_findall.html},
  WKloc = {doc/pap/BIB},
  abstract = {One of the key features of the integration of
      functional and logic languages is the access to non-deterministic
      computations from the functional part of the program.  In order to
      ensure the determinism of top-level computations in a functional logic
      program, which is usually a monadic sequence of I/O operations, one
      has to encapsulate the non-determinism (i.e., search for solutions)
      occurring in logic computations.  However, an appropriate approach to
      encapsulation can be quite subtle if subexpressions are shared, as in
      lazy evaluation strategies.  In this paper we examine the current
      approaches to encapsulate non-deterministic computations for the
      declarative multi-paradigm language Curry, show their relative
      advantages and the problems they induce. Furthermore, we present a new
      approach which combines the advantages but avoids the problems.  Our
      proposal is based on providing a primitive I/O action for
      encapsulation from which various specialized search operators can be
      derived.  In order to provide a formal foundation for this new
      approach to encapsulation, we define the operational semantics of this
      new primitive.}
}

@InCollection{Braun-1991,
  author = {S. Braun},
  title = {{Relationale Datenbanken mit multiplen Werten}},
  year = 1991,
  publisher = {Springer-Verlag},
  pages = {115--124},
  editor = {Manfred Broy},
  booktitle = {{Informatik und Mathematik. Festschrift zum 65.~Geburtstag
		  von F.L.~Bauer}},
  bibliographies = {RelMiCS}
}

@PhdThesis{BraunO-2004,
  author = 	 {Oliver Alexander Braun},
  title = 	 {Constructing Mobile Agents using Transformations},
  school = 	 {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 	 2004,
  WKloc = 	 {C-0021}
}

@InCollection{Braun-Schmidt-1984,
  author = {S. Braun and G. Schmidt},
  title = {{LINDAS --- Das Lebensmittel-, Inhaltsstoff- und
                   N\"ahrwert-Datenbanksystem}},
  year = 1984,
  volume = {BFE-R-84-04},
  series = {{Berichte der Bundesforschungsanstalt f\"ur Ern\"ahrung}},
  pages = {131--136},
  booktitle = {{Entwicklung und Benutzung von
		  N\"ahrstoff-Datenbanken in der BR Deutschland}}
}

@article{Bravenboer-Kalleberg-Vermaas-Visser-2008,
  title = "Stratego/XT 0.17. A language and toolset for program transformation",
  journal = "Science of Computer Programming",
  volume = "72",
  number = "1--2",
  pages = "52--70",
  year = "2008",
  note = "Special Issue on Second issue of experimental software and toolkits (EST)",
  issn = "0167-6423",
  DOI = "10.1016/j.scico.2007.11.003",
  DOIURL = "http://dx.doi.org/10.1016/j.scico.2007.11.003",
  URL = "http://www.sciencedirect.com/science/article/pii/S0167642308000452",
  author = "Martin Bravenboer and Karl Trygve Kalleberg and Rob Vermaas and Eelco Visser",
  keywords = "Stratego",
  keywords = "Stratego/XT",
  keywords = "Program transformation",
  keywords = "Rewriting strategies",
  keywords = "Rewrite rules",
  keywords = "Concrete syntax",
  keywords = "Dynamic rewrite rules ",
  abstract = "Stratego/XT is a language and toolset for program
                  transformation. The Stratego language provides
                  rewrite rules for expressing basic transformations,
                  programmable rewriting strategies for controlling
                  the application of rules, concrete syntax for
                  expressing the patterns of rules in the syntax of
                  the object language, and dynamic rewrite rules for
                  expressing context-sensitive transformations, thus
                  supporting the development of transformation
                  components at a high level of abstraction. The
                  \{XT\} toolset offers a collection of flexible,
                  reusable transformation components, and tools for
                  generating such components from declarative
                  specifications. Complete program transformation
                  systems are composed from these components. This
                  paper gives an overview of Stratego/XT 0.17,
                  including a description of the Stratego language and
                  \{XT\} transformation tools; a discussion of the
                  implementation techniques and software engineering
                  process; and a description of applications built
                  with Stratego/XT. "
}


@InProceedings{BreazuTannen-Kesner-Puel-1993,
  title = {A Typed Pattern Calculus},
  author = {Val Breazu-Tannen and Delia Kesner and Laurence Puel},
  pages = {262--274},
  crossref = {LICS8},
  abstract = {The theory of programming with pattern-matching function
		  definitions has been studied mainly in the framework
		  of first-order rewrite systems. We present a typed
		  functional calculus that emphasizes the strong
		  connection between the structure of whole pattern
		  definitions and their types. In this calculus
		  typechecking guarantees the absence of runtime
		  errors caused by nonexhaustive pattern-matching
		  definitions. Its operational semantics is
		  deterministic in a natural way, without the
		  imposition of ad-hoc solutions such as clause order
		  or ``best fit''.  In the spirit of the Curry-Howard
		  isomorphism, we design the calculus as a
		  computational interpretation of the Gentzen sequent
		  proofs for the intuitionistic propositional logic.
		  We prove the basic properties connecting typing and
		  evaluation: subject reduction and strong
		  normalization. We believe that this calculus offers
		  a rational reconstruction of the pattern-matching
		  features found in successful functional languages.},
  annote = {GFA}
}

@InProceedings{BreazuTannen-Meyer-1985,
  author = {Val Breazu-Tannen and Albert R. Meyer},
  title = {Lambda Calculus with Constrained Types},
  crossref = {LOP1985},
  WKloc = {A-0078},
  abstract = {Motivsted by domain equations, we consider types
		  satisfying arbitrary equational constraints thus
		  generalizing a range of situations with the finitely
		  typed case at one extreme and the type-free case at
		  the other. The abstract model theory of th
		  $\beta\eta$ type-free case is generalized. We
		  investigate the relation between lambda calculus
		  with constrained types and cartesian closed
		  categories (CCC's) at proof-theoretic and
		  model-theoretic levels. We find an adjoint
		  equivalence between the category of typed
		  $\lambda$-algebras and that of CCC's. The
		  subcategories of typed $\lambda$-models and concrete
		  cccs correspond to each other under this
		  equivalence. All these results are parameterized by
		  an arbitrary set of higher-order constants and an
		  arbitrary set of higher-order equations.},
  pages = {23--40}
}

@InProceedings{BreazuTannen-Subrahmanyam-1992,
  author = {Val Breazu-Tannen and Ramesh Subrahmanyam},
  title = {On extending computational adequacy by data abstraction},
  pages = {161--169},
  booktitle = {Proceedings of the conference on {Lisp} and functional programming,
               {June 22 - 24, 1992, San Francisco, CA, USA}},
  year = 1992,
  organisation = {ACM},
  URL = {http://www.acm.org/pubs/citations/proceedings/lfp/141471/p161-breazu-tannen/},
  abstract = {Given an abstract data type(ADT), and algebra that
              {\em specifies} it, and an implementation of the data type
              in a certain language, if the implementation is ``correct'',
              a certain principle of {\em modularity of reasoning} holds.
              Namely, one can safely reason about programs in the
              language extended by the ADT, by interpreting the
              ADT operation symbols according to the specification algebra.
              The main point of this paper is to formalize correctness
              as a local condition involving only the specification
              and the implementation and to prove the equivalence of
              such a condition to the modularity principle.
              We conduct our study in the context of a language
              without divergence (in subsection 2.1), and for
              languages with divergence and general recursion
              (in subsections 2.2 and 2.3). We also describe a
              sufficient condition under which, given an implementation,
              there may be a finite set of observational equivalences
              which imply the local condition. Further, we illustrate a
              technique for proving in a practical situation that a given
              implementation of an abstract data type is correct.}
}

@Booklet{Bredihin,
  author = {D. A. Bredihin},
  title = {Abstract Characterization of some Classes of Algebras of
		Binary Relations},
  note = {see Zbl 394.04001},
  bibliographies = {RelMiCS}
}

@Article{Bredihin-Schein-1978,
  author = {D. A. Bredihin and Boris M. Schein},
  title = {Representations of Ordered Semigroups and Lattices by
		Binary Relations},
  journal = COLL,
  volume = 39,
  year = 1978,
  pages = {1--12},
  bibliographies = {RelMiCS}
}

@MastersThesis{Brethauer-1991,
  keywords = {Formula manipulation system, computer aided proof,
		  relation algebra},
  year = 1991,
  title = {{Ein Formelmanipulationssystem zur
		  com\-pu\-ter\-ge\-st\"utz\-ten Beweis\-f\"uh\-rung in der
		  Relationenalgebra}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 43/91},
  note = {ID 43/91},
  month = DEC,
  author = {Roland Brethauer},
  bibliographies = {RelMiCS}
}

@PhDThesis{Bretz-1989,
  keywords = {error handling, functional programming language,
		  exception handling, labmda calculus},
  abstract = {Ein Sprachkonstrukt zur Ausnahmebehandlung ist eine
		  Kontrollstruktur, die es erlaubt, die
		  Standardfortsetzung eines Programms bei Erkennen
		  eines Ausnahmeereignisses durch eine
		  Ausnahmefortsetzung zu ersetzen; zur Markierung
		  verschiedener Ausnahmeausstiegspunkte f\"ur eine
		  Programm werden Ausnahmebezeichner verwendet.

                  In der vorliegenden Arbeit wird ein Konzept zur
		  Behandlung von Ausnahmen entwickelt und in eine
		  funktionale, am $\lambda$-Kalk\"ul orientierte
		  Sprache integriert. Das Konzept zur
		  Ausnahmebehandlung basiert auf dem Ersetzungsmodel
		  von Yemini und Berry, einem Vorschlag zur
		  Ausnahmebehandlung f\"ur Ausdruckssprachen.

                  Im Mittelpunkt steht die formale Beschreibung der
		  konzipierten Programmiersprache ALEX, die sich in
		  drei Teile gliedern l\"a\ss{}t: (1) die kontextfreie
		  Syntax ist durch eine BNF-Gramatik angegeben, (2)
		  die Kontextbedingungen sind als Typinferenzregeln
		  formuliert, (3) die Semantik ist denotational und
		  operational, sowohl f\"ur die verz\"ogerte als auch
		  f\"ur die vorzeitige Parameterauswertung
		  spezifiziert.

                  ALEX ist Ausgangspunkt f\"ur eine Reihe von
		  Spracherweiterungen. Im einzelnen werden default
		  handler, unparametrisierte Ausnahmen, eine variante
		  der retry-Handlerreaktion sowie meherere
		  Ausnahmevereinbarungen f\'ur eine Funktion
		  spezifiziert.

                  Gleichzeitig wird in der Arbeit ein allgemeines
		  Begriffssystem zur Beschreibung von Konzepten zur
		  Behandlung von Ausnahmen entwickelt und anhand von
		  Beispielen erl\"autert. Die eingef\"uhrten Begriffe
		  sind klar und pr\"azise beschrieben und in die
		  Begriffswelt des Software-Engineering und der
		  Programmiersprachen eingebettet. Die Darstellung der
		  Ausnahmemechanismen der Programmiersprachen PL/I,
		  CLU, Ada, CHILL, ML und VAL in der erarbeiteten
		  Nomenklatur rundet den Terminolo9gieteil ab.},
  year = 1989,
  title = {{Ein Konzept zur Ausnahmebehandlung f\"ur
		  funktionale Programmiersprachen und dessen formale
		  Beschreibung}},
  school = {Erziehungswissenschaftliche Hochschule Rheinland Pfalz},
  author = {Manfred Bretz}
}

@InProceedings{Breu-Grosu-Huber-Rumpe-Schwerin-1998,
  author = {Ruth Breu and Radu Grosu and Franz Huber and Bernhard Rumpe and
            Wolfgang Schwerin},
  title = {Systems, Views and Models of UML},
  booktitle = {The Unified Modeling Language,
               Technical Aspects and Applications},
  year = 1998,
  publisher = {Physica Verlag, Heidelberg},
  editor = {Martin Schader and  Axel Korthaus},
  URL = {http://www4.informatik.tu-muenchen.de/papers/BGHRS98.html},
  abstract = {In this paper we show by using the example of UML,
              how a software engineering method can benefit from an
              integrative mathematical foundation. The mathematical
              foundation is given by a mathematical system model. This
              model provides the basis both for integrating the various
              description techniques of UML and for implementing
              methodical support.  After describing the basic concepts
              of the system model, we give a short overview of the
              UML description techniques. Then we show how they fit
              into the system model framework and sketch an approach
              to structure the UML development process such that it
              provides methodological guidance for developers.},
  CRClassification = {D.1.5., D.2.1, D.2.4., F.3.1},
  CRGenTerms = {Design},
  pages = {93--109},
  bibliographies = {SeminarWT2000}
}

@TechReport{Breuer-1991,
  author = {Peter T. Breuer},
  title = {An analysis/synthesis language with learning strategies},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  year = 1991,
  month = JUL,
  number = {PRG-TR-13-91},
  WKloc = {A-0080},
  keywords = {REDO},
  abstract = {PARLEY is a declarative programming language based
		  on the precept that `solution synthesis' from the
		  solutions to subproblems and `problem analysis' into
		  the set of subproblems ought to be the only
		  components of a program description to concern the
		  programmer. The programming style is introduced in
		  conjunction with an operational semantics which
		  lends itself to shared data and parallel processing models.},
  bibliographies = {RelMiCS}
}

@InProceedings{Brien-1993,
  author = {STephen Brien},
  title = {The Development of {Z}},
  crossref = {SoSL93},
  pages = {1--14}
}

@Article{Brink-1977,
  author = {Chris Brink},
  title = {On {Birkhoff's} Postulates for a Relation Algebra},
  journal = JLON,
  volume = 15,
  year = 1977,
  pages = {391--394},
  OPTnote = {MR 56\#207},
  bibliographies = {RelMiCS}
}

@Article{Brink-1978,
  author = {Chris Brink},
  title = {On {Peirce's} Notation for the Logic of Relatives},
  journal = PEIRCE,
  volume = 14,
  year = 1978,
  pages = {285--304},
  bibliographies = {RelMiCS}
}

@Article{Brink-1979,
  author = {Chris Brink},
  title = {The Algebra of Relatives},
  journal = NOTRE,
  volume = 20,
  year = 1979,
  pages = {900--908},
  bibliographies = {RelMiCS}
}

@Article{Brink-1979a,
  author = {Chris Brink},
  title = {Two Axiom Systems for Relation Algebras},
  journal = NOTRE,
  volume = 20,
  year = 1979,
  pages = {909--914},
  OPTnote = {MR 80m:03101},
  bibliographies = {RelMiCS}
}

@Article{Brink-1981,
  author = {Chris Brink},
  title = {Boolean Modules},
  journal = JALG,
  volume = 71,
  year = 1981,
  pages = {291--313},
  OPTnote = {MR 83a:06020},
  bibliographies = {RelMiCS}
}

@Article{Brink-1988,
  author = {Chris Brink},
  title = {On the Application of Relations},
  journal = SAJ,
  year = 1988,
  volume = {7(2)},
  pages = {105--112},
  bibliographies = {RelMiCS}
}

@Article{Brink-1993,
  year = 1993,
  volume = 30,
  title = {Power Structures},
  pages = {177--216},
  journal = ALGU,
  author = {Chris Brink},
  bibliographies = {RelMiCS}
}

@Article{Brink-Britz-Melton-1993,
  year = 1993,
  volume = 54,
  title = {A note on fuzzy power relations},
  pages = {115--117},
  journal = Fuzzy_Sets_and_Systems,
  author = {Chris Brink and Katarina Britz and Austin Melton},
  bibliographies = {RelMiCS}
}

@Article{Brink-Britz-Schmidt-1994,
  year = 1994,
  volume = 6,
  title = {{Peirce} Algebras},
  pages = {339--358},
  journal = FACOMP,
  author = {Chris Brink and Katarina Britz and Renate Schmidt},
  bibliographies = {RelMiCS}
}

@TechReport{Brink-Britz-SchmidtR-1992,
  author = {Chris Brink and Katarina Britz and Renate A. Schmidt},
  title = {Peirce Algebras},
  institution = {Max-Planck-Institut f\"ur Informatik},
  year = 1992,
  number = {MPI-I-92-229},
  address = {Im Stadtwald, Saarbr\"ucken},
  month = JUL,
  authorsAddress = {schmidt@mpi-sb.mpg.de},
  WKloc = {C-0004},
  OPTabstract = {We present a two-sorted algebra, called a {\em
		  Peirce algebra}, of relations and sets interacting
		  with each other. In a Peirce algebra, sets can
		  combine with each other as in a Boolean algebra,
		  relations can combine with each other as in a
		  relation algebra, and in addition we have both a
		  relation-forming operator on sets (the Peirce
		  product of Boolean modules) and a set-forming
		  operator on relations (a cylindrification
		  operation). Two applications of Peirce algebras are
		  given. The first points out that Peirce algebras
		  provide a natural algebraic framework for modelling
		  certain programming constructs. The second shows
		  that the so-called {\em terminological logics}
		  arising in knowledge representation have evolved a
		  semantics best described as a calculus of relations
		  interacting with sets.},
  keywords = {Boolean modules, relation algebras, terminological
		  logics, weakest prespecifications}
}

@Article{Brink-Gabbay-Ohlbach-1995,
  year = 1995,
  volume = 29,
  title = {Towards Automating Duality},
  pages = {73--90},
  journal = Computers_and_Mathematics_with_applications,
  author = {Chris Brink and Dov Gabbay and Hans J\"urgen Ohlbach},
  bibliographies = {RelMiCS}
}

@TechReport{Brink-Rewitzky-1992,
  author = {Chris Brink and Ingrid Rewitzky},
  institution = {Dept.\null{} of Mathematics, Univ.\null{} of Cape Town},
  title = {Predicate Transformers as Power Operations},
  year = 1992,
  number = {RR 137},
  pages = {27 p.},
  bibliographies = {RelMiCS}
}

@Article{Brink-Rewitzky-1992a,
  author = {Chris Brink and Ingrid Rewitzky},
  title = {Modelling the Algebra of Weakest Preconditions},
  journal = South_African_Computer_Journal,
  year = 1992,
  volume = 6,
  pages = {11--20},
  bibliographies = {RelMiCS}
}

@Article{Brink-Rewitzky-1995,
  year = 1995,
  volume = 7,
  title = {Predicate Transformers as Power Operations},
  pages = {169--182},
  journal = FACOMP,
  author = {Chris Brink and Ingrid Rewitzky},
  bibliographies = {RelMiCS}
}

@Article{Brink-Schmidt-1992,
  year = 1992,
  volume = 23,
  title = {Subsumption computed algebraically},
  pages = {329--342},
  journal = Computers_and_Mathematics_with_applications,
  author = {Chris Brink and Renate Schmidt},
  note = {Special Issue on semantic networks in {A}rtificial {I}ntelligence},
  bibliographies = {RelMiCS}
}

@Article{Brink-Vermeulen-Pretorius-1991,
  author = {Chris Brink and J. J. C. Vermeulen and J. P. G. Pretorius},
  title = {Verisimilitude via Vietoris},
  journal = Journal_of_Logic_and_Computation,
  year = 1992,
  volume = 2,
  pages = {709--718},
  bibliographies = {RelMiCS}
}

@TechReport{Brink-Vermeulen-Pretorius-1991a,
  author = {Chris Brink and J. J. C. Vermeulen and J. P. G. Pretorius},
  title = {Verisimilitude Via Vietoris},
  institution = {Dept.\null{} of Mathematics, Univ.\null{} of Cape Town},
  year = 1991,
  number = {RR 117},
  month = MAY,
  bibliographies = {RelMiCS}
}

@Misc{Broadbery-Bonstein-2002,
  author = {Peter Broadbery and Manuel Bronstein},
  title = {A First Course on {Aldor} with \texttt{libaldor}},
  month = MAR,
  year = 2002,
  WKloc = {A-1419},
  bibliographies = {MathScheme},
  URL = {http://www.aldor.org/}
}

@InProceedings{Broadbery-GomezDiaz-Watt-1995,
  author = {Broadbery, Gomez-Diaz, Watt},
  title = {On the Implementation of Dynamic Evalution},
  booktitle = {Proc. International Symposium on Symbolic and Algebraic Computation {(ISSAC 95), 10-12 July 1995}},
  pages = {77--84},
  year = 1995,
  publisher = {ACM},
  abstract = {Dynamic evaluation is a technique for producing multiple
      results according to a decision tree which evolves with program
      execution. Sometimes it is desired to produce results for all
      possible branches in the decision tree, while on other occasions it
      may be sufficient to compute a single result which satisfies certain
      properties. This technique finds use in computer algebra where
      computing the correct result depends on recognising and properly
      handling special cases of parameters. In previous work, programs
      using dynamic evaluation have explored all branches of decision trees
      by repeating the computations prior to decision points.

      This paper presents two new implementations of dynamic evaluation
      which avoid recomputing intermediate results. The first approach uses
      Scheme ``continuations'' to record state for resuming program
      execution. The second implementation uses the Unix ``fork'' operation
      to form new processes to explore alternative branches in parallel.

      These implementations are based on modifications to Lisp- and C-based
      run-time systems for the Axiom Version 2 extension language
      (previously known as $A^{\#}$). This allows the same high-level
      source code to be compared using the ``re-evaluation,'' the
      ``continuation,'' and the ``fork'' implementations.},
  URL = {http://www.aldor.org/docs/reports/i95dynev/dyneval.html},
  WKloc = {A-1209, doc/pap/BIB},
  bibliographies = {RelMiCS, MathScheme}
}

@TechReport{Brobeil-Maly-Schmitz-Ziegler-1991,
  year = 1991,
  title = {{MC68000 --- Ein 68000-Entwicklungssystem f\"ur die Lehre}},
  number = 9102,
  month = FEB,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {U. Brobeil and S. Maly and Lothar Schmitz and T. Ziegler}
}

@InCollection{Brockenauer-Cornelsen-2001,
  author = {Ralf Brockenauer and Sabine Cornelsen},
  title = {Drawing Clusters and Hierarchies},
  crossref = {Kaufmann-Wagner-2001},
  pages = {193--227},
  chapter = 8,
  contents = {8.1 Definitions
       8.2 Clustering Methods
       8.2.1 $k$-Way Partition
       8.2.2 Structural Clustering
       8.2.3 Other Approaches
       8.3 Planar Drawings of Hierarchical Clustered Graphs
       8.3.1 Straight-Line Drawings with Convex Clusters
       8.3.2 Orthogonal Drawings with Rectangular Clusters
       8.3.3 Multilevel Visualization of Clustered Graphs
       8.4 Hierarchical Representation of Compound Graphs
       8.4.1 Conventions
       8.4.2 The Layout Algorithm
       8.5 Force-Directed Methods for Clustered Graphs
       8.5.1 Inserting Dummy Vertices
       8.5.2 Interactive Clustering
       8.5.3 Meta Layouts
       8.6 Online Graph Drawing of Huge Graphs - A Case Study
       8.7 Summary},
  URL = {http://link.springer.de/link/service/series/0558/bibs/2025/20250193.htm},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Brockmeyer-Wittich-1998,
  author = {Udo Brockmeyer and Gunnar Wittich},
  title = {Tamagotchis Need No Die -- Verification of {\sc Statemate} Designs},
  booktitle = {Tools and Algorithms for the Construction and Analysis of Systems,
               4th International Conference, TACAS'98},
  editor = {Bernhard Steffen},
  year = 1998,
  pages = {217--231},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  volume = 1384,
  bibliographies = {SeminarWT2000}
}

@Article{Brodal-Okasaki-1996,
  author = {Gerth Stølting Brodal and Chris Okasaki},
  journal = JFP,
  title = {Optimal Purely Functional Priority Queues},
  year = 1996,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#priority},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/priority.ps},
  keywords = {purely functional data structures, priority queues,
                 O(1) insert/merge/findMin, O(log n) deleteMin,
                 data-structural bootstrapping},
  month = dec,
  number = 6,
  volume = 6,
  scope = {pearls},
  WKloc = {B-0067}
}

@InProceedings{Brogi-1990,
  title = {{AND-Parallelism} without Shared Variables},
  pages = {306--321},
  crossref = {ICLP90},
  author = {Brogi Antonio}
}

@InProceedings{Brogi-Chiarelli-Mancarella-Mazotta-Pedreschi-Renso-Turini-1994,
  author = {A. Brogi and A. Chiarelli and P. Mancarella and
		  V. Mazotta and D. Pedreschi and C. Renso and F. Turini},
  title = {Implementation of Program Composition Operations},
  crossref = {PLILP1994},
  pages = {292--307},
  abstract = {This paper reports on a number of implementations
		  for a suite of composition operations of logic
		  programs. $\ldots$}
}

@InProceedings{Brogi-Turini-1994,
  title = {Semantics of Meta-Logic in an Algebra of Programs},
  author = {A. Brogi and F. Turini},
  pages = {262--270},
  crossref = {LICS9},
  abstracts = {Meta-programming is a powerful technique for extending and
      modifying the semantics of an existing object language. Along with
      the expressiveness, however, meta-programming puts forth some subtle
      semantic problems, among which the most critical is bound to the
      representation of object programs at the meta-level. We propose a
      semantic justification for a simple representation technique in the
      field of a generalised notion of meta-programming in logic. The
      generalisation consists in specifying the meta-programs with respect
      to object programs defined by program expressions. The expressions
      are defined via a rich suite of operations on logic programs. The
      technique allows one to build straightforward and concise
      meta-programs via the representation of object level variables by
      meta-level variables.}
}

@InProceedings{Bronsard-1990,
  author = {Francois Bronsard},
  title = {Axiomatization of a Functional Logic Language},
  pages = {101--116},
  abstract = {Functional logic languages incorporate logic programming
             capabilities of constraint solving within a functional language
             framework. We consider a prototypical functional logic language
             which supports definite descriptions, i.e., terms of the form
             ``the $\x$ such that $\p$''. Its semantics is defined in terms
             of flat ordered structures with and elements. These elements
             are used to represent the absence and ambiquity, respectively,
             of objects denoted by descriptions. We provide an equational
             axiomatization of the language and show that it is complete for
             this semantics.},
  crossref = {ALP1990}
}

@InCollection{Brookes-1994,
  author = {Stephen Brookes},
  title = {Fair Communicating Processes},
  crossref = {Roscoe-1994},
  pages = {59--74},
  chapter = 4,
  OPTnote = {},
  OPTannote = {}
}

@incollection{ Brookes-Geva-1992,
  author = "Stephen Brookes and Shai Geva",
  title = "Computational Comonads and Intensional Semantics",
  booktitle = "Applications of Categories in Computer Science: Proceedings {LMS} Symp., Durham, {UK}, 20--30 July 1991",
  volume = "177",
  publisher = "Cambridge University Press",
  address = "Cambridge",
  editor = "M. P. Fourman and P. T. Johnstone and A. M. Pitts",
  pages = "1--44",
  year = "1992",
  url = "citeseer.ist.psu.edu/brookes91computational.html",
  WKloc = {A-1575, doc/pap/BIB}
}

@InProceedings{Brookes-Roscoe-1984,
  author = {S.D. Brookes and A.W. Roscoe},
  title = {An improved failure model for communicating sequential processes},
  booktitle = {Proc.\null{} of the NFS-SERC Seminar on Concurrency},
  series = LNCS,
  volume = 197,
  publisher = Springer,
  year = 1984,
  pages = {281--305},
  bibliographies = {RelMiCS}
}

@Article{Brooks-1987,
  author = {Frederick P. Brooks},
  title = {No Silver Bullet},
  journal = {IEEE Computer},
  volume = 20,
  number = 4,
  pages = {10--19},
  month = apr,
  year = 1987,
  keywords = {misc se-lit reuse binder}
}

@InProceedings{Brown-Fuchs-Roble-Wadler-2001,
  author = {Allen Brown and Matthew Fuchs and Jonathan Robie and Philip Wadler},
  title = {MSL: A model for W3C XML Schema},
  booktitle = {{WWW10, Hong Kong, May 2001}},
  year = 2001,
  URL = {http://cm.bell-labs.com/cm/cs/who/wadler/topics/xml.html#msl},
  abstract = {MSL (Model Schema Language) is an attempt to formalize some
      of the core idea in XML Schema. The benefits of a formal description
      is that it is both concise and precise. MSL has already proved
      helpful in work on the design of XML Query. We expect that similar
      techniques can be used to extend MSL to include most or all of XML
      Schema.},
  WKloc = {A-1184, doc/pap/BIB},
  bibliographies = {DigBib}
}

@Article{Brown-Gurr-1993,
  author = {Carolyn Brown and Doug Gurr},
  title = {A Representation Theorem for Quantales},
  journal = JPAA,
  year = 1993,
  volume = 85,
  pages = {27--42},
  bibliographies = {RelMiCS}
}

@InProceedings{Brown-Hutton-1994,
  title = {Categories, Allegories and Circuit Design},
  author = {Carolyn Brown and Graham Hutton},
  pages = {372--381},
  crossref = {LICS9},
  WKloc = {A-0376},
  abstract = {Relational languages such as {\sc Ruby} are used to derive
		  hardware circuits from abstract specifications of
		  their behaviour. Much reasoning is done informally
		  in {\sc Ruby} using pictorial representations of
		  relational terms.  We formalise this use of pictures
		  in circuit design. We show that pictures naturally
		  form a {\em unitary pretabular
		  allegory}. Homomorphisms of pictures correspond to
		  adding new wires or circuit components. Two pictures
		  are mutually homomorphic if and only if they
		  represent equal allegorical terms. We prove
		  soundness and completeness results which guarantee
		  that deriving circuits using pictures does not lead
		  to errors.  We illustrate the use of pictures by
		  deriving the ripple adder implementation from a high
		  level, behavioural specification.},
  bibliographies = {RelMiCS, GraphCalc}
}

@InProceedings{Brown-Jeffrey-1993,
  author = {Carolyn Brown and Alan Jeffrey},
  title = {Allegories of Circuits},
  booktitle = {Proc. Logic For Computer Science},
  publisher = Springer,
  year = 1994,
  WKloc = {A-0750},
  bibliographies = {RelMiCS, GraphCalc}
}

@InProceedings{Broy-1985,
  author = {Manfred Broy},
  title = {Extensional Behaviour of Concurrent, Nondeterministic, Communicating Systems},
  pages = {229--276},
  abstract = {A simple denotational model for nondeterministic
              communicating, concurrent agents is introduced that allows
              to represent the extensional behaviour of communicating agents.
              Based on this model several concepts of program correctness
              are introduced, analysed and related to classical concepts.
              Observability concepts are discussed for an
              operational semantics given in the form of
              labelled rewriting systems. In particular the close connections
              between domain theory and fixed point theory for
              concurrent programs and concepts of correctness are
              invstigated and outlined, and generalisations of the notions
              of partial and total correctness are given.},
  crossref = {Marktoberdorf-1985}
}

@Article{Broy-1986,
  author = {Manfred Broy},
  title = {Denotational Semantics of Communicating Sequential Programs},
  journal = ACTIN,
  year = 1986,
  volume = 23,
  pages = {253--259},
  WKloc = {A-1313}
}

@Article{Broy-1987,
  author = {Manfred Broy},
  title = {Predicative Specifications for Functional Programs Describing Communicating Networks},
  journal = ACTIN,
  year = 1987,
  volume = 25,
  pages = {93--101},
  WKloc = {A-1310}
}

@InProceedings{Broy-Hinkel-Nipkow-Prehofer-Schieder-1994,
  author = {Manfred Broy and Ursula Hinkel and Tobias Nipkow and
        Christian Prehofer and Birgit Schieder},
  title = {Interpreter Verification for a Functional Language},
  booktitle = {Proc.\ 14th Conf.\ Foundations of Software Technology and
           Theoretical Computer Science},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  series = {LNCS},
  year = 1994,
  OPTorganization = {},
  publisher = {Springer-Verlag},
  OPTaddress = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {to appear},
  OPTunibwm = {},
  OPTannote = {}
}

@TechReport{Broy-Stefanescu-1996,
  author = {Manfred Broy and {Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {The Algebra of Stream Processing Functions},
  year = 1996,
  month = MAY,
  institution = {Technische Universit\"at M\"unchen, Institut f\"ur Informatik,
                 Sonderforschungsbereich 342: Methoden und Werkzeuge
                 f"ur die Nutzung paralleler Rechnerarchitekturen},
  OPTtype = {},
  OPTnumber = {TUM-I9620, SFB-Bericht Nr.~342/11/96 A},
  OPTaddress = {},
  WKloc = {A-0888}
}

@Article{Broy-Stefanescu-2001,
  author = {Manfred Broy and {Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {The Algebra of Stream Processing Functions},
  journal = 	 TCS,
  year = 	 2001,
  volume =	 258,
  pages =	 {99--129},
  bibliographies = {RelMiCS},
  WKloc = 	 {doc/pap/BIB},
  abstract = {Data flow networks are a model of concurrent
     computation. They consist of a collection of concurrent
     asynchronous processes which communicate by sending data over
     FIFO channels. In this paper we study the algebraic structure of
     the data ow networks and base their semantics on stream
     processing functions. Our algebraic theory is based on the
     \emph{calculus of flownomials}. With an \emph{additive} (or
     \emph{cantorian})interpretation the calculus gives a unified
     presentation of the classical algebraic models for control
     structures, that is,regular algebra and iteration theories. The
     kernel of the calculus is an equational axiomatization called
     basic network algebra (BNA) for flow graphs modulo graph
     isomorphism. We show that the algebra of stream processing
     functions called $\mathsf{SPF}$ (used for deterministic networks)
     and the algebra of sets of stream processing functions called
     $\mathcl{P}\mathsf{SPF}$ (used for nondeterministic networks) are
     BNA algebras. Actually they give a \emph{multiplicative} (or
     \emph{cartesian}) interpretation of the calculus of
     flownomials. As a byproduct this shows that both semantic models
     are compositional. This means the semantics of a network may be
     described in terms of the semantics of its components. (As it is
     well known this is not true for the input-output relational
     semantics of nondeterministic networks.) We also identify
     additional axioms satisfied by the branching constants in these
     two algebraic theories. For the deterministic case we study in
     addition the coarser equivalence relation on networks given by
     the input-output behavior and provide a correct and complete
     axiomatization.}
}

@Book{Broy-Wirsing-1991,
  editor =    {Broy, M. and Wirsing, M.},
  title =        {Methods of Programming, Selected Papers of the CIP-project},
  publisher =    Springer,
  year =         1991,
  volume =    544,
  series =    LNCS
}

@Article{Bruce-Meyer-Mitchell-1990,
  author = {Kim B. Bruce and Albert R. Meyer and John C. Mitchell},
  title = {The Semantics of Second-Order Lambda Calculus},
  journal = INFCOMP,
  year = 1990,
  volume = 85,
  pages = {76--134},
  number = 1,
  UniBwM = {Z484-85x-1},
  WKloc = {A-0213},
  abstract = {In the second-order (polymorphic) typed lambda
		  calculus, lambda abstraction over type variables
		  leads to terms denoting polymorphic functions.
		  Straightforward cardinality considerations show that
		  a naive set-theoretic interpretation of the calculus
		  is impossible. We give two definitions of semantic
		  models for this language and prove them equivalent.
		  Our syntactical ``environment model'' definition and
		  a more algebraic ``combinatory model'' definition
		  for the polymorphic calculus correspond to analogous
		  model definitions for untyped lambda calculus.
		  Soundness and completeness theorems are proved using
		  the environment model definition. We verify that
		  some specific interpretations of the calculus
		  proposed in the literature indeed yield models in
		  our sense.},
  annote = {large bibliography on second order lambda calculus},
  bibliographies = {RelMiCS, LogRel}
}

@InProceedings{Bruce-Mitchell-1992,
  author = {Kim Bruce and John C. Mitchell},
  title = {{PER} models of subtyping, recursive types and higher-order
          polymorphism},
  pages = {316--327},
  abstract = {We relate standard techniques for solving recursive domain
             equations to previous models with types interpreted as partial
             equivalence relations (per's) over a $D_\infty$ lambda model.
             This motivates a particular choice of type functions, which
             leads to an extension of such models to higher-order
             polymorphism. The resulting models provide natural
             interpretations for function spaces, records, recursively
             defined types, higher-order type functions, and polymorphic
             type $\forall X <: Y.A$ where the bound may be of a higher
             kind. In particular, we may combine recusion and polymorphism
             in a way that allows the bound $Y$ in $\forall X <: Y.A$ to be
             recursively defined. The model may also be used to interpret
             so-called ``F-bounded polymorphism.'' Together, these features
             allow us to represent several forms of type and type functions
             that seem to arise naturally in typed object-oriented
             programming.},
  crossref = {POPL1992},
  authorsAddress = {kin\@cs.williams.edu, mitchell\@cs.stanford.edu},
  WKloc = {A-0165}
}

@InProceedings{Bruce-Petersen-Fiech-1997,
  author = {Kim B. Bruce and Leaf Petersen and Adrian Fiech},
  title = {Subtyping is not a good ``Match'' for object-oriented languages},
  booktitle = {ECOOP '97},
  pages = {104--127},
  year = 1997,
  OPTeditor = {},
  volume = 1241,
  series = LNCS,
  publisher = Springer,
  URL = {http://www.cs.williams.edu/~kim/README.html#Match},
  abstract = {We present the design and rationale of a new
      statically-typed object-oriented language, LOOM. LOOM retains most of
      the features of the earlier language PolyTOIL. However the subtyping
      relation is dropped from LOOM in favor of the matching relation.
      ``Hash types'', which are defined in terms of matching, are
      introduced to provide some of the benefits of subtyping. These types
      can be used to provide support for heterogeneous data stuctures in
      LOOM. LOOM is considerably simpler than PolyTOIL, yet the language is
      just as expressive. The type system for the language is decidable and
      provably type safe. The addition of modules to the language provides
      better control over information hiding and allows the provision of
      access like that of C++'s friends.},
  WKloc = {A-1085, doc/pap/BIB}
}

@InProceedings{Bruce-Riecke-1987,
  WKloc = {A-0064},
  keywords = {functional polymorphism algebraic types denotational
		  semantics constructor expressions kind checking soundness},
  abstract = {Miranda has two interesting features in its typing
		  system: implicit polymorphism (also known as
		  ML-style polymorphism) and algebraic types.
		  Algebraic types create new types from old and can
		  operate on arbitrary types. This paper argues that
		  functions on types, or {\em type onstructors}, best
		  represent the meaning of algebraic types. Building
		  upon this idea, we develop a denotational semantics
		  for algebraic types. We first define a typed lambda
		  calculus that specifies type constructors. A
		  semantic model of type onstructors is then built,
		  using the ideal model as a basis. (The ideal model
		  gives the most natural semantics for Miranda's
		  implicit polymorphism.) The model is shown to be
		  sound with respect to this lambda calculus.
		  Finally,we demonstrate how to use the model to
		  interpret algebraic types, and prove that the
		  translation produces elements in the model.},
  title = {The semantics of Miranda's Algebraic Types},
  pages = {455--473},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Kim B. Bruce and Jon G. Riecke}
}

@Article{Bruck-Ryser-1949,
  author = {R. H. Bruck and H. J. Ryser},
  title = {The Nonexistence of Certain Finite Projective Planes},
  journal = CANAD,
  volume = 1,
  year = 1949,
  pages = {88--93},
  bibliographies = {RelMiCS}
}

@InProceedings{Brucker-Wolff-2002,
  author = 	 {Achim D. Brucker and Burkhart Wolff},
  title = 	 {Using Theory Morphisms for Implementing Formal Methods Tools},
  crossref =	 {TYPES2002},
  pages =	 {59--77},
  WKloc = 	 {A-1524, doc/pap/BIB},
  bibliographies = {HHOL},
  keywords = {Formal Methods, Formal Semantics, Shallow Embeddings,
              Theorem Proving, OCL},
  abstract = {Tools for a specification language can be implemented
     directly (by building a special purpose theorem prover) or by a
     conservative embedding into a typed meta-logic, which allows
     their safe and logically consistent implementation and the reuse
     of existing theorem prover engines. For being useful, the
     conservative extension approach must provide derivations for
     several thousand ``folklore'' theorems.  In this paper, we present
     an approach for deriving the mass of these theorems mechanically
     from an existing library of the meta-logic. The approach
     presupposes a structured theory morphism mapping library
     datatypes and library functions to new functions of the
     specification language while uniformly modifying some semantic
     properties; for example, new functions may have a different
     treatment of undefinedness compared to old ones.}
}

@InProceedings{BrueggemannKlein-1993,
  author = {A. Br{\"}uggemann-Klein},
  title = {Unambiguity of Extended Regular Expressions in SGML
		  Document Grammars},
  crossref = {ESA93},
  pages = {73--84},
  WKloc = {A-0355},
  abstract = {$\ldots$}
}

@InProceedings{Bruening-1993,
  author = {Stefan Br\"uning},
  title = {Search Space Pruning by Checking Dynamic Term Growth},
  crossref = {LPAR93},
  pages = {52--63},
  WKloc = {A-0120},
  abstract = {In this paper we present a method to detect non-terminating or
             failing queries based on analyzing the dynamic growth of terms.
             It overcomes restrictions known from approaches to preclude
             infinite loops in the field of logic programming. The general
             idea is to predetermine what may happen to a term while
             performing inference steps. Various well-known techniques and
             results of theory of formal languages are used. The strength of
             our technique is emphasized by the fact that using it we are
             able to decide Horn-formulas consisting of facts, two-literal
             clauses, and a goal literal all of them restricted to unary
             predicate and unary function symbols.}
}

@PhdThesis{Bruennler-2003,
  author = 	 {Kai Br{\"u}nnler},
  title = 	 {Deep Inference and Symmetry in Classical Proofs},
  school = 	 {Technische Universit\"at Dresden, Fakult\"at Informatik},
  year = 	 {2003},
  WKloc = 	 {A-1606, doc/pap/BIB},
  bibliographies = {HHOL},
  abstract = {In this thesis we see deductive systems for classical
     propositional and predicate logic which use \emph{deep
     inference}, i.e. inference rules apply arbitrarily deep inside
     formulas, and a certain \emph{symmetry}, which provides an
     involution on derivations. Like sequent systems, they have a cut
     rule which is admissible. Unlike sequent systems, they enjoy
     various new interesting properties. Not only the identity axiom,
     but also cut, weakening and even contraction are reducible to
     atomic form. This leads to inference rules that are local,
     meaning that the effort of applying them is bounded, and
     \emph{finitely generating}, meaning that, given a conclusion,
     there is only a nite number of premises to choose from. The
     systems also enjoy new normal forms for derivations and, in the
     propositional case, a cut elimination procedure that is
     drastically simpler than the ones for sequent systems.}
}

@Article{Bruni-Gadducci-2003,
  author = 	 {Bruni, Roberto and Gadducci, Fabio},
  title = 	 {Some Algebraic Laws for Spans
                  (and their connections with multirelations)},
  journal = 	 ENTCS,
  year = 	 2003,
  volume =	 44,
  number =	 3,
  pages =	 {9.1--9.19},
  bibliographies = {RelMiCS, RelMiS2001},
  abstract = {This paper investigates some key algebraic properties
     of the categories of spans and cospans (up to isomorphic
     supports) over the category Set of (small) sets and functions,
     analyzing the monoidal structures induced over both spans and
     cospans by cartesian product and disjoint union of sets. Our
     results find analogous counterparts in (and are partly inspired
     by) the theory of relational algebras, thus our paper also sheds
     some light on the relationship between (co)spans and the
     categories of (multi)relations and of equivalence relations. And,
     since (co)spans yield an intuitive presentation of dynamical
     systems with input and output interfaces, our results introduce
     an expressive, two-fold algebra that can serve as a specification
     formalism for rewriting systems and for composing software
     modules.}
}

@InProceedings{Bruni-Gadducci-Montanari-1998,
  author = {Bruni, Roberto and Gadducci, Fabio and Montanari, Ugo},
  title = {Normal Forms for Partitions and Relations},
  crossref = {WADT1998},
  pages = {31--47},
  WKloc = {A-0960},
  URL = {http://www.di.unipi.it/~bruni/publications/ListOfAbstracts.html},
  abstract = {In recent times there has been a growing interest towards
             algebraic structures which are able to express formalisms
             different from the well-known, tree-like presentation of terms.
             Many of the approaches adopted for such descriptions reveal a
             common, specific interest towards their application in the
             ``distributed and concurrent systems'' field, but an
             exhaustive comparison between them is very difficult
             because their presentations can be quite different.
             This work is a first step towards a unified view,
             which is able to recast all those formalisms into a
             more general one, where they can be easily compared.
             We introduce a general schema for describing a
             characteristic normal form for many interesting
             algebraic formalisms, and show that those normal forms
             can be thought of as arrows of suitable concrete
             monoidal categories, whose operations preserve the
             normal form itself.},
  bibliographies = {RelMiCS}
}

@Article{Bruni-Gadducci-Montanari-2000,
  author = {Bruni, Roberto and Gadducci, Fabio and Montanari, Ugo},
  title = {Normal Forms for Partitions and Relations},
  journal = TCS,
  year = 2000,
  note = {to appear},
  WKloc = {A-0961},
  URL = {http://www.di.unipi.it/~bruni/publications/ListOfAbstracts.html},
  abstract = {Recent years have seen a growing interest towards
             algebraic structures that are able to express
             formalisms different from the standard, tree-like presentation
             of terms. Many of these approaches reveal a specific interest
             towards the application to the
            ``distributed and concurrent systems'' field, but an
             exhaustive comparison between them is difficult because
             their presentations can be quite dissimilar. This work
             is a first step towards a unified view, which is able to
             recast all those formalisms into a more general one,
             where they can be easily compared. We introduce a
             general schema for describing a characteristic normal form
             for many algebraic formalisms, and show that those normal forms
             can be thought of as arrows of suitable
             concrete monoidal categories.},
  bibliographies = {RelMiCS}
}

@InProceedings{Bruni-LluchLafuente-2009,
  author =       {Roberto Bruni and Lluch Lafuente, Alberto},
  title =        {Ten Virtues of Structured Graphs},
  crossref =  {GTVMT2009},
  pages =     {1.1--1.20},
  bibliographies = {GraTraVis},
  abstract = {This paper extends the invited talk by the first author
     about the virtues of structured graphs.
     The motivation behind the talk and this paper
     relies on our experience on the development of ADR,
     a formal approach for the design of
     style-conformant, reconfigurable software systems.
     ADR is based on hierarchical graphs with interfaces
     and it has been conceived in the attempt
     of reconciling software architectures and process calculi
     by means of graphical methods.
     We have tried to write an ADR agnostic paper
     where we raise some drawbacks of flat, unstructured graphs
     for the design and analysis of software systems
     and we argue that hierarchical, structured graphs
     can alleviate such drawbacks.},
  keywords = {Hierarchical graphs, Graph Grammars, Visual Languages}
}

@Article{Bruni-Meseguer-Montanari-2001,
  author = {Roberto Bruni and Jos{\'e} Meseguer and Ugo Montanari},
  title = {Symmetric and Cartesian Double Categories as a Semantic Framework for Tile Logic},
  journal = {Math.~Struct.~in Comp.~Science},
  year = 2001,
  volume = {?},
  OPTnumber = {},
  OPTmonth = {},
  pages = {?},
  note = {to appear?},
  OPTunibwm = {INF/Z},
  WKloc = {A-0843, doc/pap/BIB},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {},
  CiteSeer = "citeseer.ist.psu.edu/bruni01symmetric.html"
}

@Article{Bruni-Meseguer-Montanari-Sassone-2001,
  author = 	 {Bruni, R., Meseguer, J., Montanari, U. and Sassone, V.},
  title = 	 {Functorial Models for Petri Nets},
  journal = 	 {Information and Computation},
  year = 	 2001,
  volume = 	 170,
  number = 	 2,
  pages = 	 {207--236},
  WKloc = 	 {doc/pap/BIB},
  URL = {http://eprints.ecs.soton.ac.uk/14742/},
  abstract = 	 {We show that although the algebraic
                  semantics of place/transition Petri nets under the
                  collective token philosophy can be fully explained
                  in terms of strictly symmetric monoidal categories,
                  the analogous construction under the individual
                  token philosophy is not completely satisfactory,
                  because it lacks universality and also
                  functoriality. We introduce the notion of pre-net to
                  recover these aspects, obtaining a fully
                  satisfactory categorical treatment, where the
                  operational semantics of nets yields an
                  adjunction. This allows us to present a uniform
                  logical description of net behaviors under both the
                  collective and the individual token philosophies in
                  terms of theories and theory morphisms in partial
                  membership equational logic. Moreover, since the
                  universal property of adjunctions guarantees that
                  colimit constructions on nets are preserved by our
                  algebraic models, the resulting semantic framework
                  has good compositional properties.}
}

@InProceedings{Bruni-Montanari-1999,
  author = {Roberto Bruni and Ugo Montanari},
  title = {Cartesian-Closed Double Categories, their Lambda-Notation, and the Pi-Calculus},
  crossref = {LICS-1999},
  pages = {246--265},
  abstract = {We introduce the notion of cartesian closed double category
             to provide mobile calculi for communicating systems with
             specific semantic models: One dimension is dedicated to
             compose systems and the other to compose their computations
             and their observations. Also, inspired by the connection
             between simply typed lambda-calculus and
             cartesian closed categories, we define a new typed framework,
             called double lambda-notation, which is able to express the
             abstraction/application and pairing/projection operations
             in all dimensions. In this development, we take the
             categorical presentation as a guidance in the interpretation
             of the formalism. A case study of the pi-calculus, where the
             double lambda-notation straightforwardly handles
             name passing and creation, concludes the presentation. },
  WKloc = {A-0847}
}

@PhDThesis{Brunning-1980,
  author = {Jacqueline Brunning},
  title = {Peirce's Development of the Algebra of Relations},
  series = Doct,
  school = Toronto,
  address = {Toronto},
  year = 1980,
  bibliographies = {RelMiCS}
}

@InProceedings{Brus-vanEekelen-vanLeer-Plasmeijer-1987,
  WKloc = {A-0024},
  abstract = {Clean is an experimental language for specifying
		  functional computations in terms of graph rewriting.
		  It is based on an extension of Term Rewriting
		  Systems (TRS) in which the terms are replaced by
		  graphs. Such a Graph Rewriting System (GRS) consists
		  of a, possibly cyclic, directed graph, called the
		  data graph and graph rewrite rules which specify how
		  this data graph may be rewritten. Clean is designed
		  to provide a firm base for functional programming.
		  In particular, Clean is suitable as an intermediate
		  language between functional languages and (parallel)
		  target machine architectures. A sequential
		  implementation of Clean on a conventional machine is
		  described and its performance is compared with other
		  systems. The results show that Clean can be
		  efficiently implemented.},
  title = {Clean --- A Language for Functional Graph Rewriting},
  pages = {364--384},
  crossref = {FPCA-1987},
  author = {T. H. Brus and van Eekelen, M. C. J. D. and van
		  Leer, M. O. and M. J. Plasmeijer},
  annote = {--- PLGnotes ---
		  No variables in the ``data graph''.
		  ``variables'' are rule parameters.
		  ``rewrite rules'': LHS = ``redex pattern'',
				     RHS = ``contractum pattern'' $|$
					   ``redirection''.
		  Redirections are references to variables.
		  Variables are leaves, and possibly conceptually individual,
		    though not even explicit as kind of nodes.
		  Matching is ... and ``identity on constants''(sic!)}
}

@Book{Bryan-1997,
  author = {Martin Bryan},
  title = {{SGML and HTML Explained}},
  year = 1997,
  publisher = {Addison Wesley Longman},
  WKloc = {B-0049}
}

@Article{Bryant-1986,
  author = 	 {R. E. Bryant},
  title = 	 {Graph-Based Algorithms for Boolean Function Manipulation},
  journal = 	 {IEEE Transactions on Computers},
  year = 	 1986,
  volume =	 {C-35},
  number =	 8,
  pages =	 {677--691},
  month =	 AUG,
  keywords = 	 {BDD}
}

@Article{Brzozowski-1964,
  author = 	 {J. Brzozowski},
  title = 	 {Derivatives of Regular Expressions},
  journal = 	 JACM,
  year = 	 1964,
  volume = 	 11,
  number = 	 4,
  bibliographies = {RelMiCS}
}

@Article{Buchberger-Jebelean-Kutsia-Maletzky-Windsteiger-2016_Theorema2,
    author = {Bruno Buchberger and Tudor Jebelean and Temur Kutsia and Alexander Maletzky and Wolfgang Windsteiger},
    title = {{Theorema 2.0}: Computer-Assisted Natural-Style Mathematics},
    language = {english},
    abstract = {The Theorema project aims at the development of a computer assistant for the working mathematician. Support should be given throughout all phases of mathematical activity, from introducing new mathematical concepts by definitions or axioms, through first (computational) experiments, the formulation of theorems, their justification by an exact proof, the application of a theorem as an algorithm, until to the dissemination of the results in form of a mathematical publication, the build up of bigger libraries of certified mathematical content and the like. This ambitious project is exactly along the lines of the QED manifesto issued in 1994 (see e.g. http://www.cs.ru.nl/~freek/qed/qed.html) and it was initiated in the mid-1990s by Bruno Buchberger. The Theorema system is a computer implementation of the ideas behind the Theorema project. One focus lies on the natural style of system input (in form of definitions, theorems, algorithms, etc.), system output (mainly in form of mathematical proofs) and user interaction. Another focus is theory exploration, i.e. the development of large consistent mathematical theories in a formal frame, in contrast to just proving single isolated theorems. When using the Theorema system, a user should not have to follow a certain style of mathematics enforced by the system (e.g. basing all of mathematics on set theory or certain variants of type theory), rather should the system support the user in her preferred flavour of doing math. The new implementation of the system, which we refer to as Theorema 2.0, is open-source and available through GitHub.},
    journal = {JFR},
    volume = {9},
    number = {1},
    pages = {149--185},
    isbn_issn = {ISSN 1972-5787},
    year = {2016},
    refereed = {yes},
    keywords = {Mathematical assistant systems, Theorema, automated reasoning, theory exploration, unification},
    length = {37},
    DOIURL = {http://dx.doi.org/10.6092/issn.1972-5787/4568},
    DOI = {10.6092/issn.1972-5787/4568}
}

@InProceedings{Buchheit-1993,
  author = {Paul Buchheit},
  title = {A Modular Approach to Natural Language Processing},
  abstract = {The INFANT System is a working natural language processing
             system that approaches language analysis through the
             integration of a large number of interdependent procedural
             modules. By themselves the modules perform specific,
             predictable, and rather mundane tasks; taken together, they
             make up an understanding system whose `beliefs' and responses
             are varied and unpredictable. Thus a type of emergent behavior
             is demonstrated by the system. This paper desribes the overall
             modular design of INFANT, and illustrates through sample
             conversations some of its capabilities and deficiencies.},
  crossref = {ACM1993}
}

@TechReport{Buchheit-Donini-Schaerf-1993,
  author = {Martin Buchheit and Francesco M. Donini and Andrea Schaerf},
  title = {Decidable Reasoning in Terminological Knowledge
		  Representation  Systems},
  number = {RR-93-10},
  year = 1993,
  type = {Research Report},
  month = {April},
  institution = DFKI,
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS},
  abstract = {Terminological Knowledge Representation Systems (TKRS)
		  are tools for designing and using knowledge bases
		  that make use of terminological languages (or
		  concept languages). We analyze from a theoretical
		  point of view a TKRS whose capabilities go beyond
		  the ones of presently available TKRS. The new
		  features studied, all of practical interest, can be
		  summarized in three main points.

                  First, we consider a highly expressive
		  terminological language, called ALCNR, including
		  general complements of concepts,  number
		  restrictions and  role conjunction.

                  Second, we allow to express inclusion statements
		  between general concepts, and terminological cycles
		  as a particular case. Third, we prove the
		  decidability of a number of desirable TKRS-deduction
		  services (like satisfiability-, subsumption- and
		  instance checking) through a sound, complete and
		  terminating calculus for reasoning in
		  ALCNR-knowledge bases. Our calculus extends the
		  general technique of constraint systems and can be
		  easily turned into a procedure using exponential
		  space. As a byproduct of the proof, we get also the
		  result that inclusion statements in ALCNR can be
		  simulated by terminological cycles, if descriptive
		  semantics is adopted.}
}

@InProceedings{Budd-1992,
  authorsAddress = {Oregon State University, budd\@cs.orst.edu},
  abstract = {Multiparadigm programming is a term used to describe
		  a style of software development that makes use of
		  facilities originally designed in support of a
		  number of different programming language paradigms.
		  In this paper we illustrate our conception of
		  multiparadigm programming, by describing how various
		  data structures can be implemented in the
		  programming language Leda. Leda is a strongly-typed
		  compiled multiparadigm programming language that we
		  have been developing over the past several years.
		  Our exposition serves both to illustrate the idea of
		  multiparadigm programming, and to describe the
		  features of the language Leda.},
  title = {Multiparadigm Data Structures in Leda},
  crossref = {ICCL92},
  author = {Timothy A. Budd}
}

@InProceedings{Budd-Pandey-1991,
  author = {Timothy A. Budd and Rajeev K. Pandey},
  title = {Compiling {APL} for Parallel and Vector Execution},
  crossref = {APL1991},
  pages = {80--87},
  WKloc = {A-1473, doc/pap/BIB},
  bibliographies = {Anand},
  abstract = {The inherent parallelism of applicative languages such
     as APL and functional languages such as FP present
     a little-exploited and somewhat unorthodox means of
     parallel programming. Here we summarize our investigation
     of a new approach to compiling such programs
     for execution on various types of parallel hardware. Our
     method centers around an intermediate form that is an
     extension of the lambda calculus. We present evidence
     that APL programs are easily translated into this intermediate
     form, and that this intermediate representation
     lends itself readily to code generation for a variety of
     parallel hardware.}
}

@InProceedings{Buendgen-1994,
  author = {Reinhard B{\"u}ndgen},
  title = {Preserving Confluence for Rewrite Systems with Built-In
          Operations},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Tuebingen}
}

@InProceedings{Buendgen-1996,
  author = {Reinhard B{\"u}ndgen},
  title = {Proof Transformation for Non-Compatible Rewriting},
  booktitle = {AISMC-3},
  pages = {160--175},
  series = {LNCS},
  volume = 1138,
  UniBwM = {KYB800/Z9380-3},
  OPTabstract = {},
  WKloc = {A-0452}
}

@Book{Buerckert-1991,
  UniBwM = {INF400/W2196},
  year = 1991,
  title = {A Resolution Principle for a Logic with Restricted
		  Quantifiers},
  series = LNAI,
  publisher = Springer,
  number = 568,
  author = {Hans-J\"urgen B\"urckert},
  bibliographies = {RelMiCS}
}

@TechReport{Buerckert-Nutt-1992,
  author = {Hans-J\"urgen B\"urckert and Werner Nutt},
  title = {On Abduction and Answer Generation through
		  Constrained Resolution},
  institution = {DFKI},
  year = 1992,
  type = {Research Report},
  number = {RR-92-51},
  month = OCT,
  WKloc = {C-0001},
  abstract = {Recently, extensions of constrained logic
		  programming and constrained resolution for theorem
		  proving have been introduced, that consider
		  constraints, which are interpreted under an open
		  world assumption. We discuss relationships between
		  applications of these approaches for query answering
		  in knowledge base systems on the one hand and
		  abduction-based hypothetical reasoning on the other
		  hand. We show both that constrained resolution can
		  be used as an operationalization of (some limited
		  form of) abduction and that abduction is the logical
		  status of an answer generation process through
		  constrained resolution, ie., it is an abductive but
		  not a deductive form of reasoning.},
  keywords = {constrained resolution, query answering, intensional
		  answers, abduction}
}

@InProceedings{Bundy-1990,
  author = {Alan Bundy},
  title = {The Use of Proof Plans in Formal Methods},
  crossref = {DISCO90},
  pages = {151--153},
  note = {invited lecture},
  keywords = {Nuprl}
}

@Article{Buneman-Fernandez-Suciu-2000,
  author = {Peter Buneman and Mary F. Fernandez and Dan Suciu},
  title = {{UnQL}: a query language and algebra for
                 semistructured data based on structural recursion},
  journal = {VLDB Journal: Very Large Data Bases},
  volume = 9,
  number = 1,
  pages = {76--110},
  month = {????},
  year = 2000,
  coden = {VLDBFR},
  ISSN = {1066-8888 (print), 0949-877X (electronic)},
  bibdate = {Wed Sep 27 10:11:55 MDT 2000},
  note = {Electronic edition.},
  URL = {http://link.springer.de/link/service/journals/00778/papers/0009001/00090076.pdf;
                 http://link.springer.de/link/service/journals/00778/bibs/0009001/00090076.htm;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/b/Buneman:Peter.html;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/f/Fernandez:Mary_F=.html;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/s/Suciu:Dan.html},
  acknowledgement = ack-nhfb
}

@Book{Bunge-1967,
  author = {Bunge, M.},
  title = {Scientific Research {I}, The Search for System},
  publisher = Springer,
  year = 1967,
  address = {Berlin},
  bibliographies = {RelMiCS}
}

@Article{Bunke-1982,
  author = {Horst Bunke},
  title = {On the Generative Power of Sequential and Parallel
		  Programmed Graph Grammars},
  journal = {Computing},
  volume = 29,
  pages = {89--112},
  year = 1982
}

@Article{Bunke-1982b,
  author = {H. Bunke},
  title = {Attributed programmed graph grammars and their application
		  to schematic diagram interpretation},
  journal = {{IEEE} pattern analysis and machine intelligence},
  volume = 4,
  number = 6,
  pages = {574--582},
  year = 1982,
  publisher = {{IEEE} Computer Society Press},
  added = {1996-08-02-14-49-29}
}

@InProceedings{Buntrock-Otto-1995,
  author = {Gerhard Buntrock and Friedrich Otto},
  title = {Growing Context-Sensitive Languages and
		  Church-Rosser Languages},
  crossref = {STACS1995},
  pages = {313--324},
  authorsAddress = {W\"urzburg, Kassel},
  WKloc = {A-0410},
  abstract = {The growing context-sensitive languages (GCSL) are
		  characterized by a nondeterministic machine model,
		  the so-called shrinking two-pushdown automaton
		  (sTPDA). Then the deterministic version of this
		  automaton (sDTPDA) is shown to characterize the
		  class of generalized Church-Rosser languages
		  (GCRL). Finally, we prove that eah growing
		  context-sensitive language is accepted in polynomial
		  time by some one-way auxiliary pushdown automaton
		  with a logarithmic space bound
		  (OW-auxPDA[log,poly]). As a consequence the class of
		  (generalized) Church-Rosser languages and the class
		  of context-free languages are incomparable under set
		  inclusion, which verifies a conjecture of McNaughton
		  et al [MNO88].}
}

@Book{Burch-1991,
  author = {Robert W. Burch},
  title = {A {Peirce} Reduction Thesis},
  note = {The foundations of topological logic,
		Philosophical Inquiries, Vol.\null{} 1},
  publisher = TexasTUP,
  address = {Lubbock, Texas},
  year = 1991,
  pages = {xvi+152},
  bibliographies = {RelMiCS}
}

@InProceedings{ Burch-Clarke-McMillan-1990,
    author = {J.R. Burch and E.M. Clarke and K.L. McMillan and D.L. Dill and L.J. Hwang},
    title = "Symbolic Model Checking: $10^{20}$ States and Beyond",
    booktitle = "Proceedings of the Fifth Annual {IEEE} Symposium on Logic in Computer Science",
    publisher = "IEEE Computer Society Press",
    address = "Washington, D.C.",
    pages = "1--33",
    year = "1990",
    url = "citeseer.ist.psu.edu/burch90symbolic.html"
}

@Article{Burch-Clarke-McMillan-1992,
  title = {Symbolic Model Checking: $10^{20}$ States and Beyond},
  author = {J. R. Burch and E. M. Clarke and K. L. McMillan and D.
                 L. Dill and L. J. Hwang},
  pages = {142--170},
  journal = IandC,
  month = JUN,
  year = 1992,
  volume = 98,
  number = 2,
  bibliographies = {SeminarWT2000}
}

@Book{Burkart-1997,
  author = {Olaf Burkart},
  title = {Automatic Verification of Sequential Infinite-State Processes},
  note = {Ph.D. Thesis, RWTH Aachen},
  series = {LNCS},
  volume = 1354,
  publisher = {Springer},
  year = 1997,
  UniBwM = {INF700/YD1322},
  keywords = {model checking}
}

@Book{Burkhardt-1980,
  author = {Hans Burkhardt},
  title = {Logik und Semiotik in der Philosophie von Leibniz},
  publisher = {Philosophia Verlag},
  year = 1980,
  address = {M\"unchen},
  bibliographies = {RelMiCS}
}

@Book{Burmeister-1986,
  author = {Peter Burmeister},
  title = {A Model Theoretic Oriented Approach to Partial
		  Algebras (Introduction to Theory and Application of
		  Partial Algebras --- Part I)},
  publisher = {Akademie-Verlag Berlin},
  year = 1986,
  volume = 32,
  series = {Mathematical Research},
  UniBwM = {INF700/N3256-1}
}

@Misc{Burmeister-Monserrat-Rossello-Valiente-1996,
  author = {P. Burmeister and M. Monserrat and F. Rossell{\'o} and G. Valiente},
  title = {Algebraic Transformation of Partial Algebras {II}:
                  Double-Pushout Approach},
  month = JUN,
  year = 1996,
  WKloc = {A-1094, doc/pap/BIB}
}

@Misc{Burmeister-Rossello-Torrens-Valiente-1996,
  author = {P. Burmeister and F. Rossell{\'o} and J. Torrens and G. Valiente},
  title = {Algebraic Transformation of Partial Algebras {I}:
                  Double-Pushout Approach},
  month = FEB,
  year = 1996,
  WKloc = {A-1066, doc/pap/BIB}
}

@Misc{Burmeister-Rossello-Valiente-1996,
  author = {Peter Burmeister and Francesc Rossell{\'o} and Gabriel Valiente},
  title = {Double-Pushout Hypergraph Rewriting through Free Completions},
  year = 1996,
  WKloc = {A-1218, doc/pap/BIB}
}

@TechReport{Burnett-Baker-1994,
  author = {Marageret M. Burnett and Marla J. Baker},
  title = {A Classification System for Visual Programming Languages},
  institution = {Department of Computer Science, Oregon State University},
  year = 1994,
  number = {93-60-14},
  address = {Corvallis, Oregeon},
  month = JUN,
  note = {revised version},
  WKloc = {A-0247}
}

@Book{Burns-Davies-1993,
  author = {Alan Burns and Geoff Davies},
  title = {Concurrent Programming},
  publisher = {Addison-Wesley},
  year = 1993,
  series = {International Computer Science Series},
  McMaster = {QA 76.642 .B87 1993},
  bibliographies = {SE3B}
}

@Book{Burris-Sankappanavar-1981,
  author = {Stanley Burris and H.P. Sankappanavar},
  title = {A Course in Universal Algebra},
  publisher = Springer,
  year = 1981,
  volume = 78,
  series = {Graduate Texts in Mathematics},
  UniBwM = {MAT200/J17736},
  URL = {http://www.math.uwaterloo.ca/~snburris/htdocs/ualg.html},
  bibliographies = {RelMiCS},
  contents = {I   Lattices
                  II  The Elements of Universal Algebra
                  III Selected Topics
                  IV  Starting from Boolean Algebras
                  V   Connections with Model Theory
                  Recent Developments and Open Problems}
}

@InProceedings{Burstall-1992,
  author = {Rod Burstall},
  title = {Extended {Calculus of Constructions} as a
		  specification language},
  crossref = {MPC1992},
  pages = 1,
  URL = {ftp://ftp.dcs.ed.ac.uk/export/lego/index.html},
  abstract = {Huet and Coquand's Calculus of Constructions, an
		  implementation of type theory, was extended by Luo
		  with sigma types, a type of pairs where the type of
		  the second component depends on the value of the
		  first one. This calculus has been implemented as
		  `Lego' by Pollack.

                  The sigma types enable one to give a compact
		  description of abstract mathematical structures such
		  as `group', to build more concrete structures of
		  them, such as `the group of integers under addition'
		  and to check that the concrete structure includes as
		  components proofs that it satisfies the axioms of
		  the abstract structures. So `group' is a sigma type
		  and `group of integers under addition' is an
		  $n$-tuple of types, operators and proofs which is an
		  element of this sigmatype. We can define functions
		  which enrich such structures or forget them to
		  simpler ones.

                  However the calculus is intentional and it is too
		  restrictive to identify the mathematical notion of
		  `set' with `type'. We will discuss how sets and
		  functions between them may be represented and the
		  notational difficulties which arise. We put forward
		  a tentative suggestion as to how these difficulties
		  might be overcome by defining the category of sets
		  and functions in the calculus, then using the
		  internal language of that category to extend the
		  type theory. This would be a particular example of a
		  more general reflection principle.}
}

@InProceedings{Burstall-1994,
  title = {Terms, Proofs, and Refinement (Extended abstract)},
  author = {Rod Burstall},
  pages = {2--7},
  crossref = {LICS9},
  WKloc = {A-0364},
  abstract = {We give a simple account of the connection between lambda
		  terms and natural deduction proofs, showing how the
		  terms can be rearranged into a form close to
		  conventional proofs, and also to less conventional
		  ``top down'' proofs.  Creating proofs interactively
		  by refinement can be seen as just keying in a lambda
		  expression one symbol at a time in response to
		  prompts from the machine.  The aim is to convey some
		  basic ideas to the uninitiated without technical or
		  pragmatic detail.}
}

@Article{Burstall-Darlington-1977,
  author = {R.M. Burstall and J. Darlington},
  title = {A transformation system for developing recursive programs},
  journal = JACM,
  DOIURL = {http://doi.acm.org/10.1145/321992.321996},
  DOI = {10.1145/321992.321996},
  year = 1977,
  volume = 24,
  number = 1,
  pages = {44--67},
  bibliographies = {RelMiCS},
  abstract = {A system of rules for transforming programs is
                  described, with the programs in the form of
                  recursion equations. An initially very simple,
                  lucid, and hopefully correct program is transformed
                  into a more efficient one by altering the recursion
                  structure. Illustrative examples of program
                  transformations are given, and a tentative
                  implementation is described. Alternative structures
                  for programs are shown, and a possible initial phase
                  for an automatic or semiautomatic
                  program-manipulation system is indicated.}
}

@InCollection{Burstall-Diaconescu-1994,
  author = {Rod Burstall and R{\smallsmile{a}}zvan Diaconescu},
  title = {Hiding and Behaviour: An institutional approach},
  crossref = {Roscoe-1994},
  pages = {75--92},
  chapter = 5,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Burstall-Goguen-1980,
  author = {R. M. Burstall and J. A. Goguen},
  title = {The semantics of {Clear}, a specification language},
  booktitle = {Proc. Copenhagen Winter School on Abstract Software
		  Specification},
  volume = 86,
  series = {LNCS},
  year = 1980,
  pages = {292--332}
}

@TechReport{Burstall-Goguen-Tarlecki-1990,
  author = {R. Burstall and J. Goguen and A. Tarlecki},
  title = {Some Fundamental Algebraic Tools for the Semantics of
      Computation. Part 3. Indexed Categories},
  institution = {Laboratory for Foundations of Computer Science},
  year = 1989,
  number = {ECS-LFCS-89-90},
  address = {University of Edinburgh, Scotland, UK},
  ftpaddress = {ftp.dcs.ed.ac.uk},
  filename = {pub/lfcsreps/89/},
  abstractfilename = {pub/lfcsreps/89/},
  filetype = {},
  URL = {http://www.dcs.ed.ac.uk/lfcs/},
  annote = {published as \cite{Burstall-Goguen-Tarlecki-1991}}
}

@Article{Burstall-Goguen-Tarlecki-1991,
  author = {R. Burstall and J. Goguen and A. Tarlecki},
  title = {Some Fundamental Algebraic Tools for the Semantics of
                 Computation: Part 3. Indexed Categories},
  journal = {Theoretical Computer Science},
  volume = 91,
  year = 1991,
  pages = {239--264},
  annote = {see \cite{Hermida-1992}}
}

@InProceedings{Burton-1989,
    author      = {F. Warren Burton},
    title       = {Indeterminate behavior with determinate semantics in
parallel programs},
    booktitle   = {FPCA '89: Proceedings of the fourth international
conference on Functional programming languages and computer architecture},
    year        = {1989},
    isbn        = {0-89791-328-0},
    pages       = {340--346},
    location    = {Imperial College, London, United Kingdom},
    doi         = {http://doi.acm.org/10.1145/99370.99402},
    publisher   = {ACM},
    address     = {New York, NY, USA},
  bibliographies = {PMC}
}

@Article{Burton-1991,
    author      = {F. Warren Burton},
    title       = {Encapsulating nondeterminacy in an abstract data type
with deterministic semantics},
    journal     = {Journal of Functional Programming},
    year        = 1991,
    volume      = 1,
    number      = 1,
    pages       = {3--20},
    bibliographies = {PMC},
    month       = {January},
    DOI = {10.1017/S0956796800000046},
    abstract = {A parallel program may be indeterminate
                so that it can adapt its behavior
                to the number of processors available.

       Indeterminate programs are hard to write, understand, modify or verify.
       They are impossible to debug,
       since they may not behave the same from one run to the next.

       We propose a new construct,
       a polymorphic abstract data type called an improving value,
       with operations that have indeterminate behavior
       but simple determinate semantics.
       These operations allow the type of indeterminate behavior
       required by many parallel algorithms.

       We define improving values
       in the context of a functional programming language,
       but the technique can be used in procedural programs as well.}
}

@Article{Burton-Cameron-1993,
  author = {F. Warren Burton and Robert D. Cameron},
  title = {Pattern matching with abstract data types},
  journal = {Journal of Functional Programming},
  year = 1993,
  volume = 3,
  number = 2,
  pages = {171--190},
  month = APR,
  WKloc = {A-0216},
  bibliographies = {PMC},
  abstract = {Pattern matching in modern functional languages is
		  tied to the representation of data. Unfortunately,
		  this is incompatible with the philosophy of abstract
		  data types.

                  Two proposals have been made to generalize pattern
		  matching to a broader class of types. The {\em laws}
		  mechanism of Miranda allows pattern matching with
		  non-free algebraic data types. More recently, Wadler
		  proposed the concept of {\em views} as a more
		  general solution, making it possible to define
		  arbitrary mappings between a physical implementation
		  and a view supporting pattern matching. Originally,
		  it was intended to include views in the new standard
		  lazy functional programming language Haskell.

                  Laws and views each offer important advantages,
		  particularly with respect to data abstraction.
		  However, if not used with great vare, they also
		  introduce serious problems in equational reasoning.
		  As a result, laws have been removed from Miranda and
		  views were not included in the final version of
		  Haskell.

                  We propose a third approach which unifies the laws
		  and views mechanisms while avoiding their problems.
		  Philosophically, we view pattern matching as a
		  bundling of case recognition and component selection
		  functions instead of a method for inverting data
		  construction. This can be achieved by removing the
		  implied equivalence between data constructors and
		  pattern constructors. In practice, we allow
		  automatic mapping into a view but not out of the
		  view. We show that equational reasoning can still be
		  used with the resulting system. In fact, equational
		  reasoning is easier, since there are fewer hidden traps.},
  nutshell = {The tension between abstract datatypes and the syntactic
		  sugar of pattern matching has long been recognized,
		  and has led to two proposed solutions: laws and
		  views. Unfortunately, these solutions cause problems
		  for equational reasoning, so they have been rejected
		  from current language designs. This paper shows that
		  just a small modification to the earlier ideas
		  allows us to have pattern matching and abstract data
		  types along with safe equational reasoning. After
		  explaining the idea, the authors give an extended
		  example showing how to use it in practice.}
}

@Article{Busatto-Kreowski-Kuske-2005,
  author =       {Giorgio Busatto and Hans-J{\"o}rg Kreowski and Sabine Kuske},
  title =        {Abstract hierarchical graph transformation},
  journal =      {Mathematical Structures in Computer Science},
  year =         2005,
  volume =    15,
  number =    4,
  pages =     {773--819},
  WKloc = {doc/pap/BIB},
  DOI =     {10.1017/S0960129505004846},
  bibliographies =      {GraTraVis},
  abstract =    {In this paper we introduce a new hierarchical graph model
     to structure large graphs into small components
     by distributing the nodes (and, likewise, edges)
     into a hierarchy of packages.
     In contrast to other known approaches,
     we do not fix the type of underlying graphs.
     Moreover, our model is equipped with
     a rule-based transformation concept
     such that hierarchical graphs are not restricted
     to being used only for the static representation of complex system states,
     but can also be used to describe dynamic system behaviour.},
  annote = {Decoupled approach to hierarchical graphs,
    using a hierarchy DAG separate from the underlying graph,
    and a ``coupling graph'' connecting the two.
    The ``coupling graph'' is really an indexed containment relation
    of items if the underlying graph
    in ``packages'' (nodes) of the hierarchy graph
    where for every underlying edge $e$ contained in some package $p$,
    all nodes with which $e$ is incident need to be contained in $p$, too.
    (A ``plain'' variant does not define containment for edges.)
    Containment is total.
    No explanation why containment need not be univalent?
    No compatibility of containment with hierarchy?
    There is ``(edge labels in the connection graph are irrelevant)''
    in the context of connection graphs for DPO rewriting (p.~805),
    but it still sounds general.


    Partial skeleton morphisms provide a view on
    graph transformation rules independent of their appproach.
    ``Tracking graph transformation approach''
    provides partial skeleton morphisms for derivation steps.

    ``Coordinated hierarchical graph transformation'':
    The three rule skeletons are ``glued'' via partial injections
    (in the paper, ``injective'' means injective and univalent)
    $\RELconv{\sim_{\mathrm{upper}}} \RELcomp r_i \RELleq r_j \RELcomp \RELconv{\sim_{\mathrm{lower}}}$
    and
    $\sim_{\mathrm{upper}} \RELcomp r_j \RELleq r_i \RELcomp \sim_{\mathrm{lower}}$.
    (This shape, not in the paper,
     means that $\sim$ is a bisimulation from $r_i$ to $r_j$.)

    Before Def.~5.6 of ``DAG-preserving'' rules:
    ``In what follows, \emph{all morphisms are injective},
       unless we explicitly state that they are not.''
    (mainly for matching morphisms in addition to rule morphisms).


    {}}
}

@Article{Bush-1945,
  author = {Vannevar Bush},
  title = {As We May Think},
  journal = {Atlantic Monthly},
  year = 1945,
  month = JUL,
  volume = 176,
  number = 1,
  pages = {101--108},
  note = {\textsf{http://www.theatlantic.com/unbound/flashbks/computer/bushf.htm}},
  WKloc = {A-1241, doc/pap/BIB}
}

@Article{Bush-Leeming-Walters-1997,
  author = {M.R. Bush and M. Leeming and R.F.C. Walters},
  title = {Computing Left {Kan} Extensions},
  journal = JSYCO,
  year = 1997,
  WKloc = {A-1258},
  abstract = {We describe a new extension of the Todd-Coxeter algorithm
      adapted to computing left Kan extensions. The algorithm is a much
      simplified version of that introduced by carmody and Walters in
      \cite{Carmody-Walters-1991,Walters-1991}. THe simplification allows
      us to give a straightforward proof of its correctness and termination
      when the extension is finite.}
}

@InProceedings{Busi-Glabbeek-Gorrieri-1994,
  author = {N. Busi and van Glabbeek, R. and R. Gorrieri},
  title = {Axiomatising {ST}-Bisimulation Equivalence},
  crossref = {PROCOMET94},
  pages = {164--183},
  keywords = {Semantics of Programming Languages; Studies of
		  Program Constructs, Formal Definition and Theory;
		  Concurrent Programming; Process Algebra}
}

@Book{Bustard-Elder-Welsh-1988,
  author = {David Bustard and John Elder and Jim Welsh},
  title = {Concurrent Program Structures},
  publisher = {Prentice Hall},
  year = 1988,
  series = {International Series in Computer Science},
  McMaster = {QA 76.6 .B8727 1988},
  bibliographies = {SE3B}
}

@Article{Buszkowski-Orlowska-1986,
  author = {W. Buszkowski and Ewa Orlowska},
  abstract = {A relational logic is given for proving database
      dependencies represented by means of binary relations.},
  title = {On the Logic of Database Dependencies},
  journal = BUPOL,
  volume = 34,
  year = 1986,
  pages = {345--354},
  note = {See also Proc.\null{} of the {{$4^{th}$} Hungarian Computer
      Science Conf., Gy\"or, Hungary 1985, 373--383}},
  bibliographies = {RelMiCS}
}

@Book{Buszkowski-Orlowska-1986a,
  author = {W. Buszkowski and Ewa Orlowska},
  title = {Relational Calculus and Data Dependencies},
  publisher = PolishC,
  address = {Warsaw},
  year = 1986,
  number = 578,
  series = {PAS Reports},
  bibliographies = {RelMiCS}
}

@InProceedings{Buszkowski-Orlowska-1994,
  author = {Buszkowski, W. and Ewa Orlowska},
  title = {Relational Representation of Dependencies in Information
      Systems.},
  booktitle = {Modeling Incomplete Information, Fundamentals and
      Applications, in preparation.},
  editor = {Orlowska, Ewa},
  year = 1994,
  annote = {The relational logic from \cite{BuszkowskiOrlowska1986} is
      investigated, some undecidablity and decidability results are
      presented.},
  bibliographies = {RelMiCS}
}

@Article{Butcher-1991,
  author = {Paul Butcher},
  title = {A Behavioural Semantics for {Linda-2}},
  year = 1991,
  volume = 6,
  publisher = BCS,
  pages = {196-204},
  number = 4,
  month = JUL,
  journal = SOFTJ,
  bibliographies = {RelMiCS}
}

@InProceedings{Buth-1994,
  author = {Karl-Heinz Buth},
  title = {Simulation of {SOS} definitions with term rewriting systems},
  crossref = {ESOP1994},
  pages = {150--164},
  abstract = {$\ldots$ This paper presents a method to transform a
		  structured operational semantics (SOS) definition
		  ${\cal S}$, given by a special form of deduction
		  system, into a term rewriting system ${\cal R}$. $\ldots$}
}

@InProceedings{Butler-Cannon-1990,
  author = {The Design of Cayley --- A Language for Modern Algebra},
  title = {G. Butler and J. Cannon},
  crossref = {DISCO90},
  pages = {10--19}
}

@Unpublished{Butz-1998,
  author = {Carsten Butz},
  title = {Finitely presented {Heyting} algebras},
  note = {\\ \textsf{http://euclid.math.mcgill.ca/butz/publications/abstracts.html\#heyting}},
  month = OCT,
  year = 1998,
  abstract = {In this paper we study the structure of
    finitely presented Heyting algebras. Using algebraic techniques
    (as opposed to techniques from proof-theory) we show that every such
    Heyting algebra is in fact co-Heyting, improving on a result of
    Ghilardi who showed that Heyting algebras free on a
    finite set of generators are co-Heyting. Along the way we give a
    new and simple proof of the finite model property.
    Our main technical tool is a
    representation of finitely presented Heyting algebras
    in terms of a colimit of finite distributive lattices.
    As applications we construct explicitly the
    minimal join-irreducible elements (the atoms)
    and the maximal join-irreducible elements of a
    finitely presented Heyting algebras in terms of a given presentation.
    This gives as well a new proof of the
    disjunction property for intuitionistic propositional logic.},
  WKloc = {A-1069, doc/pap/BIB}
}

@Unpublished{Butz-1999,
  author = {Carsten Butz},
  title = {The filter construction revisited},
  note = {http://euclid.math.mcgill.ca/butz/publications/abstracts.html\#filter},
  month = APR,
  year = 1999,
  abstract = {The filter construction, as an endo-functor on the
    category of small coherent categories, was used extensively by
    A. Pitts in a series of papers in the 80's to prove completeness
    and interpolation results. Later I. Moerdijk and E. Palmgren
    used the filter construction to construct non-standard models of
    Heyting arithmetic.

    In this paper we describe the true nature of the filter construction:
    Applied to a left-exact category it is simply the completion of
    subobject semi-lattices under filtered (and thus all) meets.
    We study filtered coherent logic which is coherent logic extended to
    arbitrary meets and the rules that existential quantification and
    binary disjunction distribute over filtered meets. This logic is
    conservative over coherent logic.

    Restricting further to first-order logic we show that the minimal models
    of Heyting arithmetic described by Moerdijk and Palmgren are nothing but
    generic models of Heyting arithmetic in which truth of
    implications between (arbitrary) conjunctions of
    finitary first-order formulae is governed by first-order variants
    of the rules described above.},
  WKloc = {A-1071, doc/pap/BIB}
}

@Manual{CADiZ,
  title = {CADiZ},
  author = {Ian Toyn},
  organization = {Department of Computer Science, University of York},
  edition = {R3.12},
  year = 2000,
  note = {URL: \urlsize{http://www.cse-euro.demon.co.uk/yse/products/cadiz/}},
  WKloc = {B-0053}
}

@Misc{CASL-Rationale-1997,
  author = {CoFI},
  title = {{\sc Casl}: The {CoFI} Algebraic Specification Language, Rationale},
  year = 1997,
  month = MAY,
  URL = {http://www.brics.dk/Projects/CoFI},
  WKloc = {A-0819}
}

@Misc{CASL-Summary-1999,
  author = {CoFI},
  title = {{\sc Casl}: The {CoFI} Algebraic Specification Language, Summary},
  year = 1999,
  month = JUL,
  URL = {http://www.brics.dk/Projects/CoFI},
  WKloc = {A-0820},
  note = {Version 1.0, superseded by \cite{CASL-Summary-2000}}
}

@Misc{CASL-Summary-2000,
  author = {CoFI},
  title = {{\sc Casl}: The {CoFI} Algebraic Specification Language, Summary},
  year = 2000,
  month = JUN,
  URL = {http://www.brics.dk/Projects/CoFI},
  note = {Version 1.0.1-DRAFT},
  WKloc = {B-0055}
}

@Misc{CASL-Summary-2001,
  author = {CoFI},
  title = {{\sc Casl}: The {CoFI} Algebraic Specification Language, Summary},
  year = 2001,
  month = MAR,
  URL = {http://www.brics.dk/Projects/CoFI},
  note = {Version 1.0.1},
  WKloc = {B-0122}
}

@Misc{CG-Standard,
  author = {John Sowa},
  title = {Conceptual Graph Standard},
  year = 1999,
  WKloc = {A-0822}
}

@Book{CIP-L,
  author = {Bauer, F.L. and  Berghammer, R. and Broy, M. and Dosch, W.
	   and Geiselbrechtinger, F. and  Gnatz, R. and Hangel, E. and
	   Hesse, W. and Krieg-Br{\"u}ckner, B. and Laut, A. and Matzner, T.
	   and M{\"o}ller, B. and Nickl, F. and Partsch, H. and Pepper, P.
	   and  Samelson, K. and Wirsing,  M. and W{\"o}ssner, H.},
  title = {The {M}unich Project {CIP}. Volume {I}: The Wide Spectrum
	  Language {CIP-L}},
  key = {CIP-L},
  series = {LNCS},
  volume = 183,
  publisher = {Springer-Verlag},
  address = {Berlin/Heidelberg/New York},
  DOI = {10.1007/3-540-15187-7},
  year = 1985
}

@Book{CIP-S,
  author = { Bauer, F.L. and Ehler, H. and Horsch, A. and
	    M{\"o}ller, B. and Partsch, H. and Paukner, O. and Pepper, P.},
  title = {The {Munich} Project {CIP}. Volume {II}: The
	  Transformation System {CIP-S}},
  key = {CIP-S},
  series = {LNCS},
  volume = 292,
  publisher = {Springer-Verlag},
  DOI = {10.1007/3-540-18779-0},
  address = {Berlin},
  year = 1987
}

@Manual{CSL-manual,
  title = {The {Caml Special Light} system,
		  Documentation and user's manual},
  author = {Xavier Leroy},
  organization = {INRIA},
  year = 1996,
  note = {URL: {\small\tt http://pauillac.inria.fr/csl/},
	       {\small\tt ftp://ftp.inria.fr/lang/caml-light/csl-1.1?.dvi.gz}}
}

@Misc{CVS,
  key = {CVS},
  title =  {Concurrent Versions System ({CVS}): The open standard for version control},
  howpublished = {Web site at \url{http://www.cvshome.org/}},
  bibliographies = {OPG}
}

@Article{Cai-Furer-Immerman-1992,
  author = {Cai, Jin-Yi and Furer, M. and Immerman, N.},
  title = {An optimal lower bound on the number of variables for graph
                   identification},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Combinatorica},
  year = 1992,
  volume = 12,
  number = 4,
  pages = {389--},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Cai-Paige-1991,
  author = {Jiazhen Cai and Robert Paige},
  title = {``Look Ma, No Hashing, And No Arrays Neither''91},
  pages = {143--154},
  abstract = {It is generally assumed that hashing is essential to many
             algorithms to efficient compilation; e.g., symbol table
             formation and maintenance, grammar manipulation, basic block
             optimization, and global optimization. This paper questions
             this assumption, and initiates development of an efficient
             alternative compiler methodology without hashing or sorting.
             Underlying this methodology are several generic algorithmic
             tools, among which special importance is given to Multiset
             Discrimination, which partitions a into blocks of duplicate
             elements. We show how multiset discrimination, together with
             other tools, can be tailored to rid compilation of hashing
             without loss in asymptotic performance. Because of the
             simplicity of these tools, our results may be of practical as
             well as theoretical interest. The various applications
             presented culminat with a new algorithm to solve iterated
             strength reduction folded with useless code elimination that
             runs in worst case asymptotic time and auxiliary space linear
             in the maximum text length of the initial and optimized
             programs.},
  crossref = {POPL1991},
  WKloc = {A-0175}
}

@InProceedings{Cai-Paige-Tarjan-1990,
  year = 1990,
  title = {More Efficient Bottom-Up Tree Pattern Matching},
  pages = {72-86},
  crossref = {CAAP90},
  author = {J. Cai and R. Paige and R. E. Tarjan}
}

@misc{ Cairo,
        author = "{Cairo Developers}",
	title = "{Cairo Graphics}",
        key = {Cairo},
        howpublished = {\url{http://www.cairographics.org}}},
	year = "2008"
}

@InProceedings{Camarao-Figueiredo-1999,
  author = {Carlos Camar{\~{a}}o and Luc{\'\i}lia Figueiredo},
  title = {Type Inference for Overloading without Restrictions, Declarations or Annotations},
  crossref = {FLOPS1999},
  pages = {37--52},
  WKloc = {A-0786}
}

@Misc{Camarao-Figueiredo-2000,
  author = {Carlos Camar{\~{a}}o and Luc{\'\i}lia Figueiredo},
  title = {Type Inference for Overloading},
  year = 2000,
  WKloc = {A-1028},
  annote = {revised version of \cite{Camarao-Figueiredo-1999}}
}

@TechReport{Camarao-Figueiredo-2001a,
  author = {Carlos Camar{\~{a}}o and Luc{\'\i}lia Figueiredo},
  title = {Overloading and Coherence},
  institution = {UFMG},
  year = 2001,
  URL = {http://www.dcc.ufmg.br/~camarao/overloading-and-coherence.ps.gz},
  WKloc = {doc/wk/bib}
}

@article{Campeanu-YuSheng-2004,
  author    = {Cezar C{\^a}mpeanu and Sheng Yu},
  title     = {Pattern Expressions and Pattern Automata.},
  journal   = IPLET,
  volume    = {92},
  number    = {6},
  year      = {2004},
  pages     = {267--274},
  ee        = {http://dx.doi.org/10.1016/j.ipl.2004.09.007},
  WKloc     = {A-1609, doc/pap/BIB},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@Book{Cap-1993,
  author = {Clemens H. Cap},
  title = {Theoretische Grundlagen der Informatik},
  publisher = {Springer-Verlag},
  year = 1993,
  address = {Wien}
}

@InProceedings{Capelle-1997,
  author = {Christian Capelle},
  title = {Block Decomposition of Inheritance Hierarchies},
  year = 1997,
  WKloc = {A-0440},
  crossref = {WG1997}
}

@InProceedings{Capretta-1995,
  author =       {Venanzio Capretta},
  title =        {Universal Algebra in Type Theory},
  crossref =  {TPHOL1999},
  pages =     {131--148,},
  DOI =      {10.1.1.22.8437},
  WKloc =      {A-1719, doc/pap/BIB},
  abstract = {We present a development of Universal Algebra inside Type Theory,
      formalized using the proof assistant Coq.
      We define the notion of a signature and of an algebra over a signature.
      We use setoids, i.e.\null{}
      types endowed with an arbitrary equivalence relation,
      as carriers for algebras.
      In this way it is possible to define the quotient of an algebra
      by a congruence.
      Standard constructions over algebras are defined
      and their basic properties are proved formally.
      To overcome the problem of defining term algebras in a uniform way,
      we use types of trees that generalize wellorderings.
      Our implementation gives tools to define new algebraic structures,
      to manipulate them and to prove their properties.}
}

@Article{Capretta-2011,
  author =       {Venanzio Capretta},
  title =        {Coalgebras in functional programming and type theory},
  journal =      TCS,
  year =         {2011},
  OPTkey =       {},
  volume =    {412},
  number =    {38},
  pages =     {5006--5024},
  OPTmonth =     {},
  OPTnote =      {},
  abstract =    {This is a survey article on the use of coalgebras in functional programming and type theory. It presents the basic theory underlying the implementation of coinductive types, families and predicates. It gives an overview of the application of corecursive methods to the study of general recursion, formal power series, tabulations of functions on inductive data. It also sketches some advanced topics in the study of the solutions to non-guarded corecursive equations and the design of non-standard type theory.}
}

@InProceedings{Carboni-Freyd-Scedrov-1987,
  WKloc = {A-0071},
  abstract = {A categorical calculus of relations is used to
		  derive a unified setting for higher order logic and
		  polymorphic lambda calculus.},
  title = {A Categorical Approach to Realizability and
		  Polymorphic Types},
  pages = {23--42},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Aurelio Carboni and Peter J. Freyd and Andre Scedrov},
  bibliographies = {RelMiCS, LogRel}
}

@Article{Carboni-Kasangian-Walters-1987,
  author = {A. Carboni and S. Kasangian and R.F.C. Walters},
  title = {An Axiomatics for Bicategories of Modules},
  journal = JPAA,
  year = 1987,
  volume = 45,
  pages = {127--141}
}

@Article{Carboni-Lack-Walters-1993,
  author = {A. Carboni and S. Lack and R.F.C. Walters},
  title = {Introduction to extensive and distributive categories},
  journal = JPAA,
  year = 1993,
  pages = {145--158},
  volume = 84
}

@Article{Carboni-Walters-1987,
  author = {Aurelio Carboni and Robert F. C. Walters},
  title = {Cartesian Bicategories {I}},
  journal = {Annals of Pure and Applied Algebra},
  volume = 49,
  pages = {11--32},
  year = 1987,
  WKloc = {A-0746}
}

@Article{Cardelli-1987,
  author = {Luca Cardelli},
  title = {Basic Polymorphic Typechecking},
  journal = {Science of Computer Programming},
  volume = 8,
  number = 2,
  OPTpages = {},
  year = 1987,
  WKloc = {A-0500}
}

@Misc{Cardelli-1995,
  author = {Luca Cardelli},
  title = {A Language with Distributed Scope},
  year = 1995,
  month = MAY,
  keywords = {Obliq},
  WKloc = {A-0536}
}

@InCollection{Cardelli-1997,
  author = {Luca Cardelli},
  title = {Type Systems},
  crossref = {HbCSE},
  chapter = 103,
  pages = {},
  OPTabstract = {},
  WKloc = {A-0498}
}

@Misc{Cardelli-Davies-1997,
  author = {Luca Cardelli and Rowan Davies},
  title = {Service Combinators for Web Computing},
  year = 1997,
  month = MAY,
  WKloc = {A-0535}
}

@InProceedings{Cardelli-Ghelli-Gordon-1999,
  author = {Luca Cardeli and Giorgio Ghelli and Andrew D. Gordon},
  title = {Mobility Types for Mobile Ambients},
  crossref = {ICALP1999},
  pages = {??},
  abstract = {An ambient is a named cluster of processes and subambients,
      which moves as a group. We describe type systems able to guarantee
      that certain ambients will remain immobile, and that certain ambients
      will not be dissolved by their environment.},
  WKloc = {A-0765}
}

@InProceedings{Cardelli-Leroy-1990,
  WKloc = {A-0106},
  abstract = {Incremental modification is a fundamental mechanism not only
      in software systems, but also in physical and mathematical systems.
      inheritance owes its importance in large measure to its flexibility
      as a discrete incremental modification mechanism. Four increasingly
      permissive properties of incremental modification realizable by
      inheritance are examined: behaviour compatibility, signature
      compatibility, name compatibility, and cancellation. inheritance for
      entities with finite sets of attributes is defined and characterized
      as incremental modification with deferred binding of self-reference.
      Types defined as predicates for type checking are contrasted with
      classes defined as templates for object generation. Mathematical,
      operational, and conceptual models of inheritance are then examined
      in detail, leading to a discussion of algebraic models of behavioral
      compatibility, horizontal and vertical signature modification,
      algorithmically defined name modification, additive and subtractive
      exceptions, abstract inheritance networks, and parametric
      polymorphism. Liketypes are defined as a symmetrical general form of
      incremental modification that provide a framework for modelling
      similarity. The combination of safe behaviorally compatible changes
      and less safe radical incremental changes in a single programming
      language is considered.},
  title = {Inheritance as an Incremental Modification Mechanism or What
      Like Is and Isn't Like},
  pages = {55--77},
  crossref = {IFIP1990},
  author = {Peter Wegner and Stanley B. Zdonik},
  WKloc = {Q-011}
}

@InProceedings{Cardelli-Martini-Mitchell-Scedrov-1991,
  title = {An Extension of system~{$F$} with Subtyping},
  author = {Luca Cardelli and Simone Martini and John C. Mitchell and
		  Andre Scedrov},
  pages = {750--770},
  crossref = {TACS1991},
  abstract = {System~$F$ is a well-known typed $\lambda$-calculus with
		  polymorphic types, which provides a basis for
		  polymorphic programming languages.  We study an
		  extension of~$F$, called $F_{<:}$, that combines
		  parametric polymorphism with subtyping.

                  The main focus of the paper is the equational theory
		  of~$F_{<:}$, which is related to PER models and the
		  notion of parametricity.  We study some categorical
		  properties of the theory when restricted to closed
		  terms, including interesting categorical
		  isomorphism.  We also investigate proof-theoretical
		  properties, such as the conservativity of typing
		  judgements with respect to~$F$.

                  We demonstrate by a set of examples how a range of
		  constructs may be encoded in~$F_{<:}$.  These
		  include record operations and subtyping hierarchies
		  that are related to features of object-oriented
		  languages.}
}

@InProceedings{Cardelli-Mitchell-1989,
  author = {Luca Cardelli and J.C. Mitchell},
  title = {Operations on Records},
  crossref = {MFPS89}
}

@Article{Cardelli-Wegner-1985,
  author = {Cardelli, Luca and Wegner, Peter},
  title = {On Understanding Types, Data Abstraction, and
		  Polymorphism},
  journal = {ACM Computing Surveys},
  year = 1985,
  volume = 17,
  number = 4,
  pages = {471--522},
  WKloc = {B-0031},
  abstract = {Our objective is to understand the notion of type
		  in programming languages,present a model of typed,
		  polymorphic programming languages that reflects
		  recent research in type theory,and examine the
		  relevance of recent research to the design of
		  practical programming languages. Object-oriented
		  languages provide both a framework and a motivation
		  for exploring the interaction among the concepts of
		  type,data abstraction, and polymorphism,since they
		  extend the notion of type to data abstraction and
		  since type inheritance is an important form of
		  polymorphism.We develop a lambda-calculus-based
		  model for type systems that allows us to explore
		  these interactions in a simple setting, unencumbered
		  by complexities of production programming
		  languages.The evolution of languages from untyped
		  universes to monomorphic and then polymorphic type
		  systems is reviewed...}
}

@InProceedings{Cardone92,
  keywords = {subtypes as ideals},
  abstract = {We present an algebraic framework for the
		  interpretation of type systems including type
		  recursion, where types are interpreted as {\em
		  partial equivalence relations} over a Scott domain.
		  We use the notion of {\em iterative algebra},
		  introduced by J.~Tiuryn [26] as a counterpart to the
		  categorical notion of {\em iterative algebraic
		  theory} by C.~C.~Elgot [15]. We show that a
		  suitable collection of partial equivalence relations
		  is closed under type constructors and forms an
		  iterative algebra. The existence of type
		  interpretations follows from the initiality, in the
		  class of iterative algebras, of the algebra of
		  regular infinite trees obtained by infinitely
		  unfolding recursive types.},
  title = {An Algebraic Approach to the Interpretation of
		  Recursive Types},
  pages = {66--85},
  crossref = {CAAP92},
  author = {Felice Cardone}
}

@PhDThesis{Cardoso-1982,
  author = {Cardoso, Rodrigo},
  title = {{Unter\-su\-chung paralleler Pro\-gramme mit
      rela\-tio\-nen\-alge\-bra\-ischen Methoden}},
  school = {TU M\"unchen},
  type = {Diplom\-arbeit under supervision of {Gunther Schmidt}},
  year = 1982,
  bibliographies = {RelMiCS}
}

@InProceedings{Carette-2004a,
  author = 	 {Jacques Carette},
  title = 	 {Understanding Expression Simplification},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {ISSAC '03},
  OPTpages = 	 {},
  year = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  note = 	 {submitted},
  WKloc = 	 {A-1515}
}

@Article{Carette-2005a,
  author = 	 {Jacques Carette},
  title = 	 {Gaussian Elimination: a case study in efficient
        genericity with MetaOCaml},
  OPTjournal = 	 {},
  year = 	 {2005},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {},
  note = 	 {submitted},
  WKloc = 	 {A-1567, doc/pap/BIB},
  abstract = {The Gaussian Elimination algorithm is in fact an
       algorithm family --- common implementations contain at least 6
       (mostly independent) "design choices". A generic implementation
       can easily be parametrized by all these design choices, but
       this usually leads to slow and bloated code. Using MetaOCaml's
       staging facilities, we show how we can produce a natural
       implementation of Gaussian Elimination which exposes its design
       choices at code-generation time, so that these choices can
       effectively be specialized away, and where the resulting code
       is quite efficient.}
}

@Article{Carette-2006,
  author = {Jacques Carette},
  title = {Gaussian Elimination: a Case Study in Efficient Genericity
with {MetaOCaml}},
  PDF = {http://www.cas.mcmaster.ca/~carette/publications/ge.pdf},
  journal = SCICOP,
  year = {2006},
  note = {accepted}
}

@inproceedings{Carette-Kiselyov-2005,
  author    = {Jacques Carette and Oleg Kiselyov},
  title     = {Multi-stage Programming with Functors and Monads: Eliminating
               Abstraction Overhead from Generic Code},
  pages     = {256--274},
  ee        = {http://dx.doi.org/10.1007/11561347_18},
  crossref  = {GPCE2005},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@Article{Carmody-Leeming-Walters-1995,
  author = {S. Carmody and Mark Leeming and R.F.C. Walters},
  title = {The {Todd-Coxeter} Algorithm and Left {Kan} Extensions},
  journal = {Journal of Symbolic Computation},
  year = 1995,
  pages = {459--488},
  volume = 19
}

@Article{Carmody-Walters-1991,
  author = {S. Carmody and R.F.C. Walters},
  title = {Computing quotients of actions of a free category},
  journal = LNM,
  year = 1991,
  pages = {63--78},
  volume = 1488
}

@MastersThesis{Caron-1997,
  author = {Andr\'e Caron},
  title = {Transformation d'expressions relationelles sans variables},
  year = 1997,
  month = JUL,
  school = {D\'epartment d'Informatique, Facult\'e des Sciences et de Genie, Universit\'e Laval, Qu\'ebec},
  WKloc = {B-0043},
  annote = {Jules Desharnais},
  bibliographies = {RelMiCS}
}

@InProceedings{Caron-Comon-Coquide-Dauchet-Jacquemard-1994,
  author = {A.C. Caron and H. Comon and J.L. Coquid\'e and M. Dauchet and F.
           Jacquemard},
  title = {Pumping, Cleaning and Symbolic Constraints Solving},
  pages = {436--449},
  crossref = {ICALP1994},
  authorsAddress = {ACC, JLC, MD: Lille; HC, JF: Paris-Sud},
  abstract = {We define a new class of tree automata which generalizes
		  both the {\em encompassment automata} of [3] and the
		  {\em automata with tests between brothers} of
		  [2]. We give a pumping lemma for these automata,
		  which implies that the emptiness of the
		  corresponding language is decidable. Then, we show
		  how to decide emptiness by means of a ``cleaning''
		  algorithm, which leads to more effective decision
		  procedures.}
}

@Article{Carpenter-Glaser-1996,
  author = 	 {Carpenter, D. B. and Glaser, H.},
  title = 	 {Some lattice-based scientific problems, expressed in Haskell},
  journal = 	 JFP,
  year = 	 1996,
  volume =	 6,
  number =	 3,
  pages =	 {419--443},
  abstract = 	 {The paper explores the application of a lazy functional
      language, Haskell, to a series of grid-based scientific
      problems---solution of the Poisson equation and Monte Carlo simulation
      of two theoretical models from statistical and particle physics. The
      implementations introduce certain abstractions of grid topology,
      making extensive use of the polymorphic features of Haskell.
      Updating is expressed naturally through use of infinite lists,
      exploiting the laziness of the language. Evolution of systems
      is represented by arrays of interacting streams.}
}

@Article{Carter-GardnerWB-2008,
  author = 	 {J. Carter and W. B. Gardner},
  title = 	 {Converting scenarios to {CSP} traces with {Mise en Scene} for requirements-based programming},
  journal = 	 {Innovations Syst.\null{} Softw.\null{} Eng.},
  year = 	 2008,
  volume = 	 4,
  pages = 	 {45--70},
  DOI = 	 {10.1007/s11334-007-0041-0},
  WKloc = {A-1703}
}

@InProceedings{Carroll-Pollock-1994,
  author = {M.C. Carroll and L. Pollock},
  title = {Composites: Trees for Data Parallel Programming},
  crossref = {ICCL94},
  pages = {43--54}
}

@InProceedings{Carstensen-Ebert-WinterA-1994,
  author = {Martin Carstensen and J\"urgenEbert and Andreas Winter},
  title = {{Deklarative Beschreibung von Graphensprachen}},
  crossref = {Honnef94},
  pages = {21--27},
  keywords = {KOGGE (KOblenzer Generator f\"ur Graphische
		  Entwicklungsumgebungen, declarative description of
		  graph languages}
}

@Book{Carter-2006,
  author =	 {Paul A. Carter},
  title = 	 {{PC} Assembly Language},
  publisher = 	 {\url{http://www.drpaulcarter.com/}},
  year = 	 2006,
  note =	 {A-1679, doc/pap/BIB/CarterPA-2006_pcasm-book*}
}

@Article{Casas-Clark-Konuru-Otto-Prouty-Walpole-1995,
  author = {Jeremy Casas and Dan L. Clark and Ravi Konuru and Steve W. Otto and Robert M. Prouty and Jonathan Walpole},
  title = {{MPVM}: A Migration Transparent Version of {PVM}},
  journal = {Computing Systems},
  year = 1995,
  volume = 8,
  number = 2,
  pages = {171--216},
  bibliographies = {ProcMig}
}

@PhDThesis{Casley-1991,
  author = {Ross Thomas Casley},
  title = {On the Specification of Concurrent Systems},
  school = {Department of Computer Science, Standford University},
  year = 1991,
  month = JAN,
  WKloc = {A-1271},
  annote = {advisor: Vaughan Pratt; parts: \cite{Casley-Crew-Meseguer-Pratt-1991}},
  bibliographies = {RelMiCS, Unsharp},
  abstract = {In models of concurrent processes constraints on the order
      of events are often represented by partial orders, and schedules of
      events are then defined using an algebra of standard operations such
      as sequential and parallel composition.

      In this dissertation the notion of parallel order is replaced by that
      of a set with a metric which takes values in a given oredered monoid.
      Partial orders are the simple case of a monoid whose two elements
      represent the presence or absence of a constraint.

      An ordered monoid can be seen as a monoidal category, and schedules
      based on it are categories enriched in the monoid. Algebraic
      operations on schedules can then be defined as constructions in the
      category of schedules.

      These definitions rely on cetrtain properties of a category of
      schedules, such as closure and completeness, To simplify proofs of
      these properties, two constructions are defined. The first creates a
      category of unlabeled schedules from a system of constraints. The
      second adds labels to unlabeled schedules. Many categories of
      interest can be constructed from simple categories using these two
      methods. The main results of the dissertation derive the required
      properties of categories so constructed from similar, more easily
      verified properties of base categories.

      Several notions of timing constraint can be viewed in a uniform way
      in this framework. An example is the Gaifman-Pratt system,
      essentially the partial order model with additional specification as
      to whether two events may occur simultaneously. It corresponds to a
      monoid whose three elements represent strict precedence, lax
      precedence (simultaneity is permitted), and absence of constraint.
      Real-valued timing constraints correspond to the additive monoid of
      the real numbers.}
}

@Article{Casley-Crew-Meseguer-Pratt-1991,
  author = {Casley, R.T and Crew, R.F. and Meseguer, J. and Pratt, V.R.},
  title = {Temporal Structures},
  journal = {Math. Structures in Comp. Sci.},
  volume = 1,
  number = 2,
  pages = {179--213},
  month = JUL,
  year = 1991,
  URL = {http://boole.stanford.edu/pub/man.ps.gz},
  bibliographies = {RelMiCS},
  WKloc = {A-1277, doc/pap/BIB/Casley-Crew-Meseguer-Pratt-1991.ps.gz},
  abstract = {We combine the principles of the Floyd-Warshall-Kleene
      algorithm, enriched categories, and Birkhoff arithmetic, to yield a
      useful class of algebras of transitive vertex-labeled spaces. The
      motivating application is a uniform theory of abstract or
      parametrized time in which to any given notion of time there
      corresponds an algebra of concurrent behaviors and their operations,
      always the same operations but interpreted automatically and
      appropriately for that notion of time. An interesting side
      application is a language for succinctly naming a wide range of
      datatypes.},
  annote = {see also \cite{Casley-1991}}
}

@InProceedings{Caspi-1993,
  author = {Paul Caspi},
  title = { Lucid Synchrone},
  booktitle = {Proc. INRIA Workshop OPOPAC, Lacanau, France},
  month = {November},
  pages = {79--96},
  publisher = {HERMES},
  address = {Paris},
  year = 1993
}

@InProceedings{Caspi-Halbwachs-Pilaud-Plaice-1984,
  author = {P.  Caspi and N. Halbwachs and D. Pilaud and J. Plaice},
  title = {Lustre: a declarative language for programming synchronous systems},
  crossref = {POPL1987},
  pages = {178--188}
}

@InProceedings{Castagna-Ghelli-Longo-1993,
  author = {Giuseppe Castagna and Giorgio Ghelli and Giuseppe Longo},
  title = {A semantics for $\lambda\&-early$: a calculus with overloading
          and early binding},
  pages = {107--123},
  abstract = {The role of $\lambda$-calculus as core functional language is
             due to its nature as ``pure'' theory of functions. In the
             present approach we use the functional expressiveness of typed
             $\lambda$-calculus and extend it with our understanding of some
             relevant features of a broadly used programming style: Object
             Oriented Programming (OOP). The core notion we focus on, yields
             a form of ``dependency on input types'' (or ``on the types of
             the inputs'') and formalizes ``overloading'' as implicitly used
             in OOP.

             The basis of this work has been laid in [CGL92a],
             where the main syntactic properties of this extension have been
             shown. In this paper, we investigate an elementary approach to
             its mathematical meaning. The approach is elementary, as we tried
             to follow the most immediate semantic intuition which underlies
             our system, yet provide a rigorous mathematical model. Indeed,
             our semantics provides an understanding of a slightly modified
             version of the system in [CGL92a]: we had, so far, to restrict
             our attention only to ``early binding''.

             In order to
             motivate our extended $\lambda$-calculus, we first survey the
             key features in OOP which inspired our work. Then we summarize
             the system presented in [CGL92a] and introduce the variant with
             ``early binding''. Finally we present the model.},
  crossref = {TLCA93},
  WKloc = {A-0181}
}

@InProceedings{Castagna-Pierce-1994,
  author = {Giuseppe Castagna and Benjamin C. Pierce},
  title = {Decidable Bounded Quantification},
  crossref = {POPL1994},
  pages = {151--163},
  keywords = {subtyping},
  abstract = {$\ldots$ variant $F_{\leq}^{\sf T}$ of $F_{\leq}$ $\ldots$}
}

@PhdThesis{Cattrall-1992,
  author = 	 {Dave M. Cattrall},
  title = 	 {The Design and
Implementation of a Relational Programming System},
  school = 	 {Dept.\null{} of Computer Science, University of York},
  year = 	 1992,
  keywords = 	 {Drusilla},
  bibliographies = {RelMiCS}
}

@InProceedings{Cattrall-Runciman-1992,
  author = {Dave M. Cattrall and Colin Runciman},
  title = {A Relational Programming System with Inferred
		  Representations},
  crossref = {PLILP1992},
  pages = {475--476},
  note = {system presentation},
  WKloc = {A-0026},
  authorsAddress = {York},
  abstract = {Relational programming was originally proposed by
		  MacLennan [4,5,6]. He advocated a language based on
		  binary relations and operators for combining and
		  manipulating relations. Such operators form a {\em
		  relational algebra} --- a set of combining forms for
		  relations which generalise a language like FP from
		  functions to relations.

		  MacLennan designed a relational language and built
		  an interpreter for it [1] but his implementation was
		  achieved at the expense of an explicit compromise to
		  relational abstraction. He split relational
		  programming into two worlds --- an intensional world
		  (relations represented by many-to-one functions) and
		  an extensional world (relations represented by
		  association lists). The relational operators were
		  similarly segregated into two groups --- one
		  applicable to intensional, the other to extensional
		  relations. This representation divide considerably
		  inhibited freedom of programmer expression.},
  annote = {RELVIEW ?
		  Drusilla language, lazy reduction semantics,
		  higher-order relations, written in Miranda,
		  typed representation inference system},
  bibliographies = {RelMiCS}
}

@InProceedings{Caucal-1992,
  title = {Monadic Theory of Term Rewritings},
  author = {Didier Caucal},
  pages = {266--273},
  crossref = {LICS7},
  WKloc = {A-0402},
  abstract = {We consider the monadic second-order theory of term
		  rewritings.  We show that the monadic theory of the
		  rewriting (or the suffix rewriting) of a ground
		  rewrite system is undecidable. Furthermore the
		  first-order theory is undecidable for the prefix
		  derivation according to a linear context-free
		  grammar on linear terms. Nevertheless we introduce a
		  new notion on terms with variables: a term is entire
		  if each of its subterms either is a variable, or is
		  without variable or has the same variables as the
		  term.  We show that the monadic theory is decidable
		  (respectively undecidable) for the prefix rewriting
		  according to a rewrite system on entire terms, with
		  an axiom (respectively without axiom).}
}

@Misc{Cazanescu-Stefanescu-1994,
  author = {V. E. C{\u{a}}z{\u{a}}nescu and Gh. {\c{S}}tef{\u{a}}nescu},
  title = {Feedback, Iteration and Repetition},
  year = 1994,
  WKloc = {A-0889},
  filename = {WS-94.ps},
  note = {updated version of paper originally written in 1988.}
}

@Article{Cazenave-1998,
  author = {Tristan Cazenave},
  title = {Synthesis of an Efficient Tactical Theorem Prover for the Game of {Go}},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 18},
  WKloc = {A-0902, 73--78},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Manual{Cederquist-etal-1993,
  title = {Version Management with {CVS}},
  author = {Per Cederquist and others},
  organization = {Signum Suport AB},
  year = 1993,
  note = {Used version: for CVS 1.11.2},
  WKloc = {A-1343}
}

@InProceedings{Cengarle-1995,
  author = {Mar{\'{\i}}a Victoria Cengarle},
  title = {Semantic Typing for Parametric Algebraic Specifications},
  crossref = {AMAST1995},
  pages = {261--276},
  OPTabstract = {},
  WKloc = {A-0622}
}

@Misc{CellSDK3_1_VMASS,
  author =    {{IBM}},
  title =     {{Cell BE SDK 3.1}},
  howpublished = {This is the latest version of IBM's Cell SDK
and includes 32 single-precision special functions
in in-lineable generated C header files,
and as a library of long-vector functions scheduled by \textsf{Coconut},
and 32 double-precision special functions
in in-lineable generated C header files,
and as a library of long-vector functions all generated by \textsf{Coconut}.},
  month =     OCT,
  year =      2008,
  annote =    {\url{http://www.ibm.com/developerworks/power/cell/downloads.html}}
}

@InProceedings{Cerioli-Reggio-1992,
  author = {M. Cerioli and G. Reggio},
  title = {Institutions for Very Abstract Specifications},
  crossref = {SADT92},
  pages = {113--127}
}

@Misc{Cerny-1999,
  author = {Eduard Cerny},
  title = {Multiway Decision Graphs {(MDG)} and Applications},
  year = 1999,
  month = APR,
  pages = 58,
  note = {slides},
  URL = {http://www.iro.umontreal.ca/~cerny/MDG.slides.990419.pdf},
  WKloc = {A-0964},
  bibliographies = {SeminarWT2000, RelMiCS}
}

@InProceedings{Cerny-Corella-Langevin-Song-Tahar-Zhou-1997,
  author = {E. Cerny and F. Corella and M. Langevin and X. Song and S. Tahar and Z. Zhou},
  title = {Automated Verification with Abstract State machines Using Multiway Decision Diagrams},
  pages = {79--113},
  WKloc = {A-0963},
  volume = 1287,
  crossref = {Kropf-1997},
  bibliographies = {SeminarWT2000, RelMiCS},
  annote = {http://griao.iro.umontreal.ca/research/index_eng.html}
}

@InProceedings{Cerrito-Kesner-1999,
  author = {Serenella Cerrito and Delia Kesner},
  title = {Pattern Matching as Cut Elimination},
  crossref = {LICS14},
  pages = {98--108},
  WKloc = {A-1402, doc/pap/BIB (long version)},
  keywords = {TPC},
  bibliographies = {PMC}
}

@Book{Cezzar-1995,
  author = {Ruknet Cezzar},
  title = {A Guide to Programming Languages: Overview and Comparison},
  publisher = {Artech House},
  year = 1995,
  address = {Boston, London},
  UniBwM = {INF400/YA7996},
  annote = {Ada, BASIC, C, COBOL, FORTRAN, Pascal, LISP, PROLOG,
		  OOP, C++, Visual Programming}
}

@InProceedings{ChaibDraa-Desharnais-Khedri-Jarras-Sayadi-Tchier-1994,
  author = {Brahim Chaib-draa and Jules Desharnais and Ridha
                  Khedri and Imed Jarras and Slim Sayadi and
                  Fairouz Tchier},
  title = {Une approche relationnelle \`a la d\'ecomposition
                  parall\`ele},
  booktitle = {Colloque sur les m\'ethodes math\'ematiques pour la
                  synth\`ese de syst\`emes informatiques {(BMW 94)}},
  publisher = {Universit\'e du Qu\'ebec \`a Montr\'eal},
  year = 1994,
  pages = {89--103},
  month = MAY,
  WKloc = {A-0438},
  bibliographies = {RelMiCS}
}

@Book{Chaitin-2004,
  author =	 {Gregory Chaitin},
  title = 	 {META MATH! The Quest for Omega},
  publisher = 	 {Pantheon Books},
  year = 	 {to appear (2004?)},
  note =	 {draft at: \url{http://www.cs.auckland.ac.nz/CDMTCS/chaitin/omega.html}},
  WKloc = 	 {doc/pap/BIB}
}

@InProceedings{Chakravarty-1999,
  author = {Manuel M. T. Chakravarty},
  title = {Lazy Lexing is Fast},
  crossref = {FUJI-1999},
  pages = {??},
  OPTabstract = {This paper introduces a set of combinators for building
      lexical analysers in a lazy functional language. During lexical
      analysis, the combinators generate a deterministic, table-driven
      analyser on the fly. Consequently, the presented method combines the
      efficiency of off-line scanner generators with the flexibility of the
      combinator approach. The method makes essential use of the lazy
      semantics of the implementation language Haskell. Finally, the paper
      discusses benchmarks of a scanner for the programming language C.},
  WKloc = {A-0812}
}

@Misc{Chakravarty-2000,
  author = {Manuel M. T. Chakravarty},
  title = {{C $\rightarrow$ {\sc Haskell}}, or Yet Another Interfacing Tool},
  OPThowpublished = {},
  OPTmonth = {},
  year = {2000?},
  OPTnote = {},
  URL = {http://www.cse.unsw.edu./au/~chak/},
  WKloc = {A-1067}
}

@Manual{Chakravarty-etal-2002a,
  author = {Manuel M. T. Chakravarty and others},
  title = {The {Hakell 98} Foreign Function Interface 1.0, An Addendum to the {Hakell 98 Report} (Release Candidate 4)},
  month = APR,
  year = 2002,
  URL = {http://www.cse.unsw.edu.au/~chak/haskell/ffi/},
  note = {http://www.cse.unsw.edu.au/\~{}chak/haskell/ffi/},
  WKloc = {A-1283}
}

@Article{Chakravarty-Keller-,
  author = 	 {Manuel M. T. Chakravarty and Gabriele Keller},
  title = {The Risks and Benefits of Teaching Purely Functional Programming in First Year},
  journal = 	 JFP,
  year = 	 {???},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  WKloc = 	 {A-1588}
}

@InProceedings{Chalin-Kiniry-Leavens-Poll-2006_JML,
  year={2006},
  isbn={978-3-540-36749-9},
  LNCSbooktitle = {FMCO 2006},
  booktitle={Formal Methods for Components and Objects},
  volume={4111},
  series=LNCS,
  editor={Boer, Frank S. and Bonsangue, Marcello M. and Graf, Susanne and de Roever, Willem-Paul},
  DOI={10.1007/11804192_16},
  title={Beyond Assertions: Advanced Specification and Verification with {JML} and {ESC/Java2}},
  DOIURL = {http://dx.doi.org/10.1007/11804192_16},
  publisher = Springer,
  author={Chalin, Patrice and Kiniry, Joseph R. and Leavens, Gary T. and Poll, Erik},
  pages={342--363},
  abstract = {Many state-based specification languages,
    including the Java Modeling Language (JML), contain at their core
    specification constructs familiar to most undergraduates:
    e.g., assertions, pre- and postconditions, and invariants.
    Unfortunately, these constructs are not sufficiently expressive
    to permit formal modular verification of programs
    written in modern object-oriented languages like Java.
    The necessary extra constructs for specifying an object-oriented module
    include (perhaps the less familiar)
    frame properties, datagroups, and ghost and model fields.
    These constructs help specifiers deal with potential problems related to,
    for example, unexpected side effects, aliasing,
    class invariants, inheritance, and lack of information hiding.
    This tutorial paper focuses on JML's realization of these constructs,
    explaining their meaning while illustrating
    how they can be used to address the stated problems.}
}

@Book{Chan-2001,
  author = {Gillian Chan},
  title = {The Carved Box},
  publisher = {Kids Can Press},
  year = 2001,
  address = {Toronto},
  ISBN = {1-55337-016-3},
  annote = {Dundas},
  URL = {http://www.kidscanpress.com/}
}

@Article{Chandru-Rao-1996,
  author = {Vijay Chandru and M. R. Rao},
  title = {Combinatorial optimization: an integer programming perspective},
  journal = {ACM Computing Surveys (CSUR)},
  volume = 28,
  number = 1,
  year = 1996,
  ISSN = {0360-0300},
  pages = {55--58},
  WKloc = {A-1467},
  doi = {http://doi.acm.org/10.1145/234313.234341},
  publisher = {ACM Press}
}

@Book{Chang-1990,
  editor = {Shi-Kuo Chang},
  title = {Principles of Visual Programming Systems},
  publisher = {Prentice hall},
  year = 1990,
  address = {Englewood Cliffs, New Jersey},
  UniBwM = {INF400/T9646}
}

@Book{Chang-Keisler-1973,
  author = {C. C. Chang and H. J. Keisler},
  title = {Model Theory},
  publisher = NoHo,
  year = 1973,
  bibliographies = {RelMiCS}
}

@InProceedings{Chang-Manna-Pnueli-1994,
  title = {Compositional Verification of Real-Time Systems},
  author = {Edward Chang and Zohar Manna and Amir Pnueli},
  pages = {458--465},
  crossref = {LICS9},
  abstract = {This paper presents a compositional proof system for the
		  verification of real-time systems.  Real-time
		  systems are modeled as timed transition modules,
		  which explicitly model interaction with the
		  environment and may be combined using composition
		  operators.

                  Composition rules are devised such that the
		  correctness of a system may be determined from the
		  correctness of its components.  These proof rules
		  are demonstrated on Fischer's mutual exclusion
		  algorithm, for which mutual exclusion and bounded
		  response are proven.}
}

@InProceedings{Charatonik-Pacholski-1994,
  title = {Negative Set Constraints with Equality},
  author = {Witold Charatonik and Leszek Pacholski},
  pages = {128--136},
  crossref = {LICS9},
  abstract = {Systems of set constraints describe relations between sets
		  of ground terms. They have been successfully used in
		  program analysis and type inference. So far two
		  proofs of decidability of mixed set constraints have
		  been given: by R.~Gilleron, S.~Tison and M.~Tommasi
		  and A.~Aiken, D.~Kozen, and E.L.~Wimmers. However,
		  both these proofs are long, involved and do not seem
		  to extend to more general set constraints.

                  Our approach is based on a reduction of set
		  constraints to the monadic class given in a recent
		  paper by L.~Bachmair, H.~Ganzinger, and U.~Waldmann.
		  We first give a new proof of decidability of systems
		  of mixed positive and negative set constraints.  We
		  explicitely describe a very simple algorithm working
		  in NEXPTIME and we give in all detail a relatively
		  easy proof of its correctness. Then, we sketch how
		  our technique can be applied to get various
		  extensions of this result. In particular we prove
		  that the problem of consistency of mixed set
		  constraints with restricted projections and
		  unrestricted diagonalization is NEXPTIME.},
  remarks = {The full problem was solved in: Witold Charatonik and
		  Leszek Pacholski.  Set constraints with projections
		  are in NEXPTIME.  In {\em Proceedings of the 35th
		  Annual Symposium on Foundations of Computer Science
		  (FOCS)}, Santa Fe, 1994 (to appear).}
}

@Book{Charniak-Riesbeck-McDermott-1980,
  author =	 {E. Charniak and C. K. Riesbeck and D. V. McDermott},
  title = 	 {Artificial Intelligence Programming},
  publisher = 	 {Lawrence Erlbaum Associates},
  year = 	 1980,
  bibliographies = {HHOL},
  annote =	 {Chapter 14 describes \emph{discrimination nets},
                  a data structure for indexing items,
                  used in Isabelle/src/Pure/net.ML}
}

@Unpublished{Chechik-1999,
  author = {Marsha Chechik},
  title = {{SC(R)$^3$}: Towards Usability of Formal Methods},
  note = {Draft distributed on the WWW},
  month = AUG,
  year = 1999,
  WKloc = {A-1034},
  bibliographies = {SpecTech}
}

@InProceedings{Chen-Hudak-Odersky-1992,
  author = {Kung Chen and Martin Odersky and Paul Hudak},
  title = {Parametric Type Classes},
  year = 1992,
  month = JUN,
  booktitle = {{ACM} Conf. on {LISP} and Functional Programming, {June} 1992},
  WKloc = {A-0785},
  abstract = {We propose a generalization to haskell's type classes where
      a class can have type parameters besides the placeholder variable. We
      show that this generalization is essential to represent container
      classes with overloaded data constructor and selector operations. We
      also show that the resulting type system has principal types and
      present unification and type reconstruction algorithms.}
}

@TechReport{Chen-Odersky-Hudak-1992,
  author = {Kung Chen and Martin Odersky and Paul Hudak},
  title = {Type Inference for Parametric Type Classes},
  year = 1992,
  month = JUN,
  institution = {Yale University},
  WKloc = {A-0092, ~kahl/doc/pap/type-inf-ptc},
  abstract = {Haskell's type classes permit the definition of
		  overloaded operators in a rigorous and (fairly)
		  general manner that integrates well with the
		  underlying Hindley-Milner type system. As a result,
		  operators that are monomorphic in other typed
		  languages can be given a more general type. Most
		  notably missing, however, are overloaded functions
		  for data selection and construction. Such overloaded
		  functions are quite useful, but the current Haskell
		  type system is not expressive enough to support
		  them.

                  We introduce the notion of parametric type classes**
		  as a significant generalization of Haskell's type
		  classes. A parametric type class is a class that has
		  type parameters in addition to the placeholder
		  variable which is always present in a class
		  declaration. Haskell's type classes are special
		  instances of parametric type classes with just a
		  placeholder but no parameters. We show that this
		  generalization is essential to represent container
		  classes with overloaded data constructor and
		  selector operations. Furthermore, through a simple
		  encoding scheme, we show that parametric type
		  classes are able to capture the notion of ``type
		  constructor variables'', thus permitting the
		  definition of overloaded operators such as map.

                  The underlying type system supporting our proposed
		  generalization is an extension of Hindley-Milner
		  type system with parametric type classes.  The range
		  of type variables are bounded by constraints. Rules
		  for satisfiability and entailment of these
		  constraints are given by a context-constrained
		  instance theory that is separate from the inference
		  rules of the type system. The decoupling of the
		  instance theory from the type inference system makes
		  our system more modular than previous work. We prove
		  that the resulting type system is decidable, and
		  provide an effective type inference algorithm to
		  compute the principal types for well-typed expressions.}
}

@TechReport{Chen-Odersky-Zenger-Zenger-1999,
  author = {Gang Chen and Martin Odersky and Christoph Zenger and Matthias Zenger},
  title = {A Functional View of Join},
  year = 1999,
  institution = {University of South Australia},
  number = {ACRC-99-016},
  WKloc = {A-0915},
  URL = {http://lampwww.epfl.ch/papers/techfunjoin.ps.gz},
  abstract = {Join calculus, usually presented as a process calculus,
                  is suitable as a foundation of both sequential and
                  concurrent programming. We give a new operational semantics
                  of join calculus, expressed as a reduction system with a
                  single reduction rule similar to beta reduction in
                  lambda calculus. We also introduce a new Hindley/Milner
                  style type system for join calculus. Compared to
                  previous work, the type system gives more accurate
                  types of composite and mutually recursive definitions.
                  The type system's soundness is established by showing
                  that our reduction rule keeps typings invariant.
                  We present an algorithm for type inference
                  and show its soundness and completeness.}
}

@InProceedings{Cheney-2005,
  author = 	 {James Cheney},
  title = 	 {Scrap your Nameplate},
  crossref =  {ICFP2005},
  pages = {\unfinished},
  WKloc = {A-1692, doc/pap/BIB},
  note = 	 {(Functional Pearl)},
  abstract = 	 {Recent research has shown how boilerplate code, or repetitive code
      for traversing datatypes, can be eliminated using generic programming
      techniques already available within some implementations of
      Haskell. One particularly intractable kind of boilerplate is nameplate,
      or code having to do with names, name-binding, and fresh
      name generation. One reason for the difficulty is that operations on
      data structures involving names, as usually implemented, are not
      regular instances of standard map, fold , or zip operations. However,
      in nominal abstract syntax, an alternative treatment of names
      and binding based on swapping, operations such as $\alpha$-equivalence,
      capture-avoiding substitution, and free variable set functions are
      much better-behaved.

      In this paper, we show how nominal abstract syntax techniques
      similar to those of FreshML can be provided as a Haskell library
      called FreshLib. In addition, we show how existing generic programming
      techniques can be used to reduce the amount of nameplate code
      that needs to be written for new datatypes involving
      names and binding to almost nothing --- in short, how to scrap your
      nameplate.}
}

@InProceedings{Cheney-Hinze-2002,
  author = {James Cheney and Ralf Hinze},
  title = {A lightweight implementation of generics and dynamics},
  crossref = {Haskell2002},
  pages = {90--104},
  URL = {http://doi.acm.org/10.1145/581690.581698},
  WKloc = {doc/pap/BIB},
  abstract = {The recent years have seen a number of proposals for
      extending statically typed languages by dynamics or generics. Most
      proposals --- if not all --- require significant extensions to the
      underlying language. In this paper we show that this need not be the
      case. We propose a particularly lightweight extension that supports
      both dynamics and generics. Furthermore, the two features are
      smoothly integrated: dynamic values, for instance, can be passed to
      generic functions. Our proposal makes do with a standard
      Hindley-Milner type system augmented by existential types. Building
      upon these ideas we have implemented a small library that is readily
      usable both with Hugs and with the Glasgow Haskell compiler.}
}

@InProceedings{Cheng-vanEmden-Parker-1993,
  WKloc = {?},
  keywords = {?},
  contents = {?},
  abstract = {?},
  title = {Applicative term rewriting systems and Prolog technology},
  pages = {?},
  crossref = {HOA1993-PP},
  author = {M.H.M. Cheng and van Emden,M.H. and  D.S. Parker}
}

@InProceedings{ChengRS91,
  title = {Higher Level Meta Programming in {Qu}-Prolog 3.0},
  pages = {285--298},
  crossref = {ICLP1991},
  author = {Cheng, Anthony S. K. and Robinson, Peter J. and John Staples}
}

@Misc{Cheno-1995,
  author = {Laurent Ch{\'e}no},
  title = {La lettre de {Caml}},
  year = 1995,
  WKloc = {B-0127}
}

@Misc{Chidlovskii-199X,
  author = {Boris Chidlovskii},
  title = {Using Regular Tree Automata as {XML} Schemas},
  year = {199?},
  WKloc = {A-0911}
}

@Article{Chin-Tarski-1948,
  author = {Louise H. Chin and Alfred Tarski},
  title = {Remarks on Projective Algebras},
  journal = BUAMS,
  volume = 54,
  year = 1948,
  pages = {80--81},
  note = {Abstract 90},
  bibliographies = {RelMiCS}
}

@Article{Chin-Tarski-1949,
  author = {Louise H. Chin and Alfred Tarski},
  title = {Distributive and Modular Laws in the Arithmetic of
		Relation Algebras},
  journal = BUAMS,
  volume = 55,
  year = 1949,
  pages = {61--62},
  note = {Abstract 69},
  bibliographies = {RelMiCS}
}

@Article{Chin-Tarski-1951,
  keywords = {relation algebra},
  year = 1951,
  volume = 1,
  title = {Distributive and Modular Laws in the Arithmetic of
		  Relation Algebras},
  pages = {341--384},
  number = 9,
  journal = UCPM,
  author = {Louise H. Chin and Alfred Tarski},
  bibliographies = {RelMiCS}
}

@Misc{Chiou-2002,
  author =	 {Chiou, C.},
  title =	 {First-order Resolution Theorem Prover in Haskell},
  howpublished = {Available at \textsf{http://www.cs.yale.edu/homes/cc392/}},
  month =	 {July},
  year =	 2002
}

@Article{Chitil-1997,
  author =       {Olaf Chitil},
  title =        {The Sigma-Semantics: A Comprehensive Semantics for Functional Programs},
  journal =      FUNDI,
  year =         1997,
  volume =    31,
  pages =     {253--294},
  URL =     {http://www.cs.kent.ac.uk/pubs/1997/1907/index.html},
  WKloc =      {doc/pap/BIB},
  abstract = {A comprehensive semantics for functional programs is presented,
    which generalizes the well-known call-by-value and call-by-name semantics.
    By permitting a separate choice between call-by value and call-by-name
    for every argument position of every function and parameterizing
    the semantics by this choice we abstract from the parameter-passing mechanism.
    Thus common and distinguishing features of all instances of the sigma-semantics,
    especially call-by-value and call-by-name semantics, are highlighted.
    Furthermore, a property can be validated
    for all instances of the sigma-semantics by a single proof.
    This is employed for proving the equivalence
    of the given denotational (fixed-point based)
    and two operational (reduction based) definitions of the sigma-semantics.
    We present and apply means for very simple proofs
    of equivalence with the denotational sigma-semantics
    for a large class of reduction-based sigma-semantics.
    Our basis are simple first-order constructor-based functional programs
    with patterns.}
}

@Misc{Chitil-1997a,
  author = {Olaf Chitil},
  title = {Adding an Optimization Pass to the {Glasgow Haskell Compiler}},
  year = 1997,
  WKloc = {A-0858}
}

@InProceedings{Chitil-1998,
  author = {Olaf Chitil},
  title = {Common Subexpressions Are Uncommon in Lazy Functional Languages},
  year = 1998,
  crossref = {IFL1997},
  pages = {53--71},
  DOI = {10.1007/BFb0055420},
  URL = {http://www.springerlink.com/content/87m4m1294j472456/},
  WKloc = {A-0676},
  annote = {contains GHC Core syntax},
  abstract = {Common subexpression elimination is a well-known
                  compiler optimisation that saves time by avoiding
                  the repetition of the same computation. In lazy
                  functional languages, referential transparency
                  renders the identification of common subexpressions
                  very simple. More common subexpressions can be
                  recognised because they can be of arbitrary type
                  whereas standard common subexpression elimination
                  only shares primitive values. However, because lazy
                  functional languages decouple program structure from
                  data space allocation and control flow, analysing
                  its effects and deciding under which conditions the
                  elimination of a common subexpression is beneficial
                  proves to be quite difficult. We developed and
                  implemented the transformation for the language
                  Haskell by extending the Glasgow Haskell
                  compiler. On real-world programs the transformation
                  showed nearly no effect. The reason is that common
                  subexpressions whose elimination could speed up
                  programs are uncommon in lazy functional languages.}
}

@Misc{Chitil-199X,
  author = {Olaf Chitil},
  title = {Common Subexpression Elimination in a Lazy Functional Language},
  year = {199?},
  WKloc = {A-0675},
  annote = {contains GHC Core syntax},
  note = {draft of \cite{Chitil-1998}}
}

@Misc{Chitil-199Z,
  author = {Olaf Chitil},
  title = {The $\varsigma$-Semantics: A Comprehensive Semantics for Functional Programs},
  year = {199?},
  WKloc = {A-0677}
}

@Article{Chitil-2005,
  author = {Chitil, Olaf},
  title = {Pretty Printing with Lazy Dequeues},
  journal = ACM-TOPLAS,
  issue_date = {January 2005},
  volume = {27},
  number = {1},
  month = jan,
  year = {2005},
  issn = {0164-0925},
  pages = {163--184},
  numpages = {22},
  url = {http://doi.acm.org/10.1145/1053468.1053473},
  doi = {10.1145/1053468.1053473},
  acmid = {1053473},
  publisher = {ACM},
  address = {New York, NY, USA},
  WKloc = {A-1482, doc/pap/BIB (preprint)},
  keywords = {Haskell, lazy functional programming},
  abstract = {There are several purely functional libraries for
     converting tree structured data into indented text, but they all make
     use of some backtracking. Over twenty years ago Oppen published a more
     efficient imperative implementation of a pretty printer. We show that
     the same efficiency is also obtainable without destructive updates by
     developing a similar but purely functional Haskell implementation with
     the same complexity bounds. At its heart lie two lazy double ended
     queues.}
}

@InProceedings{Chlipala-2010,
  author = {Chlipala, Adam},
  title = {A Verified Compiler for an Impure Functional Language},
  numpages = {14},
  url = {http://doi.acm.org/10.1145/1706299.1706312},
  doi = {10.1145/1706299.1706312},
  acmid = {1706312},
  WKloc = {A-1749, doc/pap/BIB},
  booktitle = {Proceedings of the 37th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
  series = {POPL '10},
  year = {2010},
  isbn = {978-1-60558-479-9},
  location = {Madrid, Spain},
  pages = {93--106},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {compiler verification, interactive proof assistants},
  abstract = {We present a verified compiler to an idealized assembly language
    from a small, untyped functional language with mutable references and exceptions.
    The compiler is programmed in the Coq proof assistant
    and has a proof of total correctness with respect to big-step operational semantics
    for the source and target languages.
    Compilation is staged and includes standard phases
    like translation to continuation-passing style and closure conversion,
    as well as a common subexpression elimination optimization.
    In this work, our focus has been on discovering and using techniques
    that make our proofs easy to engineer and maintain.
    While most programming language work with proof assistants
    uses very manual proof styles,
    all of our proofs are implemented as adaptive programs in Coq's tactic language,
    making it possible to reuse proofs unchanged as new language features are added.

    In this paper, we focus especially on phases of compilation
    that rearrange the structure of syntax with nested variable binders.
    That aspect has been a key challenge area in past compiler verification projects,
    with much more effort expended in the statement and proof of binder-related lemmas
    than is found in standard pencil-and-paper proofs.
    We show how to exploit the representation technique of
    parametric higher-order abstract syntax
    to avoid the need to prove any of the usual lemmas about binder manipulation,
    often leading to proofs that are actually shorter than their pencil-and-paper analogues.
    Our strategy is based on a new approach to encoding operational semantics
    which delegates all concerns about substitution to the meta language,
    without using features incompatible with general-purpose type theories like Coq's logic.}
}

@Book{Chomsky-1957,
  author = {Noam Chomsky},
  title = {Syntactic Structures},
  publisher = {Mouton},
  year = 1957,
  address = {The Hague}
}

@MastersThesis{Christiansen-2006,
  author = 	 {Jan Christiansen},
  title = 	 {A purely functional implementation of {ROBDDs} in {Haskell}},
  school = 	 {Christian-Albrechts-Universit\"at zu Kiel},
  year = 	 2006,
  type =	 {Diploma Thesis},
  month =	 FEB,
  WKloc = 	 {A-1631, doc/pap/BIB}
}

@Misc{Christiansen-Huch-2006,
  author = 	 {Jan Christiansen and Frank Huch},
  title = 	 {A purely functional implementation of {ROBDDs} in {Haskell}},
  howpublished = {Presented at TFP 2006, available via \textsf{http://www.cs.nott.ac.uk/\~{}nhn/TFP2006/}},
  month =	 APR,
  year =	 2006,
  WKloc = 	 {A-1686, doc/pap/BIB}
}

@Book{Church-1941,
  author = {Alonzo Church},
  title = {The Calculi of Lambda Conversion},
  publisher = {Princeton University Press},
  year = 1941,
  address = {Princeton},
  bibliographies = {FP},
  WKloc = {A-1493 (first three pages, includes reference to Schoenfinkel-1924 for Currying)}
}

@Book{Church-1956,
  author = {Alonzo Church},
  title = {Introduction to Mathematical Logic},
  publisher = {Princeton University Press},
  year = 1956,
  pages = {ix+378 pp.},
  address = {Princeton, N.J.},
  bibliographies = {RelMiCS, FP}
}

@Misc{Ciarpaglini-Danelutto-Folchi-Manconi-Pelagatti-199X,
  author = {S. Ciarpaglini and M. Danelutto and L. Folchi and C. Manconi and S. Pelagatti},
  title = {{ANACLETO}: A Template-Based {\sf p3l} Compiler},
  year = {199?},
  WKloc = {A-0837}
}

@InProceedings{Cicerone-Parisi-1994,
  author = {S. Cicerone and Parisi Presicce, Francesco},
  title = {Strategies in Modular System Design by Interface Rewriting},
  crossref = {ESOP1994},
  pages = {165--179},
  abstract = {The problem of designing a modular system, using a
		  set of predefined modules, with given import and
		  export interface has been reduced to the problem of
		  generating a specification in an algebraic
		  specification grammar. Here we tackle two important
		  problems connected with the generation: the strategy
		  to adopt in choosing the rewrite rules and the
		  elimination of unnecessary searches. The first is
		  investigated using a notion of similarity of
		  specifications and a definition of value to guide
		  the search algorithm; the second is solved using
		  syntactical criteria (independent of the target
		  specification) to determine that some derivation
		  sequences are superfluous. The latter development
		  has been influenced by similar work on graph grammars.}
}

@Article{CirsteaH-Faure-Fernandez-Mackie-Sinot-2007,
  author = "Horatiu Cirstea and Germain Faure and Maribel Fern{\'a}ndez and Ian Mackie and Fran{\c{c}}ois-R{\'e}gis Sinot",
  title = "From Functional Programs to Interaction Nets via the Rewriting Calculus ",
  journal = ENTCS,
  volume = 174,
  number = 10,
  pages = "39--56",
  year = 2007,
  note = {Proceedings of the Sixth International Workshop on Reduction Strategies in Rewriting and Programming (WRS 2006)},
  issn = "1571-0661",
  DOI = {10.1016/j.entcs.2007.02.046},
  DOIURL = {http://dx.doi.org/10.1016/j.entcs.2007.02.046},
  DirectURL = {http://www.sciencedirect.com/science/article/pii/S1571066107003982},
  keywords = "pattern-matching",
  keywords = "interaction nets",
  keywords = "rewriting calculus"
}

@InProceedings{CirsteaH-KirchnerC-1998,
  author = {Horatiu Cirstea and Claude Kirchner},
  title = {Combining Higher-Order and First-Order Computation Using
                 $\rho$-calculus: Towards a semantics of {{\sf ELAN}}},
  pages = {95--120},
  crossref = {FroCos1998},
  COMMENT = {Invited presentation by CK},
  WKloc = {A-1405, doc/pap/BIB},
  bibliographies = {PMC}
}

@Article{CirsteaH-KirchnerC-2001,
  author = {Horatiu Cirstea and Claude Kirchner},
  title = {The rewriting calculus --- {Part~I {\em and} II}},
  journal = {Logic Journal of the Interest Group in Pure and Applied Logics},
  year = 2001,
  volume = 9,
  number = 3,
  pages = {427--498},
  month = MAY,
  WKloc = {A-1406, A-1407, doc/pap/BIB},
  bibliographies = {PMC}
}

@Article{CirsteaH-KirchnerC-2001a,
  author = {Horatiu Cirstea and Claude Kirchner},
  title = {The rewriting calculus --- {Part~I}},
  journal = {Logic Journal of the Interest Group in Pure and
		   Applied Logics},
  year = 2001,
  volume = 9,
  number = 3,
  pages = {427--463},
  month = MAY,
  WKloc = {A-1406, doc/pap/BIB},
  bibliographies = {PMC},
  DVI = {data/RoIGPL--1.dvi.gz},
  PS = {data/RoIGPL--1.ps.gz},
  PDF = {data/RoIGPL--1.pdf},
  abstract = {The Rho-calculus integrates in a uniform and
	simple setting first-order rewriting, Lambda-calculus and
	non-deterministic computations.  Its abstraction mechanism is
	based on the rewrite rule formation and its main evaluation
	rule is based on matching modulo a theory T.

	In this first part, the calculus is motivated and its syntax
	and evaluation rules for any theory T are presented.  In the
	syntactic case, i.e. when T is the empty theory, we study its
	basic properties for the untyped case. We first show how it
	uniformly encodes Lambda-calculus as well as first-order
	rewriting derivations. Then we provide sufficient conditions
	for ensuring the confluence of the calculus.

	The rewriting calculus is a rule construction and application
	framework. As such it embeds in a uniform way term rewriting
	and Lambda-calculus. Since rule application is an explicit
	object of the calculus, it allows us also to handle the set of
	results explicitly.

	We present a simply typed version of the rewriting
	calculus. With a good choice of the type system, we show that
	the calculus is type preserving and terminating, i.e. verifies
	the subject reduction and strong normalization properties.},
  keywords = {rewriting, strategy, non-determinism, matching,
	rewriting-calculus, lambda-calculus, rule based language.}
}

@Article{CirsteaH-KirchnerC-2001b,
  author = {Horatiu Cirstea and Claude Kirchner},
  title = {The rewriting calculus --- {Part~II}},
  journal = {Logic Journal of the Interest Group in Pure and
		   Applied Logics},
  year = 2001,
  volume = 9,
  number = 3,
  pages = {465--498},
  month = MAY,
  WKloc = {A-1407, doc/pap/BIB},
  bibliographies = {PMC},
  DVI = {data/RoIGPL--2.dvi.gz},
  PS = {data/RoIGPL--2.ps.gz},
  PDF = {data/RoIGPL--2.pdf},
  abstract = {The Rho-calculus integrates in a uniform and
	simple setting first-order rewriting, Lambda-calculus and
	non-deterministic computations.  Its abstraction mechanism is
	based on the rewrite rule formation and its main evaluation
	rule is based on matching modulo a theory T.

	We have seen in the first part of this work the motivations,
	definitions and basic properties of the rho-calculus.  This
	second part is first devoted to the use of the rho-calculus
	for encoding a (conditional) rewrite relation. To this end we
	extend the calculus with a ``first'' operator whose purpose is
	to detect rule application failure. This extension allows us
	to express recursively rule application and therefore to
	encode strategy based rewriting processes. We then use this
	extended calculus to give an operational semantics to ELAN
	programs.

	We conclude with an overview of ongoing and future works on
	rho-calculus.},
  keywords = {rewriting, strategy, non-determinism, matching,
	rewriting-calculus, lambda-calculus, rule based language.}
}

@InProceedings{CirsteaH-KirchnerC-Liquori-2001,
  author = {Horatiu Cirstea and Claude Kirchner and Luigi Liquori},
  title = {The {Rho Cube}},
  pages = {166-180},
  booktitle = {Foundations of Software Science and Computation Structures,
		     \url{http://www.disi.unige.it/etaps2001}{ETAPS'2001}},
  address = {Genova, Italy},
  editor = {Honsell, Furio},
  month = APR,
  publisher = {Springer-Verlag©},
  series = {\url{http://www.springer.de/comp/lncs/index.html}{Lecture Notes in Computer Science}},
  year = 2001,
  DVI = {data/rhoCube.dvi.gz},
  PS = {data/rhoCube.ps.gz},
  PDF = {data/rhoCube.pdf},
  bibliographies = {PMC},
  WKloc = {A-1410, doc/pap/BIB},
  abstract = { The rewriting calculus, or Rho Calculus, is a
	simple calculus that uniformly integrates abstraction on
	patterns and non-determinism. Therefore, it fully integrates
	rewriting and Lambda-calculus. The original presentation of
	the calculus was "untyped".  In this paper we present a
	uniform way to decorate the terms of the calculus with types.
	This gives raise to a new presentation a la Church, together
	with nine (8+1) type systems which can be placed in a Rho-cube
	that extends the Lambda-cube of Barendregt. Due to the
	matching capabilities of the calculus, the type systems use
	only one abstraction mechanism and therefore gives an original
	answer to the identification of the standard "Lambda" and "Pi"
	abstractors.

	As a consequence, this brings matching and rewriting as the
	first class concepts of the Rho-versions of the Logical
	Framework of Harper-Honsell-Plotkin, and of the Calculus of
	Constructions of Coquand-Huet.},
  keywords = {Rewriting calculus, calculus of construction, type
	system, lambda cube, matching, rewriting}
}

@InProceedings{CirsteaH-KirchnerC-Liquori-2002,
  author = {Horatiu Cirstea and Claude Kirchner and Luigi Liquori},
  title = {Rewriting Calculus with(out) Types},
  booktitle = {Proceedings of the Fourth Workshop on Rewriting Logic
		     and Applications, {WRLA 2002}},
  year = 2002,
  editor = {Gadducci, Fabio and Montanari, Ugo},
  publisher = {Electronic Notes in Theoretical Computer Science},
  address = {Pisa (Italy)},
  month = SEP,
  COMMENT = {Invited presentation by LL},
  DVI = {data/rho-wrla.dvi.gz},
  PS = {data/rho-wrla.ps.gz},
  PDF = {data/rho-wrla.pdf},
  bibliographies = {PMC},
  WKloc = {A-1409, doc/pap/BIB},
  abstract = {The last few years have seen the development of a
	new calculus which can be considered as the outcome of the
	last decade of various researches on (higher order) term
	rewriting systems, and lambda calculi.  In the Rewriting
	Calculus (or Rho Calculus), algebraic rules are
	considered as sophisticated forms of ``lambda terms with
	patterns'', and rule applications as lambda applications with
	pattern matching facilities.

    	The calculus can be ``customized'' to work modulo
    	sophisticated algebraic theories, like commutativity,
    	associativity, associativity-commutativity, etc.  This allows
    	us to encode complex structures such as list, sets, and more
    	generally objects.  The calculus can be either be presented
    	``à la Curry'' or ``à la Church'' without sacrificing
    	readability and without complicating too much the metatheory.

    	The Rewriting Calculus could represent a \textit{lingua
    	franca} to encode many paradigms of computations together with
    	a formal basis used to build powerful theorem provers based on
    	lambda calculus and efficient rewriting, and a step towards
    	new proof engines based on the Curry-Howard isomorphism.},
  keywords = {Rewriting calculus, type system, exception handling,
		    matching}
}

@Misc{Clack-Clayman-Parrot-1995,
  author = 	 {Chris Clack and Stuart Clayman and David Parrot},
  title = 	 {Dynamic Cyclic Data Structures in Lazy Functional Languages},
  OPThowpublished = {},
  month = 	 OCT,
  year = 	 {1995},
  WKloc = {doc/pap/BIB},
  bibliographies = {Anand},
  abstract = {Many popular myths have evolved about functional
     programming lanugages and their power of expression; one such
     myth is that ``functional languages cannot construct arbitrary
     cyclic data structures at run-time'' and therefore that
     functional languages are not appropriate for problems which
     require efficient representations of dynamically-specified cyclic
     graph structures (such as a route-finding robot in a maze). This
     myth persists despite ample published work to the contrary (see
     for example cite{Allison-1989}) and is a recurring subject of debate
     [\texttt{comp.lang.functional}].

     We provide a summary of some of the related work in this area; we
     then build on this work by presenting methods for both the
     dynamic construction of complex cyclic structures and their
     direct manipulation using circular programming techniques.}
}

@Article{Claessen-1999,
  author = {Koen Claessen},
  title = {A Poor Man's Concurrency Monad},
  journal = {Journal of Functional Programming},
  year = {1999},
  volume = {9},
  OPTnumber = {3},
  OPTmonth = MAY,
  pages = {313--323},
  WKloc = {A-0671},
  OPTabstract = {},
  OPTcontents = {},
  URL = {http://www.cs.chalmers.se/~koen/pubs/}
}

@Manual{Claessen-Sheeran-2000,
  title =        {A Tutorial on {Lava}: A Hardware Description and Verification System},
  month =     APR,
  year =      2000,
  URL =      {http://www.ittc.ku.edu/Projects/SLDG/filing_cabinet/claessen00tutorial.pdf},
  WKloc =    {doc/pap/BIB}
}

@InProceedings{Claessen-Hughes-2000,
  author = {Koen Claessen and John Hughes},
  title =        {{QuickCheck}: A Lightweight Tool for Random Testing of Haskell Programs},
  crossref =  {ICFP2000},
  pages =     {269--279},
  WKloc =     {doc/pap/BIB/Claessen}
}

@InProceedings{Claessen-Hughes-2002,
  author = {Koen Claessen and John Hughes},
  title = {Testing monadic code with {QuickCheck}},
  crossref = {Haskell2002},
  pages = {65--77},
  URL = {http://doi.acm.org/10.1145/581690.581696},
  WKloc = {doc/pap/BIB},
  abstract = {QuickCheck is a previously published random testing tool for
      Haskell programs. In this paper we show how to use it for testing
      monadic code, and in particular imperative code written using the ST
      monad. QuickCheck tests a program against a specification: we show
      that QuickCheck's specification language is sufficiently powerful to
      represent common forms of specifications: algebraic, model-based
      (both functional and relational), and pre-/post-conditional.
      Moreover, all these forms of specification can be used directly for
      testing. We define a new language of monadic properties, and make a
      link between program testing and the notion of observational
      equivalence.}
}

@InProceedings{Claessen-Ljungloef-2000,
  author = {Koen Claessen, Peter Ljungl{\"o}f},
  title = {Typed Logical Variables in Haskell},
  crossref = {Haskell2000},
  URL = {http://www.cs.chalmers.se/~koen/pubs/entry-haskell00-typedlp.html},
  WKloc = {A-1200, doc/pap/BIB},
  bibliographies = {PMC},
  annote = {is about a statically typed embedding of Prolog in Haskell}
}

@InProceedings{Claessen-Sands-1999,
  author = {Koen Claessen and David Sands},
  title = {Observable Sharing for Functional Circuit Description},
  booktitle = {Proc.\null{} Asian Computer Science Conference, {ASIAN '99, Phuket, Thailand}},
  year = 1999,
  URL = {http://www.cs.chalmers.se/~koen/Papers/obs-shar.ps},
  WKloc = {A-1005, doc/pap/BIB},
  abstract = {Pure functional programming languages have been proposed as
      a vehicle to describe, simulate and manipulate circuit
      specifications. We propose an extension to Haskell to solve a
      standard problem when manipulating data types representing circuits
      in a lazy functional language. The problem is that circuits are
      finite graphs --- but viewing them as an algebraic (lazy) datatype
      makes them indistinguishable from potentially infinite regular trees.
      However, implementations of Haskell do indeed represent cyclic
      structures as graphs. The problem is that the sharing of nodes that
      creates such cycles is not observable by any function which traverses
      such a stracutre. In this paper we propose an extension to
      call-by-need languages which makes graph sharing observable. The
      extension is based on non updatable reference cells and an equality
      test (sharing detection) on this type. We show that this simple and
      practical extension has well-behaved semantic properties, which means
      that many typical source-to-source program transformation, such as
      might be performed by a compiler, are still valid in the presence of
      this extension.},
  bibliographies = {DSL, TGV}
}

@Book{Clark-1979,
  year = 1979,
  title = {The Japanese Company},
  publisher = {Charles E. Tuttle Company (Yale University)},
  author = {Rodney Clark}
}

@PhDThesis{Clark-1992,
  keywords = {natural language parser},
  year = 1992,
  title = {Lolita: An Expert System that Parses Spanish},
  school = {West Virginia University},
  author = {Clara I. Clark},
  annote = {QER92-25302}
}

@Article{Clark-2000,
  author = {Chris Clark},
  title = {Uniform Abstract Syntax Trees},
  journal = SIGPLAN,
  year = 2000,
  volume = 35,
  number = 2,
  pages = {11--16},
  month = FEB,
  bibliographies = {EdComb}
}

@Misc{Clark-Hankin-199X,
  author = {David Clark and Chris Hankin},
  title = {A lattice of Abstract Graphs},
  year = {199X},
  WKloc = {A-0475},
  keywords = {abstract reduction, term graph, Clean},
  annote = {-> relational graphs?}
}

@InProceedings{Clarke-1994,
  title = {Automatic Verification of Finite-State Concurrent Systems},
  author = {Edmund M. Clarke, Jr.},
  pages = 126,
  crossref = {LICS9},
  bibliographies = {SpecTech}
}

@Misc{ Clavel_reflection,
  author = "Manuel Clavel",
  title = "Reflection in General Logics, Rewriting Logic, and Maude",
  CiteSeer = "citeseer.ist.psu.edu/clavel98reflection.html",
  WKloc = {doc/pap/BIB},
  abstract = {This paper is a summary of my PhD dissertation in
     which: general axiomatic notions of reflective logic and of
     strategy, based on the theory of general logics, are proposed; a
     detailed proof that rewriting logic satisfies the axiomatic
     definition of reflective logic is given; and the great practical
     importance of these concepts when a reflective logic becomes
     implemented as a reflective declarative language---as it is the
     case for a system such as Maude---is illustrated in a wide range
     of...}
}

@Misc{ Clavel-Meseguer-1996,
  author = "M. Clavel and J. Meseguer",
  title = "Axiomatizing Reflective Logics and Languages",
  text = "M. G. Clavel and J. Meseguer. Axiomatizing Reflective Logics and Languages.
    Proceedings of Reflection'96, pages 263--288. Xerox PARC, 1996.",
  year = "1996",
  CiteSeer = "citeseer.ist.psu.edu/clavel96axiomatizing.html",
  WKloc = {A-1566, doc/pap/BIB},
  annote = {axiomatisation of ``entailment system'' (semantic consequence)
            and (complete) proof calculus}
}

@article{Clavel-Duran-Eker-Lincoln-MartiOliet-Meseguer-Quesada-2002,
  title = "Maude: Specification and Programming in Rewriting Logic",
  journal = TCS,
  volume = "285",
  number = "2",
  pages = "187--243",
  year = "2002",
  OPTnote = "<ce:title>Rewriting Logic and its Applications</ce:title> ",
  issn = "0304-3975",
  DOIURL = "http://dx.doi.org/10.1016/S0304-3975(01)00359-0",
  DOI = "10.1016/S0304-3975(01)00359-0",
  url = "http://www.sciencedirect.com/science/article/pii/S0304397501003590",
  author = "M. Clavel and F. Dur{\'a}n and S. Eker and P. Lincoln and N. Mart{\'{\i}}-Oliet and J. Meseguer and J.F. Quesada",
  keywords = "Maude",
  keywords = "Rewriting logic",
  keywords = "Functional modules",
  keywords = "System modules",
  keywords = "Parameterization",
  keywords = "Reflection",
  keywords = "Internal strategies ",
  abstract = {Maude is a high-level language and a high-performance system
    supporting executable specification and declarative programming
    in rewriting logic. Since rewriting logic contains equational logic,
    Maude also supports equational specification and programming
    in its sublanguage of functional modules and theories.
    The underlying equational logic chosen for Maude
    is membership equational logic,
    that has sorts, subsorts, operator overloading,
    and partiality definable by membership and equality conditions.
    Rewriting logic is reflective,
    in the sense of being able to express its own metalevel at the object level.
    Reflection is systematically exploited in Maude
    endowing the language with powerful metaprogramming capabilities,
    including both user-definable module operations
    and declarative strategies to guide the deduction process.
    This paper explains and illustrates with examples
    the main concepts of Maude's language design,
    including its underlying logic,
    functional, system and object-oriented modules,
    as well as parameterized modules, theories, and views.
    We also explain how Maude supports reflection,
    metaprogramming and internal strategies.
    The paper outlines the principles underlying the Maude system implementation,
    including its semicompilation techniques.
    We conclude with some remarks about applications,
    work on a formal environment for Maude,
    and a mobile language extension of Maude.}
}

@PhDThesis{Clayman-1993,
  author = {Stuart Clayman},
  title = {Developing and Measuring Parallel Rule-Based Systems in a Functional Programming Environment},
  school = {University of London},
  year = 1993,
  WKloc = {B-0076}
}

@Manual{Clean1997,
  title = {{Concurrent Clean}, a general purpose, higher order, pure and
      lazy functional programming language based on graph rewriting,
      designed for the development of sequential, parallel and distributed
      real world applications},
  author = {Rinus Plasmeijer and van Eekelen, Marko},
  organization = {{\sc Hiltr} --- High Level Software Tools B.V. and University of Nijmegen},
  year = 1997,
  note = {Version 1.2 (draft)},
  WKloc = {B-0107 (includes several papers)}
}

@InProceedings{Clement-1987,
  WKloc = {A-0038},
  abstract = {We describe how to express the dynamic semantics of a small
	subset of the Standard ML language in Natural Semantics. The present
	specification is based on a communication of R.~Milner that describes
	the dynamic semantics od Standard ML in a structural style, and can be
	viewed as an example of the ``programming effort'' that is necessary
	to obtain an executable version of such a specification. The main
	aspects of Natural Semantics covered concern its relationships with
	typed inference systems and with some properties of natural deduction.
	The description has been tested on a computer but we do not give here
	details on the compilation techniques.},
  title = {The Natural Dynamic Semantics of Mini-Standard ML},
  pages = {67--81},
  crossref = {TAPSOFT1987},
  author = {Dominique Cl\'ement}
}

@Manual{Clemm-199X,
  title = {The {Odin System}},
  author = {Geoffrey M. Clemm},
  abstract = {The Odin System is a simpler, more powerful, and more reliable replacement for Make. [$\ldots$]},
  year = {199?},
  note = {Odin version 1.17},
  WKloc = {A-1142}
}

@InProceedings{Clergerie-1993,
  author = {Villemonte de la Clergerie, E.},
  title = {Layer Sharing: an improved Structure-Sharing Framework},
  pages = {345--358},
  abstract = {We present in this paper a structure-sharing framework
             originally developed for a Dynamic Programming interpreter of
             Logic Programs called DyALog. This mechanism should be of
             interest for alternative execution models of PROLOG which
             maintain multiple computation branches and reuse
             sub-computations in various contexts (computation sharing).
             This category includes, besides our Dynamic Programming model,
             the tabular models (OLDT, SLDAL, XWAM), the ``magicset''
             models, and the independent AND and OR parallelism with
             solution sharing models. These models raise the problem of
             storing vast amount of data, motivating us to discard copying
             mechanisms in favor of structure-sharing mechanisms.
             Unfortunately, computation sharing requires joining computation
             branches and possible renaming some variables, which generally
             leads to complex structure-sharing mechanisms. The proposed
             ``layer-sharing'' framework succeeds however in understandable
             and to implement.},
  crossref = {POPL1993},
  WKloc = {A-0199}
}

@InProceedings{Clerici-Jimenez-Orejas-1992,
  author = {S. Cl{\'e}rici and R. Jim{\'e}nez and Fernando Orejas},
  title = {Semantic Constructions in the Specification Language
		  {Glider}},
  crossref = {SADT92},
  pages = {144--157},
  keywords = {ICARUS, inheritance, Clear}
}

@InProceedings{Cleveland-Yankelevich-1994,
  author = {Ranc Cleveland and Daniel Yankelevich},
  title = {An Operational Framework for Value-Passing Processes},
  crossref = {POPL1994},
  pages = {326--338},
  keywords = {CCS-VP}
}

@Misc{CoFI-Rationale-1997,
  author = {CoFI},
  title = {The Common Framework Initiative for Algebraic Specification and Development, Rationale},
  year = 1997,
  month = MAY,
  URL = {http://www.brics.dk/Projects/CoFI},
  WKloc = {A-0818}
}

@Article{Coccia-Gadducci-Corradini-2002,
  author = 	 {Matteo Coccia and Fabio Gadducci and Andrea Corradini},
  title = 	 {{GS-$\Lambda$} Theories: A Syntax for Higher-Order Graphs},
  journal = 	 ENTCS,
  year = 	 2002,
  volume =	 69,
  pages =	 18,
  URL = 	 {http://www.elsevier.nl/locate/entcs/volume69.html},
  WKloc = 	 {doc/pap/BIB}
}

@TechReport{Cockett-Fukushima-1992,
  author = {J. Robin B. Cockett and Tom Fukushima},
  title = {About Charity},
  institution = {University of Calgary},
  year = 1992,
  number = {92/480/18},
  month = JUN,
  URL = {mail:parin\@cpsc.ucalgary.ca},
  WKloc = {C-0002, B-0074},
  abstract = {Charity is a categorical programming language based
		  on distributive categories (in the sense of Schanuel
		  and Lawvere) with strong datatypes (in the sense of
		  Hagino). Distributive categories come with a term
		  logic which can express most standard programs; and
		  they are fundamental to computer science because
		  they permit proof by case analysis and, when strong
		  datatypes are introduced, proof by structural
		  induction.

                  Charity is functional and polymorphic in style, and
		  is strongly normalizing. As a categorical
		  programming language it provides a unique marriage
		  of computer science and mathematical thought. The
		  above aspects are particularly important for the
		  production of verified programs as the naturality of
		  morphisms gives us ``theorems for free'', termination
		  proofs are not required, and mathemathical
		  specifications can be used.}
}

@InProceedings{Cockett-Seely-1991,
  author = {J.R.B. Cockett and R.A.G. Seely},
  title = {Weakly Distributive Categories},
  WKloc = {A-0019, doc/pap/BIB},
  year = 1992,
  series = {London Mathematical Society Lecture Notes Series},
  pages = {45--65},
  number = 177,
  editor = {M.P.Fourman and P.T. Johnstone and A.M. Pitts},
  booktitle = {Applications of Categories to Computer Science},
  annote = {ftp from triples},
  bibliographies = {RelMiCS}
}

@Article{Cockett-Spooner-1995,
  author =       {J.R.B. Cockett and David A. Spooner},
  title =        {Categories for Synchrony and Asynchrony},
  journal =      ENTCS,
  year =         1995,
  booktitle =  {{MFPS XI}, Mathematical Foundations of Programming Semantics, Eleventh Annual Conference },
  volume =    1,
  pages =     {66--90},
  DOI =          {10.1016/S1571-0661(04)80005-7},
  DOIURL =       {http://dx.doi.org/10.1016/S1571-0661(04)80005-7},
  WKloc = {doc/pap/BIB, A-1743},
  abstract =    {The purpose of this paper is to show how one may construct from a synchronous interaction category, such as SProc, a corresponding asynchronous version. Significantly, it is not a simple Kleisli construction, but rather arises due to particular properties of a monad combined with the existence of a certain type of distributive law.

Following earlier work we consider those synchronous interaction categories which arise from model categories through a quotiented span construction: SProc arises in this way from labelled transition systems. The quotienting is determined by a cover system which expresses bisimulation. Asynchrony is introduced into a model category by a monad which, in the case of transition systems, adds the ability to idle. To form a process category atop this two further ingredients are required: pullbacks in the Kleisli category, and a cover system to express (weak) bisimulation.

The technical results of the paper provide necessary and sufficient conditions for a Kleisli category to have finite limits. Furthermore, they show how distributive laws can be used to induce cover systems on such Kleisli categories. These provide the ingredients for the construction of asynchronous settings.}
}

@Article{Cockett-Spooner-1997,
  author =       {J.R.B. Cockett and David A. Spooner},
  title =        {Constructing Process Categories},
  journal =      TCS,
  year =         1997,
  volume =    177,
  pages =     {73--109},
  WKloc =      {doc/pap/BIB},
  abstract =    {A method of constructing process categories as generalized relations on a category of process models is presented. The construction may be viewed as a 2-functor,
   allowing structural properties of the process categories to be derived from the underlying structure of the model categories. In particular, this allows one to
   infer the presence of linear structure in a process category.

   The construction yields Abramsky's category SProc when applied to any of the standard models of interleaved concurrency. SProc is also obtained as a process
   category upon the category of ``sets in time'' (i.e. trees) and this sheds new light on the analogy between SProc and ``relations in time''.}
}

@Article{Codd-1970,
  author = {E. F. Codd},
  title = {A Relational Model of Data for Large Shared Data Banks},
  journal = CACM,
  volume = 13,
  number = 6,
  year = 1970,
  pages = {377--387},
  bibliographies = {RelMiCS}
}

@InProceedings{Codognet-Codognet-Loia-Quagetto-1994,
  author = {C. Codognet and P. Codognet and V. Loia and M. Quagetto},
  title = {Sleepers: a Versatile High-Level Control Mechanism},
  crossref = {PLILP1994},
  pages = {308--323},
  abstract = {$\ldots$ Sleepers are computational entities
		  enabling to use an enriched communication strategy
		  among the living predicates of a Prolog computation,
		  i.e.\null{} those present on the runtime stack. $\ldots$}
}

@Book{Cohen-1980,
  editor = {I. Bernard Cohen},
  title = {{Benjamin Peirce}},
  publisher = Arno,
  year = 1980,
  address = {New York},
  bibliographies = {RelMiCS}
}

@TechReport{CohenE-1993,
  author =       {Ernie Cohen},
  title =        {Hypotheses in Kleene Algebra},
  institution =  {Bellcore},
  year =         1993,
  type =      {Technical Report},
  number =    {TM-ARH-023814, 1993},
  CiteSeer =   {http://citeseer.nj.nec.com/1688.html},
  CiteSeer =   {http://citeseer.ist.psu.edu/cohen94hypotheses.html},
  CiteSeer =   {http://citeseer.ist.psu.edu/context/218601/1688}
}

@Unpublished{CohenE-1995,
  author =       {Ernie Cohen},
  title =        {Hypotheses in Kleene Algebra},
  note =         {Cited by \cite{Desharnais-Moeller-Struth-2004_KAD.pdf} as ``Unpublished manuscript''; probably coincides with \cite{CohenE-1993}.},
  year =      1995,
  URL = {http://en.scientificcommons.org/14229},
  abstract =    {Kleene algebra
    (an Horn axiomatization of Kleene's algebra of regular events)
    has proved to be an effective tool for reasoning about programs.
    Within the algebra, we can reason succinctly
    about both ordinary safety properties
    and important program transformations such as loop unwinding,
    change of data representation, and refinement of atomicity.
    One of the nice properties of Kleene algebra is
    that its equational theory has a simple decision procedure.
    However, to use it for programming applications,
    we need to reason in the presence of
    hypotheses stating the equality of certain programs;
    such hypotheses can, in general, render the theory undecidable.
    In this note, we show that for certain important cases,
    checking an equation under hypotheses
    can be reduced to checking a related equation without hypotheses.}
}

@InProceedings{Cohen-Tamassia-1993,
  author = {R.F. Cohen and R. Tamassia},
  title = {Combine and Conquer: a General Technique for Dynamic
		  Algorithms},
  crossref = {ESA93},
  pages = {97--108},
  WKloc = {A-0356},
  keywords = {linear attribute grammar},
  abstract = {$\ldots$}
}

@InProceedings{Cohn-1996,
  author = {A. G. Cohn},
  title = {Calculi for qualitative reasoning},
  booktitle = {Artificial Intelligence and Symbolic Mathematical Computation},
  OPTeditor = {???},
  series = LNCS,
  publisher = Springer,
  pages = {124--143},
  volume = 1138,
  year = 1996
}

@Article{Cohn-Comer-1988,
  author = {Leslie Cohn and Stephen D. Comer},
  title = {An Abstract Theory of Invertible Relations},
  journal = ALGU,
  year = 1988,
  volume = 25,
  pages = {131--146},
  bibliographies = {RelMiCS}
}

@Misc{Colby-etal-199X,
  author = {Christopher Colby and others},
  title = {Design and Implementation of {Triveni}: a Process-algenraic {API} for Threads + Events},
  year = {199?},
  WKloc = {A-0742}
}

@Article{Cole-Vishkin-1986,
  title = {Deterministic Coin Tossing with Applications to
  	 Optimal Parallel List Ranking},
  author = {Richard Cole and Uzi Vishkin},
  pages = {32--53},
  journal = IandC,
  month = JUL,
  year = 1986,
  volume = 70,
  number = 1
}

@Article{Cole-Vishkin-1989,
  author = {Richard Cole and Uzi Vishkin},
  title = {Faster Optimal Parallel Prefix Sums and List Ranking},
  pages = {334--352},
  journal = IandC,
  month = JUN,
  year = 1989,
  volume = 81,
  number = 3,
  references = {Cole-Vishkin-1986}
}

@Article{ColeV91,
  title = {Approximate Parallel Scheduling. II. Applications to
      Logarithmic-Time Optimal Parallel Graph Algorithms},
  author = {Richard Cole and Uzi Vishkin},
  pages = {1--47},
  journal = IandC,
  month = MAY,
  year = 1991,
  volume = 92,
  number = 1,
  references = {Cole-Vishkin-1986}
}

@InProceedings{Collins-1975,
  author = 	 {G. E. Collins},
  title = 	 {Quantifier Elimination for Real Closed Fields by Cylindric Algebraic Decomposition},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  pages = 	 {515--532},
  year = 	 1975,
  OPTeditor = 	 {},
  volume = 	 {33},
  series = 	 LNCS,
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  bibliographies = {RelMiCS}
}

@InProceedings{Colson-Ehrhard-1994,
  title = {On Strong Stability and Higher-Order Sequentiality},
  author = {Lo{\"\i}c Colson and Thomas Ehrhard},
  pages = {103--108},
  crossref = {LICS9},
  abstract = {We propose a definition of reducibility of sequentiality
		  for the interpretations of higher-order programs and
		  prove the equivalence between this notion and strong
		  stability.}
}

@Article{Comer-1969,
  author = {Stephen D. Comer},
  title = {Finte Inseparability of some Theories of
		Cylindrification Algebras},
  journal = JSYLO,
  year = 1969,
  volume = 34,
  number = 2,
  pages = {171--176},
  bibliographies = {RelMiCS}
}

@Article{Comer-1976,
  author = {Stephen D. Comer},
  title = {Integral Relation Algebras via Pseudogroups},
  journal = NOTIC,
  volume = 23,
  year = 1976,
  pages = {p. A-659},
  bibliographies = {RelMiCS}
}

@Article{Comer-1976a,
  author = {Stephen D. Comer},
  title = {Multivalued Loops, Geometries, and Algebraic Logic},
  journal = HOUST,
  volume = 2,
  year = 1976,
  pages = {373--380},
  bibliographies = {RelMiCS}
}

@Booklet{Comer-1979,
  author = {Stephen D. Comer},
  title = {Multivalued Loops and their Connection with Algebraic
		Logic},
  year = 1979,
  pages = 173,
  bibliographies = {RelMiCS}
}

@Article{Comer-1983,
  author = {Stephen D. Comer},
  title = {Color Schemes Forbidding Monochrome Triangles},
  journal = CONNUM,
  volume = 39,
  year = 1983,
  pages = {231--236},
  bibliographies = {RelMiCS}
}

@Article{Comer-1983a,
  author = {Stephen D. Comer},
  title = {Constructions of Color Schemes},
  journal = ACTCAROL,
  volume = 24,
  year = 1983,
  pages = {39--48},
  bibliographies = {RelMiCS}
}

@Article{Comer-1983b,
  author = {Stephen D. Comer},
  title = {A New Foundation for the Theory of Relations},
  journal = NOTRE,
  volume = 24,
  year = 1983,
  pages = {181--187},
  bibliographies = {RelMiCS}
}

@Article{Comer-1983c,
  author = {Stephen D. Comer},
  title = {A Remark on Chromatic Polygroups},
  journal = CONNUM,
  volume = 38,
  year = 1983,
  pages = {85--95},
  bibliographies = {RelMiCS}
}

@InProceedings{Comer-1983d,
  author = {Stephen D. Comer},
  title = {Extension of Polygroups by Polygroups and their
		Representations Using Color Schemes},
  booktitle = {Universal Algebra and Lattice Theory, Proc.\null{} of the
		$4^{th}$ Internat.\null{} Conf., 1982},
  address = {Puebla, Mexico},
  volume = 1004,
  series = LNM,
  year = 1983,
  pages = {91--103},
  bibliographies = {RelMiCS}
}

@Article{Comer-1984,
  author = {Stephen D. Comer},
  title = {Combinatorial Aspects of Relations},
  journal = ALGU,
  volume = 18,
  year = 1984,
  pages = {77--94},
  bibliographies = {RelMiCS}
}

@Article{Comer-1984a,
  author = {Stephen D. Comer},
  title = {Polygroups Derived From Cogroups},
  journal = JALG,
  volume = 89,
  year = 1984,
  pages = {397--405},
  bibliographies = {RelMiCS}
}

@InProceedings{Comer-1985,
  author = {Stephen D. Comer},
  title = {Combinatorial Types},
  booktitle = {Algebra, Combinatorics, and Logic in Computer Science},
  series = Bolyai,
  publisher = NoHo,
  year = 1985,
  bibliographies = {RelMiCS}
}

@Booklet{Comer-1987,
  author = {Stephen D. Comer},
  title = {Weakly Regular Trees and Their Color Algebras},
  note = {Preprint, August 1987, pp. 13},
  bibliographies = {RelMiCS}
}

@InProceedings{Comer-1991,
  author = {Stephen D. Comer},
  title = {The Representation of 3-Dimensional Cylindric Algebras},
  crossref = {AL1991},
  bibliographies = {RelMiCS}
}

@Article{Comer-1991a,
  author = {Stephen D. Comer},
  title = {A Remark on Representable Positive Cylindric Algebras},
  journal = ALGU,
  volume = 28,
  year = 1991,
  pages = {150--151},
  bibliographies = {RelMiCS}
}

@Book{ComerD-1984,
  author = {Douglas Comer},
  title = {Operating System Design: The {XINU} Approach},
  publisher = Prentice,
  year = 1984,
  McMaster = {QA 76.6 .C6275 1984 v 1},
  bibliographies = {SE3B}
}

@Book{ComerD-1987,
  author = {Douglas Comer},
  title = {Operating System Design, {Volume} 2: Internetworking with {XINU}},
  publisher = Prentice,
  year = 1987,
  McMaster = {QA 76.6 .C6275 1984 v 2},
  bibliographies = {SE3B}
}

@Misc{Comon-Dauchet-Gilleron-Jacquemard-Lugiez-Tison-Tommasi-1999,
  author = {H. Comon and M. Dauchet and R. Gilleron and F.
                 Jacquemard and D. Lugiez and S. Tison and M. Tommasi},
  title = {{Tree Automata Techniques and Applications}},
  year = 1999,
  howpublished = {This electronic book is available at
                 \url{http://www.grappa.univ-lille3.fr/tata}},
  URL = {http://www.grappa.univ-lille3.fr/tata},
  keywords = {tree-automata},
  WKloc = {B-0086}
}

@Misc{Comon-Dauchet-Gilleron-Jacquemard-Lugiez-Tison-Tommasi-2002,
  author = {H. Comon and M. Dauchet and R. Gilleron and F. Jacquemard
            and D. Lugiez and S. Tison and M. Tommasi},
  title = {Tree Automata Techniques and Applications},
  howpublished = {Available on: \url{http://www.grappa.univ-lille3.fr/tata}},
  note = {release October 1st, 2002},
  year = 1997,
  WKloc = {A-1537, doc/pap/BIB},
  keywords = {TATA},
  bibliographies = {RelMiCS}
}

@Article{Comon-Haberstrau-Jouannaud-1994,
    author = "Hubert Comon and Marianne Haberstrau and Jean-Pierre Jouannaud",
    title = "Syntacticness, Cycle-Syntacticness and Shallow Theories",
    journal = "Information and Computation",
    volume = "111",
    number = "1",
    pages = "154--191",
    year = "1994",
    WKloc = {doc/pap/BIB},
    url = "citeseer.ist.psu.edu/comon94syntacticness.html"
}

@InProceedings{Comon-Treinen-1994,
  author = {H. Comon and R. Treinen},
  title = {Ordering Constraints on Trees},
  crossref = {CAAP94},
  pages = {1--14},
  note = {invited paper},
  abstract = {We survey recent results about ordering constraints
		  on trees and discuss their applications. Our main
		  interest lies in the family of {\em recursive path
		  orderings} which enjoy the properties of being
		  total, well-founded and compatible with the tree
		  constructors. The paper includes some new results,
		  in particular the undecidability of the theory of
		  lexicographic path orderings in case of a non-unary
		  signature.},
  annote = {{\def\sat{|\kern-.3mu=}
                  A constrained formula is a pair $(\phi,c)$
		  (actually written $\phi|c$) where $\phi$ is a formula
		  in some first-order logic built upon a set ${\cal
		  Q}$ of predicate symbols and a set ${\cal F}'$ of
		  function symbols, and $c$ is a formula (called {\em
		  constraint}) in some constraint system over ${\cal
		  P} \subseteq {\cal Q}$, ${\cal F} \subseteq {\cal
		  F}'$. As sketched above, any constraint system comes
		  with a satisfaction relation $\sat$ such that, for
		  any assignment $\sigma$ of the free variables of
		  $c$, $\sigma \sat c$ iff $c\sigma$ holds in the
		  given interpretation. Then, $\phi | c$ can simply be
		  interpreted as the (possibly infinite ) set of
		  formulae $$[\![ \phi | c ]\!] = \{ \phi\sigma |
		  \sigma \sat c \} $$}}
}

@Book{Computeralgebra-1993,
  year = 1993,
  title = {{Computeralgebra in Deutschland: Bestandsaufnahme,
		  M\"oglichkeiten, Perspektiven}},
  publisher = GI,
  editor = {Fachgruppe Compteralgebra der GI, DMV und GAMM},
  annote = {tubibmue},
  address = {Passau, Heidelberg},
  bibliographies = {RelMiCS}
}

@Article{Conradi-Westfechtel-1998,
  author = {Reidar Conradi and Bernhard Westfechtel},
  title = {Version models for software configuration management},
  journal = ACMCS,
  year = 1998,
  volume = 30,
  number = 2,
  pages = {232--282},
  URL = {http://doi.acm.org/10.1145/280277.280280},
  WKloc = {doc/pap/BIB},
  abstract = {After more than 20 years of research and practice in
      software configuration management (SCM), constructing consistent
      configurations of versioned software products still remains a
      challenge. This article focuses on the version models underlying both
      commercial systems and research prototypes. It provides an overview
      and classification of different versioning paradigms and defines and
      relates fundamental concepts such as revisions, variants,
      configurations, and changes. In particular, we focus on intensional
      versioning, that is, construction of versions based on configuration
      rules. Finally, we provide an overview of systems that have had
      significant impact on the development of the SCM discipline and
      classify them according to a detailed taxonomy.}
}

@InProceedings{Consel-1993,
  author = {Charles Consel},
  title = {A Tour of {Schism}: {A} Partial Evaluation System For
                 Higher-Order Applicative Languages},
  pages = {145--154},
  crossref = {PEPM1993}
}

@InProceedings{Consel-Danvy-1993,
  author = {Charles Consel and Olivier Danvy},
  title = {Tutorial Notes on Partial Evalution},
  pages = {493--501},
  abstract = {The last years have withnessed a flurry of new results in the
             area of partial evalution. These tutorial notes survey the
             field and present a critical assessment of the state of the
             art.},
  crossref = {POPL1993},
  WKloc = {A-0203}
}

@InProceedings{Constable-1991,
  title = {Type Theory as a Foundation for Computer Science},
  author = {Robert L. Constable},
  pages = {226--243},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {We discuss some of the boons as well as shortcomings of
		  constructive type theory as a foundation for
		  computer science. Certain new concepts are offered
		  for tailoring these theories to this task including
		  an idea for collecting objects into subtypes and a
		  proposal for using logic variables and treating them
		  as part of the definition of the logic.}
}

@Misc{ Constable-1994,
  author = "R. Constable",
  title = "Using reflection to explain and enhance type theory",
  text = "Robert L. Constable. Using reflection to explain and enhance type theory.
    In Helmut Schwichtenberg, editor, Proof and Computation, volume 139 of NATO Advanced Study Institute, International Summer School held in Marktoberdorf, Germany, July 20-August 1, NATO Series F, pages 65-- 100. Springer, Berlin, 1994",
  year = "1994",
  url = "citeseer.ist.psu.edu/constable94using.html",
  WKloc = {A-1569, doc/pap/BIB},
  abstract = {The five lectures at Marktoberdorf on which these
     notes are based were about the architecture of problem solving
     environments which use theorem provers. Experience with these
     systems over the past two decades has shown that the prover must
     be extensible, yet it must be kept safe. We examine a way to
     safely add new decision procedures to the Nuprl prover. It relies
     on a reflection mechanism and is applicable to any
     tactic-oriented prover with sufficient reflection. The lectures
     explain...}
}

@Book{Constable-etal-1986,
  author = 	 {Robert L. Constable and S. F. Allen and H. M. Bromley and W. R. Cleaveland and J. F. Cremer and R. W. Harper and D. J. Howe and T. B. Knoblock and N. P. Mendler and P. Panagaden and J. T. Sasaki and S. F. Smith},
  title = 	 {Implementing Mathematics with the Nuprl Proof Development System},
  publisher = 	 {Prentice Hall},
  year = 	 {1986},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  bibliographies = {HHOL}
}

@InProceedings{Contejean-2004,
  author =       {Evelyne Contejean},
  title =        {A Certified {AC} Matching Algorithm},
  crossref =  {RTA2004},
  pages =     {70--84},
  WKloc = {A-1732, doc/pap/BIB},
  DOI =      {10.1007/978-3-540-25979-4_5},
  SpringerURL = {http://www.springerlink.com/content/yrqwlwtdnybk38wb/},
  abstract = {In this paper, we propose a matching algorithm
      for terms containing some function symbols
      which can be either free, commutative or associative-commutative.
      This algorithm is presented by inference rules
      and these rules have been formally proven sound and complete, and decreasing
      in the COQ proof assistant
      while the corresponding algorithm is implemented in the CiME system.
      Moreover some preparatory work has been done in COQ,
      such as proving that checking the equality of two terms
      modulo some commutative and associative-commutative theories is decidable.}
}

@InProceedings{Contejean-2007,
  author = {Evelyne Contejean},
  publisher = Springer,
  crossref = {JouannaudFestschrift2007},
  pages = {259--269},
  abstract = {In this paper we present the part of the Coccinelle library which
    deals with list permutations. Indeed permutations naturally arise
    when formally modeling rewriting in COQ, for instance RPO with
    multiset status and equality modulo AC. Moreover the needed
    permutations are up to an equivalence relation, and may be used to
    inductively define the same relation (equivalence modulo RPO). This
    is why we introduce the notion of permutation w. r. t. an arbitrary
    relation. The advantages of our approach are a very simple inductive
    definition (with only 2 constructors), the adequacy with the
    mathematical definition, the ability to define a relation using
    recursively permutations up to this very relation, and a fine
    grained modularity (if $\mathtt{R}$ enjoys a property, so does
    $\mathtt{permut~R}$).}
}

@Book{Conway-1971,
  author = {J[ohn] H[orton] Conway},
  title = {Regular Algebra and Finite Machines},
  publisher = {Chapman and Hall},
  year = 1971,
  address = {London},
  WKloc = {A-1495},
  McMaster = {QA 267.5 .S4 C65},
  bibliographies = {RelMiCS}
}

@TechReport{Conway-Pausch-Passarella-1991,
  author = {Matt Conway and Randy Pausch and Kimberly Passarella},
  title = {A Tutorial for {SUIT}, the Simple User Interface
                 Toolkit, Version 2.0},
  institution = {Department of Computer Science, University of
                 Virginia},
  number = {CS-91-24},
  month = oct,
  year = 1991,
  note = {Mon, 28 Aug 1995 21:06:39 GMT},
  WKloc = {B-0113}
}

@InProceedings{Cook-Harcourt-1994,
  author = {T.A. Cook and E. Harcourt},
  title = {A Functional Specification Language for Instruction
		  Set Architectures},
  crossref = {ICCL94},
  pages = {11--19}
}

@InProceedings{Cook-Launchbury-1997,
  author = 	 {Byron Cook and John Launchbury},
  title = 	 {Disposable Memo Functions},
  crossref =	 {Haskell1997},
  WKloc = 	 {A-1586},
  abstract = {We formalize the meaning of lazy memo-functions in
     Haskell with an extension to th lazy $\lambda$-calculus,
     Haskell's computational model. The semantics enable reasoning
     about memoization's effect on space and time complexity. based on
     the semantics, we present a prototype implementation that
     requires no changes to the garbage-collector; memo-tables are
     simply reclaimed when no references to them remain.}
}

@InProceedings{CookJr-1992,
  author = {Grant O. Cook, Jr.},
  title = {Code generation in ALPAL using symbolic techniques},
  booktitle = {Papers from the international symposium on Symbolic and algebraic computation},
  year = 1992,
  ISBN = {0-89791-489-9},
  pages = {27--35},
  location = {Berkeley, California, United States},
  doi = {http://doi.acm.org/10.1145/143242.143260},
  publisher = {ACM Press},
  bibliographies = {Anand}
}

@InProceedings{Cooper-Hall-Kennedy-1992,
  author = {Keith D. Cooper and Mary W. Hall and Ken Kennedy},
  title = {Procedure Cloning},
  pages = {96--105},
  crossref = {ICCL92},
  authorsAddress = {Rice University, Houston},
  abstract = {Procedure Cloning is an interprocedural optimization
		  where the compiler creates specialized copies of
		  procedure bodies. To clone a procedure, the compiler
		  replicates it and then divides the incoming calls
		  between the original procedure and the copy. By
		  carefully partitioning the call sites, the compiler
		  can ensure that each clon inherits an environment
		  that allows for better code optimization. Subsequent
		  optimization tailors the various procedure bodies.

		  This paper examines the problem of procedure
		  cloning. It describes an experiment where cloning
		  was required to enable other transformations. It
		  presents a three-phase algorithm for deciding how to
		  clone a program, and analyzes the algorithm's
		  complexity. Finally, it presents a set of
		  assumptions that bound both the running time of the
		  algorithm and the expansion in code size.},
  annote = {--- HOPSnotes ---
		  special application of partial evaluation}
}

@Article{Cooperman-MaXiaoqin-2002,
  author = {Gene Cooperman and Xiaoqin Ma},
  title = {Overcoming the memory wall in symbolic algebra: a faster permutation multiplication},
  journal = {ACM SIGSAM Bulletin},
  volume = 36,
  number = 4,
  year = 2002,
  ISSN = {0163-5824},
  pages = {1--4},
  WKloc = {SIGSAM},
  bibliographies = {Anand},
  doi = {http://doi.acm.org/10.1145/641239.641241},
  publisher = {ACM Press}
}

@Article{Copeland-1955,
  author = {Copeland, A. H. Sr.},
  title = {A Note on Cylindric and Polyadic Algebras},
  journal = MICH,
  volume = 3,
  year = 1955,
  pages = {155--157},
  bibliographies = {RelMiCS}
}

@Article{Copilowish-1948,
  author = {I. M. Copilowish},
  title = {Matrix Development of the Calculus of Relations},
  journal = JSYLO,
  volume = 13,
  year = 1948,
  pages = {193--203},
  bibliographies = {RelMiCS}
}

@Manual{Coq-2012,
  author      = {{Coq} {Development} {Team}, The},
  title       = {The {Coq} Reference Manual, version 8.4},
  month       = Aug,
  year        = {2012},
  note        = {Available electronically at \url{http://coq.inria.fr/doc}}
}

@InProceedings{CoquandC-1993,
  author = {Catarina Coquand},
  title = {From Semantics to Rules: a Machine Assisted Analysis},
  crossref = {CSL93},
  pages = {91--105},
  WKloc = {A-0346},
  keywords = {ALF, $\lambda\sigma$, explicit substitutions}
}

@Misc{CoquandC-2000,
  author =	 {Catarina Coquand},
  title =	 {Agda},
  year =	 2000,
  note =	 {\textsf{http://www.cs.chalmers.se/\~{}catarina/agda/}},
  URL = 	 {http://www.cs.chalmers.se/~catarina/agda/},
  bibliographies = {HHOL}
}

@InProceedings{Coquand-1986,
  author =       {Thierry Coquand},
  title =        {An Analysis of Girard's Paradox},
  crossref =  {LICS1986},
  CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.37.3153},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  WKloc =   {doc/pap/BIB (CiteSeer Draft)},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@InProceedings{Coquand-1992,
  author = 	 {Thierry Coquand},
  title = 	 {Pattern Matching with Dependent Types},
  booktitle =	 {Electronic Proceedings of the
    Third Annual {BRA} Workshop on Logical Frameworks
    ({B{\oring{a}}stad, Sweden})},
  year =	 1992,
  editor =	 {Bengt Nordstr{\"o}m and Kent Petersson and Gordon Plotkin},
  bibliographies = {PMC}
}

@Misc{Coquand-1996,
  author = {Thierry Coquand},
  title = {An Algorithm for Type-Checking Dependent Types},
  year = 1996,
  WKloc = {A-0591}
}

@Misc{Coquand-1999,
  author = {Thierry Coquand},
  title = {Structured Type Theory},
  year = 1999,
  month = JUN,
  URL = {http://www.cs.chalmers.se/~coquand/STT.ps.Z},
  WKloc = {A-1543, doc/pap/BIB}
}

@Misc{CoquandC-Coquand-Norell-Takeyama-2005,
  author =	 {Catarina Coquand and Thierry Coquand and Ulf Norell and Makoto Takeyama},
  title =	 {Agda2 Core Language Proposal},
  howpublished = {slides at Agda implementors' meeting},
  month =	 {Aug.~30},
  year =	 2005,
  WKloc = 	 {A-1652}
}

@InProceedings{Coquand-Dybjer-1994,
  author = {Thierry Coquand and Peter Dybjer},
  title = {Inductive Definitions and Type Theory --- an Introduction (Preliminary version)},
  crossref = {FSTTCS94},
  pages = {60--76},
  OPTabstract = {},
  WKloc = {A-0571}
}

@Misc{Coquand-Nordstrom-Smith-vonSydow-1994,
  author = {Thierry Coquand and Bengt Nordstr{\"o}m and Jan
		  M. Smith and von Sydow, Bj\"orn},
  title = {Type Theory and Programming},
  howpublished = {ftp://ftp.cs.chalmers.se/pub/papers?},
  year = 1994,
  month = JUN,
  WKloc = {A-0380},
  abstract = {This paper gives an introduction to type theory,
		  focusing on its recent use as a logical framework
		  for proofs and programs. The first two sections give
		  a background to type theory intended for the reader
		  who is new to the subject. The following presents
		  Martin-L\"of's monomorphic type theory and an
		  implementation, ALF, of this theory. Finally, a few
		  small tutorial examples in ALF are given.}
}

@Article{Coquand-Pollack-Takeyama-2005,
  author = 	 {Thierry Coquand and Randy Pollack and Makoto Takeyama},
  title = 	 {A Logical Framework with Dependently Typed Records},
  journal = 	 FUNDI,
  year = 	 2005,
  volume =	 65,
  number =	 {1--2},
  pages =	 {113--134},
  bibliographies = {HHOL},
  annote = {see \url{http://unit.aist.go.jp/cvs/Agda/} for Agda}
}

@InProceedings{Corbin-Bidoit-1983,
  keywords = {DAG unification},
  year = 1983,
  title = {A rehabilitation of Robinson's Unification Algorithm},
  publisher = {Elsevier Science Publishers (North Holland)},
  pages = {909--914},
  editor = {R.E.A. Mason},
  booktitle = {Information Processing 83},
  author = {J. Corbin and M. Bidoit}
}

@Book{Cormen-Leiserson-Rivest-Stein-2001,
  author =	 {Thomas H. Cormen and Charles E. Leiserson and Ronald L. Rivest and Clifford Stein},
  title = 	 {Introduction to Algorithms},
  publisher = 	 {McGraw-Hill},
  year = 	 2001,
  edition =	 {2nd edition},
  note = 	 {In the departmental office}
}

@InProceedings{Corney-Gough-1994,
  author = {Diane Corney and John Gough},
  title = {Type Test Elimination using Typeflow Analysis},
  WKloc = {A-0524},
  booktitle = {Programming Languages and System Architectures},
  editor = {J{\"u}rg Gutknecht},
  publisher = {Springer Verlag},
  series = {Lecture Notes in Computer Science},
  volume = 782,
  month = mar,
  year = 1994,
  pages = {137--150},
  refs = 9,
  checked = 19941104,
  source = {main library},
  abstract = {Programs written in languages of the {\em Oberon}
                 family usually contain runtime tests on the dynamic
                 type of variables. In some cases it may be desirable to
                 reduce the number of such tests. Typeflow analysis is a
                 static method of determining bounds on the types that
                 objects may posses at runtime. We show that this
                 analysis is able to reduce the number of tests in
                 certain plausible circumstances. Furthermore, the same
                 analysis is able to detect certain program errors at
                 compile time, which would normally only be detected at
                 program execution. This paper introduces the concepts
                 of typeflow analysis and details its use in the
                 reduction of runtime overhead in {\em Oberon-2}.}
}

@Misc{Corradini-199X,
  author = {Andrea Corradini},
  title = {A Complete Calculus for Equational Deduction in Coalgebraic Specification},
  year = {199?},
  WKloc = {A-0759}
}

@InProceedings{Corradini-Asperti-1992,
  author = {Andrea Corradini and Andrea Asperti},
  title = {A Categorical Model for Logic Programs: Indexed
		  Monoidal Categories},
  crossref = {REX92},
  pages = {110--137},
  abstract = {We propose a simple notion of model for Logic
		  Programs based on indexed monoidal categories. On
		  the one hand our proposal is consistent with
		  well-known techniques for providing a categorical
		  semantics for logical systems. On the other hand, it
		  allows us to keep the effectiveness of the Horn
		  Clause Logic fragment of first order logic. This is
		  shown by providing an effective construction of the
		  initial model of a program, obtained through the
		  application of a general methodology aimed at
		  defining a categorical semantics for structured
		  transition systems. Thus the declarative view (as
		  logical theory) and the operational view (as
		  structured transition system) of a logic program are
		  reconciled in a highly formal framework, which
		  provides interesting hints to possible
		  generalizations of the logic programming paradigm.},
  keywords = {Category Theory, Logic Prggramming, Categorical
		  Logic, Structured Transition Systems, Model Theory}
}

@InProceedings{Corradini-DeNicola-1994,
  author = {Flavio Corradini and De Nicola, Rocco},
  title = {Distribution and Locality of Concurrent Systems},
  pages = {154--165},
  crossref = {ICALP1994},
  authorsAddress = {Universita di Roma la Sapienza},
  abstract = {A new semantics for process description languages that
		  discriminates according to the distribution in space
		  of processes is proposed. $\ldots$}
}

@Unpublished{Corradini-Drewes-1995,
  author = {Andrea Corradini and Frank Drewes},
  title = {{(Cyclic)} Term Graph Rewriting is Adequate for
		  Rational Parallel Term Rewriting},
  year = 1995,
  OPTpages = {},
  note = {Draft},
  OPTauthorsaddress = {},
  WKloc = {A-0615},
  OPTkeywords = {},
  OPTabstract = {},
  OPTannote = {submitted (and presented?) for RTA95}
}

@Misc{Corradini-Ehrig-Heckel-Padberg-Wolter-1996,
  author = {Andrea Corradini and Hartmut Ehrig and Reiko Heckel and Julia Padberg and Uwe Wolter},
  title = {Loose Semantics and Parallel Composition of Typed Graph Transformation Systems},
  year = 1996,
  WKloc = {A-0494}
}

@InProceedings{Corradini-Ehrig-Loewe-Montanari-Padberg-1995,
  author = {Andrea Corradini and Hartmut Ehrig and Martin L\"owe
		  and Ugo Montanari and Julia Padberg},
  title = {The Category of Typed Graph Grammars and its Adjunctions
		  with Categories of Derivations},
  crossref = {GG1994},
  pages = {56--74},
  WKloc = {GraGra '94, PP, pp.~296--301}
}

@TechReport{Corradini-Ehrig-Loewe-Montanari-Rossi-1992,
  author = {Andrea Corradini and Hartmut Ehrig and Michael L\"owe and Ugo
           Montanari and Francesca Rossi},
  title = {Note on Standard Representation of Graphs and Graph Derivations},
  year = 1992,
  abstract = {Considering abstract graphs and abstract graph derivations as
             isomorphism classes of graphs and graph derivations there are
             serious problems concerning representation independence in view
             of an abstract semantics of graph transformations. These
             problems can be solved by the choice of a standard
             representation of graphs and of standard isomorphisms, leading
             to a category {\bf AGRAPHS} of abstract graphs, where abstract
             graph morphisms are defined as equivalence classes of concrete
             morphisms up to standard isomorphisms. A standard
             representation of graphs also determines a skeleton subcategory
             of {\bf GRAPHS}, called {\bf SGRAPHS}, and a functor $F$ to it,
             such thath {\bf AGRAPHS} is exactly the quotient category of
             {\bf GRAPHS} with respect to $F$. As a consequence, categories
             {\bf SGRAPHS} and {\bf AGRAPHS} are isomorphic, and both are
             suitable to define in a consistent way a notion of abstract
             sequential composition of graph transformations, which is an
             important step towards abstract semantics of graph derivations.
             Moreover, the equivalence among these categories and {\bf
             GRAPHS} guarantees the consistency between the result of
             abstract graph derivations and the corresponding concrete graph
             derivations in {\bf GRAPHS}. Although in this note we
             concentrate on categories of graphs (which provide our
             motivating examples), the problems and solutions we present
             here apply to arbitrary categories as well, and in particular
             to the categories used in the framework of High Level
             Replacement Systems.},
  number = {92-25},
  institution = {Technische Universit\"at Berlin},
  type = {Forschungsbericht des Fachbereichs Informatik},
  WKloc = {A-0224},
  annote = {published as \cite{Corradini-Ehrig-Loewe-Montanari-Rossi-1993b}}
}

@InProceedings{Corradini-Ehrig-Loewe-Montanari-Rossi-1993a,
  author = {Andrea Corradini and Hartmut Ehrig and Martin L\"owe
		  and Ugo Montanari and Francesca Rossi},
  title = {Abstract Graph Derivations in the Double Pushout Approach},
  crossref = {GTCS93},
  pages = {86--103},
  abstract = {In the algebraic theory of graph grammars, it is
		  common practice to present some notions or results
		  ``up to isomorphism''. This allows one to reason
		  about graphs and graph derivations without worrying
		  about representation-dependent details.

                  Motivated by a research activity aimed at providing
		  graph grammars with a truly-concurrent semantics, we
		  front in this paper the problem of formalizing what
		  does it mean precisely to reason about graph
		  derivations ``up to isomorphism''. This needs the
		  definition of a suitable equivalence on derivations,
		  which should be consistent with the relevant
		  definitions and results, in the sense that they
		  should extend to equivalence classes. After showing
		  that a naive equivalence is not satisfactory, we
		  propose two requirements for equivalences on
		  derivations which allow the sequential composition
		  of derivations and guarantee the uniqueness of
		  canonical derivation, respectively. Three new
		  equivalences are introduced, the third of which is
		  shown to satisfy both requirements. We also define a
		  new category having the abstract derivations as
		  arrows, which is, in our view, a fundamental step
		  towards the definition of a truly-concurrent
		  semantics for graph grammars.},
  WKloc = {A-0288},
  annote = {see \cite{Adamek-Herrlich-Strecker-1990} for
		  ``Abstract and Concrete Categories''}
}

@InProceedings{Corradini-Ehrig-Loewe-Montanari-Rossi-1993b,
  author = {Andrea Corradini and Hartmut Ehrig and Martin L\"owe
		  and Ugo Montanari and Francesca Rossi},
  title = {Note on Standard Representations of Graphs and Graph
		  Derivations},
  crossref = {GTCS93},
  pages = {104--118},
  abstract = {We show that a naive notion of abstract graph
		  derivations, based on the idea that two derivations
		  are equivalent iff they are isomorphic, does not
		  allow to extend some relevant properties of concrete
		  derivations to abstract ones. This is the main
		  motivation for the introduction of {\em standard
		  representations} of graphs, which are used to define
		  a (more restricted) notion of equivalence among
		  graph morphisms, direct derivations and graph
		  derivations. The properties of the resulting
		  category of abstrat graphs are investigated in
		  depth, and the relationship with skeleton
		  subcategories of {\bf GRAPHS} is worked out.},
  WKloc = {A-0289},
  annote = {roughly the same as
		  \cite{Corradini-Ehrig-Loewe-Montanari-Rossi-1992}}
}

@InProceedings{Corradini-Ehrig-Loewe-Montanari-Rossi-1993b-x,
  author = {Andrea Corradini and Hartmut Ehrig and Martin L\"owe
		  and Ugo Montanari and Francesca Rossi},
  title = {Note on Standard Representations of Graphs and Graph
		  Derivations},
  pages = {104--118},
  WKloc = {A-0289},
  annote = {roughly the same as
		  \cite{Corradini-Ehrig-Loewe-Montanari-Rossi-1992}},
  booktitle = {Graph Transformations in Computer Science,
		  Proc. International Workshop {Dagstuhl Castle,
		  Germany, January 1993}},
  year = 1993,
  editor = {Hans J\"urgen Schneider and Hartmut Ehrig},
  volume = 776,
  UniBwM = {INF700/Y3000},
  series = {LNCS},
  publisher = {Springer-Verlag}
}

@InProceedings{Corradini-Ehrig-Loewe-Montanari-Rossi-1994,
  author = {Andrea Corradini and Hartmut Ehrig and Martin L"owe and
		  Ugo Montanari and Francesca Rossi},
  title = {An Event Structure Semantics for Safe Graph Grammars},
  crossref = {PROCOMET94},
  pages = {417--439},
  keywords = {Mathematical Logic and Formal Languages; Grammars
		  and Other Rewriting Systems; Discrete Mathematics;
		  Graph Theory}
}

@InProceedings{Corradini-Gadducci-1995,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {CPO Models for Infinite Term Rewriting},
  crossref = {AMAST1995},
  pages = {368--384},
  OPTabstract = {},
  WKloc = {A-0623}
}

@TechReport{Corradini-Gadducci-1998,
  author = {Corradini, A. and Gadducci, F.},
  title = {Rewriting on cyclic structures},
  institution = {Dipartimento di Informatica, Pisa},
  year = 1998,
  number = {TR-98-05},
  note = {Extended abstract in Z. \'Esik, ed.,
           {\em Fixed Points in Computer Science},
                  satellite workshop of MFCS'98}
}

@Article{Corradini-Gadducci-1999-APTG,
  author = {Corradini, Andrea and Gadducci, Fabio},
  title = {An Algebraic Presentation of Term Graphs, via GS-Monoidal
              Categories},
  journal = {Applied Categorical Structures},
  year = 1999,
  volume =	 7,
  number =	 4,
  pages = {299--331},
  publisher = {Kluwer},
  WKloc = {A-0855, doc/pap/BIB},
  PDFURL = {http://resolver.scholarsportal.info/resolve/09272852/v07i0004/299_aapotgvgc&form=pdf&file=file.pdf},
  bibliographies = {Coconut}
}

@Article{Corradini-Gadducci-2002,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {Functorial Semantics for Multi-Algebras and Partial Algebras, with Applications to Syntax},
  year = 2002,
  journal = {Theoretical Computer Science},
  volume =	 286,
  number =	 2,
  pages =	 {293--322},
  annote = {(short version: \cite{Corradini-Gadducci-1999wadt98})},
  WKloc = {A-0813, doc/pap/BIB},
  URL = {http://resolver.scholarsportal.info/resolve/03043975/v286i0002/293_afsfmapawats&form=pdf&file=file.pdf},
  abstract = {Multi-algebras allow to model nondeterminism in an algebraic
      framework by interpreting operators as functions from individual
      arguments to sets of possible results. We propose a functorial
      presentation of various categories of multi-algebras and partial
      algebras, analogous to the classical presentation of algebras over a
      signature $\Sigma$ as cartesian functors from the algebraic theory
      over $\Sigma$ to \cat{Set}. We introduce two different notions of
      theory over a signature, both having a structure weaker than
      cartesian, and we consider functors from them to \cat{Rel} or
      \cat{Pfn}, the categories of sets and relations or partial functions,
      respectively.

      Next we discuss how the functorial presentation provides guidelines
      for the choice of syntactical notions for a class of algebras, and as
      an application we argue that the natural generalization of usual
      terms are ``conditioned terms'' for partial algebras, and ``term
      graphs'' for multi-algebras.},
  bibliographies = {Coconut}
}

@Article{Corradini-Gadducci-2005,
  author = 	 {Andrea Corradini and Fabio Gadducci},
  title = 	 {On Term Graphs as an Adhesive Category},
  journal = 	 ENTCS,
  year = 	 2005,
  volume =	 127,
  number =	 5,
  pages =	 {43--56},
  month =	 MAY,
  WKloc = 	 {doc/pap/BIB},
  DOI = {http://dx.doi.org/10.1016/j.entcs.2005.02.014},
  abstract = {The recent interest in bisimulation congruences for
      reduction systems, stimulated by the research on general (often
      graphical) frameworks for nominal calculi, has brought forward
      many proposals for categorical formalisms where relevant
      properties of observational equivalences could be auto-
      matically verified.

      Interestingly, some of these formalisms also identified suitable
      categories where the standard tools and techniques developed for
      the double-pushout approach to graph transformation
      [A. Corradini, U. Montanari, F. Rossi, H. Ehrig, R. Heckel and
      M. L\"owe, Algebraic approaches to graph transformation I: Basic
      concepts and double pushout approach, in: G. Rozenberg, editor,
      Handbook of Graph Grammars and Computing by Graph
      Transformation, I: Foundations, World Scientific, 1997,
      pp. 163--246] could be recast, thus providing a valid
      alternative to the High-Level Replacement Systems paradigm
      [H. Ehrig, A. Habel, H.-J. Kreowski and F. Parisi-Presicce,
      Parallelism and concurrency in highlevel replacement systems,
      Mathematical Structures in Computer Science 1 (1991)
      361--404].

      In this paper we consider the category of term graphs, and we
      prove that it (partly) fits in the general framework for
      adhesive categories, developed in [S. Lack, and
      P. Soboci{\'n}ski, Adhesive categories, in: I. Walukiewicz,
      editor, Foundations of Software Science and Computation
      Structures, Lect. Notes in Comp. Sci. 2987 (2004),
      pp. 273--288, P. Soboci{\'n}ski, ``Deriving
      bisimulation congruences from reduction systems'',
      Ph.D. thesis, BRICS, Department of Computer Science, University
      of Aaurhus (2004)], extended in [H. Ehrig, A. Habel, J. Padberg
      and U. Prange, Adhesive high-level replacement categories and
      systems, in: G. Engels and F. Parisi-Presicce, editors, Graph
      Transformation, Lect. Notes in Comp. Sci. (2004)] and applied to
      reduction systems in [V. Sassone, and P. Soboci{\'n}ski,
      Congruences for contextual graph-rewriting, Technical Report
      RS-04-11, BRICS, Department of Computer Science, University of
      Aarhus (2004)]. The main technical achievement concerns the
      proof that the category of term graphs is actually
      quasi-adhesive, obtained by proving the existence of suitable
      Van Kampen squares. }
}

@Article{Corradini-Gadducci-1999b,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {Rewriting on cyclic structures: Equivalence between the
      operational and the categorical description},
  journal = RAIRO-I,
  OPTyear = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTmonth = {},
  OPTpages = {},
  note = {to appear},
  OPTunibwm = {INF/Z},
  WKloc = {A-0814},
  abstract = {We present a categorical formulation of the rewriting of
      possibly cyclic term graphs, based on a variation of algebraic
      2-theories. We show that this presentation is equivalent to the
      well-accepted operational definition proposed by Barendregt et
      alii---but for the case of {\em circular redexes\/}, for which we
      propose (and justify formally) a different treatment. The categorical
      framework allows us to model in a concise way also automatic garbage
      collection and rules for sharing/unsharing and folding/unfolding of
      structures, and to relate term graph rewriting to other rewriting
      formalisms.},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Corradini-Gadducci-1999wadt98,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {Functorial Semantics for Multi-Algebras},
  pages = {78--90},
  year = 1999,
  booktitle = {Recent Trends in Algebraic Development Techniques},
  editor = {J. L. Fiadeiro},
  series = LNCS,
  volume = 1589,
  publisher = Springer,
  WKloc = {A-0756},
  bibliographies = {RelMiCS, GraphCalc, Coconut}
}

@Misc{Corradini-Gadducci-1997,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {A 2-Categorical Presentation of Term Graph Rewriting},
  year = {1997},
  WKloc = {A-0757}
}

@Misc{Corradini-Gadducci-199X,
  author = {Andrea Corradini and Fabio Gadducci},
  title = {Rational Term Rewriting},
  year = {199?},
  WKloc = {A-0755}
}

@TechReport{Corradini-Gadducci-Kahl-2000,
  author = {Corradini, Andrea and Gadducci, Fabio and Kahl, Wolfram},
  title = {{Term Graph Syntax for Multi-Algebras}},
  institution = {Dipartimento di Informatica, Universit\`a di Pisa},
  year = 2000,
  number = {TR-00-04},
  URL = {http://ncstrl.cs.cornell.edu:80/Dienst/UI/1.0/Display/ncstrl.unipi_it/TR-00-04},
  abstract = {Multi-algebras allow to model nondeterminism in an
     algebraic framework by interpreting operators as functions from
     individual arguments to sets of possible results. Starting from a
     functorial presentation of multi-algebras based on {\em gs-monoidal
     theories}, we argue that specifications for multi-algebras should be
     based on the notion of {\em term graphs} instead of on standard
     terms. We consider the simplest case of (term graph) equational
     specification, showing that it enjoys an unrestricted form of
     substitutivity.  We discuss the expressive power of equational
     specification for multi-algebras, and we sketch possible extensions of
     the calculus.},
  bibliographies = {RelMiCS, GraphCalc, Coconut, Kahl}
}

@InProceedings{Corradini-Gadducci-Kahl-Koenig-2002,
  author = {Andrea Corradini and Fabio Gadducci and Wolfram Kahl and Barbara K{\"o}nig},
  title = {Inequational Deduction as Term Graph Rewriting},
  crossref = {TERMGRAPH2002},
  pages = {31--44},
  URL = {http://dx.doi.org/10.1016/j.entcs.2002.09.004},
  bibliographies = {RelMiCS, GraphCalc, Coconut, Kahl}
}

@Misc{Corradini-Gadducci-Montanari-199X,
  author = {Andrea Corradini and Fabio Gadducci and Ugo Montanari},
  title = {Relating Two Categorical Models of Term Rewriting},
  year = {199?},
  WKloc = {A-075}
}

@InProceedings{Corradini-Heckel-Montanari-1999,
  author = {Andrea Corradini and Reiko Heckel and Ugo Montanari},
  title = {Tile Transition Systems as Structured Coalgebras},
  pages = {??},
  crossref = {FCT-1999},
  WKloc = {A-0846},
  OPTabstract = {}
}

@InCollection{Corradini-Heindel-Hermann-Koenig-2006,
  year={2006},
  ISBN={978-3-540-38870-8},
  booktitle={Graph Transformations},
  LNCSbooktitle={ICGT 2006},
  volume={4178},
  series=LNCS,
  editor={Corradini, Andrea and Ehrig, Hartmut and Montanari, Ugo and Ribeiro, Leila and Rozenberg, Grzegorz},
  DOI={10.1007/11841883_4},
  title={Sesqui-Pushout Rewriting},
  DOIURL={http://dx.doi.org/10.1007/11841883_4},
  publisher={Springer Berlin Heidelberg},
  author={Corradini, Andrea and Heindel, Tobias and Hermann, Frank and K{\"o}nig, Barbara},
  pages={30--45},
  language={English},
  abstract = {Sesqui-pushout (SqPO) rewriting --- ``sesqui'' means ``one and a half'' in Latin
    --- is a new algebraic approach to abstract rewriting in any category.
    SqPO rewriting is a deterministic and conservative extension of double-pushout (DPO) rewriting,
    which allows to model ``deletion in unknown context'',
    a typical feature of single-pushout (SPO) rewriting, as well as cloning.

    After illustrating the expressiveness of the proposed approach
    through a case study modelling an access control system,
    we discuss sufficient conditions for the existence of final pullback complements
    and we analyze the relationship between SqPO and the classical DPO and SPO approaches.}
}

@InProceedings{Corradini-Montanari-1991,
  author = {Andrea Corradini and Ugo Montanari},
  title = {An Algebra of Graphs and Graph Rewriting},
  pages = {236--260},
  crossref = {CTCS1991},
  WKloc = {A-0098},
  abstract = {In this paper we propose an axiomatization of `partially
	abstract graphs', i.e., of suitable classes of monomorphisms in
	a category of graphs, which may be interpreted as graphs having
	both a concrete part and an abstract part (defined up to
	isomorphism). Morphisms between pa-graphs are pushout squares.
	We show that the basic notions of the algebraic theory of graph
	grammars \cite{Ehrig-1978} (instantiated to a suitable category of graphs)
	can be rephrased in a natural way using partially abstract graphs.
	The terms of the algebra we propose are built over a small set of
	operators, including parallel composition, substitution application,
	and restriction. By equipping the algebra of terms with a
	categorical structure (arrows are equivalence classes of monadic
	contexts), we show that there is a full and faithful embedding
	(with a right adjoint) of the category of partially abstract graphs
	into the category of (well-formed) terms. This embedding is
	exploited to show that rewriting (in the sense of term rewriting
	systems) over this algebra models faithfully the direct derivations
	of graphs, described by a double pushout construction along the
	guidelines of \cite{Ehrig78}. In particular, we show that also graph
	productions having non-discrete gluing graphs can be represented as
	term rewrite rules without loss of information, unlike a similar
	approach proposed in [BC87].}
}

@Article{Corradini-Montanari-1992,
  author = {Andrea Corradini and Ugo Montanari},
  title = {An Algebraic Semantics for Structured Transition Systems and its Application to Logic Programs},
  journal = {Theoretical Computer Science},
  year = 1992,
  volume = 103,
  OPTnumber = {},
  OPTmonth = {},
  pages = {51--106},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0827},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Corradini-Montanari-Rossi-1996,
  author =    {Andrea Corradini and Ugo Montanari and Francesca Rossi},
  title =     {Graph Processes},
  journal =   FUNDI,
  year =      1996,
  volume =    26,
  number =    {3, 4},
  pages =     {241--266},
  abstract = {We first give a new definition of graph grammars, which,
    although following the algebraic double-pushout approach,
    is more general than the classical one
    because of the use of a graph of types
    where all involved graphs are mapped to.
    Then, we develop a process-based semantics for such (typed) graph grammars,
    in the line of processes as normally used
    for providing a semantics to Petri nets.
    More precisely, we represent each equivalence class of derivations
    as a graph process, which can be seen as an acyclic graph grammar,
    plus a mapping of its items onto the items of the given grammar.
    Finally, we show that such processes represent
    exactly the same equivalence classes of derivations as defined in [3].
    Therefore graph processes are attractive alternative representatives
    for such classes.
    The advantage of dealing with graph processes
    instead of equivalence classes
    (or also representatives belonging to the equivalence classes)
    is that dependency and/or concurrency of derivation steps
    is directly represented.}
}

@InProceedings{Corradini-Montanari-Rossi-Ehrig-Heckel-Loewe-1990,
  author =       {Andrea Corradini and Ugo Montanari and Francesca Rossi and Hartmut Ehrig and Michael L{\"o}we},
  title =        {Graph Grammars and Logic Programming},
  crossref =  {GG1990},
  OPTkey =       {},
  DOI = {10.1007/BFb0017392},
  SpringerURL = {http://www.springerlink.com/content/w704x6p74n100391/},
  pages =     {221--237},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  abstract = {In this paper we investigate the relationship
    between the algebraic definition of graph grammars and logic programming.
    In particular, we show that the operational semantics of any logic program
    can be faithfully simulated by a particular context-free hypergraph grammar.
    In the process of doing that, we consider the issue of representing terms,
    formulas, and clauses as particular graphs or graph productions,
    by first evaluating the approaches
    already proposed for Term Rewriting Systems (TRS),
    and then by giving an original extension of those approaches,
    to be able to deal with the unique features of logic programming.
    Actually, not only does our representation of definite clauses by
    graph productions allow us to deal correctly with logical unification,
    but also it overcomes some of the problems encountered
    by other approaches for representing TRS's as graph grammars.
    The main result of the paper states the soundness and completeness
    of the representation of clauses by productions,
    and this correspondence is extended to entire computations,
    showing how a context-free grammar (over a suitable category of graphs)
    can be associated with a logic program.
    The converse holds as well,
    i.e. given any context-free graph grammar (over that category),
    a logic program can be extracted from it.}
}

@InCollection{Corradini-Montanari-Rossi-Ehrig-Heckel-Loewe-1997,
  author = {Andrea Corradini and Ugo Montanari and Francesca Rossi and Hartmut Ehrig and Reiko Heckel and Michael L{\"o}we},
  title = {Algebraic Approaches to Graph Transformation, Part {I}:
     Basic Concepts and Double Pushout Approach},
  crossref = {HBGraTraI},
  chapter = 3,
  pages = {163--245}
}

@InProceedings{Corradini-Montanari-Rossi-Ehrig-Loewe-1990,
  author = {Andrea Corradini and Ugo Montanari and Francesca Rossi and Hartmut Ehrig and Michael L{\"o}we},
  title = {Graph Grammars and Logic Programming},
  pages = {221-237},
  crossref = {GG1990},
  keywords = {Logic Programming, Graph Grammars, Term Rewriting Systems,
	Hypergraphs, Jungles, Dags},
  contents = {0. Introduction
	1. Background
	2. Representing formulas by graphs
	3. Representing clauses by graph productions
	4. Logic programs as graph grammars and vice versa
	5. Conclusions and future work
	6. References},
  abstract = {In this paper we investigate the relationship between the
	algebraic definition of graph grammars and logic programming. In
	particular, we show that the operational semantics of any logic
	program can be faithfully simulated by a particular context-free
	hypergraph grammar. In the process of doing that, we consider the
	issue of representing terms, formulas, and clauses as particular
	graphs or graph productions, by first evaluating the approaches
	already proposed for Term Rewriting Systems (TRS), and then by
	giving an original extension of those approaches, to be able to
	deal with the unique features of logic programming. Actually, not
	only does our representation of definite clauses by graph productions
	allow us to deal correctly with logical unification, but also it
	overcomes some of the problems encountered by other approaches for
	representing TRS's as graph grammars. The main result of the paper
	states the soundness and completeness of the representation of
	clauses by productions, and this correspondence is extended to entire
	computations, showing how a context-free grammar (over a suitable
	category of graphs) can be associated with a logic program. The
	converse holds as well, i.e.\ given any context-free graph grammar
	(over that category), a logic program can be extracted from it.}
}

@Article{Corradini-Rossi-1991,
  author = {Andrea Corradini and Francesca Rossi},
  title = {Hyperedge replacement jungle rewriting for
		  term-rewriting systems and logic programming},
  crossref = {GraTra91},
  journal = TCS,
  year = 1993,
  volume = 109,
  number = {1--2},
  pages = {7--48},
  DOIURL = {http://dx.doi.org/10.1016/0304-3975(93)90063-Y},
  DOI = {10.1016/0304-3975(93)90063-Y},
  WKloc = {A-0148, doc/pap/BIB},
  abstract = {We introduce hyperedge replacement jungle rewriting,
		  a graph-rewriting formalism suitable for modeling
		  the manipulation of terms and similar structures,
		  and investigate its expressive power by showing that
		  it can model both term-rewriting systems and logic
		  programming in a faithful way. For term-rewriting
		  systems we prove the soundness of their jungle
		  representation, and a result of completeness
		  w.r.t.~applicability which is stronger than similar
		  results in the related literature, since it works
		  also for non-left-linear rules. For logic
		  programming both soundness and completeness hold.},
  annote = {\cite{Corradini-Wolz-1993} claims this paper shows that
		  jungle rewriting faithfully models TRSs, also in the
		  case of non-left-linear rewrite rules.}
}

@InProceedings{Corradini-Rossi-1991InProc,
  author = {Andrea Corradini and Francesca Rossi},
  title = {Hyperedge replacement jungle rewriting for
		  term-rewriting systems and logic programming},
  crossref = {GraTra91},
  pages = {7--48},
  DOIURL = {http://dx.doi.org/10.1016/0304-3975(93)90063-Y},
  DOI = {10.1016/0304-3975(93)90063-Y},
  WKloc = {A-0148, doc/pap/BIB},
  abstract = {We introduce hyperedge replacement jungle rewriting,
		  a graph-rewriting formalism suitable for modeling
		  the manipulation of terms and similar structures,
		  and investigate its expressive power by showing that
		  it can model both term-rewriting systems and logic
		  programming in a faithful way. For term-rewriting
		  systems we prove the soundness of their jungle
		  representation, and a result of completeness
		  w.r.t.~applicability which is stronger than similar
		  results in the related literature, since it works
		  also for non-left-linear rules. For logic
		  programming both soundness and completeness hold.},
  annote = {\cite{Corradini-Wolz-1993} claims this paper shows that
		  jungle rewriting faithfully models TRSs, also in the
		  case of non-left-linear rewrite rules.}
}

@InCollection{Corradini-Rossi-1993,
  author = {Andrea Corradini and Francesca Rossi},
  title = {A New Term Graph Rewriting Formalism:
           Hyperedge Replacement Jungle Rewriting},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  pages = {}
}

@InProceedings{Corradini-Rossi-Parisi-1991,
  author = {Andrea Corradini and Francesca Rossi and Francesco
		  Parisi-Presicce},
  title = {Logic Programming as Hypergraph Rewriting},
  pages = {275--295},
  crossref = {CAAP91},
  WKloc = {A-0205},
  authorsAddress = {AC\&FR: Universit\`a di Pisa, PP: Universit\`a
		  dell'Aquila},
  abstract = {Logic Programming and (Hyper-)Graph Rewriting are
		  two well known fields of Computer Science. In this
		  paper we show how to model logic program
		  computations through algebraic techniques familiar
		  to the graph rewriting community. Clauses of a logic
		  program are represented by graph productions, goals
		  by suitable hypergraphs (called jungles), and
		  resolution steps by an algebraic construction
		  involving three pushouts. The correspondence between
		  the two formalisms is further analyzed by providing
		  a precise algebraic characterization of
		  specialization and unfolding of clauses.}
}

@InProceedings{Corradini-Wolz-1993,
  author = {Andrea Corradini and Dietmar Wolz},
  title = {Jungle Rewriting: An Abstract Description of a Lazy
		  Narrowing Machine},
  crossref = {GTCS93},
  pages = {119--137},
  abstract = {We propose to use Jungle Rewriting, a graph
		  rewriting formalism, as a formal specification tool
		  for the LANAM, an abstract machine implementing lazy
		  narrowing. Three jungle rewriting systems which
		  model a narrowing program at different levels of
		  abstraction are presented. The first system is
		  proved to be essentially equivalent (from an
		  operational point of view) to the given program. The
		  second one contains only rules which correspond to
		  elementary actions of a narrowing interpreter, while
		  the third one also models the control issues of the
		  abstract machine. We formally prove the equivalence
		  of the first and second systems, while the
		  relationship between the second and the third one is
		  sketched. The last system provides an abstract
		  description of the compiled code of the LANAM, which
		  is outlined.},
  WKloc = {A-0290},
  annote = {Claims the introduction of jungle rewriting for
		  \cite{Corradini-Rossi-1991} --- see \cite{Hoffmann-Plump-1988}.

                  Their encoding of narrowing might be a fun way
                  to treat conditional rules, but raises domination problems.}
}

@InProceedings{Corsini-Griffault-Rauzy-1993,
  author = {Marc-Michel Corsini and Alain Griffault and Antoine Rauzy},
  title = {Yet Another Application for {Toupie}: Verification of
		  Mutual Exclusion Algorithms},
  crossref = {LPAR93},
  pages = {86--97},
  WKloc = {A-0121},
  bibliographies = {RelMiCS},
  abstract = {Toupie is a finite domain $\u$-calculus model checker that
             uses extended decision diagrams to represent relations and
             formulae. In recent papers, we have demonstracted that such a
             language can model and solve difficult problems, such as AI
             Puzzles, Abstract Interpretation of Logic Programs with very
             good running times. Hereafter we show how, in Toupie, one can
             handle transition systems and check properties of Mutual
             Exclusion Algorithms.}
}

@Misc{Cortellessa-DelGobbo-Shereshevsky-Desharnais-Mili-2003,
  author = {V. Cortellessa and Del Gobbo, D. and M. Shereshevsky and J. Desharnais and A. Mili},
  title = {Relational Characterizations of System fault Tolerance},
  year = 2003,
  month = NOV,
  WKloc = {A-1535},
  annote = {reviewed May 2004 for SciCoP}
}

@InProceedings{Courcelle-1987,
  abstract = {Certain recursive definitions can be represented by
	context-free graph grammars. The grammar associated with a recursive
	definition generates the set of its {\em computation graphs}.
	Properties of recursive definitions expressible as monadic
	second-order logical properties of their computation graphs, are
	decidable. Applications to recursive applicative program schemes and
	to recursive queries in relational data bases are given.},
  title = {On Using Context-Free Graph Grammars for Analyzing Recursive Definitions},
  pages = {83-122},
  crossref = {ProgFutGenCompII},
  author = {Bruno Courcelle},
  bibliographies = {RelMiCS}
}

@InCollection{Courcelle-1990,
  author = {Bruno Courcelle},
  title = {Graph Rewriting: An Algebraic and Logic Approach},
  chapter = 5,
  crossref = {HBThCS-b},
  pages = {193--242},
  WKloc = {Q-003; Reference updates and corrections: A-1164},
  bibliographies = {RelMiCS}
}

@Article{Courcelle-1992,
  author = {Bruno Courcelle},
  title = {The monadic second-order logic of graphs VII: Graphs
		  as relational structures},
  journal = {Theoretical Computer Science},
  year = 1992,
  volume = 101,
  number = 1,
  pages = {3--33},
  bibliographies = {RelMiCS},
  abstract = {Relational structures form a unique framework in which
               various types of graphs and hypergraphs can be
               fromalized and studied. We define operations on
               structures that are compatible with monadic second-order
               logic, and that are powerful enough to represent context-
               free graph and hypergraph grammars of various types,
               namely, the so-called hyperedge replacement, C-edNCE,
               and separated handle rewriting grammars. Several results
               concerning monadic second-order properties of the
               generated sets are obtained in a uniform way. We also
               give a logical characterization of the equational sets
               of structures that generalies the ones obtained by
               Engelfriet and Courcelle for hyperedge replacement and C-
               edNCE sets of graphs.}
}

@Misc{Courcelle-1998,
  author = {Bruno Courcelle},
  title = {The monadic second-order logic of graphs XII: Planar graphs and planar maps},
  month = APR,
  year = 1998,
  WKloc = {A-1167}
}

@Misc{Courcelle-1999,
  author = {Bruno Courcelle},
  title = {A Monadic Second-Order Definition of the Structure of Convex Hypergraphs},
  month = SEP,
  year = 1999,
  WKloc = {A-1165}
}

@Misc{Courcelle-1999a,
  author = {Bruno Courcelle},
  title = {Hierarchical graph decompositions defined by grammars and logical formulas},
  howpublished = {Summary of invited lecture at RTA 99, Trento, Italy},
  month = JUL,
  year = 1999,
  WKloc = {A-1168}
}

@Misc{Courcelle-199X,
  author = {Bruno Courcelle},
  title = {Reecriture de Graphes: Orientation Bibliographique},
  year = {199?},
  WKloc = {A-0575}
}

@Misc{Courcelle-2000,
  author = {Bruno Courcelle},
  title = {The monadic second-order logic of graphs XIV: Uniformly sparse graphs and edge set quantifications},
  month = JAN,
  year = 2000,
  WKloc = {A-1166}
}

@Article{Courcelle-Engelfriet-Rozenberg-1993,
  author = {Courcelle, Bruno and Engelfriet, Joost and
		  Rozenberg, Grzegorz},
  title = {Handle-Rewriting Hypergraph Grammars},
  journal = {Journal of Computer and System Sciences},
  year = 1993,
  volume = 46,
  number = 2,
  pages = {218--270},
  abstract = {We introduce the handle-rewriting hypergraph grammars,
               based on the replacement of handles, i.e., of
               subhypergraphs consisting of one hyperedge together with
               its incident vertices. This extends hyperedge
               replacement, where only the hyperedge is replaced. A HH
               grammar is separated if nonterminal handles do not
               overlap. The S-HH grammars are context-free, and the
               sets they generate can be characterized as the NLC-like
               vertex-rewriting C-edNCE graph grammars that are also
               context-free.}
}

@Article{Courcelle-Franchi-Zannettacci-1982,
  author = {Bruno Courcelle and Franchi-Zannettacci, P.},
  title = {Attribute Grammars and Recursive Program Schemes {I} and
		  {II}},
  journal = {Theoretical Computer Science},
  volume = 17,
  pages = {163--191, 235--257},
  year = 1982,
  UniBwM = {Z2120-17}
}

@InProceedings{Courcelle-Lagergren-1993,
  author = {Bruno Courcelle and J. Lagergren},
  title = {Recognizable Sets of Graphs of Bounded Tree-Width},
  crossref = {GTCS93},
  pages = {138--152},
  abstract = {We establish that a set of finite graphs of
		  tree-width at most $k$ is recognizable (with respect
		  to the algebra of graphs with an unbounded number of
		  sources) {\em if and only if} it is recognizable
		  with respect to the algebra of graphs with at most
		  $k$ sources. We obtain a somewhat stronger result
		  for sets of simple finite graphs of tree-width at most $k$.}
}

@InProceedings{Courcelle-Mosbah-1991,
  author = {B. Courcelle and M. Mosbah},
  title = {Monadic second-order evaluations on
		  tree-decomposable graphs},
  crossref = {GraTra91},
  pages = {49--82},
  WKloc = {A-0149},
  abstract = {Every graph generated by a hyperedge replacement
		  graph-grammar can be represented by a tree, namely
		  the derivation tree of the derivation sequence that
		  produced it. Certain functions on graphs can be
		  computed recursively on the derivation trees of
		  these graphs. By using monadic second-order logic
		  and semiring homomorphisms, we describe in a single
		  formalism a large class of such functions.
		  Polynomial and even linear algorithms can be
		  constructed for some of these functions. We unify
		  similar results obtained by Tamikazawa
		  et~al.~(1982), Bern et~al.~(1987). Arnborg
		  et~al.~(1991) and Habel et~al.~(1989)}
}

@InProceedings{Courcelle-Nivat-1976,
  year = 1976,
  title = {Algebraic Families of Interpretations},
  pages = {137--146},
  month = {Oct.~25--27},
  booktitle = {Proc.\null{} {$17^{th}$} Annual IEEE Symposium on Foundations of Computer Science},
  author = {Bruno Courcelle and M. Nivat},
  address = {Houston, Texas},
  bibliographies = {RelMiCS}
}

@InProceedings{Courtney-2003,
  author = 	 {Antony Courtney},
  title = 	 {Functionally Modeled User Interfaces},
  crossref =  {Interactive2003},
  OPTpages = 	 {},
  WKloc = 	 {doc/pap/BIB},
  abstract = {Fruit is a new user interface toolkit based on a
     formal model of user interfaces. This formal basis enables us to
     write simple, concise executable specifications of interactive
     applications. This paper presents the Fruit model and several
     example specifications. We consider one example (a basic media
     controller) in detail, and contrast the executable specification
     style of Fruit with a more traditional rapid prototype
     implementation using an imperative, object-oriented toolkit
     (Java/Swing) to show the benefits of our approach.}
}

@InProceedings{Courtney-Nilsson-Peterson-2003,
  author = 	 {Antony Courtney and Henrik Nilsson and John Peterson},
  title = 	 {The Yampa Arcade},
  crossref =  {Haskell2003},
  OPTpages = 	 {},
  WKloc = 	 {A-1590, doc/pap/BIB},
  bibliographies = {FP},
  abstract = {Simulated worlds are a common (and highly lucrative)
     application domain that stretches from detailed simulation of
     physical systems to elaborate video game fantasies. We believe
     that Functional Reactive Programming (FRP) provides just the
     right level of functionality to develop simulated worlds in a
     concise, clear and modular way. We demonstrate the use of FRP in
     this domain by presenting an implementation of the classic Space
     Invaders game in Yampa, our most recent Haskell-embedded
     incarnation of FRP.}
}

@Book{Cousineau-Mauny-1998,
  author = {Guy Cousineau and Michel Mauny},
  title = {The Functional Approach to Programming with Caml},
  publisher = {Cambridge University Press},
  year = 1998,
  URL = {http://pauillac.inria.fr/cousineau-mauny/main.html},
  ISBN = {0-521-57183-9 (hardcover), 0-521-57681-4 (paperback)}
}

@Article{Cousot-1996,
  title = {Abstract Interpretation},
  author = {Patrick Cousot},
  journal = {ACM Computing Surveys},
  pages = {324--328},
  month = jun,
  year = 1996,
  volume = 28,
  number = 2,
  WKloc = {A-1302, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/234528.234740},
  intro = {The main idea of abstract interpretation is that program static
      analyzers effectively compute an approximation of the program
      semantics so that the specification of program analyzers should be
      formally derivable from the specification of the semantics.}
}

@Article{Cousot-1997,
  author = {Patrick Cousot},
  title = {Constructive Design of a Hierarchy of Semantics of a Transition System by Abstract Interpretation},
  journal = ENTCS,
  volume = 6,
  year = 1997,
  URL = {http://www.elsevier.nl/locate/entcs/volume6.html},
  note = {25 pages},
  annote = {short version of \cite{Cousot-2000}},
  abstract = {We construct a hierarchy of semantics by successive abstract
      interpretations. Starting from a maximal trace semantics of a
      transition system, we derive a big-step semantics, termination and
      nontermination semantics, natural, demoniac and angelic relational
      semantics and equivalent nondeterministic denotational semantics, D.
      Scott's deterministic denotational semantics,
      generalized/conservative/liberal predicate transformer semantics,
      generalized/total/partial correctness axiomatic semantics and
      corresponding proof methods. All semantics are presented in uniform
      fixpoint form and the correspondence between these semantics are
      established through composable Galois connection.},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Unpublished{Cousot-2000,
  author = {Patrick Cousot},
  title = {Constructive Design of a Hiertarchy of Semantics of a Transition System by Abstract Interpretation},
  note = {56 pages, long version of \cite{Cousot-1997}},
  OPTkey = {},
  OPTmonth = {},
  OPTyear = {},
  OPTannote = {},
  WKloc = {A-1102, doc/pap/BIB/Cousot-TCS-3779.ps.gz},
  bibliographies = {RelMiCS}
}

@InProceedings{Cousot-Cousot-1977,
  author = {Patrick Cousot and Radhia Cousot},
  title = {Abstract Interpretation: {A} Unified Lattice Model for
                 Static Analysis of Programs by Construction or
                 Approximation of Fixpoints},
  crossref = {POPL1977},
  pages = {238--252}
}

@InProceedings{Cousot-Cousot-1992,
  author = {Patrick Cousot and Radhia Cousot},
  title = {Comparing the Galois Connection and
		  Widening/Narrowing Approaches to Abstract Interpretation},
  authorsAddress = {PC: LIENS, DMI, \'Ecole Normale Sup\'erieure,
		  Paris, cousot\@dmi.ens.fr;
		  RC: LIX, \'Ecole Polytechnique, Palaiseau,
		  radhia\@polytechnique.fr},
  abstract = {The use of infinite abstract domains with widening
		  and narrowing for accelerating the convergence of
		  abstract interpretations is shown to be more
		  powerful than the Galois connection approach
		  restricted to finite lattices (or lattices
		  satisfying the chain condition).},
  pages = {269--295},
  note = {invited lecture},
  crossref = {PLILP1992},
  bibliographies = {RelMiCS}
}

@InProceedings{Cousot-Cousot-1994,
  author = {Patrick Cousot and Radhia Cousot},
  title = {Higher-Order Abstract Interpretation (and
		  Application to Comportment Analysis Generalizing
		  Strictness, Termination, Projection, and {PER}
		  Analysis of Functional Languages)},
  crossref = {ICCL94},
  pages = {95--112},
  note = {invited talk},
  WKloc = {A-0390}
}

@InProceedings{Cousot-Cousot-1994a,
  author = {Patrick Cousot and Radhia Cousot},
  title = {Galois Connection Based Abstract Interpretations for Strictness Analysis},
  crossref = {Bjorner-Broy-Pottosin-1994},
  pages = {98--127},
  WKloc = {A-0998}
}

@Article{Cousot-Cousot-2002,
  author = {Patrick Cousot and Radhia Cousot},
  title = {Parsing as Abstract Interpretation of Grammar Semantics},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  WKloc = {doc/pap/BIB},
  bibliographies = {FP},
  note = {to appear},
  abstract = {Earley's parsing algorithm is shown to be an abstract interpretation of a refinement of the derivation semantics of context free grammars.}
}

@Book{Couturat-1914,
  author = {L. Couturat},
  title = {The Algebra of Logic},
  note = {Engl.\null{} translation, by L.\null{} G.\null{} Robinson, of
      L'algebre de la logique, Gauthier-Villars, Paris, 1905, pp.\null{}
      100},
  publisher = OpenPubl,
  address = {London},
  year = 1914,
  pages = {xiv+98},
  bibliographies = {RelMiCS}
}

@InProceedings{Cox-Pietrzykowski-1985,
  title = {Surface Deduction: A Uniform Mechanism for Logic Programming},
  pages = {220--229},
  crossref = {SLP85},
  author = {Cox, P. T. and Pietrzykowski, T.}
}

@InProceedings{Craig-1965,
  author = {William Craig},
  title = {Boolean Notions Extended to Higher Dimensions},
  crossref = {TM1965},
  pages = {55--69},
  bibliographies = {RelMiCS}
}

@Book{Craig-1974,
  author = {William Craig},
  title = {Logic in Algebraic Form},
  publisher = NoHo,
  address = {Amsterdam},
  year = 1974,
  bibliographies = {RelMiCS}
}

@InCollection{Craig-1974a,
  author = {William Craig},
  title = {Unification and Abstraction in Algebraic Logic},
  pages = {6--57},
  OPTnote = {MR 51\#12521},
  crossref = {Daigneault-1974},
  bibliographies = {RelMiCS}
}

@InProceedings{Craig-1974b,
  author = {William Craig},
  title = {Diagonal Relations},
  crossref = {TarskiSymp1974},
  pages = {91--104},
  bibliographies = {RelMiCS}
}

@Article{Crangle-Suppes-1987,
  author = {Colleen Crangle and Patrick Suppes},
  year = 1987,
  title = {Context-fixing semantics for instructable robots},
  journal = {International Journal of Man-Machine Studies},
  volume = 27,
  pages = {371--400},
  bibliographies = {RelMiCS}
}

@Article{Crangle-Suppes-1989,
  year = 1989,
  volume = {XIV},
  title = {Geometrical semantics for spatial prepositions},
  pages = {399--422},
  journal = Midwest_Studies_in_Philosophy,
  author = {Colleen Crangle and Patrick Suppes},
  bibliographies = {RelMiCS}
}

@Misc{Criterion-1.1.0.0,
  OPTkey =       {},
  author =    {Bryan O'Sullivan},
  title =     {Criterion-1.1.0.0},
  URL = {http://www.serpentine.com/criterion},
  OPTmonth =     {},
  year =      {2015},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Crochemore-Hancart-Lecroq-2001,
  author = {Maxime Crochemore and Christophe Hancart and Thierry Lecroq},
  title = {A unifying look at the {Apostolico-Giancarlo}
             string matching algorithm},
  journal = {J.~Discrete Algs.},
  journalURL = {http://www.dcs.kcl.ac.uk/journals/jda/},
  newJournalURL = {http://www.elsevier.com/inca/publications/store/6/7/2/7/1/1/index.htt},
  year = {2003?},
  volume = 2,
  number = 1,
  pages = {?},
  month = {?},
  CiteSeer = {http://citeseer.nj.nec.com/crochemore00unifying.html},
  URL = {http://www-igm.univ-mlv.fr/~lecroq/articles/jda1.ps.gz},
  WKloc = {A-1464},
  note = {to appear}
}

@Unpublished{Crossley-Sheperdson-1993,
  author = {John N. Crossley and Jonh C. Sheperdson},
  title = {Extracting Programs from Proofs by an Extension of
		  the {Curry-Howard} Process},
  year = 1993,
  WKloc = {B-0013},
  abstract = {In this paper we provide a general framework for
		  extracting programs from proofs directly. By this we
		  mean that we give (i) an explicit transformation,
		  without intermediate coding, from conventional
		  systems such as predicate calculus into programs,
		  (ii) a direct proof of strong normalization (which
		  shows that the process of reductionor compilation of
		  the programs terminates), (iii) a new concept of
		  verifiability which encompasses Prawitz's [1971]
		  notion of validity and the familiar notion of truth
		  in a model (see e.g.~Mendelson [1964]), (iv) a
		  general approach to existence and disjunction
		  properties, and definability of functions and we
		  canvass (v) the practicalities of implementing our
		  procedures. One of our aims is to develop a method
		  of producing programs which are automatically
		  correct. This correctness is guaranteed by the proof
		  which gives rise to the program.

                  Although our work utilizes the natural deduction
		  approach, the methodology can easily be transferred
		  to e.g.~Gentzen- or Hilbert-style systems of logic.
		  Indeed, we believe the present approach is much more
		  ``user-friendly'' than earlier treatments.

                  In a sequel we plan to extend this work to higher
		  order logic.}
}

@InProceedings{Crow-DiVito-1996,
  author = {Judith Crow and Di Vito, Ben L.},
  title = {Formalizing Space Shuttle Software Rewuirements},
  booktitle = {Proc.\null{} Formal Methods in Software Practice {(FMSP '96)}},
  OPTcrossref = {},
  OPTkey = {},
  pages = {40--48},
  year = 1996,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1136},
  bibliographies = {SpecTech}
}

@Misc{Crow-Owre-Rushby-Shankar-Srivas-1995,
  author = {Judy Crow and Sam Owre and John Rushby and Natarajan Shankar and Mandayam Srivas},
  title = {A Tutorial Introduction to {PVS}, Presented at {WIFT '95: Workshop on Industrial-Strength Formal Specification Techniques, Boca Raton, Florida, April 1995}},
  year = 1995,
  month = JUN,
  URL = {http://www.csl.sri.com/sri-csl-fm.html},
  WKloc = {A-0817}
}

@Misc{Cruz-Tamassia-,
  author = {Isabel F. Cruz and Roberto Tamassia},
  title = {Graph Drawing Tutorial},
  howpublished = {Slides available on WWW},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1351}
}

@Article{Crvenkovic-Dolinka-Esik-2000,
  author = 	 {S. Crvenkovi{\'c} and I. Dolinka and Z. {\'E}sik},
  title = {The variety of {Kleene} algebras with conversion is not finitely based},
  journal = 	 TCS,
  year = 	 2000,
  volume =	 230,
  pages =	 {235--245},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Given an arbitrary set $A$, one obtains the full
     Kleene algebra of binary relations over $A$ by considering the
     operations of union, composition, reflexive-transitive closure,
     conversion, and the empty set and the identity relation as
     constants. Such algebras generate the variety of Kleene algebras
     (with conversion). As a result of a general analysis of
     identities satisfied by varieties having an involution operation,
     we prove that the variety of Kleene algebras with conversion has
     no finite equational axiomatization. In our argument we make use
     of the fact that the variety of Kleene algebras without
     conversion is not finitely based and that, relatively to this
     variety, the variety of Kleene algebras with conversion is
     finitely axiomatized.},
  annote = {Cites \cite{Esik-Bernatsky-1995} for including the inclusion
     $x \RELleq x \RELcomp \RELconv{x} \RELcomp x$
     (formulated as an equation using $+$)
     besides the involution equations for a finite axiomatisation of
     the variety $\RELconv{\mathcal{KA}}$ relative to
     the variety $\mathcal{KA}$ of Kleene algebras of \emph{relations}.}
}

@InProceedings{Crvenkovic-Madarasz-1989,
  author = {S. Crvenkovi{\'c} and R. Madarasz},
  title = {On Semigroup-Relation Algebras},
  booktitle = {Algebra and Logic, Proc.\null{} Conf., 1987},
  address = {Sarajevo, Yugoslavia},
  year = 1989,
  pages = {17--28},
  note = {Zbl 727.20044},
  bibliographies = {RelMiCS}
}

@Article{CsuhajVarju-Vaszil-2001,
  author = {Erzs{\'e}bet Csuhaj-Varj{\'u} and Gy{\"o}rgy Vaszil},
  title = {On Context-free Parallel Communicating Grammar Systems: Synchronization, Communication, and Normal Forms},
  journal = TCS,
  volume = 255,
  pages = {511--538},
  year = 2001,
  WKloc = {A-1104, doc/pap/BIB}
}

@Article{Cubric-Dybjer-Scott-199X,
  author = {D. {\u{C}}ubri{\'{c}} and P. Dybjer and Philip Scott},
  title = {Normalization and the Yoneda Embedding},
  journal = {Math. Struct. in Comp. Science},
  year = {199?},
  volume = {???},
  OPTnumber = {},
  OPTmonth = {},
  pages = {???},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0914},
  URL = {http://www.site.uottawa.ca/~phil/extra/papers/paper_final.pub.ps},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@inproceedings{Cuoq-KirchnerF-Kosmatov-Prevosto-Signoles-Yakobowski-2012,
 author = {Cuoq, Pascal and Kirchner, Florent and Kosmatov, Nikolai and Prevosto, Virgile and Signoles, Julien and Yakobowski, Boris},
 title = {Frama-C: a software analysis perspective},
 booktitle = {Proceedings of the 10th international conference on Software Engineering and Formal Methods},
 series = {SEFM'12},
 year = {2012},
 isbn = {978-3-642-33825-0},
 location = {Thessaloniki, Greece},
 pages = {233--247},
 numpages = {15},
 DOIURL = {http://dx.doi.org/10.1007/978-3-642-33826-7_16},
 doi = {10.1007/978-3-642-33826-7_16},
 acmid = {2404250},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
}

@Book{Curien-1986,
  author = {{Curien, P.-L.}},
  title = {Categorical Combinators, Sequential Algorithms, and
                 Functional Programming},
  series = {Research Notes in Theoretical Computer Science},
  publisher = {Pitman},
  address = {London},
  year = 1986,
  pages = {xviii+300},
  ISBN = {0-273-08722-3}
}

@Article{Curien-1986a,
  author = {P.-L. Curien},
  title = {Categorical Combinators},
  journal = {Information and Control},
  volume = 69,
  number = {1-3},
  pages = {189--254},
  year = 1986,
  keywords = {functional logic},
  abstract = {The author's main aim is to present the connection
                 between $\lambda$-calculus and cartesian closed
                 categories both in an untyped and purely syntactic
                 setting.}
}

@Book{Curien-1993,
  year = 1993,
  title = {Categorical Combinators, Sequential Algorithms,
                   and Functional Programming},
  publisher = BIRKH,
  series = {Progress in Theoretical Computer Science},
  pages = {XX, 403},
  edition = 2,
  author = {Pierre-Luis Curien},
  address = {Boston}
}

@Article{Curien-1993a,
  author =       {Pierre-Luis Curien},
  title =        {Substitution up to Isomorphism},
  journal =      FUNDI,
  year =         1993,
  WKloc =       {A-1738, doc/pap/BIB},
  bibliographies = {DepObj},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Curien-1998,
  author = {Pierre-Luis Curien},
  title = {Abstract Boehm trees},
  journal = MSCS,
  year = 1998,
  volume = 8,
  number = 6,
  WKloc = {doc/pap/BIB/Curien-1998.dvi.gz, doc/pap/BIB/Curien-1998.ps.gz}
}

@Unpublished{Curien-2008,
  author =       {Pierre-Louis Curien},
  title =        {Category theory: a programming language-oriented introduction},
  note =         {\unfinished},
  WKloc =       {A-1757},
  month =     OCT,
  year =      {2008},
  annote =    {Contents:
    \begin{enumerate}
    \item Categories, functors, natural transformations
    \item String diagrams
    \item Kan extensions
    \item Algebras, coalgebras, bialgebras
    \item Lambda calculus and categories
    \end{enumerate}}
}

@InProceedings{Curien-Ghelli-1991,
  title = {Subtyping + Extensionality: Confluence of $\beta\eta$top
		  reduction in~{$F_\leq$}},
  author = {Pierre-Louis Curien and Giorgio Ghelli},
  pages = {731--749},
  crossref = {TACS1991},
  abstract = {We contribute to the syntactic study of~${\bf F}_\leq$, a
		  variant of second order $\lambda$-calculus~{\bf F}
		  which appears as a paradigmatic kernel language for
		  polymorphism and subtyping.  The type system
		  of~${\bf F}_\leq$ has a maximum type Top and bounded
		  quantification.  We endow this language with the
		  familiar $\beta$-rules (for terms and types), to
		  which we add extensionality rules: the $\eta$-rules
		  (for terms and types), and a rule (top) which
		  equates all terms of type Top.  These rules are
		  suggested by the axiomatization of cartesian closed
		  categories.  We show that this theory $\beta\eta{\rm
		  top}_\leq$ is decidable, by exhibiting an
		  effectively weakly normalizing and confluent
		  rewriting system for it. Our proof of confluence
		  relies on the confluence of a corresponding
		  system~$\bf F_1$ (the extension of~{\bf F} with a
		  terminal type), and follows a general pattern that
		  we investigate for itself in a separate paper.}
}

@InProceedings{Curien-Hardin-Rios-1992,
  author = {P.-L. Curien and T. Hardin and A. R{\'\i}os},
  title = {Strong Normalization of Substitutions},
  WKloc = {A-0027},
  keywords = {inflation, CCL},
  authorsAddress = {C: CNRS and LIENS, curien\@dmi.ens.fr; H: INRIA
		  Rocquencourt and CNAM, hardin\@margaux.inria.fr,
		  A. Rios: LIENS, Paris, rios@dmi.ens.fr},
  abstract = {$\lambda\sigma$-calculus is an extended
		  $\lambda$-calculus where substitutions are handled
		  explicitly. We prove the strong normalization of its
		  subcalculus $\sigma$ which computes substitutions.},
  pages = {209--217},
  crossref = {MFCS1992}
}

@InProceedings{Curien-Herbelin-2000,
  author = {Pierre-Luis Curien and Hugo Herbelin},
  title = {The duality of computation},
  booktitle = {Proc. International Conference on Functional
    Programming, September 2000, Kyoto},
  year = 2000,
  publisher = {World Scientific},
  WKloc = {doc/pap/BIB/Curien-Herbelin-2000_icfp.dvi.gz, doc/pap/BIB/Curien-Herbelin-2000_icfp.ps.gz}
}

@InProceedings{Curien-Qian-Shi-1996,
  author = {R{\'e}gis Curien and Zhenyu Qian and Hui Shi},
  title = {Efficient Second-Order Matching},
  pages = {317--331},
  crossref = {RTA96},
  WKloc = {A-0554}
}

@Book{Currie-1999,
  author = {Ed Currie},
  title = {The Essence of {Z}},
  publisher = {Prentice Hall Europe},
  year = 1999,
  series = {The Essence of Computing Series},
  note = {ISBN 0-13-749839-X, 187 pages},
  URL = {http://www.pearsoneduc.com/titles/013749839x.html},
  WKloc = {owned, \lent{Cameron Hotchkies}}
}

@Article{Curry-1930,
  author = {H.B. Curry},
  title = {{Grundlagen der Kombinatorischen Logik}},
  journal = {American Journal of Mathematics},
  year = 1930,
  volume = 52,
  pages = {509--536, 789--834},
  keywords = {combinatory logic functional},
  bibliographies = {FP}
}

@Article{Curry-1941,
  author = {H.B. Curry},
  title = {The Paradox of Kleene and Rosser},
  journal = {Transactions of the American Mathematical Society},
  year = 1941,
  volume = 50,
  pages = {454--516},
  keywords = {combinatory logic functional}
}

@Article{Curry-1942,
  author = {H.B. Curry},
  title = {The Inconsistency of Certain Formal Logics},
  journal = {Journal of Symbolic Logic},
  year = 1942,
  volume = 7,
  pages = {115--117},
  keywords = {combinatory logic currys paradox functional}
}

@Article{Curry-1968,
  author = {H.B. Curry},
  title = {Recent Advances in Combinatory logic},
  journal = {Bulletin de la Soci\'et\'e Math\'ematique de Belgique},
  year = 1968,
  volume = 20,
  pages = {288--298}
}

@Book{Curry-Feys-1968,
  title = {Combinatory Logic, Volume I},
  author = {Curry, Haskell Brookes and Feys, Robert},
  year = 1958,
  series = {Studies in Logic and the Foundations of Mathematics},
  pages = {xvi+417},
  publisher = NoHo,
  address = {Amsterdam},
  note = {Second printing 1968},
  comments = {with two sections by William Craig},
  bibliographies = {FP}
}

@Book{Curry-Hindley-Seldin-1972,
  title = {Combinatory Logic, Volume II},
  author = {Curry, Haskell Brookes and Hindley, J. Roger and Seldin,
		  Jonathan P.},
  year = 1972,
  series = {Studies in Logic and the Foundations of Mathematics},
  pages = {xiv+520},
  publisher = NoHo,
  address = {Amsterdam},
  ISBN = {0-7204-2208-6},
  bibliographies = {FP},
  McMaster = {Mills Storage BC 135 .C865}
}

@PhDThesis{Curtis-1996,
  author = {Sharon Curtis},
  title = {A Relational Approach to Optimization Problems},
  school = {University of Oxford},
  year = 1996,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  OPTmonth = {},
  bibliographies = {RelMiCS},
  WKloc = {B-0099},
  keywords = {dynamic programming, greedy strategies, combinatorial optimization, graph calculus}
}

@InProceedings{Curtis-Lowe-1995,
  author = {Sharon Curtis and Gavin Lowe},
  title = {A Graphical Calculus},
  pages = {214--231},
  WKloc = {A-0458},
  crossref = {MPC1995},
  bibliographies = {RelMiCS, GraphCalc}
}

@Misc{Curtis-Lowe-1995a,
  author = {Sharon Curtis and Gavin Lowe},
  title = {Proofs with Graphs},
  year = 1995,
  month = OCT,
  WKloc = {A-0457},
  bibliographies = {RelMiCS, GraphCalc}
}

@Article{Czarnecki-Helsen-2006,
  author =       {Krzysztof Czarnecki and Simon Helsen},
  title =        {Feature-based survey of model transformation approaches},
  journal =      {IBM Systems Journal},
  year =         {2006},
  volume =    {45},
  number =    {3},
  pages =     {621-–645},
  DOI =  {10.1147/sj.453.0621},
  DOIURL =  {http://dx.doi.org/10.1147/sj.453.0621},
  WKloc =      {doc/pap/BIB},
  abstract =    {Model transformations are touted to play a key role
    in Model Driven Development$^{\mathrm{TM}}$.
    Although well-established standards for creating metamodels
    such as the Meta-Object Facility exist,
    there is currently no mature foundation
    for specifying transformations among models.
    We propose a framework for the classification
    of several existing and proposed model transformation approaches.
    The classification framework is given as a feature model
    that makes explicit the different design choices for model transformations.
    Based on our analysis of model transformation approaches,
    we propose a few major categories in which most approaches fit.}
}

@Article{DAgostino-1992,
  author={D'Agostino, Marcello},
  title={Are tableaux an improvement on truth-tables?},
  year={1992},
  issn={0925-8531},
  journal={Journal of Logic, Language and Information},
  volume={1},
  number={3},
  DOI={10.1007/BF00156916},
  DOIURL={http://dx.doi.org/10.1007/BF00156916},
  publisher={Kluwer Academic Publishers},
  keywords={tableaux; truth-tables; computational complexity; theorem-proving},
  pages={235--252},
  language={English},
  abstract = {We show that Smullyan's analytic tableaux
    cannot $p$-simulate the truth-tables.
    We identify the cause of this computational breakdown
    and relate it to an underlying semantic difficulty which is common
    to the whole tradition originating in Gentzen's sequent calculus,
    namely the dissonance between cut-free proofs and the Principle of Bivalence.
    Finally we discuss some ways in which this principle can be
    built into a tableau-like method without affecting its ``analytic'' nature.},
  WKloc = {A-1758}
}

@Manual{DIMACS-1993,
  title = 	 {Satisfiability Suggested Format},
  organization = {DIMACS},
  month =	 MAY,
  year =	 1993,
  keywords = 	 {SAT file format},
  WKloc = 	 {A-1628}
}

@Misc{DUMA,
  author =	 {Hayati Ayg\"un},
  title =	 {{D.U.M.A.} --- Detect Unintended Memory Access},
  howpublished = {\textsf{http://duma.sourceforge.net/}},
  year =	 2005
}

@InCollection{Dahl-1994,
  author = {Ole-Johan Dahl},
  title = {Monitors Revisited},
  crossref = {Roscoe-1994},
  pages = {93--103},
  chapter = 6,
  OPTnote = {},
  OPTannote = {}
}

@Article{DalLago-Martini-2012,
  author =       {Ugo Dal Lago and Simone Martini},
  title =        {On Constructor Rewrite Systems and the Lambda Calculus},
  journal =      LMCS,
  year =         2012,
  volume =    8,
  number =    {3:12},
  paperId = 12,
  DOI = {10.2168/LMCS-8(3:12)2012},
  DirectURL = {http://www.lmcs-online.org.libaccess.lib.mcmaster.ca/ojs/viewarticle.php?id=588},
  pages =     {1--27},
  abstract  =    {We prove that orthogonal constructor term rewrite systems and lambda-calculus with weak (i.e., no reduction is allowed under the scope of a lambda-abstraction) call-by-value reduction can simulate each other with a linear overhead. In particular, weak call-by- value beta-reduction can be simulated by an orthogonal constructor term rewrite system in the same number of reduction steps. Conversely, each reduction in a term rewrite system can be simulated by a constant number of beta-reduction steps. This is relevant to implicit computational complexity, because the number of beta steps to normal form is polynomially related to the actual cost (that is, as performed on a Turing machine) of normalization, under weak call-by-value reduction. Orthogonal constructor term rewrite systems and lambda-calculus are thus both polynomially related to Turing machines, taking as notion of cost their natural parameters.}
}

@TechReport{Dami-1993,
  author = {Laurent Dami},
  editor = {D. Tsichritzis},
  title = {The {HOP} Calculus},
  institution = {Centre Universitaire d'Informatique, University of
                 Geneva},
  type = {Visual Objects},
  pages = {151--212},
  month = jul,
  year = 1993,
  keywords = {olit hop osg vo93},
  abstract = {A new calculus is presented for modelling
                 object-oriented constructs. The main features of the
                 calculus are: interaction by names, unification of
                 types and values, operators for combinations and
                 alternations of terms. With a limited set of syntactic
                 constructs a surprisingly large range of features can
                 be modelled, including not only object-oriented
                 constructs but also abstract data types, recursive and
                 dependent types and concurrency. The syntax and
                 operational semantics of the calculus are presented,
                 together with numerous programming examples. Through
                 comparisons with the lambda calculus, we argue that
                 interaction by names is fundamentally more expressive
                 than traditional functional abstraction and
                 application. In particular, it becomes possible to
                 treat the parameters of an abstraction independently
                 while doing a fixed-point operation, which is of great
                 convenience for modelling object-oriented systems.
                 Finally, an approach to type-checking is presented.
                 Although not totally mature yet, it shows how types and
                 values are merged in a single preorder over terms, and
                 how this preorder can be used to prevent type errors.}
}

@InProceedings{Damm-Helbig-1994,
  author = {W. Damm and J. Helbig},
  title = {Linking Visual Formalisms: A Compositional Proof
		  System for Statecharts Based on Symbolic Timing
		  Diagrams},
  crossref = {PROCOMET94},
  pages = {337--356},
  keywords = {Compositionality in Concurrency}
}

@InProceedings{Dams-Grumberg-Gerth-1994,
  author = {D. Dams and O. Grumberg and R. Gerth},
  title = {Abstract Interpretation of Reactive Systems:
		  Abstractions Preserving $ \forall\mbox{CTL}^* $, $
		  \exists\mbox{CTL}^* $ and $ \mbox{CTL}^* $},
  crossref = {PROCOMET94},
  pages = {561--581},
  keywords = {$ \mbox{CTL}^* $; Model Checking; Nondetermism;
		  Transition System; Abstract Interpretation;
		  Preservation; Approximation; Simulation}
}

@inproceedings{Danielsson-2010,
  author    = {Nils Anders Danielsson},
  title     = {Total parser combinators},
  crossref = {ICFP2010},
  OPTbooktitle = {ICFP},
  OPTyear      = {2010},
  pages     = {285--296},
  DOI    = {10.1145/1863543.1863585},
  DOIURL    = {http://doi.acm.org/10.1145/1863543.1863585},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@inproceedings{Danielsson-2013,
  author    = {Nils Anders Danielsson},
  title     = {Correct-by-construction pretty-printing},
  OPTcrossref = {DTP2013},
  booktitle = {Dependently-typed programming, {DTP 2013}},
  year      = {2013},
  pages     = {1--12},
  DOI    = {10.1145/2502409.2502410},
  DOIURL    = {http://dx.doi.org/10.1145/2502409.2502410},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@InBook{Danielsson-Norell-2011,
  author="Danielsson, Nils Anders and Norell, Ulf",
  editor="Scholz, Sven-Bodo and Chitil, Olaf",
  title="Parsing Mixfix Operators",
  bookTitle="Implementation and Application of Functional Languages: 20th International Symposium, IFL 2008, Hatfield, UK, September 10-12, 2008. Revised Selected Papers",
  year="2011",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="80--99",
  isbn="978-3-642-24452-0",
  DOI="10.1007/978-3-642-24452-0_5",
  DOIURL="http://dx.doi.org/10.1007/978-3-642-24452-0_5"
}

@proceedings{ICFP2010a,
  editor    = {Paul Hudak and
               Stephanie Weirich},
  title     = {Proceeding of the 15th ACM SIGPLAN international conference
               on Functional programming, ICFP 2010, Baltimore, Maryland,
               USA, September 27-29, 2010},
  booktitle = {ICFP},
  publisher = {ACM},
  year      = {2010},
  isbn      = {978-1-60558-794-3},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@TechReport{Danvy-1998a,
  author = {Olivier Danvy},
  title = {Functional Unparsing},
  year = 1998,
  month = MAY,
  institution = {BRICS},
  number = {RS-98-12},
  WKloc = {A-0532}
}

@Article{Danvy-Glueck-Thiemann-1998,
  author = {Olivier Danvy and Robert Gl{\"u}ck and Peter Thiemann},
  title = {1998 Symposium on Partial Evaluation},
  journal = {ACM Computing Surveys},
  year = 1998,
  volume = 30,
  number = 2,
  month = JUN,
  pages = {285 - 290},
  WKloc = {A-0902},
  note = {Articles of this symposium are Vol.~3es}
}

@InProceedings{Danvy-Rose-1998,
  author = {Olivier Danvy and Kristoffer H{\/{o}}gsbro Rose},
  title = {Higher-Order Rewriting and Partial Evaluation},
  crossref = {RTA-1998},
  pages = {},
  OPTabstract = {},
  keywords = {CRS},
  WKloc = {A-0476}
}

@InProceedings{Danvy-Vestergaard-1996,
  author = {Olivier Danvy and Ren{\'e} Vestergaard},
  title = {Semantics-Based Compiling: A Case Study in Type-Directed Partial Evaluation},
  crossref = {PLILP1996},
  pages = {182--197},
  OPTabstract = {},
  WKloc = {A-0445}
}

@Article{Darlington-1971,
  author = {Jared L. Darlington},
  title = {A partial mechanization of second-order logic},
  pages = {91--100},
  booktitle = {Machine Intelligence},
  address = {New York},
  year = 1971,
  volume = 6,
  publisher = {American Elsevier}
}

@InProceedings{Darlington-1973,
  author = {Jared L. Darlington},
  title = {Automatic Program Synthesis in Second-Order Logic},
  pages = {537--542},
  ISBN = {0-934613-58-3},
  editor = {Nils J. Nilsson},
  booktitle = {Proceedings of the 3rd International Joint Conference
                 on Artificial Intelligence},
  address = {Standford, CA},
  month = aug,
  year = 1973,
  publisher = {William Kaufmann}
}

@InProceedings{Darlington-While-1987,
  author = {John Darlington and Lyndon While},
  title = {Controlling the Behaviour of Functional Language Systems},
  pages = {278--300},
  crossref = {FPCA-1987},
  keywords = {temporal language SIAN},
  abstract = {We present a methodology that allows temporal
		  constraints to be imposed on the behaviour of
		  term-rewriting systems and in particular allows the
		  evaluation order of pure functional programs to be
		  constrained. This permits the use of these languages
		  in applications such as operating systems and
		  real-time control, where control of evaluation order
		  is necessary to achieve a correct implementation.

		  This control is achieved, in the declarative spirit,
		  by utilising a temporal logic that allows the user
		  to specify, at a high level, the temporal conditions
		  his program must satisfy. The temporal logic is a
		  meta-language which talks about events occurring in
		  the execution of the associated program. The program
		  together with the temporal constraints is then
		  automatically transformed to produce a single
		  program that is guaranteed to behave correctly on
		  any implementation. These techniques are of
		  particular interest when developing programs for
		  parallel asynchronous machines such as ALICE [Cripps
		  et al, 1987] that can exhibit genuinely
		  non-deterministic evaluation (even of deterministic
		  programs).

		  We detail the temporal specification language, the
		  transformations used and their implementation and
		  give an example showing the use of the methodology
		  with an illustration of its execution on the
		  parallel graph reduction machine ALICE.},
  bibliographies = {RelMiCS}
}

@Article{Darragh-Cleary-Witten-1993,
  author = {Darragh and Cleary and Witten},
  title = {Bonsai: A Compact Representation of Trees},
  journal = {Software Practice and Experience},
  year = 1993,
  volume = 23,
  number = 3,
  pages = 277,
  month = MAR
}

@InProceedings{Dassow-1994,
  author = {J\"urgen Dassow},
  title = {Decision Problems for Edge Grammars},
  crossref = {MFCS94},
  pages = {286--295},
  abstract = {Edge grammars generate pairs of words which are
		  interpreted as edges of graphs. We prove that it is
		  undecidable whether or not all graphs generated by a
		  linear edge grammar are connected Hamiltonian,
		  Eulerian, planar, $k$-node-colourable, bipartite and
		  trees, respectively. Moreover, for linear edge
		  grammars, it is undecidablewhether or not the
		  generated set of graphs contains a connected or
		  Hamiltonian or Eulerian graph.}
}

@Misc{Data.Vector,
  author =    {Roman Leshchinskiy},
  title =     {The vector package},
  OPThowpublished = {Haskell package available at},
  howpublished = {\url{http://hackage.haskell.org/package/vector}},
  month =     SEP,
  year =      2013,
  OPTnote =      {Version 0.10.9.1, last accessed 2014-05-15}
}

@InProceedings{Dau-HerethCorreia-2003b,
  author = 	 {Frithjof Dau and Hereth Correia, Joachim},
  title = 	 {Nested Concept Graphs with Cuts: Mathematical Foundations},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1676, doc/pap/BIB},
  OPTbibliographies = {RelMiCS},
  anote = {``nested'' brings set (relation) comprehensions}
}

@Article{Dave-2003,
 author = {Maulik A. Dave},
 title = {Compiler Verification: A Bibliography},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {28},
 number = {6},
 year = {2003},
 issn = {0163-5948},
 pages = {2--2},
 doi = {http://doi.acm.org/10.1145/966221.966235},
 publisher = {ACM Press},
 WKloc = {doc/pap/BIB},
 bibliography = {Anand}
}

@InProceedings{Davenport-1990,
  author = {J. H. Davenport},
  title = {Current Problems in Computer Algebra Systems Design},
  crossref = {DISCO90},
  pages = {1--9}
}

@InProceedings{Davenport-Trager-1990,
  author = {J. Davenport and B.M. Trager},
  title = {Scratchpad's View of Algebra {I}: Basic Commutative Algebra},
  crossref = {DISCO90},
  pages = {40--54}
}

@Book{Davey-Priestley-1990,
  author = {B. A. Davey and H. A. Priestley},
  title = {Introduction to Lattices and Order},
  year = 1990,
  publisher = CambridgeUP,
  UniBwM = {MAT160/T13677},
  McMaster = {QA 171.5 .D38 1990},
  contents = {1 Ordered Sets
		2 Lattices and complete lattices
		3 CPOs, algebraic lattices and domains
		4 Fixpoint theorems
		5 Lattices as algebraic structures
		6 Modular and distributive lattices
		7 Boolean algebras and their applications
		8 Representation theory: the finite case
		9 Ideals and filters
		10 Representation theory: the general case
		11 Formal concept analysis
		Appendix: further reading},
  bibliographies = {RelMiCS}
}

@Book{Davie-1992,
  author = {Davie, Antony J.T.},
  title = {An Introduction to Functional Programming Systems
		  Using Haskell},
  publisher = {Cambridge University Press},
  year = 1992,
  ISBN = {0-521-25830-8, 0-521-27724-8},
  volume = 27,
  series = {Cambridge Computer Science Texts},
  UniBwM = {INF400/W14106},
  abstract = {Here is an introduction to functional programming and
               its associated systems. A unique feature is its use of
               the language Haskell for teaching both the rudiments and
               the finer points of the functional technique. haskell is
               a new, internationally agreed and accepted functional
               language that is designed for teaching, research and
               applications, that has a complete formal description,
               that is feely available, and that is based on ideas that
               have a wide consensus. Thus is encapsulates some of the
               main thrusts of functional programming itself, which is
               a style of programming designed to confront the software
               crisis directly. Programs written in functional
               languages can be built up from smaller parts, and they
               can also be proved correct, important when software has
               to be reliable. Moreover, a certain amount of
               parallelism can be extracted from functional languages
               automatically. This book serves as an introduction both
               to functional programming and Haskell, and will be most
               useful to students, teachers and researchers in either
               of these areas. An especially valuable feature are the
               chapters on programming and implementation, along with a
               large number of exercises.},
  bibliographies = {FP}
}

@Article{Davies-1971,
   author = 	{R. O. Davies and A. Hayes and G. Rousseau},
   title = 	{Complete Lattices and the Generalized {Cantor} Theorem},
   journal = 	{Proceedings of the AMS},
   year = 	1971,
   volume = 	27,
   pages = 	{253--258}
}

@PhDThesis{Davis-1950,
  author = {Chandler Davis},
  title = {Lattices and Modal Operators},
  school = Harvard,
  address = {Cambridge, MA},
  year = 1950,
  bibliographies = {RelMiCS}
}

@Article{Davis-1954,
  author = {Chandler Davis},
  title = {Modal Operators, Equivalence Relations, and Projective
		Algebras},
  journal = AJM,
  volume = 76,
  year = 1954,
  pages = {747--762},
  bibliographies = {RelMiCS}
}

@Book{Davis-1958,
  author = {Martin Davis},
  title = {Computability and Unsolvability},
  publisher = McGraw,
  address = {New York},
  year = 1958,
  bibliographies = {RelMiCS}
}

@Book{Davis-1964,
  author = {Martin Davis},
  title = {The Undecidable},
  publisher = {??},
  year = 1964,
  OPTbibliographies = {RelMiCS}
}

@Article{Davis-1966,
  author = {A. S. Davis},
  title = {An Axiomatization of the Algebra of Transformations over a Set},
  journal = MANN,
  volume = 164,
  year = 1966,
  pages = {372--377},
  bibliographies = {RelMiCS}
}

@Article{Davis-1982,
  author = {Martin Davis},
  title = {Why {G\"odel} didn't have {Church's} thesis},
  journal = IandC,
  year = {1982 (1952?)},
  volume = 54
}

@Article{Davis-1987,
  author = {E. Davis},
  title = {Constraint Propagation with Interval Labels},
  journal = AI,
  volume = 32,
  pages = {281--331},
  year = 1987,
  bibliographies = {RelMiCS}
}

@InProceedings{Davison-1993,
  author = {Andrew Davison},
  title = {Parsing with DCG-Terms},
  crossref = {LPAR93},
  pages = {98--109},
  WKloc = {A-0122},
  abstract = {Definite Clause Grammars (DCGs) are powerful adjuncts to
             prolog, permitting the clear and succinct encoding of parsers
             and other grammar related programs. Normally, a DCG is
             incorporated into a prolog program by being treated as a
             apecial set predicates. This paper utilises DCGs in a different
             way, by augmenting the unification algorithm with DCG-terms.
             More importantly, this notation allows token lists to be
             constructed and deconstructed by reference to nonterminal
             names. Examples in the parsing domain are used to illustracte
             these ideas, and show how programs are improved. In particular,
             seraches for sublists of tokens of different types can be
             expressed simply, without having to complicate the underlying
             DCG. Also, the DCG part of a program is more independent of the
             other code, which makes both easier to debug and modify.}
}

@InProceedings{Dawar-Hella-1994,
  title = {The Expressive Power of Finitely Many Generalized Quantifiers},
  author = {Anuj Dawar and Lauri Hella},
  pages = {20--29},
  crossref = {LICS9},
  abstract = {We consider extensions of first order logic (FO) and fixed
      point logic (FP) by means of generalized quantifiers in the sense of
      Lindstr\"om [1966]. We show that adding a finite set of such
      quantifiers to FP fails to capture PTIME, even over a fixed
      signature. This strengthens results of Hella [1992] and Kolaitis and
      Vaananen [1994]. We also prove a stronger version of this result for
      PSPACE, which enables us to establish a weak version of a conjecture
      formula ted by Kolaitis and Vardi [1992]. These results are obtained
      by defining a notion of element type for bounded variable logics with
      finitely many generalized quantifiers. Using these, we characterize
      the classes of finite structures over which the infinitary logic with
      bounded number of variables extended by a finite set of generalized
      quantifiers~{\bf Q} is no more expre ssive than first order logic
      extended by the quantifers in~{\bf Q}.}
}

@Article{Dawson-2014,
  author = {Dawson, Ryan},
  title = {Why Making Software is So Difficult},
  journal = {SIGSOFT Softw.\null{} Eng.\null{} Notes},
  issue_date = {July 2014},
  volume = {39},
  number = {4},
  month = aug,
  year = {2014},
  issn = {0163-5948},
  pages = {1--5},
  numpages = {5},
  DOIURL = {http://doi.acm.org/10.1145/2632434.2632442},
  DOI = {10.1145/2632434.2632442},
  acmid = {2632442},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {agile, design, development, estimation, lifecycle, waterfall},
  WKloc = {doc/pap/BIB},
  abstract = {This paper develops a metaphor that expresses
    what is so difficult about software production.
    The purpose of the metaphor is
    to be applicable to real-world software production situations,
    especially on larger or more complex projects,
    where decisions are made in ways that neglect
    some of the aspects of software production that make it so difficult,
    especially that seeing how to solve a problem
    can be immensely difficult to do from the beginning
    and as a result one sometimes has to abandon a whole approach and start again.
    The use of the metaphor is justified with reference to related literature
    and by application to certain characteristic situations.}
}

@InProceedings{Day-Launchbury-Lewis-1999,
  title = {Logical Abstractions in {Haskell}},
  author ={Nancy A. Day and John Launchbury and Jeff Lewis},
  crossref = {Haskell1999},
  URL = {http://www.cs.uwaterloo.ca/~nday/Papers/haskellworkshop99.html},
  WKloc = {doc/pap/BIB},
  abstract = {We describe a generalization of the Haskell Boolean
      type, which allows us to use existing decision procedures for
      reasoning about logical expressions. In particular, we have
      connected Haskell with a Binary Decision Diagram (BDD) package
      for propositional logic, and the Stanford Validity Checker for
      reasoning in quantifier-free, first-order logic. We have defined
      referentially transparent interfaces to these packages allowing
      the user to ignore the details of their imperative
      implementations. We found that having a tight connection between
      the provers and Haskell allows Haskell to serve as a
      meta-language enhancing the capabilities of the provers. We
      illustrate the use of these packages for reasoning about a sort
      algorithm and a simple microprocessor model. In the sort
      example, the parametric nature of Haskell's polymorphism is used
      to lift the result of the BDD analysis to arbitrary datatypes.}
}

@PhdThesis{DeMarchi-2003,
  author =       {De Marchi, Federico},
  title =        {Monads in Coalgebra},
  school =       {University of Leicester},
  year =         {2003},
  URL =      {http://gradworks.umi.com/U4/93/U493785.html},
  abstract = {Universal algebra has long been regarded as a fundamental tool in studying semantics of programming languages. Within this paradigm, one can formulate statements regarding the correctness of a program by looking at the interpretations of the code in any model for the language.

While this provides a description of finite computations, other models have to be introduced in order to provide a semantics for recursion and infinite computations in general. This leads to a study of rational and infinite terms. Such terms arise by a dual construction to that of the finite ones. namely, while the latter form an initial algebra, the former are a final coalgebra.

For this reason, it is natural to approach the study of infinite terms by dualising the categorical model of universal algebra. This leads to various different constructions, which are worth of investigation. In this thesis we approach two of them. In one case, we introduce the notions of cosignature, coequation and comodel, in the spirit of the theory of coalgebraic specification. In the second we focus on the properties of monads which can model infinitary computations. Such monads we call guarded, and include, amongst others, the monads of finite terms, infinite terms, rational terms and term graphs. As a byproduct of identifying this notion, we can solve algebraic systems of equation, which are an abstract counterpart to the notion of a recursive program scheme.

Many guarded monads we encounter are obtained by collecting, in an appropriate sense, a suitable family of coagebras. These examples are all instances of a general theorem we present, which tells under which conditions we can define a monad by a colimit operation, and when such comonads are guarded.

The level of abstraction allowed by the use of the categorical formalism allows us to instantiate some of the results in different categories, obtaining a monadic semantics for rational and infinite parallel term rewriting.}
}

@InProceedings{DeNicola-Labella-1994,
  author = {De Nicola, Rocco and Anna Labella},
  title = {A Completeness Theorem for Nondeterministic Kleene Algebras},
  crossref = {MFCS94},
  pages = {536--545},
  abstract = {A generalization of Kleene Algebras (structures with
		  $+,\dot,{}^*,0$ and $1$ operators) is considered to
		  take into account possible nondeterminism expressed
		  by the $+$ operator. It is shown that essentially
		  the same complete axiomatization of Salomaa is
		  obtained except for the elimination of the
		  distribution $P\dot(Q+R) = P\dot Q+P\dot R$ and the
		  idempotence law $P+P=P$. The main result is that an
		  algebra obtained from a suitable category of
		  labelled trees plays the same role as the algebra of
		  regular events. The algebraic semantics and the
		  axiomatization are the extended by adding $\Omega$
		  and the $||$ operator, and the whole set of laws is
		  used as a touchstone for starting a discussion over
		  the laws for deadlock, termination and divergance
		  proposed for models of concurrent systems.}
}

@Article{DeNicolaA-Missikoff-2016,
  author = {De Nicola, Antonio and Missikoff, Michele},
  title = {A Lightweight Methodology for Rapid Ontology Engineering},
  journal = CACM,
  issue_date = {March 2016},
  volume = {59},
  number = {3},
  month = feb,
  year = {2016},
  issn = {0001-0782},
  pages = {79--86},
  numpages = {8},
  DOIURL = {http://doi.acm.org/10.1145/2818359},
  DOI = {10.1145/2818359},
  acmid = {2818359},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {Ampersand},
  abstract = {UPON Lite focuses on users, typically domain experts
    without ontology expertise, minimizing the role of ontology engineers.},
}

@Article{DePaoli-Goscinski-1998,
  author = {De Paoli, D. and Goscinski, A.},
  title = {The RHODOS Migration Facility},
  journal = {Journal of Systems and Software},
  year = 1998,
  volume = 40,
  pages = {51--65},
  bibliographies = {ProcMig}
}

@Book{Dean-1997,
  author = {Neville Dean},
  title = {The Essence of Discrete Mathematics},
  publisher = {Prentice Hall Europe},
  year = 1997,
  series = {The Essence of Computing Series},
  note = {ISBN 0-13-345943-8, 197 pages},
  URL = {http://www.pearsoneduc.com/titles/0133459438.html},
  WKloc = {owned, \lent{Mark Lawford}}
}

@InProceedings{Dean-Ghemawat-2004,
  author = 	 {Jeffrey Dean and Sanjay Ghemawat},
  title = 	 {MapReduce: Simplified Data Processing on Large Clusters},
  booktitle = {OSDI'04: Sixth Symposium on Operating System Design and Implementation,
San Francisco, CA, December, 2004},
  year = 	 2004,
  URL = {http://labs.google.com/papers/mapreduce.html},
  WKloc = {doc/pap/BIB},
  abstract = 	 {MapReduce is a programming model and an associated
                  implementation for processing and generating large
                  data sets. Users specify a map function that
                  processes a key/value pair to generate a set of
                  intermediate key/value pairs, and a reduce function
                  that merges all intermediate values associated with
                  the same intermediate key. Many real world tasks are
                  expressible in this model, as shown in the paper.

                  Programs written in this functional style are
                  automatically parallelized and executed on a large
                  cluster of commodity machines. The run-time system
                  takes care of the details of partitioning the input
                  data, scheduling the program's execution across a
                  set of machines, handling machine failures, and
                  managing the required inter-machine
                  communication. This allows programmers without any
                  experience with parallel and distributed systems to
                  easily utilize the resources of a large distributed
                  system.

                  Our implementation of MapReduce runs on a
                  large cluster of commodity machines and is highly
                  scalable: a typical MapReduce computation processes
                  many terabytes of data on thousands of
                  machines. Programmers find the system easy to use:
                  hundreds of MapReduce programs have been implemented
                  and upwards of one thousand MapReduce jobs are
                  executed on Google's clusters every day.}
}

@Article{Dean-Ghemawat-2008,
 author = {Jeffrey Dean and Sanjay Ghemawat},
 title = {MapReduce: Simplified Data Processing on Large Clusters},
 journal = {Commun. ACM},
 volume = {51},
 number = {1},
 year = {2008},
 issn = {0001-0782},
 pages = {107--113},
 doi = {http://doi.acm.org/10.1145/1327452.1327492},
 WKloc = {doc/pap/BIB},
 publisher = {ACM},
 address = {New York, NY, USA},
 annote = {Google}
}

@InProceedings{Debbabi-Faour-Tawbi-1996,
  author =       "M. Debbabi and A. Faour and N. Tawbi",
  title =        "Efficient Type-Based Control-Flow Analysis of
                     Higher-Order Concurrent Programs",
  booktitle =    "Proceedings of the International Workshop on
		  Functional and Logic Programming, IFL '96",
  publisher = Springer,
  series = LNCS,
  volume = 1268,
  pages = {247--266},
  month = SEP,
  year = 1996
}

@Article{Dechter-Pearl-1988,
  author = {R. Dechter and J. Pearl},
  title = {Network-based Heuristics for
		Constraint-satisfaction Problems},
  journal = AI,
  volume = 34,
  year = 1988,
  pages = {1--38},
  bibliographies = {RelMiCS}
}

@Book{Dedekind-1963,
  author = {Dedekind, Richard},
  title = {Essays on the Theory of Numbers.},
  publisher = Dover,
  year = 1963,
  note = {reprinted: Open Court Publishing Company, 1901. Translation
	  by W.W.\null{} Beman of \emph{Stetigkeit und irrationale Zahlen}
	  (1872) and \emph{Was sind und was sollen die Zahlen?} (1888)},
  bibliographies = {RelMiCS}
}

@Misc{Degano-Gorrieri-Rosolini-,
  author = {P. Degano and R. Gorrieri and G. Rosolini},
  title = {Graphs and Event Refinement},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  WKloc = {A-1256},
  abstract = {A general notion of refinement of event structures is
      proposed, that allows to refine freely both the events and the
      relations of causality and conflict. It is based on the presentation
      of an event structure as a symmetric graph of posets, and of its
      configurations as sections of graph homomorphisms.}
}

@TechReport{Degtyarev-Voronkov-1995,
  author = {Anatoli Degtyarev and Andrei Voronkov},
  title = {Reduction of Second-Order Unification to
		  Simultaneous Rigid {$E$}-Unification},
  institution = {Computing Science Department, Uppsala University},
  year = 1995,
  type = {UPMAIL Technical Report},
  number = 109,
  month = JUN,
  authorsAddress = {\{anatoli,voronkov\}\@csd.uu.se},
  WKloc = {A-0403},
  abstract = {The simultaneous rigid $E$-unification problem is
		  used in automated reasoning with equality. In our
		  previous paper we proved the undecidability of this
		  problem by reduction of monadic
		  semi-unification. Here we give a simpler and more
		  intuitive proof of the undecidability by reduction
		  of second-order unification.},
  annote = {``bound variables'' in second order languages are
		  really zero-ary placeholders as successors of
		  schemavariables, here called ``free variables''.}
}

@Misc{Dekker-1994,
  author = {Anthony H. Dekker},
  title = {The Game of {Life}: A {CLEAN} Programming Tutorial
		  and Case Study},
  year = 1994,
  authorsAddress = {tdekker\@iscs.nus.sg},
  file = {~kahl/doc/pap/LIFE},
  WKloc = {A-0245},
  abstract = {This report presents a tutorial for the CLEAN
		  Functional Programming Language, in the form of a
		  stepwise development of two programs for animating
		  Conway's Game of Life. The power of the novel and
		  elegant input/output and Graphical User Interface
		  (GUI) facilities of CLEAN are demonstrated by this
		  example, which contains most of the features
		  required in useful applications
		  software. Suggestions on suitable programming style
		  are also included.}
}

@Article{Dekkers-1988,
  WKloc = {A-0033},
  abstract = {Consider types built up from a base type $0$ using
		  the operation $\tfun$. A type $\sigma$ is reducible
		  to a type $\tau$, notation $\sigma \leq \tau$, iff
		  there exists a closed term $M$ in $\sigma \tfun \tau$
		  such that for all closed $N_1, N_2$ in $\sigma$ we
		  have $N_1 =_{\beta\eta} N_2 \zeq MN_1 =_{\beta\eta} MN_2$.
		  Two types are equivalent iff each is reducible to
		  the other. In \cite{Statman-1980} is
		  shown that the equivalence classes of types are well
		  ordered in type $\omega+2$ or $\omega+3$. The paper
		  does not decide if it is $\omega+2$ or $\omega+3$
		  because it is not clear whether $\mu \equiv
		  (((0\tfun 0)\tfun 0)\tfun 0)\tfun 0 \tfun 0$ and
		  $\nu \equiv (0\tfun 0)\tfun(0\tfun 0)\tfun 0 \tfun 0$
		  are equivalent. We show that $\mu$ and $\nu$ are not
		  equivalent and conclude that the equivalence classes
		  are ordered in type $\omega+3$.},
  year = 1988,
  volume = 77,
  title = {Reducibility of Types in Typed Lambda Calculus:
		  Comment on a Paper by {Richard} {Statman}},
  pages = {131--137},
  number = 2,
  month = {may},
  journal = {Information and Computation},
  author = {Wil Dekkers}
}

@Misc{DelCastillo-Glaesser-1998,
  author = {Del Castillo, Giuseppe and Uwe Gl{\"a}sser},
  title = {Machine-Supported Execution and Validation of High-Level Abstract State Machine Models},
  year = 1998,
  WKloc = {A-0696},
  annote = {ASM, evolving algebras}
}

@InProceedings{DelGobbo-Mili-2003,
  author = {Del Gobbo, D. and A. Mili},
  title = {An Application of Relational Algebra: Specification of a Fault Tolerant Flight Control System},
  booktitle = {Electronic Notes in Theoretical Computer Science},
  volume = 44,
  issue = 3,
  publisher = {Elsevier Science Publishers},
  editor = {Wolfram Kahl and David L. Parnas and Gunther Schmidt},
  year = 2003,
  bibliographies = {RelMiS2001, RelMiCS}
}

@InProceedings{Delest-Dubernard-1994,
  author = {M. Delest and J-Ph. Dubernard},
  title = {q-Grammars: Results, Implementation},
  crossref = {STACS1994},
  pages = {377--388}
}

@Article{Delobel-1978,
  author = {C. Delobel},
  title = {Normalization and Hierarchical Dependencies in the Relational
      Data Model},
  journal = ACM-TDS,
  volume = 2,
  number = 3,
  year = 1978,
  pages = {201--222},
  bibliographies = {RelMiCS}
}

@InProceedings{Demeyer-Mens-Wermelinger-2001,
  author = {Serge Demeyer and Tom Mens and Michel Wermelinger},
  title = {Towards a Software Evolution Benchmark},
  booktitle = {Proceedings of International Workshop on Principles of Software Evolution {(IWPSE2001), September 2001}},
  year = 2001,
  URL = {http://win-www.uia.ac.be/u/sdemey/Pubs/Deme01n/},
  WKloc = {doc/pap/BIB},
  abstract = {Case-studies are extremely popular in rapidly evolving
      research disciplines such as software engineering because they allow
      for a quick but fair assessment of new techniques. Unfortunately, a
      proper experimental set-up is rarely the case: all too often
      case-studies are based on a single small toy-example chosen to favour
      the technique under study. Such lack of scientific rigor prevents
      fair evaluation and has serious consequences for the credibility of
      our field. In this paper, we propose to use a representative set of
      cases as a benchmark for comparing various techniques dealing with
      software evolution. We hope that this proposal will launch a
      consensus building process that eventually must lead to a
      scientifically sound validation method for researchers investigating
      reverse- and re-engineering techniques.}
}

@InProceedings{Demoen-GarciaDeLaBanda-Stuckey-1999,
  author = {B. Demoen and Garcia de la Banda, M. and P.J. Stuckey},
  title = {Type constraint solving for parametric and ad-hoc polymorphism},
  booktitle = {Proceedings of the {22nd Australian Computer Science Conference}},
  pages = {217--228},
  year = 1999,
  editor = {J. Edwards},
  month = JAN,
  publisher = Springer,
  URL = {http://www.csse.monash.edu.au/~mbanda/hal/index.html#publications}
}

@Article{Demri-Orlowska-1995,
  author = {Demri, S. and Ewa Orlowska},
  title = {Logical Analysis of Demonic Nondeterministic Programs},
  journal = TCS,
  year = 1996,
  volume = 166,
  month = DEC,
  note = {To appear},
  bibliographies = {RelMiCS}
}

@Book{Demri-Orlowska-2002,
  author = {Stephane Demri and Ewa Orlowska},
  title = {Incomplete Information: Structure, Inference, Complexity},
  publisher = Springer,
  year = 2002,
  URL = {http://www.springer.de/cgi/svcat/search_book.pl?isbn=3-540-41904-7},
  series = {EATCS Monographs in Theoretical Computer Science},
  ISBN = {3-540-41904-7},
  bibliographies = {RelMiCS},
  abstract = {This monograph presents a systematic, exhaustive and
      up-to-date overview of formal methods and theories for data analysis
      and inference inspired by the concept of rough set. The book studies
      structures with incomplete information from the logical, algebraic
      and computational perspective. The formalisms developed are
      non-invasive in that only the actual information is needed in the
      process of analysis without external sources of information being
      required. The book is intended for researchers, lecturers and
      graduate students who wish to get acquainted with the rough set style
      approach to information systems with incomplete information.}
}

@Article{Demri-Orlowska-Rewitzky-1994,
  author = {Demri, S. and Orlowska, Ewa and Rewitzky, Ingrid},
  title = {Towards Reasoning about {Hoare} Relations.},
  journal = ANMAAI,
  volume = 12,
  year = 1994,
  pages = {265--289},
  annote = {Relational formalisation is given for a program logic for
      reasoning about {Hoare}-Style programs.},
  bibliographies = {RelMiCS}
}

@InProceedings{Demuth-Hussmann-Schmitz-Zschaler-1998,
  author = {B. Demuth and H. Hussmann and L. Schmitz and S. Zschaler},
  title = {Using a Framework to Teach OOP},
  year = 1998,
  booktitle = {{OOPSLA'98} Educators' Symposium, {Vancouver, Canada}},
  OPTpages = {},
  OPTeditor = {}
}

@InProceedings{Demuth-Hussmann-Schmitz-Zschaler-1999,
  author = {Birgit Demuth and Heinrich Hussmann and Lothar Schmitz and Steffen Zschaler},
  title = {{Erfahrungen mit einem frameworkbasierten Softwarepraktikum}},
  year = 1999,
  booktitle = {{Software-Engineering im Unterricht der Hochschulen SEUH '99}},
  pages = {21--30},
  editor = {Bj\"orn Dreher and Christoph Schulz and Debora Weber-Wulff},
  publisher = {Teubner-Verlag}
}

@InProceedings{Demuth-Schmitz-Zschaler-1999,
  author = {Birgit Demuth and Lothar Schmitz and Steffen Zschaler},
  title = {{Verkaufsanwendungen auf Basis des Anwendungsframeworks SalesPoint}},
  year = 1999,
  pages = {255--272},
  editor = {S. Maffeis and F. Toenniessen and C. Zeidler},
  booktitle = {{Erfahrungen mit Java. Projekte aus Industrie und Hochschule}},
  publisher = {dpunkt-Verlag}
}

@Misc{Denker-Meseguer-Talcott-,
  author = {G. Denker and J. Meseguer and C. Talcott},
  title = {Protocol Specification and Analysis in {Maude}},
  WKloc = {A-1157},
  bibliographies = {SpecTech}
}

@Article{Dennis-1980,
  author = {Jack B. Dennis},
  title = {Dataflow Supercomputers},
  journal = {Computer},
  volume = 13,
  number = 11,
  pages = {48--56},
  month = NOV,
  year = 1980
}

@InProceedings{Dennis-1985,
  author = {Jack B. Dennis},
  title = {Data Flow Computation},
  pages = {345--398},
  contents = {1. Models of Data Flow Computation (p. 346)\\
              2. Static Flow Computation (p. 355)\\
              3. Functional Programming for Data Flow Computation (p. 364)\\
              4. VIM: An Experimental Computer System to Support General Functional Programming (p. 370)\\
              5. The Scenario Theory for Non-determinate Computation (p. 382)},
  URL = {http://www.crpc.rice.edu/CRPC/newsletters/win98/pcp_dennis.html},
  OPTwkloc = {A-09},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Denniston-Melton-Saladi-Soudamini-2006,
  author = 	 {Jeffrey Denniston and Austin Melton and Srikanth Saladi and Jidesh Soudamini},
  title = 	 {{Galois} Connections and Functions Lifted from Relations},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1672},
  OPTbibliographies = {RelMiCS, PMC}
}

@InProceedings{Derichsweiler-1999,
  author = {Frank Derichsweiler},
  title = {{VisualWorks f\"ur Linux:
                   Eine industrietaugliche Smalltalk-Entwicklungsumgebung}},
  crossref = {LinuxTag-1999}
}

@InProceedings{Derichsweiler-1999b,
  author = {Frank Derichsweiler},
  title = {{Strategy Driven Program Transformation within the
        {\bf H}igher {\bf O}bject {\bf P}rogramming {\bf S}ystem {\sc HOPS}}},
  crossref = {Kirchhundem-1999},
  pages = {165--172},
  abstract = {In the context of the integrated abstract transformational
     program development environment \HOPS{} we present an approach to
     mechanising transformation of programs in term-graph representation
     by means of strategies and transformation programs.}
}

@InProceedings{Derichsweiler-1999c,
  author = {Frank Derichsweiler},
  title = {{Strategiegesteuerte Programmtransformation im Higher Object Programming System {\sc HOPS}}},
  crossref = {Informatiktage-1999},
  pages = {89--91},
  abstract = {Im Kontext der integrierten Entwicklungsumgebung HOPS,
      in welcher Programme in einer Termgraphrepräsentation dargestellt
      und auf hohem Abstraktionsniveau erstellt und transformiert werden,
      diskutieren wir unseren Ansatz zur Automatisierung der
      Programmtransformation durch die Verwendung von Strategien.}
}

@PhDThesis{Derichsweiler-2002,
  author = {Frank Derichsweiler},
  title = {{Strategiegesteuerte Transformation von Termgraphen}},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 2002,
  month = JUN,
  WKloc = {A-1344 (previous version), doc/pap/BIB}
}

@InProceedings{Derichsweiler-Kahl-2000,
  author = {Frank Derichsweiler and Wolfram Kahl},
  title = {Program Generation via Declarative Term Graph Attribution},
  crossref = {FMTools-2000PP},
  pages = {99--103},
  remark = {Extended Abstract}
}

@InProceedings{Derrick-Boiten-1998,
  author = {John Derrick and Eerke Boiten},
  title = {Testing Refinements by Refining Tests},
  crossref = {ZUM1998},
  pages = {265--283},
  bibliographies = {RelMiS},
  WKloc = {A-1327}
}

@InCollection{Dershowitz-Jouannaud-1990,
  author = {Nachum Dershowitz and Jean-Pierre Jouannaud},
  title = {Rewrite Systems},
  chapter = 6,
  crossref = {HBThCS-b},
  pages = {243--320},
  WKloc = {Q-003, B-0083}
}

@InProceedings{Dershowitz-Jouannaud-1990-x,
  author = {Nachum Dershowitz and Jean-Pierre Jouannaud},
  title = {Rewrite Systems},
  chapter = 6,
  pages = {243--320},
  WKloc = {Q-003},
  editor = {van Leeuwen, Jan},
  booktitle = {Handbook of Theoretical Computer Science},
  publisher = {Elsevier Science Publishers B.~V.},
  year = 1990,
  volume = {B}
}

@InProceedings{Dershowitz-Jouannaud-Klop,
  author = {Nachum Dershowitz and J.-P. Jouannaud and Jan Willem Klop},
  title = {More Problems in Rewriting},
  crossref = {RTA93},
  pages = {468--487},
  WKloc = {A-0136},
  abstract = {Two years ago, in the proceedings of the previous conference,
             we presented a list of open problems in the theory of rewriting
             [Dershowitz et al., 1991a]. This time, we report on progress
             made during the intervening time, and then list some new
             problems. (A few additional questions on the subject appear in
             the back of [Diekert, 1990].) We also mention a couple of
             long-standing open problems which have recently been answered.
             The last section contains a partisan list of interesting areas
             for future research. A new, comprehensive survey of the field
             is [Klop,1992].

             Five of the forty-four problems listed in
             [Dershowitz et al., 1991a] have been solved and some progress
             has been made on ten more.}
}

@InProceedings{Dershowitz-Kaplan-1989,
  author = {Nachum Dershowitz and St\'ephane Kaplan},
  title = {Rewrite, {\small Rewrite, {\small Rewrite, {\small
		  Rewrite, {\small Rewrite $\ldots$}}}}},
  crossref = {POPL1989},
  pages = {250--259},
  WKloc = {A-0208},
  abstract = {The theory of term rewriting systems has important
		  applications in abstract data type specifications
		  and functional programming languages. We begin here
		  a study of properties of systems that are not
		  necessarily terminating, but allow for infinite
		  derivations that have a limit. In particular, we give
		  conditions for the existence of a limit and for its
		  uniqueness.

                  Term rewriting systems are directed equations used
		  to compute by repeatedly replacing equal terms in a
		  given formula, as long as possible. As a programming
		  language, term rewriting systems have the full power
		  of Turing machines. For one approach to their use in
		  computing, see [O'Donnell-1977], [Chew-1980], and
		  [O'Donnell-1985]. The theory of rewriting is an
		  outgrowth of the study of the lambda calculus and
		  combinatory logics. For surveys of the theory of
		  rewriting, see [Huet-Oppen-1980], [Klop-1987], or
		  \cite{Dershowitz-Jouannaud-1990}; our notations are
		  consistent with the latter.}
}

@InProceedings{Dershowitz-Mitra-1993,
  WKloc = {?},
  keywords = {?},
  contents = {?},
  abstract = {?},
  title = {Higher-order unification for convergent systems},
  pages = {?},
  crossref = {HOA1993-PP},
  author = {Nachum Dershowitz and S. Mitra}
}

@PhDThesis{Desharnais-1989,
  author = {Jules Desharnais},
  title = {Abstract Relational Semantics},
  school = {School of Computer Science, McGill University,
                  Montr{\'e}al},
  year = 1989,
  month = JUL,
  WKloc = {A-0951},
  bibliographies = {RelMiCS}
}

@InProceedings{Desharnais-1997a,
  author = {Jules Desharnais},
  title = {Monomorphic Characterization of $n$-ary Direct Products},
  crossref = {RelMiCS1997-PP},
  pages = {359--368},
  bibliographies = {RelMiCS}
}

@Article{Desharnais-1999,
  author = {Jules Desharnais},
  title = {Monomorphic Characterization of $n$-ary Direct Products},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  pages = {275--288},
  month = DEC,
  WKloc = {A-0824},
  DOI = {10.1016/S0020-0255(99)00020-1},
  DOIURL = {http://dx.doi.org/10.1016/S0020-0255(99)00020-1},
  OLDURL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/34/abstract.html},
  bibliographies = {RelMiCS}
}

@InProceedings{Desharnais-2003,
  author = {Jules Desharnais},
  title = {{Kleene} ALgebras with Relations},
  crossref = {RelMiCS2003},
  pages = {8--20},
  WKloc = {A-1664},
  note = {(Invited Talk)},
  bibliographies = {RelMiCS, RelMiCS7}
}

@Article{Desharnais-Belkhiter-Sghaier-Tchier-Jaoua-Mili-Zaguia-1995,
  author = {Jules Desharnais and Nadir Belkhiter and Ben Mohamed
                  Sghaier, Salah and Fairouz Tchier and Ali Jaoua and
                  Ali Mili and Nejib Zaguia},
  title = {Embedding a Demonic Semilattice in a Relation Algebra},
  journal = TCS,
  volume = 149,
  year = 1995,
  pages = {333--360},
  bibliographies = {RelMiCS},
  WKloc = {A-0988}
}

@Misc{Desharnais-Frappier-Khedri-Mili-199X,
  author = {Jules Desharnais and Marc Frappier and Ridha
                  Khedri and Ali Mili},
  title = {Integration of Sequential Scenarios},
  year = {199?},
  WKloc = {A-0439},
  bibliographies = {RelMiCS}
}

@InCollection{Desharnais-Hodgson-Mullins-1997,
  author = {Jules Desharnais and Bernard Hodgson and John Mullins},
  title = {Linear Logic},
  chapter = 7,
  pages = {106--114},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@InProceedings{Desharnais-Jaoua-Belkhiter-Tchier-1992,
  author = {Jules Desharnais and Ali Jaoua and Nadir Belkhiter
                  and Fairouz Tchier},
  title = {Data Refinement in a Relation Algebra},
  booktitle = {{Second Maghrebian Conf.\null{} on Software
                  Engineering and Artificial Intelligence, Tunis, Tunisia}},
  year = 1992,
  pages = {222--236},
  month = APR,
  organization = {Fondation Nationale de la Recherche Scientifique},
  bibliographies = {RelMiCS}
}

@Article{Desharnais-Jaoua-Mili-Boudriga-Mili-1993,
  author = {Jules Desharnais and Ali Jaoua and Fatma Mili and
                  Noureddine Boudriga and Ali Mili},
  title = {A Relational Division Operator: The Conjugate
                  Kernel},
  WKloc = {A-0032},
  authorsAddress = {JD\&AJ: Universit\'e Laval, Quebec; FM: Oakland
                  University, Rochester; NB\&AM: Tunis II},
  abstract = {We discuss a binary operator on relations, which has
                  division like properties. We review the mathematical
                  properties of this operator, then investigate its
                  relevance to program construction.},
  journal = TCS,
  year = 1993,
  volume = 114,
  pages = {247--272},
  annote = {$\kappa(R,Q) = \relnot{\relnot{R}\rtrans{Q}} \reland
                  \RL{}\rtrans{Q}$},
  bibliographies = {RelMiCS}
}

@InProceedings{Desharnais-Jipsen-Struth-2009,
  author =       {Jules Desharnais and Peter Jipsen and Georg Struth},
  title =        {Domain and Antidomain Semigroups},
  crossref =  {RelmiCS2009},
  pages =     {73--87},
  bibliographies = {RelMiCS, RelMiCS11},
  OPTabstract =    {}
}

@InProceedings{Desharnais-Khedri-Mili-1998,
  author = {Jules Desharnais and Ridha Khedri and Ali Mili},
  title = {Interpretation of Tabular Expressions Using Arrays of Relations},
  crossref = {RelMiCS1998},
  pages = {3--14},
  WKloc = {A-1448},
  bibliographies = {RelMiCS, RelMiS, RelMiCS4}
}

@InProceedings{Desharnais-Khedri-Mili-1998a,
  author = {Jules Desharnais and Ridha Khedri and Ali Mili},
  title = {Relational Semantics of Tabular Expressions: Talk Abstract},
  year = 1998,
  crossref = {RelMiCS1998-PP},
  WKloc = {A-0769},
  bibliographies = {RelMiCS, RelMiS, SeminarWT2000}
}

@InProceedings{Desharnais-Madhavji-1990,
  author = {Jules Desharnais and Nazim H. Madhavji},
  title = {Abstract Relational Specifications},
  pages = {267--284},
  crossref = {IFIP1990},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@InProceedings{Desharnais-Mili-1984,
  author = {Jules Desharnais and Ali Mili},
  title = {Relations as the Basis for Program Specification,
                  Analysis and Design},
  booktitle = {{Proc.\null{} Internat.\null{} Workshop on Models and
                  Languages for Specification and Design, Orlando, FL}},
  year = 1984,
  pages = {137--139},
  month = MAR,
  bibliographies = {RelMiCS}
}

@Article{Desharnais-Mili-Mili-1993,
  author = {Jules Desharnais and Ali Mili and Fatma Mili},
  title = {On the Mathematics of Sequential Decompositions},
  journal = SCICOP,
  year = 1993,
  volume = 20,
  pages = {253--289},
  bibliographies = {RelMiCS}
}

@InCollection{Desharnais-Mili-Mili-Mullins-Slimani-1996,
  author = {Jules Desharnais and Ali Mili and Rym Mili and John
                  Mullins and Yahia Slimani},
  title = {Semantics of Concurrent Programming},
  booktitle = {Handbook of Parallel and Distributed Computing},
  publisher = McGraw,
  year = 1996,
  editor = {Albert Zomaya},
  series = {Computer Engineering Series},
  pages = {24--58},
  address = {New York, NY},
  bibliographies = {RelMiCS}
}

@InCollection{Desharnais-Mili-Nguyen-1997,
  author = {Jules Desharnais and Ali Mili and Thanh Tung Nguyen},
  title = {Refinement and Demonic Semantics},
  chapter = 11,
  pages = {166--183},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@TechReport{Desharnais-Moeller-2000,
  author = 	 {Jules Desharnais and Bernhard M{\"o}ller},
  title = 	 {Characterizing Determinacy in {Kleene} Algebras},
  institution =  {Institut f\"ur Informatik. Universit\"at Augsburg},
  year = 	 {2000},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {2000-5},
  OPTaddress = 	 {},
  OPTmonth = 	 MAY,
  note = 	 {(long version of \cite{Desharnais-Moeller-2001})},
  WKloc = 	 {doc/pap/BIB}
}

@Article{Desharnais-Moeller-2001,
  author = 	 {Jules Desharnais and Bernhard M{\"o}ller},
  title = 	 {Characterizing Determinacy in {Kleene} Algebras},
  journal = 	 {Information Sciences},
  year = 	 2001,
  volume =	 139,
  pages =	 {253--273},
  WKloc = 	 {A-1581, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Elements of Kleene algebras can be used, among
       others, as abstractions of the input-output semantics of
       nondeterministic programs or as models for the association of
       pointers with their target objects. In the first case, one
       seeks to distinguish the subclass of elements that correspond
       to deterministic programs. In the second case one is only
       interested in functional correspondences, since it does not
       make sense for a pointer to point to two different objects. We
       discuss several candidate notions of determinacy and clarify
       their relationship. Some characterizations that are equivalent
       in the case where the underlying Kleene algebra is an
       (abstract) relation algebra are not equivalent for general
       Kleene algebras.}
}

@TechReport{Desharnais-Moeller-2002a,
  author = {Jules Desharnais and Bernhard M{\"o}ller},
  title = {Least Reflexive Points of Relations},
  institution = {Universit\"at Augsburg, Institut f\"ur Informatik},
  year = 2002,
  number = {2002-13},
  month = JUN,
  WKloc = {A-1374, doc/pap/BIB},
  annote = {presented by Jules in Brock, 2002-09-21},
  bibliographies = {RelMiCS}
}

@Article{Desharnais-Moeller-2005,
  author = 	 {Jules Desharnais and Bernhard M{\"o}ller},
  title = 	 {Least Reflexive Points of Relations},
  journal = HOSYCO,
  year = 2005,
  volume = 18,
  DOI = {10.1007/s10990-005-7006-5},
  URL = {http://www.springerlink.com/content/uh82h06554473623/},
  WKloc = {doc/pap/BIB},
  pages = {51--77},
  abstract = {Assume a partially ordered set $(S,\leq)$ and a relation
                  $R$ on $S$. We consider various sets of conditions
                  in order to determine whether they ensure the
                  existence of a least reflexive point, that is, a
                  least $x$ such that $xRx$. This is a generalization
                  of the problem of determining the least fixed point
                  of a function and the conditions under which it
                  exists. To motivate the investigation we first
                  present a theorem by Cai and Paige giving conditions
                  under which iterating $R$ from the bottom element
                  necessarily leads to a minimal reflexive point; the
                  proof is by a concise relation-algebraic
                  calculation. Then, we assume a complete lattice and
                  exhibit sufficient conditions, depending on whether
                  $R$ is partial or not, for the existence of a least
                  reflexive point. Further results concern the
                  structure of the set of all reflexive points; among
                  other results we give a sufficient condition for
                  these to form a complete lattice, thus generalizing
                  Tarski's classical result to the nondeterministic
                  case.},
  bibliographies =  {RelMiCS}
}

@InCollection{Desharnais-Moeller-2008,
  author = 	 {Jules Desharnais and Bernhard M{\"o}ller},
  title = 	 {Least reflexive points of relations},
  editor = {O. Danvy and H. Mairson and F. Henglein and A. Pettorossi},
  publisher = Springer,
  year = 2008,
  pages = {215--228},
  abstract = {Assume a partially ordered set $(S,\leq)$ and a relation
                  $R$ on $S$. We consider various sets of conditions
                  in order to determine whether they ensure the
                  existence of a least reflexive point, that is, a
                  least $x$ such that $xRx$. This is a generalization
                  of the problem of determining the least fixed point
                  of a function and the conditions under which it
                  exists. To motivate the investigation we first
                  present a theorem by Cai and Paige giving conditions
                  under which iterating $R$ from the bottom element
                  necessarily leads to a minimal reflexive point; the
                  proof is by a concise relation-algebraic
                  calculation. Then, we assume a complete lattice and
                  exhibit sufficient conditions, depending on whether
                  $R$ is partial or not, for the existence of a least
                  reflexive point. Further results concern the
                  structure of the set of all reflexive points; among
                  other results we give a sufficient condition for
                  these to form a complete lattice, thus generalizing
                  Tarski's classical result to the nondeterministic
                  case.},
  booktitle = 	 {Automatic program development --- A tribute to {Robert Paige}},
  bibliographies =  {RelMiCS}
}

@TechReport{Desharnais-Moeller-Struth-2003,
  author = {Jules Desharnais and Bernhard M{\"o}ller and Georg Struth},
  title = {Kleene Algebra with Domain},
  institution = {Universit\"at Augsburg, Institut f\"ur Informatik},
  year = 2003,
  number = {2003-7},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Article{Desharnais-Moeller-Struth-2004,
  author = {Jules Desharnais and Bernhard M{\"o}ller and Georg Struth},
  title     = {{Modal Kleene Algebra and Applications --- A Survey}},
  journal   = {JoRMiCS},
  volume    = 1,
  pages     = {93--131},
  year      = 2004,
  WKloc     = {doc/pap/BIB, A-1755 Laval TR},
  bibliographies = {RelMiCS},
  JournalURL = {http://www.JoRMiCS.org/}
}

@Article{Desharnais-Moeller-Struth-2006,
  author = {Jules Desharnais and Bernhard M{\"o}ller and Georg Struth},
  title = {{Kleene} Algebra with Domain},
  journal = 	 {ACM Transactions on Computational Logic},
  year = 	 2006,
  volume =	 7,
  number =	 4,
  pages = 	 {798--833},
  OPTmonth =	 OCT,
  WKloc = 	 {A-1665},
  abstract = {We propose Kleene algebra with domain (KAD), an
     extension of Kleene algebra by simple equational axioms for a
     domain and a codomain operation. KAD considerably augments the
     expressiveness of Kleene algebra, in particular for the
     specification and analysis of programs and state transition
     systems. We develop the basic calculus, present the most
     interesting models and discuss some related theories. We
     demonstrate applicability by two examples: algebraic
     reconstructions of Noethericity and propositional Hoare logic
     based on equational reasoning. }
}

@Article{Desharnais-Moeller-Tchier-2006,
  author = 	 {Jules Desharnais and Bernhard M{\"o}ller and Fairouz Tchier},
  title = 	 {{Kleene} under a Modal Demonic Star},
  journal = 	 {Journal of Logic and Algebraic Programming},
  year = 	 2006,
  volume =	 66,
  pages =	 {127--160},
  WKloc = 	 {A-1666}
}

@TechReport{Desharnais-Tchier-Khedri-1994,
  author = {Jules Desharnais and Fairouz Tchier and Ridha
                  Khedri},
  title = {Demonic Relational Semantics of Sequential Programs},
  institution = LAVAL,
  year = 1994,
  type = {Research Report},
  number = {DIUL-RR-9406},
  address = {Qu{\'e}bec, QC, G1K 7P4, Canada},
  month = DEC,
  bibliographies = {RelMiCS}
}

@InProceedings{Despeyroux-Hirschowitz-1994,
  author = {Jo\"elle Despeyroux and Andr\'e Hirschowitz},
  title = {Higher-Order Abstract Syntax with Induction in Coq},
  crossref = {LPAR94},
  authorsAddress = {INRIA, Sophia-Antipolis; CNRS, University of Nice},
  pages = {159--173},
  WKloc = {A-0358},
  abstract = {}
}

@Misc{Despeyroux-Pfenning-Schuermann-199X,
  author = {Jo\"elle Despeyroux and Frank Pfenning and Carsten Sch{\"u}rmann},
  title = {Primitive Recursion for higher-Order Abstract Syntax},
  year = {199?},
  WKloc = {A-0695}
}

@Article{Desrosiers-Iglewski-Obaid-1995,
  author = {B. Desrosiers and M. Iglewski and A. Obaid},
  title = {Utilisation de la m\'ethode de traces pour la
                  d\'efinition formelle d'un protocole de communication},
  journal = Electronic_Journal_on_Networks_and_Distributed_Processing,
  year = 1995,
  volume = 2,
  month = SEP,
  pages = {57--73},
  bibliographies = {RelMiCS}
}

@Article{Detlefs-Nelson-Saxe-2005,
  author = {Detlefs, David and Nelson, Greg and Saxe, James B.},
  title = {Simplify: a theorem prover for program checking},
  journal = JACM,
  volume = {52},
  number = {3},
  year = {2005},
  issn = {0004-5411},
  pages = {365--473},
  DOIURL = {http://doi.acm.org/10.1145/1066100.1066102},
  publisher = {ACM},
  address = {New York, NY, USA},
  WKloc = {A-1736, doc/pap/BIB},
  bibliographies = {Coconut},
  annote = {Used by \cite{Joshi-Nelson-ZhouYunhong-2006}.},
  abstract = {This article provides a detailed description
      of the automatic theorem prover Simplify,
      which is the proof engine of the Extended Static Checkers ESC/Java and ESC/Modula-3.
      Simplify uses the Nelson--Oppen method
      to combine decision procedures for several important theories,
      and also employs a matcher to reason about quantifiers.
      Instead of conventional matching in a term DAG,
      Simplify matches up to equivalence in an E-graph,
      which detects many relevant pattern instances
      that would be missed by the conventional approach.
      The article describes two techniques,
      error context reporting and error localization,
      for helping the user to determine the reason that a false conjecture is false.
      The article includes detailed performance figures
      on conjectures derived from realistic program-checking problems.}
}

@Article{Deutsch-1985,
  author = {David Deutsch},
  title = {Quantum theory, the {Church-Turing} principle
            and the universal quantum computer},
  journal = {Proc. R. Soc. Lond.},
  volume = {A400},
  pages = {97-117},
  year = 1985
}

@InProceedings{Deutsch-1993,
  author = {Alain Deutsch},
  title = {On determining lifetime and aliasing of dynamically allocated
          data in higher-order functional specifications},
  abstract = {We present a static analysis method for determining aliasing
             and lifetime of dynamically allocated data in lexically scoped,
             higher-order, strict and polymorphic languages with first class
             continuations. The goal is program transformations that
             introduce imperative constructs such as destructive updatings,
             stack allocations and explicit deallocations in order to reduce
             the run-time memory management overhead. Our method is based on
             an operational model of higher order functional programs from
             which we construct statically computable abstractions using the
             abstract interpretation framework. Our method provides a
             solution to a problem left open [Hudak 86] : determing
             isolation of data in the case of higher order languages with
             structured data.},
  crossref = {POPL1993},
  WKloc = {A-0169}
}

@Book{Deutsch-1994,
  author = {Markus Deutsch},
  title = {{Unternehmenserfolg mit EDI}},
  publisher = {vieweg},
  year = 1994,
  series = {{Zielorientierts Business-Computing}},
  UniBwM = {INF190/YA3584}
}

@InProceedings{Deutsch-Fernandez-Florescu-Levy-Suciu-1998,
  author = {A. Deutsch and M. F. Fernandez and D. Florescu and
                 A.~Y. Levy and D. Suciu},
  title = {{``XML-QL: A Query Language for XML''}},
  booktitle = {WWW The Query Language Workshop (QL)},
  address = {Cambridge, MA},
  year = 1998,
  month = dec # {,},
  note = {{\sf http://www.w3.org/TR/1998/NOTE-xml-ql-19980819/}},
  WKloc = {A-1234}
}

@Article{Devienne-1990,
  author = {P. Devienne},
  title = {Weighted Graphs: A Tool For Studying The Halting Problem
               And Time Complexity In Term Rewriting Systems And Logic
               Programming},
  journal = {Theoretical Computer Science},
  year = 1990,
  volume = 75,
  number = 1,
  pages = {157--215},
  abstract = {This study is based on the halting and complexity
               problems for a simple class of logic programs in prolog-
               like languages. Any prolog program can be expressed in
               the form of an overlap of some simpler programs whose
               structures are basic and can be studied formally. The
               simplest recursive rules are studied here and the
               weighted graph is introduced to charcterize their
               behaviour. This new sytactic object, the weighted graph,
               generalises the directed graph. Unfoldings of directed
               graphs generate infinite regular trees that I generalise
               by weighting the arrows and putting periods on the
               variables. The weights along a branch are added during
               unfolding and the results (modulo of the period) indexes
               variables. Hence, their interpretations are non-regular
               trees because of the infinity of variables. This paper
               presents some of the formal properties of these graphs,
               finite and infinite interpretation and unification.}
}

@Book{Devlin-1997,
  author = {Keith Devlin},
  title = {Goodbye {Descartes}: The End of Logic and the Search for a New Cosmology of the Mind},
  publisher = {Basic Books},
  year = 1997,
  address = {New York}
}

@Book{Devlin-2000,
  author = {Keith Devlin},
  title = {The Math Gene: How Mathematical Thinking Evolved and Why Numbers Are Like Gossip},
  publisher = {Basic Books},
  year = 2000
}

@Article{Devlin-2001,
  author = {Keith Devlin},
  title = {Viewpoint: the real reason why software engineers need math},
  journal = {Communications of the ACM},
  volume = 44,
  number = 10,
  year = 2001,
  ISSN = {0001-0782},
  pages = {21--22},
  WKloc = {A-1489, doc/pap/BIB},
  doi = {http://doi.acm.org/10.1145/383845.383851},
  publisher = {ACM Press},
  bibliographies = {SWE, SE3B, SE3E, SE2S}
}

@Book{Dewdney-1989,
  author = {A. K. Dewdney},
  title = {The Turing Omnibus, 61 Excursions in Computer Science},
  UniBwM = {INF001/S12975},
  year = 1989,
  publisher = {Computer Science Press}
}

@InProceedings{DiCosmo-1992,
  author = {Di Cosmo, Roberto},
  title = {Type Isomorphisms in a Type-Assignment Framework},
  crossref = {POPL1992},
  pages = {200--210},
  abstract = {This paper contains a full treatment of isomorphic
		  types for languages equipped with an ML style
		  polymorphic type inference mechanism. Surprisingly
		  enough, the results obtained ontradict the
		  commonplace feeling that (the core of) ML is a
		  subset of second order $\lambda$-calculus: we can
		  provide an isomorphism of types that holds in the
		  core ML language, but not in second order
		  $\lambda$-calculus. This new isomorphism not only
		  allows to provide a complete (and decidable)
		  axiomatisation of all the types isomorphic in ML
		  style languages, a relavant issue for the {\em type
		  as specifications} paradigm in library searches, but
		  also suggest a natural extension that in a sens
		  completes the type-inference mechanism in ML. This
		  extension is easy to implement and allows to get a
		  further insight it the nature of the {\em let}
		  polymorphic construct.}
}

@InProceedings{DiCosmo-Ghani-1997,
  author = {Di Cosmo, Roberto and Neil Ghani},
  title = {On modular properties of higher order extensional lambda calculi},
  crossref = {ICALP1997},
  pages = {237--247},
  WKloc = {A-0868}
}

@InProceedings{DiCosmo-Kesner-1994,
  author = {Di Cosmo, Roberto and Delia Kesner},
  title = {Combining First Order Algebraic
          Rewriting Systems, Recursion and Extensional Lambda Calculi},
  pages = {462--472},
  crossref = {ICALP1994},
  authorsAddress = {RDC: LIENS-ENS; DK: Paris-Sud},
  WKloc = {A-0357},
  abstract = {It is well known that confluence and strong
		  normalization are preserved when combining
		  left-linear {\em algebraic rewriting systems} with
		  simply typed lambda calculus. It is equally well
		  known that confluence fails when adding either the
		  usual extensional rule for $\eta$, or recursion
		  together with the usual contraction rule for
		  surjective pairing.

                  We show that {\em confluence} and {\em
		  normalization} are {\em modular} properties for the
		  combination of {\em left-linear} algebraic rewriting
		  systems with typed lambda calculi enriced with {\em
		  expansive} extensional rules for $\eta$ and
		  surjective pairing. For that, we use a transition
		  technique allowing to simulate expansions without
		  expansion rules. We also show that confluence is
		  maintained in a modular way when adding {\em
		  fixpoints}. This result is also obtained by a simple
		  translation technique allowing to simulate bounded
		  recursion with $\beta$-reduction.}
}

@InProceedings{DiCosmo-Kesner-1995,
  author = {Di Cosmo, Roberto and Delia Kesner},
  title = {Rewriting with Extensional Polymorphic $\lambda$-Calculus},
  crossref = {CSL1995},
  pages = {215--232},
  WKloc = {A-1411, doc/pap/BIB}
}

@InProceedings{DiGianantonio-Honsell-1993,
  author = {Pietro Di Gianantonio and Furio Honsell},
  title = {An Abstract Notion of Application},
  pages = {124--138},
  abstract = {Many concrete notions of function application, suitable for
             interpreting typed lambda calculi with recursive types, have
             been introduced in the literature. These arise in different
             fields such as set theory, multised theory, type theory and
             functor theory and are apparently unrelated. In this paper we
             introduce the general concept of {\em applicative exponential
             structure} and show that it subsumes all these notions. Our
             approach is based on a generalization of the notion of
             {\em intersection type}. We construe all these structures in a
             finitary way, so as to be able to utilize uniformly a general
             form of type assignment system for defining the interpretation
             function. Applicative exponential structures are just
             combinatory algebras, in general. Our approach suggests a wide
             variety of entirely new concrete notions of function
             application; e.g.in connection with boolean sets. Applicative
             exponential structures can be used for modeling various forms
             of non-deterministic operators.},
  crossref = {TLCA93},
  WKloc = {A-0182}
}

@Article{DiNola-Lettieri-1989,
  author = {Di Nola, Antonio and Ada Lettieri},
  title = {Relation Equations in Residuated Lattices},
  journal = RENDCIRC,
  volume = 38,
  year = 1989,
  pages = {246--256},
  bibliographies = {RelMiCS}
}

@Book{DiNola-Pedryzc-Sanchez-Sessa-1989,
  author =	 {Di Nola, A. and W. Pedryzc and E. Sanchez and S. Sessa},
  title = 	 {Fuzzy Relation Equations and their Application to Knowledge Engineering},
  publisher = 	 {Kluwer Academic Publishers},
  year = 	 1989,
  address =	 {Dordrecht},
  bibliographies = {RelMiCS}
}

@Article{DiNola-Pedryzc-Sessa-1995,
  author = 	 {Di Nola, A. and W. Pedryzc and S. Sessa},
  title = 	 {Fuzzy Relational Structures: The State-of-Art},
  journal = 	 {Fuzzy Sets and Systems},
  year = 	 1995,
  volume =	 75,
  pages =	 {241--262},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = { Fuzzy relational structures, built with the aid of
     fuzzy sets and fuzzy relations, constitute a fundamental class of
     constructs arising in fuzzy set theory. They give rise to fuzzy
     relation equations. In this paper we will develop the concepts of
     single and multilevel relational structures. Different logic
     operations will be discussed in detail. Aspects of the knowledge
     representation will be studied and illustrated with several
     schemes of applications.}
}

@Misc{DiasdeAlmeida-1995,
  author = {Dias de Almeida, Jos{\'e} Jo{\~a}o},
  title = {{NLlex} --- a tool to generate lexical analyzers for natural language},
  year = {199?},
  WKloc = {A-0711}
}

@Misc{DiasdeAlmeida-199X,
  author = {Dias de Almeida, Jos{\'e} Jo{\~a}o},
  title = {{YaLG} --- extending {\sc dcg} for {NLP}},
  year = {199?},
  WKloc = {A-0710},
  keywords = {natural language}
}

@Misc{DiasdeAlmeida-Pinto-1994,
  author = {Dias de Almeida, Jos{\'e} Jo{\~a}o and Ulisses Pinto},
  title = {{Jspell} --- um m\'odulo para an\'alise l\'exica gen\'erica de linguagem natural},
  year = 1994,
  WKloc = {A-0589}
}

@InProceedings{Diatchki-JonesMP-Hallgren-2002,
  author = {Iavor S. Diatchki and Mark P. Jones and Thomas Hallgren },
  title = {A formal specification of the {Haskell 98} module system },
  crossref = {Haskell2002},
  pages = {17--28},
  WKloc = {A-1362, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/581690.581692},
  abstract = {Many programming languages provide means to split large
      programs into smaller modules. The module system of a language
      specifies what constitutes a module and how modules interact.This
      paper presents a formal specification of the module system for the
      functional programming language Haskell. Although many aspects of
      Haskell have been subjected to formal analysis, the module system
      has, to date, been described only informally as part of the Haskell
      language report. As a result, some aspects of it are not well
      understood or are under-specified; this causes difficulties in
      reasoning about Haskell programs, and leads to practical problems
      such as inconsistencies between different implementations. One
      significant aspect of our work is that the specification is written
      in Haskell, which means that it can also be used as an executable
      test-bed, and as a starting point for Haskell implementers.}
}

@InProceedings{Diaz-1992,
  keywords = {layout, linear arrangements, numbering, VLSI layout,
		  placement problem, routing problem, MINLA, MINCUT,
		  BANDWIDTH, MINSUMCUT, $\delta$-OPERATOR,PRAM},
  authorsAddress = {Universitat Polit\`ecnica Catalunya, Barcelona},
  abstract = {In this paper we survey the results and open
		  questions about some graph layout problems.},
  title = {Graph Layout Problems},
  pages = {14--23},
  note = {invited lecture},
  crossref = {MFCS1992},
  author = {J. D\'\i{}az}
}

@Article{Diaz-Petit-Serna-2002,
  author = {Josep D{\'{\i}}az and Jordi Petit and Maria Serna},
  title = {A survey of graph layout problems},
  journal = {ACM Computing Surveys (CSUR)},
  volume = 34,
  number = 3,
  year = 2002,
  ISSN = {0360-0300},
  pages = {313--356},
  WKloc = {A-1466},
  doi = {http://doi.acm.org/10.1145/568522.568523},
  publisher = {ACM Press}
}

@TechReport{Dickmanns-1992,
  year = 1992,
  type = {Interner {B}ericht},
  title = {{KRONOS} {B}enutzerhandbuch},
  note = {mit KRONOS Configuration Manual},
  month = {Okt.},
  institution = {UniBwM/INF},
  author = {Dickmanns, Dirk},
  address = {Neubiberg}
}

@TechReport{Dickmanns-1993,
  year = 1993,
  type = {submitted to DAGM},
  title = {R"aumliche {B}ewegungserkennung zum {G}reifen
             eines freischwebenden {K}"orpers im {A}ll},
  month = {August},
  institution = {UniBwM/INF},
  author = {Dickmanns, Dirk},
  address = {Neubiberg}
}

@TechReport{Dickmanns-1993a,
  year = 1993,
  type = {Internal report},
  title = {The {D}ynamic {D}atabase
           {P}rogrammer's {R}eference},
  month = {May},
  institution = {UniBwM/INF},
  author = {Dickmanns, Dirk},
  address = {Neubiberg}
}

@TechReport{Dickmanns-1993b,
  year = 1993,
  type = {Internal report},
  title = {The {D}ynamic {D}atabase {U}ser's {G}uide},
  month = {May},
  institution = {UniBwM/INF},
  author = {Dickmanns, Dirk},
  address = {Neubiberg}
}

@InProceedings{DickmannsED-Behringer-Bruedigam-Dickmanns-Thomanek-vonHolt-1993,
  year = 1993,
  title = {An all-transputer visual
             autobahn--autopilot/copilot},
  note = {also in TAT 93 at Aachen},
  booktitle = {Proc. ICCV, Berlin},
  author = {Dickmanns, Ernst Dieter and
             Behringer, Reinhold and
             Br\"udigam, Claus and
             Dickmanns, Dirk and
             Thomanek, Frank and
             von Holt, Volker}
}

@Book{Dieudonne-1992,
  author = {Jean Dieudonn\'e},
  title = {Mathematics --- The Music of Reason},
  publisher = {Springer-Verlag},
  year = 1992,
  note = {Orig.: ``{\em Pour l'honneur de l'esprit humain}'',
		  Hachette 1987},
  UniBwM = {MAT001/W12524}
}

@Article{Dijkstra-1974,
  author = {E. W. Dijkstra},
  title = {A simple axiomatic basis for programming language constructs},
  journal = INDAG,
  year = 1974,
  volume = 36,
  pages = {1--15},
  bibliographies = {RelMiCS}
}

@Article{Dijkstra-1975,
  author = {E. W. Dijkstra},
  title = {Guarded commands, nondeterminacy and formal derivation of programs},
  journal = CACM,
  year = 1975,
  volume = 18,
  pages = {453--457},
  bibliographies = {RelMiCS}
}

@Book{Dijkstra-1976,
  author = {E. W. Dijkstra},
  title = {A Discipline of Programming},
  publisher = Prentice,
  year = 1976,
  UniBwM = {INF400/D13835},
  bibliographies = {RelMiCS},
  WKloc = {owned, \lent{Ridha}}
}

@InProceedings{Dijkstra-1985,
  author = {Edsger W. Dijkstra},
  title = {On the Nature of Computing Science},
  pages = {1--3},
  crossref = {Marktoberdorf-1985},
  note = {(dinner speech)}
}

@InProceedings{Dijkstra-1985a,
  author = {Edsger W. Dijkstra},
  title = {The Image Construction in Computerized Axial Tomography},
  pages = {503--506},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Dijkstra-1985b,
  author = {Edsger W. Dijkstra},
  title = {The Distributed Snapshot of {K.M. Chandy and L. Lamport}},
  pages = {513--517},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Dijkstra-1992,
  author = {Edsger W. Dijkstra},
  title = {On the Economy of doing Mathematics},
  pages = {2--10},
  abstract = {Every honest scientist regularly does some soul searching; he
             then tries to analyse how well he is doing and if his progress
             is still towards his original goal. Two years ago, at the
             previous conference on the Mathematics of Program Construction,
             I observed some of such soul searching among the scientists
             there present, and I observed more doubt than faith. The first
             part of this talk is therefore devoted to my explanation of why
             I think most of those doubts unjustified.},
  crossref = {MPC1992},
  WKloc = {A-0233}
}

@InCollection{Dijkstra-1994,
  author = {Edsger W. Dijkstra},
  title = {On the Design of Calculational Proofs},
  crossref = {Roscoe-1994},
  pages = {105--117},
  chapter = 7,
  OPTnote = {},
  OPTannote = {}
}

@Book{Dijkstra-Feijen-1984,
  author = {E.W. Dijkstra and W.H.J. Feijen},
  title = {Een Methode van Programmeren},
  publisher = {Academic Service},
  address = {Den Haag},
  year = 1984,
  note = {Also available as {\em A Method of Programming},
		  Addison-Wesley, Reading, Mass., 1988.},
  bibliographies = {RelMiCS}
}

@InProceedings{Dijkstra-Feijen-vanGasteren-1985,
  author = {Edsger W. Dijkstra and W.H.J. Feijen and van Gasteren, A.J.M},
  title = {Derivation of a Termination Detection Algorithm for Distributed Computations},
  pages = {507--512},
  crossref = {Marktoberdorf-1985}
}

@Book{Dijkstra-Scholten-1990,
  author = {Edsger W. Dijkstra and Carel S. Scholten},
  title = {Predicate Calculus and Program Semantics},
  year = 1990,
  series = {Texts and Monographs in Computer Science},
  publisher = Springer,
  UniBwM = {MAT006/T7575},
  bibliographies = {RelMiCS},
  contents = {Preface
		1 On Structures
		2 On substitution and replacement
		3 On functions and equality
		4 On our proof format
		5 The calculus of boolean structures
		6 Some properties of predicate transformers
		7 Semantics of straight-line programs
		8 Equations in predicates and their extreme solutions
		9 Semantics of repetitions
		10 Operational considerations
		11 Converse predicate transformers
		12 The strongest postcondition
		Index}
}

@InProceedings{Dijkstra-vanGasteren-1985,
  author = {Edsger W. Dijkstra and van Gasteren, A.J.M},
  title = {A Simple Fix Point Argument without the Restriction to Continuity},
  pages = {519--525},
  OPTabstract = {To the best of our knowledge, the above argument is the
      first one to connect well-foundedness in its full generality directly
      to a non-operational notion of termination, i.e. to the strongest
      solution of a fix-point equation. Its simplicity should dispel the
      myth that the restriction to continuity for the sake of convenience
      is justified.},
  OPTwkloc = {A-09},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{DijkstraA-Schrage-Swierstra-1999,
  author = {A. Dijkstra and M.M. Schrage and S.D. Swierstra},
  title = {{SKIT}, An Open Architecture for Courseware Authoring},
  booktitle = {{FIE 99}, Frontiers in Education Conference, {San Juan}},
  year = 1999,
  ISBN = {0-7803-5643-8},
  publisher = {IEEE},
  WKloc = {doc/pap/BIB/DijkstraA-Schrage-Swierstra-1999.ps}
}

@Book{Diller-1994,
  author = {A. Diller},
  title = {{Z}: An Introduction to Formal Methods},
  publisher = {John Wiley \& Sons},
  year = 1994,
  edition = {2nd},
  length = 374,
  URL = {http://www.blackwell.co.uk/cgi-bin/bb_item?0471939730},
  ISBN = {0-471-93973-0},
  annote = {This book offers a comprehensive tutorial to Z from the
                  practical viewpoint. Many natural deduction style proofs
                  are presented and exercises are included. Z as defined in
                  the 2nd edition of {\em The Z Notation} \cite{z:spiv92} is
                  used throughout. \par Contents: Tutorial introduction;
                  Methods of reasoning; Case studies; Specification
                  animation; Reference manual; Answers to exercises;
                  Glossaries of terms and symbols; Bibliography.},
  UniBwM = {INF560-Mag/V969}
}

@InProceedings{Dinesh-1993,
  author = {T. B. Dinesh},
  title = {Type-Checking Revisited: Modular Error Handling},
  OPTcrossref = {SoSL93},
  pages = {216--231},
  ISBN = {3-540-19854-7},
  editor = {D. J. Andrews and J. F. Groote and C. A. Middelburg},
  booktitle = {Proceedings of the International Workshop on Semantics
                 of Specification Languages ({SOSL}'93)},
  month = oct,
  series = {Workshops in Computing},
  publisher = {Springer},
  address = {London, UK},
  year = 1994,
  keywords = {ASF, SDF, ASF+SDF}
}

@TechReport{Dinesh-1993a,
  author = {T. B. Dinesh},
  institution = {ASF+SDF Group, University of Amsterdam},
  title = {Type Checking Revisited: Modular Error Handling},
  year = 1993,
  WKloc = {A-0523},
  abstract-url = {ftp://ftp.cwi.nl/pub/gipe/reports/abstracts/Din93.html},
  URL = {ftp://ftp.cwi.nl/pub/gipe/reports/Din93.ps.Z},
  scope = {imptype},
  abstract = {Static-semantics determines the validity of a program, while
      a type-checker provides more specific type error information.
      Type-checkers are specified based on the static semantics
      specification, for the purpose of identifying and presenting type
      errors in invalid programs.

      We discuss a style of algebraically specifying the static semantics
      of a language which facilitates automatic generation of a
      type-checker and a language specific error reporter. Such a
      specification can also be extended in a modular manner to yield
      human-readable error messages.}
}

@TechReport{Dinesh-Tip-1992,
  author = {T. B. Dinesh and Frank Tip},
  title = {Animators and error reporters for generated
                 programming environments},
  pages = 98,
  institution = {Centrum voor Wiskunde en Informatica (CWI)},
  number = {CS-R9253},
  ISSN = {ISSN 0169-118X},
  year = 1992,
  keywords = {Animators, debugging, error messages, error reporters,
                 interpreters, programming environments, origin
                 tracking, type-checking.},
  URL = {ftp://ftp.cwi.nl/pub/CWIreports/AP/CS-R9253.ps.Z},
  WKloc = {B-0093},
  abstract = {We study animators and error reporters for generated
                 programming environments. An $<$i$>$error
                 reporter$<$/i$>$ is a tool for indicating the exact
                 position of a type-error in the source text. An
                 $<$i$>$animator$<$/i$>$ visualizes program execution;
                 typically, it highlights the statement that is
                 currently executing. Applications of both tools are
                 mainly to be found in the areas of debugging and
                 tutoring. Instead of explicitly extending language
                 specifications with these facilities, we claim that
                 error reporters and animators can be generated from
                 existing specifications for type-checkers and
                 interpreters with little effort; to this end, a simple
                 pattern-matching mechanism is used in conjunction with
                 origin tracking, a generic tracing technique. In this
                 paper, we discuss our claim, and at the same time
                 investigate the limitations and deficiencies of origin
                 tracking. Our techniques are illustrated using an
                 example language named CLaX, a Pascal relative. The
                 full specifications of the CLaX syntax, type-checker
                 and interpreter are included in appendices.},
  note = {AP (Department of Software Technology)},
  note = {CS-R9253},
  note = {Fri, 26 Sep 1997 08:05:23 GMT},
  contents = {(not listed)}
}

@InProceedings{Dinesh-Tip-1997,
  author = {T. B. Dinesh and Frank Tip},
  title = {A Slicing-Based Approach for Locating Type Errors},
  pages = {77--88},
  ISBN = {1-880446-89-8},
  booktitle = {Proceedings of the Conference on Domain-Specific
                 Languages ({DSL}-97)},
  month = oct # {15--17~},
  publisher = {USENIX Association},
  address = {Berkeley},
  year = 1997
}

@Article{Dinesh-Ueskuedarh-1997,
  author = {T. B. Dinesh and S. M. Ueskuedarh},
  title = {Share-Where Maintenance in Visual Algebraic
                 Specifications},
  journal = {Lecture Notes in Computer Science},
  volume = 1345,
  pages = {297--??},
  year = 1997,
  coden = {LNCSD9},
  ISSN = {0302-9743},
  bibdate = {Tue Apr 28 08:51:33 MDT 1998}
}

@Article{Dipert-1981,
  author = {Randall R. Dipert},
  title = {Set-Theoretical Representations of Ordered Pairs
		and the Logic of Relations},
  journal = CANADP,
  volume = 12,
  pages = {353--373},
  year = 1981,
  bibliographies = {RelMiCS}
}

@Article{Dipert-1984,
  author = {Randall R. Dipert},
  title = {Review},
  note = {of {Peirce}: Studies in Logic by Members of the
		Johns Hopkins University},
  journal = HIST,
  year = 1984,
  volume = 4,
  pages = {1--9},
  bibliographies = {RelMiCS}
}

@Article{Dipert-1984a,
  author = {Randall R. Dipert},
  title = {{Peirce}, {Frege}, the Logic of Relations, and {Church}'s
		Theorem},
  journal = HIST,
  year = 1984,
  volume = 5,
  pages = {49--66},
  bibliographies = {RelMiCS}
}

@Article{Dipert-1984b,
  author = {Randall R. Dipert},
  title = {Review},
  note = {of Peirce: Studies in Logic by Members of the
		Johns Hopkins University},
  journal = PEIRCE,
  year = 1984,
  volume = 20,
  pages = {469--472},
  bibliographies = {RelMiCS}
}

@TechReport{Dixon-1993,
  author = {Clare Dixon},
  title = {A Graph-Based Approach to Resolution in Temporal Logic},
  institution = {Manchester University, Department of Computer Science},
  year = 1993,
  month = SEP,
  number = {UMCS-93-9.2},
  URL = {ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-93-9-2.ps.Z},
  directory = {~kahl/doc/pap/manchester},
  WKloc = {B-0023},
  abstract = {An approach to the mechanisation of temporal logics,
		  based on a form of clausal resolution has been
		  developed.  Temporal formulae incorporating both
		  past-time and future-time temporal operators are
		  converted into Separated Normal Form, and then both
		  non-temporal and temporal resolution rules are
		  applied.  The temporal resolution rule attempts to
		  match conditions which must be eventually satisfied
		  with sets of formulae which together imply that the
		  condition will never be satisfied.  In this
		  dissertation the sets of formulae which are used in
		  the application of the temporal resolution rule are
		  identified using graph theoretic techniques.  This
		  is achieved by first extracting formulae or
		  combinations of formulae that could possibly stop a
		  condition being satisfied and then using them to
		  build a graph.  Secondly any loops identified during
		  the construction of the graph will mean that the
		  condition can never be satisfied, and are used in
		  the application of the temporal resolution rule thus
		  deriving further formulae.  This temporal resolution
		  method has been combined with sub-programs
		  performing translation to normal form and
		  non-temporal resolution to produce an integrated
		  resolution based temporal theorem prover.  The main
		  algorithm cycles through these three procedures
		  until false is derived, (the original formula was
		  satisfiable).},
  annote = {Nodes are formulae; edges are derived from implication.}
}

@Article{Dixon-Fleuriot-2005,
  author = 	 {Lucas Dixon and Jacques Fleuriot},
  title = 	 {A Proof-Centric Approach to Mathematical Assistants},
  journal = 	 JAL,
  year = 	 {2005},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  keywords = 	 {Isabelle, Isar, IsaPlanner},
  WKloc = 	 {A-1647, doc/pap/BIB}
}

@InProceedings{DjiDjev-Pantziou-Zaroliagis-1995,
  author = {H.N. DjiDjev and G.E. Pantziou and Ch.D. Zaroliagis},
  title = {On-line and Dynamic Algorithms for Shortest Path Problems},
  crossref = {STACS1995},
  pages = {193--205},
  keywords = {planar graph, separator decomposition},
  abstract = {We describe algorithms for finding shortest paths
		  and distances in a plnar digraph which exploit the
		  particular topology of the input graph. An important
		  feature of our algorithms is that they can work in
		  an dynamic environment, where the cost of any edge
		  can be changed or the edge can be deleted. Our data
		  structures can be updated after any such change in
		  only polylogarithmic time, while a single-pair query
		  is answered in sublinear time. We also describe the
		  first parallel algorithms for solving the dynamic
		  version of the shortest path problem.}
}

@InCollection{Dobrogost-Anand-Kahl-2013,
  author =       {Michal Dobrogost and Christopher Kumar Anand and Wolfram Kahl},
  title =        {Verified Multicore Parallelism Using Atomic Verifiable Operations},
  booktitle =    {Multicore Technology: Architecture, Reconfiguration, and Modeling},
  pages =        {107--151},
  publisher = {CRC Press},
  year =      2013,
  editor =    {Muhammad Yasir Qadri and Stephen J. Sangwine},
  INBOOKURL =      {http://www.crcpress.com/product/isbn/9781439880630}
}

@Book{Doets-vanEijck-2004,
  author =	 {Kees Doets and van Eijck, Jan},
  title = 	 {The Haskell Road to Logic, Math and Programming},
  year = 	 2004,
  publisher = 	 {King's College Publications},
  URL = {http://homepages.cwi.nl/~jve/HR/},
  note = {Full text at (apparently inofficial link):
      http://homepages.cwi.nl/~jve/HR/HR.pdf}
}

@InProceedings{Doh-SchmidtDA-1992,
  author = {Kyung-Goo Doh and David A. Schmidt},
  title = {Extraction of Strong Typing Laws from Action
		  Semantics Definitions},
  crossref = {ESOP1992},
  pages = {151--166},
  abstract = {We describe a method that automatically extracts a
		  type checking semantics, encoded as a set of type
		  inference rules, from an action semantics definition
		  of a programming language. The type inference rules
		  are guaranteed to enforce strong typing, since they
		  are based on an underlying metasemantics for action
		  semantics, which uses typing functions and natural
		  transformations to give meaning. Next, we use the
		  type checking semantics to extract a dynamic
		  semantics definition from the original action
		  semantics definition. We present an example.}
}

@InProceedings{Dold-1995,
  author = {Axel Dold},
  title = {Representing, Verifying and Applying Software Development Steps Using the {PVS} System},
  crossref = {AMAST1995},
  pages = {431--445},
  OPTabstract = {},
  WKloc = {A-0624 --- abandoned in Tunis 2008},
  bibliographies = {Coconut}
}

@InProceedings{Dold98,
  author =    {Axel Dold},
  title  =    "Software Development in {PVS} using Generic Development Steps",
  series =    LNCS,
  number =    "1766",
  pages  =    "146--161",
  booktitle = "Generic Programming --- International Seminar on Generic Programming",
  publisher = Springer,
  URL = {http://www.informatik.uni-ulm.de/ki/sw-development.html},
  WKloc = {doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = {This paper is concerned with a mechanized formal
     treatment of the transformational software development process in
     a unified framework. We utilize the PVS system to formally
     represent, verify and correctly apply generic software
     development steps and development methods from different existing
     transformational approaches. We illustrate our approach by
     representing the well-known divide-and-conquer paradigm, two
     optimization steps, and by formally deriving a mergesort
     program.}
}

@PhdThesis{Dold-2000,
  author = 	 {Axel Dold},
  title = 	 {Formal Software Development using Generic Development Steps},
  school = 	 {Universit{\"a}t Ulm},
  year = 	 2000,
  URL = {http://www.informatik.uni-ulm.de/ki/Dold/phd.html},
  WKloc = {doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = { This dissertation is concerned with a mechanized
     formal treatment of the transformational software development
     process in a unified framework.

     As a formal vehicle, the specification and verification system
     PVS is utilized to integrate development steps and development
     methods from different existing transformational
     approaches. Integration comprises the formalization (that is, a
     representation in the PVS specification language), the
     verification, and the correct application of the generic
     development steps. Transformations of different kind and
     complexity are integrated into this framework. They include
     well-known algorithmic paradigms and problem solving strategies
     such as global-search and divide-and-conquer, as well as
     transformations for the modification of functional specifications
     such as transformations from the Bird-Meertens Formalism like
     fusion or Horner's rule, transformation steps for optimizing
     recursive functions, transformations on the level of procedural
     programs, and implementations of data structures.

     All software artifacts are represented generically within
     parameterized PVS theories which specify the semantic
     requirements on the parameters by means of assumptions and define
     the result of the transformation. Based on the semantic
     requirements, correctness of the generic step can be proved once
     and for all. Application of such a development step to a given
     problem is then carried out by providing a concrete instantiation
     for the parameters and verifying that it satisfies the theory
     requirements.

     Some of the problem-solving strategies are organized by means of
     taxonomies (that is, hierarchies of specializations for specific
     problem classes and / or data structures). This approach greatly
     improves the applicability of the transformations and leads to an
     elegant structure of the algorithmic knowledge captured in the
     development steps.

     As a basis for the realization of refinement hierarchies and for
     formal software development in PVS, a notion of refinement
     between parameterized PVS theories and a development methodology
     is presented which is based on correct partial instantiations of
     parameterized theories.

     The usability of the approach is illustrated by many examples of
     different complexity. They include, among others, the derivation
     of several search and sorting algorithms, the derivation of a
     Prolog interpreter, the implementation of a symbol table used in
     compilers, and finally, as a larger non-trivial case-study: the
     construction of a compiler program from a given relational
     compiling specification specifying the translation of a Common
     Lisp-like language into a stack-based intermediate language.}
}

@InProceedings{Doligez-Leroy-1993,
  author = {Damien Doligez and Xavier Leroy},
  title = {A concurrent, generational garbage collector for a
      multithreaded implementation of ML},
  crossref = {POPL1993},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  abstract = {This paper presents the design and implementation of a ``quasi
      real-time'' garbage collector for Concurrent Caml Light, an
      implementation of ML with threads. This two-generation system
      combines a fast, asynchronous copying collector on the young
      generation with a non-disruptive concurrent marking collector on
      the old generation. This design crucially relies on the ML
      compile-time distinction between mutable and immutable
      objects.},
  URL = {http://pauillac.inria.fr/~xleroy/publi/concurrent-gc.ps.gz},
  WKloc = {doc/pap/BIB/Doligez-Leroy-1993.ps.gz}
}

@Unpublished{Doornbos-1993a,
  author = {H. Doornbos},
  title = {Mathematical induction made calculational},
  note = {Submitted to Computer Science in the Netherlands `93},
  WKloc = {A-0142},
  abstract = {In this note a concise formulation of mathematical induction
		is presented. This formulation is in terms of relations only,
		without mentioning the related objects. It is shown that
		the induction principle in this form lends itself very well
		for use in calculational proofs. As a non-trivial example
		a proof of a generalization of Newman's lemma is given.},
  bibliographies = {RelMiCS}
}

@InProceedings{Doornbos-1994,
  author = {H. Doornbos},
  title = {A Relational Model of Programs without the
		  Restriction to {Egli-Milner}-Monotone Constructs},
  crossref = {PROCOMET94},
  pages = {357--376 or 363--382 \unfinished},
  keywords = {Logics and Meanings of Programs; Specifying and
		  Verifying and Reasoning about Programs; Semantics of
		  Programming Languages; Studies of Program
		  Constructs; Semantics of Nondeterminism and
		  Concurrency},
  bibliographies = {RelMiCS}
}

@PhDThesis{Doornbos-1996,
  author = {H. Doornbos},
  title = {Reductivity arguments and program construction},
  school = {Eindhoven Univ.\null{} of Technology, Dept.\null{} of
                  Mathematics and Computing Science},
  year = 1996,
  month = JUN,
  bibliographies = {RelMiCS}
}

@InProceedings{Doornbos-Backhouse-1995,
  author = {Henk Doornbos and Roland Backhouse},
  title = {Induction and Recursion on Datatypes},
  crossref = {MPC1995},
  pages = {242--256},
  WKloc = {A-0707, doc/pap/BIB},
  bibliographies = {RelMiCS},
  DOI = {10.1007/3-540-60117-1_14},
  URL = {http://www.springerlink.com/content/f3555j71245883g6/},
  abstract = {A new induction principle is introduced.
     The principle is based on a property of relations, called reductivity,
     that generalises the property of admitting induction
     to one relative to a given datatype.
     The principle is used to characterise
     a broad class of recursive equations that have a unique solution
     and is also related to standard techniques
     for proving termination of programs.
     Methods based on the structure of the given datatype
     for constructing reductive relations are discussed.

     The principle is formulated in the point-free calculus of relations
     guaranteeing concision without loss of precision
     and thus ensuring its amenability to calculation.}
}

@InCollection{Doornbos-vanGasteren-Backhouse-1997,
  author = {Henk Doornbos and Netty van Gasteren and Roland Backhouse},
  title = {Programs and Datatypes},
  chapter = 10,
  pages = {150--165},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Article{Doornbos-vonKarger-1998,
  title = {On the Union of Well-Founded Relations},
  author = {Henk Doornbos and Burkart von Karger},
  pages = {195--201},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {We give a criterion for the union of well-founded (i.e.,
      noetherian) relations to be well-founded, generalizing results of
      Geser and of Bachmair-Dershowitz. The proof is written in a
      calculational style and is conducted entirely in regular algebra.},
  bibliographies = {RelMiCS}
}

@InProceedings{Dosch-1993,
  WKloc = {?},
  keywords = {?},
  contents = {?},
  abstract = {?},
  title = {The undefined function differs from the pointwise
                  undefined function},
  pages = {?},
  crossref = {HOA1993-PP},
  author = {W. Dosch}
}

@Book{Dosch-Schmidt-1990,
  year = 1990,
  title = {{Entwicklung von Programmen f\"ur geometrische Objekte}},
  publisher = {Interner Bericht, Institut f\"ur Mathematik, Universit\"at Augsburg},
  pages = 245,
  month = JAN,
  editor = {Walter Dosch and Gunther Schmidt}
}

@Article{Dougherty-1992,
  author = {Dougherty, Daniel J.},
  title = {Adding Algebraic Rewriting to the Untyped Lambda Calculus},
  journal = {Information and computation},
  year = 1992,
  volume = 101,
  number = 2,
  pages = {251--},
  month = DEC
}

@InProceedings{Dougherty-1993,
  author = {D. J. Dougherty},
  title = {Some Lambda Calculi with Categorical Sums and Products},
  crossref = {RTA93},
  pages = {137--151},
  WKloc = {A-0130},
  abstract = {We consider the simply typed $\lambda$-calculus with
		  primitive recursion operators and types
		  corresponding to categorical products and
		  coproducts. The standard equations corresponding to
		  extensionality and to surjectivity of pairing and
		  its dual are oriented as expansion rules. Strong
		  normalization and ground (base-type) confluence is
		  proved for the full calculus; full confluence is
		  proved for the calculus omitting the rule for strong
		  sums. In the latter case, fixed-point constructors
		  may be added while retaining confluence.}
}

@InProceedings{Dougherty-Gutierrez-2000,
  author = {Dan Dougherty and Claudio Guti{\'e}rrez},
  title = {Normal Forms and Reduction for Theories of Binary Relations},
  crossref = {RTA2000},
  pages = {95--109},
  bibliographies = {RelMiCS, GraphCalc},
  WKloc = {A-1006},
  abstract = {We consider equational theories of binary relations,
    in a language expressing composition, converse, and lattice operations.
    We treat the equations valid in the standard model of sets
    and also define a hierarchy of equational axiomatisations
    stratifying the standard theory.
    By working directly with a presentation of relation-expressions
    as \emph{graphs} we are able to define a notion of reduction
    which is confluent and strongly normalising,
    in sharp contrast to traditional treatments based on first-order terms.
    As consequences we obtain normal forms, decidability of the
    decision problem for equality for each theory.
    in particular we show a non-deterministic polynomial-time upper bound
    for the complexity of the decision problems.}
}

@Article{Dougherty-Gutierrez-2006,
  author = 	 {Daniel J. Dougherty, Claudio Guti{\'e}rrez},
  title = 	 {Normal forms for binary relations},
  journal = 	 TCS,
  year = 	 2006,
  volume = 	 360,
  number = 	 {1--3},
  pages = 	 {228--246}
}

@Article{Douglis-Ousterhout-1991,
  author = {Frederick Douglis and John K. Ousterhout},
  title = {Transparent Process Migration: Design Alternatives and th {Sprite} Implementation},
  journal = {Software Practice and Experience},
  year = 1991,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  bibliographies = {ProcMig},
  WKloc = {A-1254, doc/pap/BIB}
}

@InProceedings{Dowek-1991,
  author = {C. Dowek},
  title = {A second-order pattern matching algorithm for the cube
                 of typed {$\lambda$}--calculi},
  pages = {151--160},
  ISBN = {3-540-54345-7},
  editor = {Andrzej Tarlecki},
  booktitle = {Proceedings of Mathematical Foundations of Computer
                 Science. ({MFCS} '91)},
  month = sep,
  series = {LNCS},
  volume = 520,
  publisher = {Springer},
  address = {Berlin, Germany},
  year = 1991
}

@InProceedings{Dowek-1992,
  title = {Third Order Matching is Decidable},
  author = {Gilles Dowek},
  pages = {2--10},
  crossref = {LICS7},
  source = {ftp://theory.lcs.mit.edu/pub/meyer/lics.bib}
}

@Article{Dowek-1993,
  WKloc = {A-0009},
  abstract = {We prove that the pattern matching problem is
		  undecidable in polymorphic $\lambda$-calculi [as
		  Girard's system $F$ in Girard (1972, 1989)] and
		  calculi supporting inductive types [as G\"odel's
		  system $T$ in G\"odel (1958) and girard (1989)] by
		  reducing Hilbert's tneth problem to it. More
		  generally, pattern matching is undecidable in all
		  the calculi in which primitive recursive functions
		  can be fairly represented in a precised sense.},
  year = 1993,
  volume = 107,
  title = {The Undecidability of Pattern Matching in Calculi
		  where Primitive Recirsive Functions are Representable},
  pages = {349--356},
  journal = {Theoretical Computer Science},
  author = {Gilles Dowek}
}

@Book{Dowek-2009,
  author =    {Gilles Dowek},
  title =     {Principles of Programming Languages},
  publisher = Springer,
  year =      2009,
  series =    {Undergraduate Topics in Computer Science},
  DOIURL =    {http://dx.doi.org/10.1007/978-1-84882-032-6}
}

@Article{Downey-Sethi-Tarjan-1980,
  keywords = {combinatorial, discrete, graph},
  abstract = {Let G be a directed graph such that for each vertex V in
               G, successors of V are ordered. Let C be any equivalence
               relation on the vertices of G. The congruence closure C*
               of C is the finest equivalence relation containing C and
               such that any two vertices having corresponding
               successors equivalent under C* are themselves
               equivalent under C*. Efficient algorithms are described
               for computing congruence closure in the general case and
               in the following two special cases : (I) G under C* is
               acyclic, and (II) G is acyclic and C identifies a single
               pair of vertices. the use of these algorithms to test
               expression equivalence (a problem central to program
               verification) and to test losslessness of joins in
               relational databases is described.},
  year = 1980,
  volume = 27,
  title = {Variations on the Common Subexpression Problem},
  pages = {758-771},
  number = 4,
  journal = {Journal of the ACM},
  author = {Downey, Peter J. and Sethi, Ravi and Tarjan, Robert Endre}
}

@Article{Downey-Sethi-Tarjan-1980review,
  keywords = {combinatorial, discrete, graph},
  abstract = {Let G be a directed graph such that for each vertex V in
               G, successors of V are ordered. Let C be any equivalence
               relation on the vertices of G. The congruence closure C*
               of C is the finest equivalence relation containing C and
               such that any two vertices having corresponding
               successors equivalent under C* are themselves
               equivalent under C*. Efficient algorithms are described
               for computing congruence closure in the general case and
               in the following two special cases : (I) G under C* is
               acyclic, and (II) G is acyclic and C identifies a single
               pair of vertices. the use of these algorithms to test
               expression equivalence (a problem central to program
               verification) and to test losslessness of joins in
               relational databases is described.},
  year = 1981,
  volume = 22,
  title = {Variations on the Common Subexpression Problem},
  number = 2,
  journal = {Computing Reviews},
  author = {Downey, Peter J. and Sethi, Ravi and Tarjan, Robert Endre}
}

@Book{Dowty-1979,
  author = {D. R. Dowty},
  title = {Word Meaning and Montague Grammar},
  publisher = Reidel,
  year = 1979,
  bibliographies = {RelMiCS}
}

@Misc{Drepper-Molnar-2003,
  author = {Ulrich Drepper and Ingo Molnar},
  title = {The {Native POSIX Thread Library} for {Linux}},
  month = JAN,
  year = 2003,
  WKloc = {A-1442},
  bibliographies = {SE3B}
}

@Article{Dresher-Ore-1938,
  author = {M. Dresher and Oystein Ore},
  title = {Theory of Multigroups},
  journal = AJM,
  volume = 60,
  year = 1938,
  pages = {705--733},
  bibliographies = {RelMiCS}
}

@InProceedings{Drewes-1991,
  author = {F. Drewes},
  title = {Recognising $k$-connected hypergraphs in cubic time},
  crossref = {GraTra91},
  pages = {83--122}
}

@InProceedings{Drewes-1994,
  author = {Frank Drewes},
  title = {A Lower Bound on the Growth of Functions Computed by
		  Tree Transducers},
  crossref = {CAAP94},
  pages = {100--114}
}

@InProceedings{Drewes-Hoffmann-Janssens-Minas-VanEetvelde-2007,
   author = {F. Drewes and B. Hoffmann and D. Janssens and M. Minas and Van Eetvelde, N.},
   title = {Shaped Generic Graph Transformation},
   pages = {201--216},
   crossref = {AGTIVE2007}
}

@InProceedings{Drewes-Hoffmann-Plump-2000,
  author = {Frank Drewes and Berthold Hoffmann and Detlef Plump},
  title = {Hierarchical Graph Transformation},
  pages = {98--113},
  crossref = {FoSSaCS2000},
  annote = {abstract on p. 143 of \cite{GraTra2000}, long version:
            \cite{Drewes-Hoffmann-Plump-2000a}}
}

@TechReport{Drewes-Hoffmann-Plump-2000a,
  author = {Frank Drewes and Berthold Hoffmann and Detlef Plump},
  title = {Hierarchical Graph Transformation},
  institution = {Fachbereich  Mathematik / Informatik, Universit\"at Bremen},
  year = 2000,
  number = {1/00},
  WKloc = {A-0979},
  annote = {Short version published as \cite{Drewes-Hoffmann-Plump-2000}.}
}

@Article{Drewes-Hoffmann-Plump-2002,
  author = {Frank Drewes and Berthold Hoffmann and Detlef Plump},
  title = {Hierarchical Graph Transformation},
  pages = {249--283},
  journal = {Journal on Computer and System Sciences},
  year = 2002,
  volume = 64,
  number = 2,
  WKloc = {doc/pap/BIB},
  bibliographies = {GraTraVis},
  annote = {short version: \cite{Drewes-Hoffmann-Plump-2000};
            previous long version: \cite{Drewes-Hoffmann-Plump-2000a}}
}

@InCollection{Drewes-Kreowski-Habel-1997,
  author = {Frank Drewes and Hans-J{\"o}rg Kreowski and Annegret Habel},
  title = {Hyperedge Replacement Graph Grammars},
  crossref = {HBGraTraI},
  chapter = 2,
  pages = {95--162}
}

@InProceedings{Dreyer-Harper-Chakravarty-2007,
  author = {Derek Dreyer and Robert Harper and Manuel M. T. Chakravarty and Gabriele Keller},
  title = {Modular type classes},
  crossref = {POPL2007},
  pages = {63--70},
  doi = {http://doi.acm.org/10.1145/1190216.1190229},
  abstract = {ML modules and Haskell type classes have proven to be
                  highly effective tools for program
                  structuring. Modules emphasize explicit
                  configuration of program components and the use of
                  data abstraction. Type classes emphasize implicit
                  program construction and ad hoc polymorphism. In
                  this paper, we show how the implicitly-typed style
                  of type class programming may be supported within
                  the framework of an explicitly-typed module language
                  by viewing type classes as a particular mode of use
                  of modules. This view offers a harmonious
                  integration of modules and type classes, where type
                  class features, such as class hierarchies and
                  associated types, arise naturally as uses of
                  existing module-language constructs, such as module
                  hierarchies and type components. In addition,
                  programmers have explicit control over which type
                  class instances are available for use by type
                  inference in a given scope. We formalize our
                  approach as a Harper-Stone-style elaboration
                  relation, and provide a sound type inference
                  algorithm as a guide to implementation.}
}

@InProceedings{Dridi-Neumann-1999,
  author = {Fredj Dridi and Gustav Neumann},
  title = {How to implement Web-based Groupware Systems based on {WebDAV}},
  booktitle = {{Proc.\null{} WETICE '99, IEEE 8th Intl.\null{} Workshop on Enabling Technologies: INfrastructure for Collaborative Enterprises, Stanford, CA, June 1999}},
  year = 1999,
  WKloc = {A-1121}
}

@Book{Drosten-1989,
  UniBwM = {INF700/S4950},
  WKloc = {Q-002},
  year = 1989,
  volume = 210,
  title = {{Termersetzungssysteme: Grundlagen der Prototyp-Generierung
      algebraischer Spezifikationen}},
  series = {Informatik-Fachberichte},
  publisher = {Springer},
  author = {Klaus Drosten}
}

@InProceedings{DuBois-Loidl-Trinder-2002.ps,
  author = {Du Bois, Andr{\'e} Rauber and Hans-Wolfgang Loidl and Phil W. Trinder},
  title = {Thread Migration in a Parallel Graph Reducer},
  crossref = {IFL2002},
  OPTpages = {},
  WKloc = {A-1446, doc/pap/BIB},
  bibliographies = {PMC},
  OPTannote = {}
}

@InProceedings{Dube-Beaudoin-200X,
  author = 	 {Danny Dub{\'e} and Vincent Be{\'a}udoin},
  title = 	 {Imploving {LZ77} Bit Recycling using All Matches},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1704},
  OPTannote = 	 {}
}

@Article{Dube-Feeley-2005,
  author = 	 {Danny Dub{\'e} and Marc Feeley},
  title = 	 {{BIT}: A Very Compact {Scheme} System for Microcontrollers},
  journal = 	 {},
  year = 	 {2005},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  WKloc = 	 {A-1705},
  OPTannote = 	 {}
}

@InProceedings{Dubiner-Galil-Magen-1990,
  author = {M. Dubiner and Z. Galil and E. Magen},
  title = {Faster Tree Pattern Matching},
  year = 1990,
  pages = {145--150},
  booktitle = {Proc.\null{} of the Symposium on Foundations of Computer
               Science (FOCS'90)},
  bibliographies = {RelMiCS}
}

@Article{Dublish-1990,
  year = 1990,
  volume = 36,
  title = {Some Comments on the Subtree Isomorphism Problem for Ordered Trees},
  pages = {273-275},
  journal = {Information Processing Letters},
  author = {P. Dublish}
}

@InProceedings{Dubois-Rouaix-Weis-1995,
  author = {Catherine Dubois and Fran{\c{c}}ois Rouaix and Pierre Weis},
  title = {Extensional Polymorphism},
  crossref = {POPL1995},
  pages = {???},
  OPTabstract = {We present the extensional polymorphism, a framework to
                  typecheck ad hoc polymorphic functions. This formalism is
                  compatible with parametric polymorphism, and supports a
                  large class of functions defined by
                  structural pattern matching on types.},
  URL = {ftp://ftp.inria.fr/INRIA/Projects/cristal/Pierre.Weis/generics.dvi.Z},
  WKloc = {A-0903},
  annote = {to be extended in Jun Furuse's PhD thesis, due summer 2000;
            see also \cite{Furuse-Weis-2000}.}
}

@Booklet{Duentsch-1988,
  author = {Ivo D{\"u}ntsch},
  title = {On {G}alois Closed Algebras of Binary Relations},
  note = {Preprint, 1988, pp.\null{} 19},
  bibliographies = {RelMiCS}
}

@InProceedings{Duentsch-1998,
  author = {Ivo D{\"u}ntsch and Hui Wang and Steve McKloskey},
  title = {Relation Algebras in Spatial Reasoning},
  crossref = {RelMiCS1998-PP},
  pages = {63--68},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Duentsch-Schmidt-Winter-2000,
  author = {D{\"u}ntsch, Ivo and Schmidt, Gunther and Winter, Michael},
  title = {A necessary relation algebra for mereotopology},
  journal = {Studia Logica},
  year = 2000,
  OPTvolume = {},
  OPTnumber = {},
  OPTmonth = {},
  OPTpages = {1--18},
  note = {in print},
  OPTunibwm = {INF/Z},
  OPTurl = {},
  OPTabstract = {},
  OPTcontents = {},
  WKloc = {A-1065, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@InProceedings{Duentsch-Wang-McKloskey-1998,
  author = {D{\"u}ntsch, Ivo and Wang, Hui and McKloskey, Steve},
  title = {Relation Algebras in Spational Reasoning},
  crossref = {RelMiCS1998-PP},
  pages = {63--68}
}

@Article{Duesterwald-Gupta-Soffa1997,
  author = {Evelyn Duesterwald and Rajiv Gupta and Mary Lou Soffa},
  title = {A Practical Framework for Demand-Driven Interprocedural Data Analysis},
  journal = ACM-TOPLAS,
  volume = 19,
  year = 1997,
  number = 6,
  local-URL = {/Users/Shared/papers/inBibtex/p992-duesterwald.pdf},
  pages = {992--1030},
  month = {November},
  annote = {Anand: Does not analyse pointer aliasing.},
  bibliographies = {Anand},
  abstract = {The high cost and growing importance of interprocedural
     data flow analysis have led to an increased interest in demand-driven
     algorithms. In this article, we present a general framework for
     developing demand-driven interprocedural data flow analyzers and
     report our experience in evaluating the performance of this
     approach. A demand for data flow information is modeled as a set of
     queries. The framework includes a generic demand-driven algorithm that
     determines the response to a query by iteratively applying a system of
     query propagation rules. The propagation rules yield precise responses
     for the class of distributive nite data flow problems. We also
     describe a two-phase framework variation to accurately handle
     nondistributive problems. A performance evaluation of our
     demand-driven approach is presented for two data flow problems,
     namely, reaching-de nitions and copy constant propagation. Our
     experiments show that demand-driven analysis performs well in
     practice, reducing both time and space requirements when compared with
     exhaustive analysis.},
  keywords = {Copy constant propagation, data flow analysis, def-use chains,
    demand-driven algorithms, distributive data flow frameworks,
    interprocedural data flow analysis,
    program optimizations},
  ACMcats = {D.3.4 [Programming Languages]: Processors|compilers; optimization;
    D.2.2 [Software Engineering]: Tools and Techniques;
    H.3.4 [Information Storage and Retrieval]: Systems and Software|question-answering}
}

@TechReport{Duggan-1993a,
  author = {Dominic Duggan},
  title = {Unification with Extended Patterns},
  institution = {{U}niversity of {W}aterloo},
  year = 1993,
  number = {{CS}-93-37},
  address = {Waterloo, Ontario, Canada},
  month = {July},
  URL = {http://nuada.uwaterloo.ca/dduggan/papers.html},
  note = {Revised March 1994 and September 1994},
  WKloc = {B-0024},
  abstract = {An extension of Miller's algorithm for decidable
		  higher-order unification is presented.  The
		  extension considered is the addition of products and
		  impredicative polymorphism.  Miller's pattern
		  restriction is generalized by allowing repeated
		  occurrences of variables to appear as arguments to
		  free function variables, provided such variables are
		  prefixed by distinct sequences of projections.  This
		  extended pattern restriction is the basis or the
		  definition of a higher-order version \cite{Duggan-1993b} of the
		  $\lambda\sigma$-calculus. The algorithm is
		  verified to terminate, and is shown to be sound and
		  complete.}
}

@TechReport{Duggan-1993b,
  author = {Dominic Duggan},
  title = {Higher-Order Substitutions},
  institution = {{U}niversity of {W}aterloo},
  year = 1993,
  number = {{CS}-93-44},
  address = {Waterloo, Ontario, Canada},
  month = {October},
  URL = {http://nuada.uwaterloo.ca/dduggan/papers.html},
  note = {Revised September 1994},
  WKloc = {B-0025,A-0518},
  abstract = {The $\lambda\sigma$-calculus is a concrete
		  $\lambda$-calculus of explicit substitutions,
		  designed for reasoning about implementations of
		  $\lambda$-calculi. Higher-order abstract syntax is
		  an approach to metaprogramming which explicitly
		  captures the variable-binding aspects of programming
		  language constructs. A new calculus of explicit
		  substitutions for higher-order abstract syntax is
		  introduced, allowing a high-level description of
		  variable binding in object languages while also
		  providing substitutions as explicit
		  programmer-manipulable data objects. The new
		  calculus is termed the $\lambda\sigma\beta_0$
		  calculus, since it makes essential use of an
		  extension of $\beta_0$-unification (described in
		  another paper). Termination and confluence are
		  verified for the $\lambda\sigma\beta_0$-calculus
		  similarly to the $\lambda\sigma$-calculus, and an
		  efficient implementation is given in terms of
		  first-order renaming substitutions. The verification
		  of confluence makes use of a verified extension of
		  Nipkow's higher-order critical pairs lemma to the
		  forms of rewrite rules required for the statement of
		  the $\lambda\sigma\beta_0$-calculus. This lemma,
		  combined with the implementability of substitutions
		  and the extended $\beta_0$-unification algorithm,
		  provides the first extension of Knuth-Bendix
		  completion to higher-order rewrite systems.}
}

@TechReport{Duggan-Bent-1994,
  author = {Dominic Duggan and Frederick Bent},
  title = {Explaining Type Inference},
  institution = {University of Waterloo},
  year = 1994,
  number = {CS-94-14},
  address = {Waterloo, Ontario, Canada},
  month = {January},
  URL = {http://nuada.uwaterloo.ca/dduggan/papers.html},
  note = {Revised March 1994},
  abstract = {Type inference is the compile-time process of
		  reconstructing missing type information in a program
		  based on the usage of its variables. ML and Haskell
		  are two languages where this aspect of compilation
		  has enjoyed some popularity, allowing type
		  information to be omitted while static type checking
		  is still performed. Type inference may be expected
		  to have some application in the prototyping and
		  scripting languages which are becoming increasingly
		  popular. A difficulty with type inference is the
		  confusing and sometimes counter-intuitive
		  diagnostics produced by the type checker as a result
		  of type errors.  A modification of the
		  Hindley-Milner type inference algorithm is
		  presented, which allows the specific reasoning which
		  led to a program variable having a particular type
		  to be recorded for type explanation. This approach
		  is close to the intuitive process used in practice
		  for debugging type errors.}
}

@TechReport{Duggan-Ophel-1994,
  author = {Dominic Duggan and John Ophel},
  title = {Kinded Parametric Overloading},
  institution = {University of Waterloo, Department of Computer Science},
  year = 1994,
  number = {CS-94-35},
  month = {September},
  note = {Supersedes CS-94-15 and CS-94-16, March 1994, and CS-93-32,
                  August 1993},
  URL = {ftp://nuada.uwaterloo.ca/pub/papers/kinded-parametric-overloading.ps.gz
         http://nuada.uwaterloo.ca/dduggan/papers.html#kinds-paper},
  abstract = {The combination of overloading and parametric
		  polymorphism has received some attention in the
		  functional programming community.  The main approach
		  has been that of Haskell type classes.  An approach
		  to the type-checking and semantics of parametric
		  overloading is presented, based on using structured
		  types to constrain type variables.  {\em Open kinds}
		  constrain type variables by sets of operations and
		  are useful for the incremental development of
		  reusable procedures (in a similar manner to Haskell
		  classes), while {\em closed kinds} constrain type
		  variables by sets of types (essentially providing a
		  type-safe form of dynamic typing).  The type system
		  includes a rule for ``closing up'' an open kind to a
		  closed kind.  Applications of these faciities
		  include local overloading, the combination of
		  parametric overloading with a Standard ML-like
		  module system, and an optimization which replaces
		  call-site closure construction with dynamic
		  dispatching based on explicit type tags.  A set
		  difference operation for kinds allows the
		  unambiguous typing of overlapping overload
		  instances.  The system of kinds is provided in some
		  detail.  A type system and type inference algorithm
		  are sketched.  An operational semantics is provided
		  and used to verify semantic soundness.  This
		  operational semantics is also used to verify the
		  correctness of the removal of call-site closure construction.}
}

@InProceedings{Dunn-1991,
  author = {J. M. Dunn},
  title = {Gaggle Theory: An Abstraction of {Galois} Connections and Residuation with Applications to Negation and Various Logical Operations},
  crossref = {JELIA1990},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  bibliographies = {RelMiCS}
}

@Article{Duran-1998,
  title = {Some Classes Containing a Fork Algebra Equivalent Variety Involving Projections},
  author = {Juan E. Dur{\'a}n},
  pages = {203--226},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {Some varieties that are extensions of relational algebras
      with two constants that play the role of projections are studied. The
      classes have as a subvariety the abstract fork algebra ({\bf AFA})
      equivalent variety involving projections. They are obtained by
      weakening some laws valid in {\bf AFA}. Some applications of the
      varieties in the literature and in the specification of abstract data
      types are exhibited. For each of the classes obtained, an answer is
      given to the question: ``Is the relational reduct of the class
      representable?''. For the subvarieties formed with the models that
      have a repressentable relational reduct, a repressentation theorem is
      proved. For them the finitization problem is studied. Next the
      varieties presented are compared by means of the inclusion order. For
      each class the problem of characterizing finite models is considered.
      Simple models in the varieties are studied. Finally, the existence of
      equivalent classes with a binary operation like fork is studied.},
  bibliographies = {RelMiCS}
}

@InCollection{Duval-Echahed-Prost-Ribeiro-2014,
  author={Duval, Dominique and Echahed, Rachid and Prost, Frederic and Ribeiro, Leila},
  title={Transformation of Attributed Structures with Cloning},
  pages={310--324},
  DOI={10.1007/978-3-642-54804-8_22},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-54804-8_22},
  year={2014},
  isbn={978-3-642-54803-1},
  LNCSbooktitle={FASE 2014},
  booktitle={Fundamental Approaches to Software Engineering},
  volume={8411},
  series=LNCS,
  editor={Gnesi, Stefania and Rensink, Arend},
  publisher={Springer Berlin Heidelberg},
  language={English},
  abstract = {Copying, or cloning, is a basic operation
    used in the specification of many applications in computer science.
    However, when dealing with complex structures, like graphs,
    cloning is not a straightforward operation since a copy of a single vertex
    may involve (implicitly) copying many edges.
    Therefore, most graph transformation approaches forbid the possibility of cloning.
    We tackle this problem by providing a framework for graph transformations with cloning.
    We use attributed graphs and allow rules to change attributes.
    These two features (cloning/changing attributes) together
    give rise to a powerful formal specification approach.
    In order to handle different kinds of graphs and attributes,
    we first define the notion of attributed structures in an abstract way.
    Then we generalise the sesqui-pushout approach of graph transformation
    in the proposed general framework and give appropriate conditions
    under which attributed structures can be transformed.
    Finally, we instantiate our general framework with different examples,
    showing that many structures can be handled and that the proposed framework
    allows one to specify complex operations in a natural way.}
}

@TechReport{Dwyer-1994,
  author = {Barry Dwyer},
  title = 	 {Programming Using Binary Relations: a proposed programming language},
  institution =  {Dept.\null{} of Computer Science, University of Adelaide},
  year = 	 1994,
  number =	 {94-04},
  note = 	 {\url{http://www.cs.adelaide.edu.au/~dwyer/}},
  keywords = 	 {Libra},
  bibliographies = {RelMiCS}
}

@TechReport{Dwyer-1995,
  author = {Barry Dwyer},
  title = 	 {{LIBRA}: A Lazy Interpreter of Binary Relational Algebra},
  institution =  {Dept.\null{} of Computer Science, University of Adelaide},
  year = 	 1995,
  number =	 {95-10},
  note = 	 {\url{http://www.cs.adelaide.edu.au/~dwyer/}},
  keywords = 	 {Libra},
  bibliographies = {RelMiCS}
}

@InProceedings{Dwyer-1998,
  author = {Barry Dwyer},
  title = {Relational Programming in Libra},
  pages = {35--58},
  abstract = {Libra is a general-purpose programming language based on the
                   algebra of binary relations.  It attempts to unify functional
                   and logic programming, retaining the advantages of both, and
                   avoiding some of the problems.  It has all the features needed
                   of a programming language, and a straightforward semantic
                   interpretation.  Since program specifications are easily
                   expressed as relations, it offers a simple path from a
                   specification to a program and from the program to its proof
                   of correctness.  The algebra of binary relations has several
                   operators whose effects are like those of familiar procedural
                   language constructs, for example, relational composition is
                   analogous to sequential execution. The Libra language is
                   illustrated by its application to a simple programming exercise.
                   Some conclusions are drawn.},
  editor = {Ali Jaoua and Peter Kempf and Gunther Schmidt},
  booktitle = {Using Relational Methods in Computer Science},
  year = 1998,
  month = JUL,
  series = {Technical Report Nr.\null{} 1998-03},
  publisher = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  bibliographies = {RelMiCS}
}

@PhDThesis{Dwyer-1999,
  author = {Barry Dwyer},
  title = {The Automatic Design of Batch Processing Systems},
  abstract = {Batch processing is a means of improving the efficiency of
      transaction processing systems. Despite the maturity of this field,
      there is no rigorous theory that can assist in the design of batch
      systems. This thesis proposes such a theory, and shows that it is
      practical to use it to automate system design. This has important
      consequences; the main impediment to the wider use of batch systems
      is the high cost of their development and maintenance. The theory is
      developed twice: informally, in a way that can be used by a systems
      analyst, and formally, as a result of which a computer program has
      been developed to prove the feasibility of automated design.

      Two important concepts are identified, which can aid in the
      decomposition of any system: 'separability', and 'independence'.
      Separability is the property that allows processes to be joined
      together by pipelines or similar topologies. Independence is the
      property that allows elements of a large set to be accessed and
      updated independently of one another. Traditional batch processing
      technology exploits independence when it uses sequential access in
      preference to random access. It is shown how the same property allows
      parallel access, resulting in speed gains limited only by the number
      of processors. This is a useful development that should assist in the
      design of very high throughput transaction processing systems.

      Systems are specified procedurally by describing an ideal system,
      which generates output and updates its internal state immediately
      following each input event. The derived systems have the same
      external behaviour as the ideal system except that their outputs and
      internal states lag those of the ideal system arbitrarily. Indeed,
      their state variables may have different delays, and the systems as
      whole may never be in consistent state.

      A 'state dependency graph' is derived from a static analysis of a
      specification. The reduced graph of its strongly-connected components
      defines a canonical process network from which all possible
      implementations of the system can be derived by composition. From
      these it is possible to choose the one that minimises any imposed
      cost function. Although, in general, choosing the optimum design
      proves to be an NP-complete problem, it is shown that heuristics can
      find it quickly in practical cases.},
  school = {Department of Computer Science, University of Adelaide},
  year = 1999,
  month = FEB,
  bibliographies = {RelMiCS},
  URL = {http://www.cs.adelaide.edu.au/~dwyer/thesis.html},
  WKloc = {doc/pap/BIB}
}

@Article{Dwyer-Hatcliff-Nanda-1998,
  author = {Matthew Dwyer and John Hatcliff and Muhammad Nanda},
  title = {Using Partial Evaluation to Enable Verification of Concurrent Software},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 22},
  WKloc = {A-0902, 79--84},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Dybjer-1996,
  author =       {Peter Dybjer},
  title =        {Internal Type Theory},
  OPTcrossref =  {TYPES1995},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {1158},
  OPTnumber =    {},
  OPTseries =    {LNCS},
  OPTaddress =   {},
  WKloc =     {A-1737, doc/pap/BIB},
  bibliographies = {DepObj},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Earley-1971,
  author = {Jay Earley},
  title = {Toward an Understanding of Data Structures},
  journal = {Communications of the ACM},
  year = 1971,
  volume = 14,
  number = 10,
  pages = {617--627},
  abstract = {This paper presents a notation and formalism for
		  describing the semantics of data structures. This is
		  based on directed graphs with named edges and
		  transformations on these graphs. In addition, an
		  implementation facility is described which could be
		  part of a programming language, which allows a
		  programmer who has expressed the semantics of an
		  algorithm in terms of the graphs to then specify the
		  implementation of some of his data structures in
		  order to gain efficiency.}
}

@InProceedings{Eastaughffe-1998,
  author = 	 {Katherine A. Eastaughffe},
  title = 	 {Support for Interactive Theorem Proving: Some Design Principles and Their Application},
  OPThowpublished = {found on CiteSeer},
  OPTbooktitle = {Informal Proceedings of ``User Interfaces for Theorem Provers 1998''},
  OPTeditor = {Yves Bertot},
  month = 	 MAY,
  year = 	 1998,
  abstract = {This paper proposes a set of guidelines for use in
     the design of automated support for theorem proving. in
     particular they are aimed at graphical user interfaces to
     existing interactive proof engines. The application of these
     guidelines to the design of a graphical user interface to
     Isabelle is described.},
  URL = {http://citeseer.nj.nec.com/14431.html},
  keywords = {XIsabelle},
  WKloc = 	 {A-1523, doc/pap/BIB},
  bibliographies = {HHOL}
}

@Article{Eastman-1973,
  author = {C. M. Eastman},
  title = {Automated Space Planning},
  journal = AI,
  volume = 4,
  year = 1973,
  pages = {41--64},
  bibliographies = {RelMiCS}
}

@Book{Eastward,
  author = {John Eastward},
  title = {Oxford Practice Grammar},
  publisher = OUP,
  year = {},
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTedition = {New edition},
  OPTmonth = {},
  OPTnote = {},
  bibliographies = {Liping}
}

@Article{Eaton-1940,
  author = {J. E. Eaton},
  title = {Theory of Cogroups},
  journal = DUKE,
  volume = 6,
  year = 1940,
  pages = {101--107},
  bibliographies = {RelMiCS}
}

@InProceedings{Ebert-Franzke-1994,
  author = 	 {J{\"u}rgen Ebert and Angelika Franzke},
  title = 	 {A Declarative Approach to Graph Based Modeling},
  crossref =	 {WG1994},
  pages =	 {38--50},
  bibliographies = {GXL}
}

@InProceedings{Ebert-Kullbach-WinterA-1999,
  author = 	 {J{\"u}rgen Ebert and Bernt Kullbach and Andreas Winter},
  title = 	 {\textsf{\slshape GraX} --­ An Interchange Format for Reengineering Tools},
  crossref =  {WCRE1999},
  OPTkey = 	 {},
  OPTbooktitle = {},
  pages = 	 {89--98},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {GXL},
  abstract = {Current research in software reengineering offers a great
      amount of tools specialized on certain reengineering tasks.
      The definition of a powerful common interchange format is
      a key issue to provide interoperability between these tools.
      This paper discusses aspects of data interchange formats
      for exchanging reengineering related data. It proposes a
      graph-based format to exchange both application specific
      concepts and data by XML documents.}
}

@InProceedings{Ebner-Brandner-Scholz-Krall-Wiedermann-Kadlec-2008,
  author = {Ebner, Dietmar and Brandner, Florian and Scholz, Bernhard and Krall, Andreas and Wiedermann, Peter and Kadlec, Albrecht},
  title = {Generalized instruction selection using SSA-graphs},
  booktitle = {LCTES '08: Proceedings of the 2008 ACM SIGPLAN-SIGBED conference on Languages, compilers, and tools for embedded systems},
  year = {2008},
  isbn = {978-1-60558-104-0},
  pages = {31--40},
  location = {Tucson, AZ, USA},
  doi = {http://doi.acm.org/10.1145/1375657.1375663},
  publisher = {ACM},
  address = {New York, NY, USA},
  OPTcrossref =  {LCTES2008},
  bibliographes = {Coconut},
  abstract = {Instruction selection is a well-studied compiler phase
     that translates the compiler's intermediate representation of programs
     to a sequence of target-dependent machine instructions
     optimizing for various compiler objectives (e.g. speed and space).
     Most existing instruction selection techniques are limited
     to the scope of a single statement or a basic block
     and cannot cope with irregular instruction sets
     that are frequently found in embedded systems.

     We consider an optimal technique for instruction selection
     that uses Static Single Assignment (SSA) graphs
     as an intermediate representation of programs
     and employs the Partitioned Boolean Quadratic Problem (PBQP)
     for finding an optimal instruction selection.
     While existing approaches are limited to instruction patterns
     that can be expressed in a simple tree structure,
     we consider complex patterns producing multiple results at the same time
     including pre/post increment addressing modes, div-mod instructions,
     and SIMD extensions frequently found in embedded systems.
     Although both instruction selection on SSA-graphs and PBQP
     are known to be NP-complete,
     the problem can be solved efficiently - even for very large instances.

     Our approach has been implemented in LLVM for an embedded ARMv5 architecture.
     Extensive experiments show speedups of up to 57\% on typical DSP kernels
     and up to 10\% on SPECINT 2000 and MiBench benchmarks.
     All of the test programs could be compiled within less than half a minute
     using a heuristic PBQP solver that solves 99.83\% of all instances optimally.}
}

@Article{Eco-2003a,
  author = 	 {Umberto Eco},
  title = 	 {Vegetal and Mineral Memory: The Future of Books},
  journal = 	 {Al-Ahram Weekly},
  year = 	 2003,
  month =	 NOV,
  WKloc = 	 {A-1506}
}

@InProceedings{Edalat-1994,
  title = {Domain Theory and Integration},
  author = {Abbas Edalat},
  pages = {115--124},
  crossref = {LICS9},
  abstract = {We present a domain-theoretic framework for measure theory
      and integration of bounded real-valued functions with respect to
      bounded Borel measures on compact metric spaces. The probabilistic
      power domain of the upper space of a compact metric space is an
      $\omega$-continuous dcpo with a basis consisting of linear
      combinations of point valuations (simple valuations). The set of
      normalised Borel measures of the metric space can be embedded into
      the maximal elements of this dcpo. The image of the embedding
      consists of all normalised valuations on the upper space which are
      supported in the set of maximal elements of the upper space, i.e. the
      singletons of the metric space. This implies that any bounded Borel
      measure on the compact metric space can be obtained as the least
      upper bound of an $\omega$-chain of simple valuations on the upper
      space. We thus have a constructive framework for measure theory. We
      use this setting to define a new notion of integral of a bounded
      real-valued function with respect to a bounded Borel measure on a
      compact metric space. Our new theory of integration, which we call
      R-integration, generalises the theory of Riemann integration. By
      using a chain of simple valuations, whose lub is the given Borel
      measure, we can obtain increasingly better approximations to the
      value of the integral, similar to the way the Riemann integral is
      obtained in calculus by using step functions. We show that all the
      basic results in the theory of Riemann integration can be extended in
      this more general setting, indicating that we have a faithful
      generalisation of the Riemann integral. Furthermore, with this notion
      of integration, the value of the integral, when it exists, coincides
      with the Lebesgue integral of the function, showing that we have a
      sound generalisation. A basic difference is that in the theory of
      Lebesgue integration of a real-valued function, it is the {\em
      function\/} which is approximated by simple functions, whereas in our
      new framework, it is the Borel {\em measure\/} which is approximated
      with simple valuations. This crucial distinction has important
      consequences in practice when we have a fixed measure and need to
      integrate functions which are continuous almost everywhere with
      respect to this measure. An immediate area for application is in the
      theory of iterated function systems with probabilities (fractals). We
      will also show by an example using the Cantor space that the integral
      of a function can be computed much more simply in our framework than
      in that of Lebesgue integration.}
}

@Article{Eder-1985,
  author =   {Elmar Eder},
  title =    {Properties of substitutions and unifications},
  journal =  JSYCO,
  year =     1985,
  volume =   1,
  pages =    {31--46},
  abstract = {In this paper the set of first order substitutions
    with a partial ordering “more general than” is investigated.
    It is proved that the set of equivalence classes of idempotent substitutions
    together with an added greatest element is a complete lattice.
    A simultaneous unification of finitely many finite sets of terms
    can be reduced to unifying each of the sets of terms separately
    and then building the supremum of the most general unifiers in this lattice.
    This saves time in an automatic proof procedure when combined
    with the concept of weak unification also introduced in this paper.}
}

@TechReport{Efremidis-1994,
  pages = 123,
  year = 1994,
  type = {Technical Report},
  number = {TR94-1434},
  title = {On Program Transformations},
  bibdate = {August 26, 1994},
  author = {Sofoklis G. Efremidis},
  WKloc = {B-0115},
  abstract = {In understanding complex algorithms, the notions of
                 encapsulation and modularization have played a key
                 role. An algorithm is broken into several parts or
                 modules, and understanding of each part is independent
                 of others. In addition, each part contains details that
                 are not needed by other parts and so can be hidden from
                 them. Programming languages provide support for
                 encapsulation and modularization in many different
                 forms. Early programming languages provided the
                 procedure and function as a means for modularization.
                 Later, files were introduced as a means of modularizing
                 programs. More sophisticated mechanisms were then
                 introduced, like modules, packages, structures, and
                 classes. In all these cases, the interface to a module
                 remained the procedure or function call. Programs that
                 use such modules contain calls to functions and
                 procedures for communicating with a module. Ideally,
                 using the operations that are provided by a module
                 should be done in exactly the same way as using
                 operations that are provided by modules should be easy
                 to intermix. In addition, substituting one module for
                 another that has the same functionality but different
                 implementation should involve a minimal amount of
                 effort. Recently, a new programming language, Polya,
                 has been designed, which attempts to support
                 modularization and at the same time incorporate the
                 operations that are provided by the modules in the
                 programming language itself. This is done by a
                 sophisticated type-definition facility and a mechanism
                 for transforming programs at the source-program level.
                 This thesis studies mechanisms for program
                 transformation at the source program level, in the
                 context of Polya. Program transformation is based on a
                 set of transformation rules that prescribe how a part
                 of a program is to be transformed, and a set of
                 directives that prescribe which program variables are
                 to be transformed. We first give an algorithm for
                 processing program transformations as described by the
                 transform construct. The algorithm constructs a
                 coordinate transformation of an abstract program based
                 on a set of transforms and transform directives for
                 transforming program variables. We then study the
                 problem of transforming expressions that have compound
                 types. Both the type constructor and the component
                 expressions of the original expression may be
                 transformed. No extra rules need be added to the bodies
                 of transforms that transform the type constructor and
                 the component expressions. In the sequel we investigate
                 the problem of transforming procedures and functions
                 that have parameters that need to be transformed.
                 Finally, the problem of transforming
                 program-transformation rules is studied. The program
                 transformation techniques are applied to two well-known
                 algorithms. The algorithms are source programs, which
                 are subsequently transformed to programs of
                 conventional programming languages, and then compiled
                 and run.},
  language = {English},
  institution = {Cornell University, Computer Science Department},
  month = jun,
  copyright = {Sofoklis Efremidis 1994 - All Rights Reserved}
}

@InProceedings{Ehrig-1978,
  author = {Hartmut Ehrig},
  title = {Introduction to the Algebraic Theory of Graph Grammars},
  crossref = {GG1978},
  pages = {1--69}
}

@InProceedings{Ehrig-1978-x,
  author = {Hartmut Ehrig},
  title = {Introduction to the Algebraic Theory of Graph Grammars},
  pages = {1--69},
  UniBwM = {INF700/Z7829-1},
  year = 1978,
  volume = 73,
  booktitle = {Graph-Grammars and Their Application to Computer Science and Biology, International Workshop},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer-Verlag},
  month = NOV,
  KEY = {GG '78},
  editor = {Volker Claus and Hartmut Ehrig and Grzegorz Rozenberg},
  address = {Bad Honnef}
}

@TechReport{Ehrig-1978a,
  year = 1978,
  title = {Introduction to the Algebraic Theory of Graph
		  Grammars (A Survey)},
  number = {78-28},
  note = {Prepared for GG '78},
  institution = {Technische Universit\"at Berlin, Fachbereich 20 --
		  Informatik},
  author = {Hartmut Ehrig}
}

@InProceedings{Ehrig-1986,
  author = {Hartmut Ehrig},
  title = {{Tutorial Introduction to the Algebraic Approach of
		  Graph-Grammars}},
  crossref = {GG1986},
  pages = {3--14}
}

@InProceedings{Ehrig-Bardohl-1994,
  author = {Hartmut Ehrig and Roswitha Bardohl},
  title = {Specification Techniques Using Dynamic Abstract Data Types and Application to Shipping Software},
  booktitle = {Proc. Shanghai Workshop on Software Technology},
  year = 1994,
  pages = {70--85},
  abstract = {The concept of dynamic abstract data types was recently
      proposed as a dynamic extension of the well-known concept of abstract
      data types, motivated by several recent approaches in the areas of
      algebraic specifications, object-orientation, evolving algebras and
      graph transformations. The basic idea of dynamic abstract data types
      is to extend abstract data types by dynamic operations which are
      transformations between abstract data types. In this paper we review
      the concept of dynamic abstract data types, and discuss different
      specification techniques using this concept, especially a new version
      based on algebraic graph transformations. This version allows to
      define transformations between states of the system by rules in the
      sense of graph grammars. This new kind of specification techniques is
      applied to the shipping software problem, where a semiformal
      specification based on entity-relationship diagrams is translated
      into a formal specification using dynamic abstract data types. The
      main ideas of the specifications and transformations are presented in
      this paper.}
}

@InCollection {Ehrig-EhrigK-Ermel-Hermann-Taentzer-2007,
   author = {Ehrig, Hartmut and Ehrig, Karsten and Ermel, Claudia and Hermann, Frank and Taentzer, Gabriele},
   affiliation = {Department of Computer Science, Technical University Berlin Germany},
   title = {Information Preserving Bidirectional Model Transformations},
   booktitle = {Fundamental Approaches to Software Engineering},
   series = LNCS,
   editor = {Dwyer, Matthew and Lopes, Antónia},
   publisher = Springer,
   isbn = {},
   pages = {72--86},
   volume = {4422},
   DOIURL = {http://dx.doi.org/10.1007/978-3-540-71289-3_7},
   DOI = {10.1007/978-3-540-71289-3_7},
   abstract = {Within model-driven software development, model transformation has become a key activity. It refers to a variety of operations modifying a model for various purposes such as analysis, optimization, and code generation. Most of these transformations need to be bidirectional to e.g. report analysis results, or keep coherence between models. In several application-oriented papers it has been shown that triple graph grammars are a promising approach to bidirectional model transformations. But up to now, there is no formal result showing under which condition corresponding forward and backward transformations are inverse to each other in the sense of information preservation. This problem is solved in this paper based on general results for the theory of algebraic graph transformations. The results are illustrated by a transformation of class models to relational data base models which has become a quasi-standard example for model transformation.},
   year = {2007}
}

@Book{Ehrig-EhrigK-Prange-Taentzer-2006,
  author =    {Hartmut Ehrig and Karsten Ehrig and Ulrike Prange and Gabriele Taentzer},
  title =        {Fundamentals of Algebraic Graph Transformation},
  publisher =    Springer,
  year =         {2006},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  DOIURL =      {http://link.springer.com/book/10.1007%2F3-540-31188-2},
  DOI  =    {10.1007/3-540-31188-2}
}

@book{Ehrig-Ermel-Golas-Hermann-2015_GMT,
 author = {Ehrig, Hartmut and Ermel, Claudia and Golas, Ulrike and Hermann, Frank},
 title = {Graph and Model Transformation: General Framework and Applications},
 year = {2015},
 isbn = {3662479796, 9783662479797},
 publisher = Springer,
 WKloc = {doc/pap/BIB},
 DirectURL = {http://link.springer.com/book/10.1007/978-3-662-47980-3},
 DOI = {10.1007/978-3-662-47980-3},
 abstract = {This book is a comprehensive explanation of graph and
                  model transformation. It contains a detailed
                  introduction, including basic results and
                  applications of the algebraic theory of graph
                  transformations, and references to the historical
                  context. Then in the main part the book contains
                  detailed chapters on M-adhesive categories,
                  M-adhesive transformation systems, and
                  multi-amalgamated transformations, and model
                  transformation based on triple graph grammars. In
                  the final part of the book the authors examine
                  application of the techniques in various domains,
                  including chapters on case studies and tool
                  support. The book will be of interest to researchers
                  and practitioners in the areas of theoretical
                  computer science, software engineering, concurrent
                  and distributed systems, and visual modelling.}
}

@InProceedings{Ehrig-Habel-Kreowski-Parisi-1990,
  author = {Hartmut Ehrig and Annegret Habel and Hans-J\"org
		  Kreowski and Francesco Parisi-Presicce},
  title = {From Graph Grammars to High Level Replacement Systems},
  pages = {269--287},
  crossref = {GG1990},
  abstract = {The algebraic approach to graph
		  grammars---well-known in the literature for several
		  types of graphs and structures---is extended to
		  include several new types of replacement systems,
		  especially the replacement of algebraic
		  specifications which were recently	introduced for
		  a rule-based approach to modular system design. This
		  leads to the new concept of high level replacement
		  systems which is formulated in an axiomatic
		  algebraic framework based on categories and
		  double-pushouts. In this paper only basic notions
		  like productions, derivations, parallel and
		  sequential independence are introduced for
		  high-level replacement systems leading to
		  Church-Rosser and Parallelism Theorems previously
		  shown in the literature for special cases only.}
}

@InCollection{Ehrig-Heckel-Korff-Loewe-Ribeiro-Wagner-Corradini-1997,
  author = {Hartmut Ehrig and Reiko Heckel and Martin Korff and Michael L{\"o}we and Leila Ribeiro and Annika Wagner and Andrea Corradini},
  title = {Algebraic Approaches to Graph Transformation, Part {II}:
     Single Pushout Approach and Comparison with Double Pushout Approach},
  crossref = {HBGraTraI},
  chapter = 4,
  pages = {247--312}
}

@InProceedings{Ehrig-Heckel-Llabres-Orejas-Padberg-Rozenberg-1998,
  author = {Hartmut Ehrig and Reiko Heckel and Merc{\`e} Llabre{\'e}s and Fernando Orejas and Julia Padberg and Grzegorz Rozenberg},
  title = {Double-Pullback Graph Transitions: A Rule-Based Framework with Incomplete Information},
  crossref = {TAGT1998},
  pages = {85--102}
}

@InProceedings{Ehrig-Korff-Loewe-1990,
  author = {Hartmut Ehrig and Martin Korff and Michael L{\"o}we},
  title = {Tutorial Introduction to the Algebraic Approach of
		  Graph Grammars Based on Double and Single Pushouts},
  pages = {24--37},
  crossref = {GG1990},
  WKloc = {A-0272},
  contents = {1 General Format of Productions and Direct Derivations
                   2 An Introductory Example
                   3 Towards an Algebraic Version
                   4 Algebraic Version of Productions and Direct Derivations
                   5 Definitions of Graphs and Graph Morphisms
                   6 Explicit Version of Gluing and Gluing Diagram
                   7 Algebraic Version of Gluing and Gluing Diagram
                   8 Essentials of the Algebraic Approach
                   9 Examples
                   10 Single Pushout Approach
                   11 References},
  abstract = {The gluing construction on which the algebraic
		  notion of a derivation is based operationally
		  provides a simple and intuitive understanding of
		  graph rewriting. Inheriting the powerful toolbox of
		  category theory, its abstract version as a (single
		  resp. double) pushout leads to highly compact and
		  elegant proofs especially for the basic
		  constructions of sequential and parallel independent
		  derivations as well as for concurrent and
		  amalgamated productions respectively.}
}

@Article{Ehrig-Kreowski-MaggioloSchettini-Rosen-Winkowski-1981,
  author = {Ehrig, Hartmut and Kreowski, Hans-J{\"o}rg and Maggiolo-Schettini, Andrea and Rosen, Barry K. and Winkowski, Jozef},
  affiliation = {Fachbereich Informatik Technische Universität Berlin 1000 Berlin 10 Federal Republic of Germany},
  title = {Transformations of structures: An algebraic approach},
  WKloc = {doc/pap/BIB},
  journal = {Theory of Computing Systems},
  publisher = {Springer New York},
  issn = {1432-4350},
  pages = {305--334},
  volume = {14},
  issue = {1},
  DOIURL = {http://dx.doi.org/10.1007/BF01752403},
  DOI = {10.1007/BF01752403},
  abstract = {This paper introduces a new mathematical approach to transformations of structures, where the concept of  structure  is extremely general. Many structures and transformations that arise in biology as well as computer science are special cases of our concepts. A structure may be changed by finding an occurrence of a pattern and replacing it by another pattern as specified by a rule. To prove theorems about long sequences of applications of complicated rules, we need precise and tractable mathematical definitions of rules and how to apply them. This paper presents such definitions and three fundamental theorems, together with brief remarks on applications to control flow analysis, record handling, and evaluation of recursively defined functions. Unlike some previous efforts toward a rigorous theory of transformations of structures, this paper uses ideas and results from abstract algebra to minimize the need for elaborate constructions.},
   year = {1981}
}

@Article{Ehrig-Kreowski-Mahr-Padawitz-1983,
  author = {Hartmut Ehrig and Hans-J\"org Kreowski and B. Mahr
		  and Peter Padawitz},
  title = {Algebraic Implementation of Abstract Data Types},
  journal = {Theoretical Computer Science},
  volume = 20,
  number = 2,
  pages = {207--266},
  abstract = {Starting with a review of the theory of algebraic
               specifications in the sense of the adj-group a new
               theory of algebraic implementations of abstract data
               types is presented. While main concepts of this new
               theory were given already at several conferences this
               paper provides the full theory of algebraic
               implementations developed in berlin except of complexity
               considerations which are given in a separate paper. The
               new concept of algebraic implementations includes
               implementations for algorithms in specific programming
               and on the other hand it meets also the requirements for
               stepwise refinement of structured programs and software
               systems as introduced by Dijkstra and Wirth. On the
               syntactical level an algebraic implementations to a
               system of recursive programs while the semantical level
               is defined by algebraic constructions, called synthesis
               restriction and identification. Moreover the concept
               allows composition of implementations and a rigorous
               study of correctness. The main results of the paper are
               different kinds of correctness criteria which are
               applied to a number of illustrating examples including
               the implementation of sets by hash-tables. Algebraic
               implementations of larger systems like a histogram or a
               parts system are given in separate case studies which,
               however are not included in this paper.}
}

@InProceedings{Ehrig-Kreowski-Taentzer-1993,
  author = {Hartmut Ehrig and Hans-J\"org Kreowski and Gabriele Taentzer},
  title = {Canonical Derivations for High-Level Replacement Systems},
  crossref = {GTCS93},
  pages = {153--169},
  abstract = {Canonical derivations, previously studied for string
		  and graph grammars only, are generalized from graph
		  grammars to high-level replacement systems, short
		  HLR-systems. These systems were recently introduced
		  to provide a common categorical framework for
		  different types of replacement systems on complex
		  objects, including graphs, hypergraphs, structures
		  and algebraic specifications. It turns out that
		  basic results concerning synthesis and analysis of
		  parallel derivation sequences in HLR-systems,
		  obtained in previous papers, can be extended to
		  construct canonical parallel derivation sequences
		  which are optimal w.r.t.\null{} leftmost
		  parallelism. The main results show the existence and
		  uniqueness of canonical derivations under weak
		  assumptions for the underlying categories of
		  HLR-systems. These results are specialized to
		  graphs, hypergraphs, Petri nets, algebraic
		  specifications and others by classifying the
		  underlying categories with respect to the
		  assumptions. This leads to interesting new results
		  for most of the corresponding HLR-systems.},
  WKloc = {A-0291}
}

@InProceedings{Ehrig-Loewe-1991,
  author = {Hartmut Ehrig and Martin L\"owe},
  title = {{The ESPRIT Basic Research Working Group COMPUGRAPH
		  ``Computing by Graph Transformation'': a survey}},
  crossref = {GraTra91},
  pages = {3--6},
  WKloc = {A-0147}
}

@InProceedings{Ehrig-Loewe-1991a,
  author = {Hartmut Ehrig and Martin L\"owe},
  title = {Parallel and Distributed Derivations in the
		  Single-Pushout Approach},
  crossref = {GraTra91},
  pages = {123--144},
  WKloc = {A-0150},
  abstract = {Parallel and distributed derivations are introduced
		  and studied in the single-pushout approach, which
		  models rewriting by pushout constructions in
		  appropriate categories of partial morphisms. We
		  present a categorical framework for this approach in
		  an axiomatic way. Models of this categorical
		  framework are among others: graphs, hypergraphs,
		  relational structures, and algebraic specifications
		  with suitable partial morphisms. Several new results
		  concerning parallelism and distributed parallelism
		  are presented which are even new in the example categories.},
  bibliographies = {RelMiCS}
}

@TechReport{Ehrig-Loewe-1992,
  author = {Hartmut Ehrig and Michael L\"owe},
  title = {{COMPUGRAPH} Computing by Graph Transformation: Final Report,
          {ESPRIT Basic Research Working Group No. 3299}},
  year = 1992,
  number = {92-08},
  institution = {Technische Universit\"at Berlin},
  month = MAR,
  type = {Forschungsbericht des Fachbereichs Informatik}
}

@Book{Ehrig-Mahr-1985,
  author = {Hartmut Ehrig and Bernd Mahr},
  title = {Fundamentals of Algebraic Specification (Volumes 1 and 2)},
  year = 1985,
  publisher = Springer
}

@Book{Ehrig-Mahr-1985a,
  author = {Hartmut Ehrig and Bernd Mahr},
  title = {Fundamentals of Algebraic Specification 1: Equations and Initial Semantics},
  year = 1985,
  publisher = Springer,
  McMaster = {QA 76.9 .D35 E37 1985 V.1},
  McMasterID = {39005020172320}
}

@Book{Ehrig-Mahr-1985b,
  author = {Hartmut Ehrig and Bernd Mahr},
  title = {Fundamentals of Algebraic Specification 2: Module Specifications and Constraints},
  year = 1985,
  publisher = Springer,
  McMaster = {QA 76.9 .D35 E37 1985 V.2},
  McMasterID = {39005020172338}
}

@Book{Ehrig-Mahr-Cornelius-GrosseRhode-Zeitz-2001,
  author = {Hartmut Ehrig and Bernd Mahr and Felix Cornelius and Martin Gro{\ss}e-Rhode and Philip Zeitz},
  title = {{Mathematisch-strukturelle Grundlagen der Informatik}},
  publisher = Springer,
  series = {Springer-Lehrbuch},
  year = 2001,
  ISBN = {3-540-41923-3},
  edition = {2. Auflage},
  UniBwM = {MAT001/YG4259},
  note = {XXI+621 S.},
  URL = {http://www.springer.de/cgi-bin/search_book.pl?isbn=3-540-41923-3},
  abstract = {In fünf sorgfältig aufeinander abgestimmten Teilen behandelt
      das Buch die wesentlichen mathematischen Elemente der formalen
      Spezifikation von Systemen und der Aussagen- und Prädikatenlogik, die
      für das Verständnis des formalisierten Problemlösens entscheidend und
      damit für Informatiker unerläßlich sind. Eine Einführung in die
      intuitive Mengentheorie vermittelt zunächst notwendige mathematische
      Grundlagen. Motiviert durch das Konzept von Datenstrukturen und
      abstrakten Datentypen werden dann algebraische Strukturen in der
      Informatik behandelt. Danach werden Aussagen- und Prädikatenlogik aus
      der Sicht der Mathematik und Informatik dargestellt. Schließlich
      führt die Kategorientheorie für Informatiker in die Welt der
      abstrakten Behandlung mathematischer Strukturen ein. Die Neuauflage
      wurde erweitert um Darstellungen zur Modellalgebra und zur
      Implementierung. Übungsaufgaben wurden ergänzt.}
}

@TechReport{Ehrig-Orejas-Padberg-1999,
  author = {Ehrig, Hartmut and Orejas, Fernando and Padberg, Julia},
  title = {Relevance, Integration and Classification of Specification Formalism and Formal Specification Techniques},
  institution = {FB Informatik, TU Berlin},
  year = 1999,
  number = {99/13},
  WKloc = {A-1182, doc/pap/BIB},
  bibliographies = {SpecTech}
}

@Article{Ehrig-Padberg-Prange-Habel-2006,
  author = {Hartmut Ehrig and Julia Padberg and Ulrike Prange and Annegret Habel},
  title =        {Adhesive High-Level Replacement Systems: A New Categorical Framework for Graph Transformation},
  journal =      FUNDI,
  year =         2006,
  volume =    74,
  number =    1,
  pages =     {1--29},
  DirectURL =     {http://iospress.metapress.com/content/f89c8ba4nbeq1xc4/},
  WKloc =      {doc/pap/BIB},
  abstract =    {Adhesive high-level replacement (HLR) systems are introduced
     as a new categorical framework for
     graph transformation in the double pushout (DPO) approach,
     which combines the well-known concept of HLR
     systems with the new concept of adhesive categories
     introduced by Lack and Soboci{\'n}ski.

     In this paper we show that most of the HLR properties,
     which had been introduced to generalize some basic results
     from the category of graphs to high-level structures,
     are valid already in adhesive HLR categories.
     This leads to a smooth categorical theory of HLR systems
     which can be applied to a large variety of graphs and other visual models.
     As a main new result in a categorical framework
     we show the Critical Pair Lemma
     for the local confluence of transformations.
     Moreover we present a new version of embeddings and extensions
     for transformations in our framework of adhesive HLR systems.},
  keywords = {high-level replacement systems, adhesive categories,
     adhesive HLR categories, graph transformation,
     local confluence of transformations, Critical Pair Lemma}
}

@InProceedings{Ehrig-Padberg-Ribeiro-1992,
  author = {Hartmut Ehrig and Julia Padberg and L. Ribeiro},
  title = {Algebraic High Level Nets: {Petri} Nets Revisited},
  crossref = {SADT92},
  pages = {188--206},
  keywords = {restaurant of dining philosophers}
}

@Unpublished{Ehrig-Parisi-1991,
  author = {Hartmut Ehrig and Francesco Parisi-Presicce},
  title = {Interactions between Graph Grammars and Modular
		  System Design},
  year = 1991,
  note = {(draft)}
}

@TechReport{Ehrig-Parisi-1991a,
  author = {Hartmut Ehrig and Francesco Parisi-Presicce},
  title = {Nonequivalence of Categories for Equational
		  Algebraic Specifications in View of
		  High-Level-Replacement Systems},
  institution = {TU Berlin},
  year = 1991,
  type = {Forschungsbericht des Fachbereichs Informatik},
  number = {91/16},
  month = SEP,
  abstract = {Five different alternatives for the definition of
		  standard equational algebraic specifications and
		  corresponding specification morphisms leading to
		  different categories are studied and compared with
		  each other. Although intuitively they seem to be
		  equivalent it turns out that they lead to three
		  different equivalent classes of categories. In fact,
		  it is shown that the construction of pushouts and
		  pullbacks is significantly different in these three
		  cases. Although the corresponding specification
		  logics are all semantical equivalent in a weak sense
		  three of them are semantical inconsistent
		  w.r.t.~pullback constructions. The nonequivalence of
		  the equational categories has also significant
		  implications for the corresponding
		  high-level-replacement (HLR) system. In fact, only
		  for one type -- with the class M of all injective
		  specification morphisms -- the corresponding
		  HLR-properties are satisfied which allow to conclude
		  the corresponding Church-Rosser, Parallelism- and
		  Concurrency Theorems for High-Level-Replacement
		  systems. In the other cases only subsets of these
		  properties are satisfied depending on the choice of
		  the distinguished class M of morphisms for the
		  productions. In some cases counterexamples to
		  previous claims in the literature are shown while
		  the validity of some other properties is still open.}
}

@InProceedings{Ehrig-Parisi-1993,
  author = {Hartmut Ehrig and Francesco Parisi-Presicce},
  title = {Interaction between Algebraic Specification Grammars
		  and Modular System Design},
  pages = {217--224},
  crossref = {AMAST1993},
  WKloc = {A-1210}
}

@InProceedings{Ehrig-Pfender-Schneider-1973,
  author = {Hartmut Ehrig and Michael Pfender and Hans J\"urgen Schneider},
  title = {Graph Grammars: An Algebraic Approach},
  year = 1973,
  pages = {167--180},
  booktitle = {Proc.\null{} IEEE Conf.\null{} on Automata and Switching Theory, {SWAT '73}},
  conferenceaddress = {Iowa City}
}

@InProceedings{Ehrig-Prange-Taentzer-2004,
  author =       {Hartmut Ehrig and Ulrike Prange and Gabriele Taentzer},
  title =        {Fundamental Theory for Typed Attributed Graph Transformation},
  crossref =  {ICGT2004},
  pages =     {161--177},
  abstract =    {The concept of typed attributed graph transformation is most significant for modeling and meta modeling in software engineering and visual languages, but up to now there is no adequate theory for this important branch of graph transformation. In this paper we give a new formalization of typed attributed graphs, which allows node and edge attribution. The first main result shows that the corresponding category is isomorphic to the category of algebras over a specific kind of attributed graph structure signature. This allows to prove the second main result showing that the category of typed attributed graphs is an instance of ldquoadhesive HLR categoriesrdquo. This new concept combines adhesive categories introduced by Lack and Sobocinacuteski with the well-known approach of high-level replacement (HLR) systems using a new simplified version of HLR conditions. As a consequence we obtain a rigorous approach to typed attributed graph transformation providing as fundamental results the Local Church-Rosser, Parallelism, Concurrency, Embedding and Extension Theorem and a Local Confluence Theorem known as Critical Pair Lemma in the literature.}
}

@Misc{Ehrig-Taentzer-1996,
  author = {Hartmut Ehrig and Gabriele Taentzer},
  title = {Computing by Graph Transformation, A Sirvey and Annotated bibliography},
  month = MAR,
  year = 1996,
  keywords = {CompuGraph},
  WKloc = {B-0079}
}

@InProceedings{EhrigK-Ermel-Haensgen-Taentzer-2004,
    author = {Karsten Ehrig and Claudia Ermel and Stefan H{\"a}nsgen and Gabriele Taentzer},
    title = {Towards Graph Transformation based Generation of Visual Editors using {Eclipse}},
    booktitle = {Visual Languages and Formal Methods, Elec.\null{} Notes Theor.\null{} Comp.\null{} Sci.\null{} vol.~127},
    year = {2004},
    pages = {127--143},
    publisher = {Elsevier},
  keywords = {Tiger}
}

@Article{Eijck-1994,
  author = {Eijck, J.~van},
  journal = FACOMP,
  pages = {766--787},
  title = {Presupposition failure --- a comedy of errors},
  volume = {6A},
  year = 1994,
  bibliographies = {RelMiCS}
}

@InProceedings{Eklund-2003,
  author = 	 {Patrik Eklund},
  title = 	 {Monads and powerset algebras},
  crossref =  {RelMiCS2003-PP},
  pages = 	 {40-42},
  authorURL = {http://www.cs.umu.se/~peklund}
}

@InProceedings{Eklund-Gaehler-1992,
  author = 	 {Patrik Eklund and W. G{\"a}hler},
  title = 	 {Fuzzy filter functors and convergence},
  booktitle = {Applications of category theory to fuzzy subsets},
  pages = 	 {109--136},
  year = 	 1992,
  editor = 	 {S. E. Rodabaugh and others},
  series = 	 {Theory and Decision Library B},
  publisher = {Kluwer},
  annote = 	 {Contains a counter-example for monad composition, according to \cite{Eklund-Gaehler-2004}.}
}

@InProceedings{Eklund-Gaehler-2004,
  author = 	 {Patrik Eklund and W. G{\"a}hler},
  title = 	 {Partially ordered monads and powerset {Kleene} algebras},
  booktitle = 	 {Proc.\null{} 10th Information Processing and Management of Uncertainty in Knowledge Based Systems Conference (IPMU 2004)},
  authorURL = {http://www.cs.umu.se/~peklund},
  year = 	 {2004},
  WLloc = {doc/pap/BIB},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Eklund-Galan-2007,
  author = 	 {P. Eklund and M. A. Gal{\'a}n},
  title = 	 {The Rough Powerset Monad },
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {37th International Symposium on Multiple-Valued Logic (ISMVL'07)},
  OPAC = {http://ieeexplore.ieee.org.libaccess.lib.mcmaster.ca/servlet/opac?punumber=4215911},
  ISBN = {0-7695-2831-7},
  pages = 	 {27--32},
  year = 	 {2007},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  DOI = 	 {http://doi.ieeecomputersociety.org/10.1109/ISMVL.2007.55},
  WKloc = {doc/pap/BIB},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  abstract = 	 {Rough sets provide a good environment to deal with
                  vagueness and uncertainty situations. In this paper
                  we show how monads can be used to generalize and
                  interpret rough situations. In particular, the
                  partially ordered ordinary power set monad turns out
                  to contain sufficient structure in order to provide
                  rough set operations.}
}

@InProceedings{Eklund-Galan-OjedaAciego-Valverde-2000,
  author = {P. Eklund and M. A. Gal{\'a}n and Ojeda-Aciego, M. and A. Valverde},
  title = {Set functors and generalised terms},
  booktitle = {Proc.\null{} IPMU 2000, 8th Information Processing and
Management of Uncertainty in Knowledge-Based Systems Conference,
vol. III},
  OPTcrossref =  {},
  pages = 	 {1595--1599},
  year = 	 {2000},
  WKloc = 	 {doc/pap/BIB},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  annote = 	 {Theorem 4.1 seems to correspond to the \textsf{swap} result
                  in \cite{Jones-Duponcheel-1993}.}
}

@Article{Eklund-Galan-Medina-OjedaAciego-Valverde-2001,
  author = {P. Eklund and M. A. Gal{\'a}n and J. Medina and Ojeda Aciego, M. and A. Valverde},
  title = 	 {A graphical approach to monad compositions},
  journal = 	 ENTCS,
  year = 	 {2001},
  volume = 	 {40},
  pages = 	 {145--160},
  DOI = 	 {http://dx.doi.org/10.1016/S1571-0661(05)80041-6},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {In this paper we show how composite expressions
                  involving natural transformations can be pictorially
                  represented in order to provide graphical proof
                  support for providing monad compositions. Examples
                  are drawn using powerset monads composed with the
                  term monad.}
}

@Article{Eklund-Galan-Medina-OjedaAciego-Valverde-2002,
  author = {P. Eklund and M. A. Gal{\'a}n and J. Medina and Ojeda Aciego, M. and A. Valverde},
  title = 	 {A categorical approach to unification of generalised terms},
  journal = 	 ENTCS,
  year = 	 2002,
  volume = 	 66,
  number = 	 5,
  pages = 	 {41--51},
  note = 	 {Special Issue: UNCL'2002, Unification in Non-Classical Logics (ICALP 2002 Satellite Workshop)},
  DOI = 	 {http://dx.doi.org/10.1016/S1571-0661(04)80513-9}
}

@Article{Eklund-Galan-Medina-OjedaAciego-Valverde-2002a,
  author = {P. Eklund and M. A. Gal{\'a}n and J. Medina and Ojeda Aciego, M. and A. Valverde},
  title = 	 {Set functors, $L$-Fuzzy Set Categories, and Generalized Terms},
  journal = 	 {Computers and Mathematics with Applications},
  year = 	 2002,
  volume = 	 43,
  number = 	 {6--7},
  pages = 	 {693--705},
  DOI = {http://dx.doi.org.libaccess.lib.mcmaster.ca/10.1016/S0898-1221(01)00314-5},
  WKloc = {doc/pap/BIB},
  abstract = 	 {In this work, we generalize previous constructions
                  of fuzzy set categories, introduced in [1], by
                  considering L-fuzzy sets in which the values of the
                  characteristic functions run on a completely
                  distributive lattice, rather than in the unit real
                  interval. Later, these L-fuzzy sets are used to
                  define the L-fuzzy categories, which are proven to
                  be rational. In the final part of the paper, the
                  L-fuzzy functors given by the extension principles
                  are provided with a structure of monad which is
                  used, together with the functorial definition of the
                  term monad, to provide monad compositions as a basis
                  for a notion of generalised terms. }
}

@Article{Eklund-Galan-Medina-OjedaAciego-Valverde-2004,
  author = {P. Eklund and M. A. Gal{\'a}n and J. Medina and Ojeda Aciego, M. and A. Valverde},
  title = 	 {Similarities between powersets of terms},
  journal = 	 {Fuzzy Sets and Systems},
  year = 	 2004,
  volume = 	 144,
  number = 	 1,
  pages = 	 {213--225},
  URL = {http://sevein.matap.uma.es/~aciego/moa-publi.html#powerset-fss},
  WKloc = {doc/pap/BIB (draft)},
  abstract = 	 {Generalisation of the foundational basis for
                  many-valued logic programming builds upon
                  generalised terms in form of powersets of terms. A
                  categorical approach involving set and term functors
                  as monads allows for a study of monad compositions
                  that provide variable substitutions, and
                  compositions thereof. In this paper, substitutions
                  and unifiers appear as constructs in Kleisli
                  categories related to particular composed powerset
                  term monads. Specifically, we show that a frequently
                  used similarity-based approach to fuzzy unification
                  is compatible with the categorical approach, and can
                  be adequately extended in this setting; also some
                  examples are included in order to illuminate the
                  definitions.}
}

@InProceedings{Eklund-Helgesson-2009,
  author =       {Patrik Eklund and Robert Helgesson},
  title =        {Composing Partially Ordered Monads},
  crossref =  {RelMiCS2009},
  pages =     {88-102},
  bibliographies = {RelMiCS, RelMiCS11},
  abstract =      {Composition of the
     many-valued powerset partially ordered monad with the term monad
     provides extensions to non-classical relations
     and also new examples for Kleene algebras.}
}

@PhDThesis{Elliot-1990,
  author = {Conal M. Elliot},
  title = {Extensions and Applications of Higher-order Unification},
  school = {Carnegie Mellon University},
  year = 1990,
  WKloc = {A-1171}
}

@Misc{Elliot-199X,
  author = {Conal M. Elliot},
  title = {Functional Images},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1173}
}

@InProceedings{Elliot-Finne-deMoor-2000,
  author = {Conal Elliot and Sigbj{\oslash}rn Finne and de Moor, Oege},
  title = {Compiling Embedded Languages},
  crossref = {SAIG2000},
  pages = {9--27},
  WKloc = {A-1172, doc/pap/BIB},
  URL = {http://link.springer.de/link/service/series/0558/bibs/1924/19240009.htm},
  abstract = {Functional languages are particularly well-suited to the
      implementation of interpreters for domain-specific embedded languages
      (DSELs). We describe an implemented technique for producing
      \emph{optimizing compilers} for DSELs, based on Kamin's idea of DSELs
      for program generation. The technique uses a data type of syntax for
      basic types, a set of smart constructors that perform rewriting over
      those types, some code motion transformations, and a back-end code
      generator. Domain-specific optimization results from chains of
      rewrites on basic types. New DSELs are defined directly in terms of
      the basic syntactic types, plus host language functions and tuples.
      This definition style makes compilers easy to write and, in fact,
      almost identical to the simplest embedded interpreters. We illustrate
      this technique with a language \emph{Pan} for the computationally
      intensive domain of image synthesis and manipulation.}
}

@InProceedings{Elliott-Hudak-1997,
      title        = {Functional Reactive Animation},
      url          = {http://conal.net/papers/icfp97/},
      author       = "Conal Elliott and Paul Hudak",
      booktitle    = "International Conference on Functional Programming",
      year         = 1997,
  annote = {Fran}
}

@Article{Ellis-1872,
  author = {A. J. Ellis},
  title = {On the Algebraical Analogues of Logical Relations},
  journal = PROLON,
  year = {1872--3},
  bibliographies = {RelMiCS}
}

@Book{Ellis-Stroustrup-1990,
  UniBwM = {INF590/V9313},
  year = 1990,
  title = {The Annotated {C$++$} Reference Manual},
  publisher = {Addison-Wesley},
  author = {Margaret A. Ellis and Bjarne Stroustrup}
}

@InCollection{Emerson-1990,
  author = {E. Allen {Emerson}},
  title = {Temporal and Modal Logic},
  pages = {995--1072},
  crossref = {HBThCS-b},
  WKloc = {A-0937},
  bibliographies = {SpecTech},
  PreprintURL = {http://www.cs.utexas.edu/users/emerson/Pubs/handbook3.ps}
}

@Misc{Emerson-Trefler-199X,
  author = {E. Allen Emerson and Richard J. Trefler},
  title = {Parametric Quantitative Temporal Reasoning},
  year = {199?},
  WKloc = {A-0637}
}

@Article{Emoto-FischerS-HuZhenjiang-2012,
  author =       {KntoEmoto and Sebastian Fischer and Zhenjiang Hu},
  title =        {Filter-embedding Semiring Fusion Programming with {MapReduce}},
  journal =      {Formal Aspects of Computing},
  year =         {2012},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  note =         {(to appear \unfinished)},
  WKloc =        {A-1741 semi-final version}
}

@Misc{Empiricus-1XY,
  OPTkey =       {},
  author =    {Sextus Empiricus},
  OPTtitle =     {},
  OPThowpublished = {},
  OPTmonth =     {},
  OPTyear =      {},
  OPTnote =      {},
  annote =    {Brian R Gaines <gaines@cpsc.ucalgary.ca> on cg list on 2010-12-07:

    Thus, one can show that the semantic networks from which conceptual
    graphs draw the meanings of their terms can, in general, be
    represented with syllogistic primitives rather than truth-functional
    connectives often taken to have been defined by Boole (although the
    definitions go back at least to Sextus Empiricus in the second century AD).}
}

@MastersThesis{Endres-Mueller-1991,
  keywords = {SHOPS, HOPS},
  year = 1991,
  title = {Objektorientierte {Implementierung} eines
		  {Programmiersystems} mit zweischichtig typisierter
		  {DAG-Sprache} und {Regelanwendung}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  type = {Diplomarbeit},
  number = {ID 45/91},
  month = DEC,
  author = {Klaus Endres and Frank M{\"u}ller}
}

@InProceedings{Engelfriet-1994,
  author = {J. Engelfriet},
  title = {Graph Grammars and Tree Transducers},
  crossref = {CAAP94},
  pages = {15--37},
  note = {invited paper},
  WKloc = {A-0351},
  abstract = {Regular tree grammars can be used to generate
		  graphs, by generating expressions that denote
		  graphs. Top-down and bottom-up tree transducers are
		  a tool for proving properties of such graph
		  generating tree grammars.}
}

@Article{Engelfriet-Heyker-1992,
  author = {Joost Engelfriet and Linda Heyker},
  title = {Context-Free Hypergraph Grammars Have the Same
		  Term-Generating Power as Attribute Grammars},
  journal = {Acta Informatica},
  volume = 29,
  pages = {161--210},
  year = 1992,
  UniBwM = {Z265-29},
  annote = {contains some useful early-history bibliography!}
}

@Article{Engelfriet-Leih-Rozenberg-1988,
  author = {Joost Engelfriet and George Leih and Grzegorz Rozenberg},
  title = {Apex Graph-Grammars and Attribute Grammars},
  journal = {Acta Informatica},
  volume = 25,
  pages = {537--571},
  year = 1988,
  UniBwM = {Z265-25}
}

@InCollection{Engelfriet-Rozenberg-1997,
  author = {Joost Engelfriet and Grzegorz Rozenberg},
  title = {Node Replacement Graph Grammars},
  crossref = {HBGraTraI},
  chapter = 1,
  pages = {1--94}
}

@Misc{Engelfriet-Vereijken-1994,
  author = {Joost Engelfriet and Jan Joris Vereijken},
  title = {Concatenation of Graphs},
  month = SEP,
  year = 1994,
  WKloc = {doc/pap/BIB},
  abstract = {... linear Hyperedge Replacement Systems ...}
}

@Misc{Engelfriet-vanOostrom-1996,
  author = {Joost Engelfriet and van Oostrom, Vincent},
  title = {Logical Description of Context-Free Graph Languages},
  OPThowpublished = {},
  OPTmonth = {},
  year = 1996,
  OPTnote = {},
  WKloc = {A-1221, doc/pap/BIB}
}

@MastersThesis{Engelhardt-1990,
  keywords = {HOPS2},
  year = 1990,
  title = {Fallstudien am HOPS-System},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  type = {Diplomarbeit},
  number = {ID 11/90},
  author = {Christian G. Engelhardt}
}

@InProceedings{Engels-Heckel-2000,
  author = {Gregor Engels and Reiko Heckel},
  title = {Graph Transformations as Unifying Formal Framework for System Modelling and Model Evolution},
  crossref = {ICALP2000},
  pages = {127--150},
  WKloc = {A-1354, doc/pap/BIB (draft?)},
  OPTannote = {}
}

@Article{Engels-Heckel-Taentzer-Ehrig-1997,
  author = {Gregor Engels and Reiko Heckel and Gabriele Taentzer and Hartmut Ehrig},
  title = {A Combined Reference Model- and View-based Approach to System Specification},
  journal = {Intl.\null{} Journal of Software and Knowledge Engineering},
  year = 1997,
  volume = 7,
  number = 4,
  pages = {457--477},
  keywords = {double-pullback transitions}
}

@Book{Engels-Schaefer-1989,
  author = {Gregor Engels and W. Sch\afer},
  title = {Programmentwicklungsumgebungen, Konzepte und Realisierung},
  publisher = {B.G.Teubner},
  year = 1989,
  series = {Leitf\"aden der angewandten Informatik},
  address = {Stuttgart},
  annote = {cited in \cite{Andries-Engels-1993} as source for
		  hybrid syntax-directed editors}
}

@InProceedings{Engels-Schuerr-1995,
  author = {Gregor Engels and Andy Schürr},
  title = {Hierarchical Graphs, Graph Types, and Meta Types},
  crossref = {SEGRAGRA-1995},
  pages = {75-84}
}

@InProceedings{Engler-1996,
  author = {Dawson R. Engler},
  title = {{VCODE}: A Retargetable, Extensible, Very Fast Dynamic Code Generation System},
  crossref = {PLDI1996},
  pages = {160--170},
  WKloc = {A-1468}
}

@InProceedings{Engler_magik,
  author = {Dawson Engler},
  title = {Incorporating application semantics and control into compilation},
  OPTcrossref = {},
  OPTkey = {},
  booktitle = {First Conference on Domain-Specific Languages},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  bibliographies = {Anand},
  URL = {http://www.pdos.lcs.mit.edu/~engler/magik-abstract.html},
  abstract = {Programmers have traditionally been passive users of
      compilers, rather than active exploiters of their transformational
      abilities. This paper presents magik, a system that allows
      programmers to easily and modularly incorporate application-specific
      extensions into the compilation process.

      The magik system gives programmers two significant capabilities.
      First, it provides mechanisms that implementors can use to
      incorporate application semantics into compilation, thereby enabling
      both optimizations and semantic checking impossible by other means.
      Second, since extensions are invoked during the translation from
      source to machine code, code transformations (such as software fault
      isolation~\cite{wahbe:sandbox}) can be performed with full access to
      the symbol and data flow information available to the compiler
      proper, allowing them both to exploit source semantics and to have
      their transformations (automatically) optimized as any other code.}
}

@Article{Eppstein-1992,
  author = {David Eppstein},
  title = {Parallel Recognition of Series-Parallel Graphs},
  pages = {41--55},
  journal = IandC,
  month = MAY,
  year = 1992,
  volume = 98,
  number = 1,
  references = {Cole-Vishkin-1986}
}

@inproceedings{Erdweg-Kats-Rendel-KaestnerC-Ostermann-Visser-2011,
 author = {Erdweg, Sebastian and Kats, Lennart C.L. and Rendel, Tillmann and K\"{a}stner, Christian and Ostermann, Klaus and Visser, Eelco},
 title = {Growing a language environment with editor libraries},
 booktitle = {Proceedings of the 10th ACM international conference on Generative programming and component engineering},
 series = {GPCE '11},
 year = {2011},
 isbn = {978-1-4503-0689-8},
 location = {Portland, Oregon, USA},
 pages = {167--176},
 numpages = {10},
 DOIURL = {http://doi.acm.org/10.1145/2047862.2047891},
 doi = {10.1145/2047862.2047891},
 acmid = {2047891},
 publisher = {ACM},
 OPTaddress = {New York, NY, USA},
 keywords = {DSL embedding, language extensibility, language workbench, library},
 bibliographies = {EdComb},
 WKloc = {doc/pap/BIB}
}

@InProceedings{ Eriksson-1992,
    author = "Lars-Henrik Eriksson",
    title = "A Finitary Version of the Calculus of Partial Inductive Definitions",
    booktitle = "Proceedings of the Second International Workshop on Extensions of Logic Programming",
    publisher = "Springer-Verlag LNAI 596",
    address = "Stockholm, Sweden",
    editor = "L.-H. Eriksson and L. Halln{\"a}s and P. Schroeder-Heister",
    pages = "89--134",
    year = "1992",
    url = "citeseer.ist.psu.edu/eriksson92finitary.html"
}

@InProceedings{ Eriksson-1993,
  author = "Lars-Henrik Eriksson",
  title = "Finitary Partial Inductive Definitions as a General Logic",
  crossref = {ELP93},
  pages = {94--119},
  DOI = {10.1007/3-540-58025-5_52},
  URL = {http://www.springerlink.com/content/y592u860utq75407/},
  CiteSeer = "citeseer.ist.psu.edu/eriksson94finitary.html",
  abstract = {We describe how the calculus of partial inductive
                  definitions is used to represent logics. This
                  calculus includes the powerful principle of
                  definitional reflection. We describe two
                  conceptually different approaches to representing a
                  logic, both making essential use of definitional
                  reflection. In the deductive approach, the logic is
                  defined by its inference rules. Only the succedent
                  rules (in a sequent calculus setting --- introduction
                  rules in a natural deduction setting) need be
                  given. The other rules are obtained implicitly using
                  definitional reflection. In the semantic approach,
                  the logic is defined using its valuation
                  function. The latter approach often provides a more
                  straightforward representation of logics with simple
                  semantics but complicated proof systems.}
}

@InProceedings{ Eriksson-1994,
  author = "Lars-Henrik Eriksson",
  title = "Pi: An Interactive Derivation Editor for the Calculus of Partial Inductive Definitions",
  crossref = {CADE1994},
  pages = "821--825",
  CiteSeer = "citeseer.ist.psu.edu/eriksson94pi.html",
  DOI = {10.1007/3-540-58156-1_68},
  URL = {http://www.springerlink.com/content/yk75uk401883pg30/},
  abstract = {Pi is a system for the interactive construction and
                  editing of formal derivations in the calculus of
                  finitary partial inductive definitions. This
                  calculus can be used as a logical framework where
                  object logics are specified, turning Pi into a
                  derivation system for a particular object
                  logic. Noteworthy features of Pi include: a graphic
                  user interface where derivations are presented in
                  tree form, direct manipulation of the derivation
                  tree structure by selection using a mouse, and the
                  ability to edit existing derivations by cutting and
                  pasting as well as by changing the formulae
                  occurring in a derivation. A simple facility for
                  automatic theorem proving has been designed.}
}

@Misc{Erkoek-Launchbury-2000a,
  author = {Levent Erk{\"o}k and John Launchbury},
  title = {A Recursive {\tt do} for {Haskell}: Design and Implementation},
  year = 2000,
  WKloc = {A-1128}
}

@Misc{Erkoek-Launchbury-2000b,
  author = {Levent Erk{\"o}k and John Launchbury},
  title = {Recursive Monadic Bindings: Proofs and Details},
  month = APR,
  year = 2000,
  WKloc = {A-1127},
  note = {proofs for \cite{Erkoek-Launchbury-2000a}}
}

@InProceedings{Erkoek-Launchbury-2002,
  author = {Levent Erk{\"o}k and John Launchbury},
  title = {A recursive do for {Haskell}},
  crossref = {Haskell2002},
  pages = {29--37},
  DOIURL = {http://doi.acm.org/10.1145/581690.581693},
  DOI = {10.1145/581690.581693},
  WKloc = {doc/pap/BIB},
  abstract = {Certain programs making use of monads need to perform
      recursion over the values of monadic actions. Although the
      do-notation of Haskell provides a convenient framework for monadic
      programming, it lacks the generality to support such recursive
      bindings. In this paper, we describe an enhanced translation schema
      for the donotation and its integration into Haskell. The new
      translation allows variables to be bound recursively, provided the
      underlying monad comes equipped with an appropriate fixed-point
      operator.}
}

@Misc{Ermel-1999,
  author = {Claudia Ermel},
  title = {The {\sc Agg} Environment: A Short Manual},
  year = 1999,
  WKloc = {A-0657}
}

@Book{Erne-1982,
  year = 1982,
  title = {Einf\"uhrung in die Ordnungstheorie},
  publisher = BI,
  author = {Marcel Ern{\'e}},
  address = {Mannheim, Wien, Z\"urich},
  UniBwM = {I-LB451:Ern\'e},
  bibliographies = {RelMiCS},
  contents = {0. Mengen und Zahlen
		1. Relationen und Abbildungen
		2. H\"ullen- und Kernsysteme
		3. Algebra der Relationen
		4. Ordnungsrelationen
		5. Halbverb\"ande und Verb\"ande
		6. Homomorphismen
		7. Abschnitte, Schnitte und Ideale
		8. Lineare Ordnungen
		9. Wohlordnungen und Maximalprinzipien
		10. Kettenbedingungen
		11. Ordinalzahlen
		Index
		Symbolverzeichnis
		Literatur}
}

@Misc{Erne-2005,
  author =    {Marcel Ern{\'e}},
  title =     {Categories of Contexts},
  howpublished = {Preprint, \url{http://www.iazd.uni-hannover.de/~erne/preprints/CatConts.pdf}},
  year =      2005,
  WKloc =    {doc/pap/BIB},
  bibliographies =    {RelMiCS}
}

@Article{Ershov-1958,
  author = 	 {Ershov, A.P},
  title = 	 {On Programming of Arithmetic Operations},
  journal = 	 {Doklady, AN USSR},
  year = 	 1958,
  volume = 	 118,
  number = 	 3,
  pages = 	 {427--430},
  doi = {http://doi.acm.org/10.1145/368892.368907},
  note = 	 {transl.\null{} Friedman, M.D., Commun.\null{} ACM 1, 8 (Aug. 1958), 3--6},
  annote = 	 {Invention of hash consing}
}

@InProceedings{Ershov-1981,
  author = {A.P. Ershov},
  title = {The Transformational Machine: Theme and Variations},
  booktitle = {Mathematical Foundations of Computer Science,
      {\v{S}}trbsk{\'{e}} Pleso, Czechoslovakia (Lecture Notes in Computer
      Science, vol. 118)},
  editor = {J. Gruska and M. Chytil},
  publisher = Springer,
  year = 1981,
  pages = {16-32},
  annote = {English version of
                \cite{Ershov:1982:Transformational:Russian}.}
}

@Article{Ershov-1982,
  author = {A.P. Ershov},
  title = {Mixed Computation: Potential Applications and Problems for
      Study},
  journal = {Theoretical Computer Science},
  year = 1982,
  volume = 18,
  pages = {41-67},
  annote = {English version of
                \cite{Ershov:1980:MixedComputation:Russian}.}
}

@InProceedings{Ershov-1985,
  author = {A.P. Ershov},
  title = {On Mixed Computation: Informal Account of the Strict and
      Polyvariant Computational Schemes},
  crossref = {Marktoberdorf-1985},
  pages = {107--120},
  annote = {An account of the general idea of mixed computation
                   and a comparative analysis of results published in
                   \cite{Bulyonkov:1984:Polyvariant}, \cite{Itkin-1983c},
                   and \cite{Ostrovsky:1980:Application:Novosibirsk}.}
}

@InProceedings{Ershov-Itkin-1977,
  author = {A.P. Ershov and V.E. Itkin},
  title = {Correctness of Mixed Computation in {Algol}-like Programs},
  booktitle = {Mathematical Foundations of Computer Science, Tatransk\'{a}
      Lomnica, Czechoslovakia},
  editor = {J. Gruska},
  publisher = Springer,
  series = LNCS,
  volume = 53,
  year = 1977,
  pages = {59-77},
  annote = {A formal treatment of mixed computation of programs in
      Algol-like languages. Three definitions of mixed computation are
      presented. The first one involves the smallest number of forcible
      suspensions but is incorrect in the general case. The other two are
      universally correct but require more forcible suspensions.}
}

@InProceedings{Erwig-1997,
  author = {Martin Erwig},
  title = {Functional Programming with Graphs},
  crossref = {ICFP1997},
  pages = {52--65},
  OPTabstract = {},
  WKloc = {A-0578},
  note = {See also \url{http://web.engr.oregonstate.edu/~erwig/fgl/}}
}

@Misc{Erwig-199X,
  author = {Martin Erwig},
  title = {Fully Persistent Graphs --- Which One to Choose?},
  year = {199?},
  WKloc = {A-0682}
}

@Misc{Erwig-199Y,
  author = {Martin Erwig},
  title = {The Categorical Imperative --- Or: How to Hide Your State Monads},
  year = {199?},
  WKloc = {A-0683}
}

@Misc{Erwig-199Z,
  author = {Martin Erwig},
  title = {Visual Semantics --- Or: What Youe See is What You Compute},
  year = {199?},
  WKloc = {A-0684},
  annote = {VEX}
}

@Article{Erwig-199Za,
  author = {Martin Erwig},
  title = {Diets for Fat Sets},
  journal = {Journal of Functional Programming},
  year = {199?},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0685},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Erwig-PeytonJones-2000,
  author = {Martin Erwig and Peyton Jones, Simon},
  title = {Pattern Guards and Transformational Patterns},
  crossref = {Haskell2000},
  pages = {12.1-12.27},
  OPTnote = {},
  WKloc = {A-1114, doc/pap/BIB},
  bibliographies = {FP},
  abstract = {We propose three extensions to patterns and pattern
     matching in Haskell. The first, pattern guards, allows the guards
     of a guarded equation to match patterns and bind variables, as
     well as to test boolean condition. For this we introduce a
     natural generalization of guard expressions to guard qualifiers.

     A frequently-occurring special case is that a function should be
     applied to a matched value, and the result of this is to be
     matched against another pattern. For this we introduce a
     syntactic abbreviation, transformational patterns, that is
     particularly useful when dealing with views. These proposals can
     be implemented with very modest syntactic and implementation
     cost. They are upward compatible with Haskell; all existing
     programs will continue to work.

     We also offer a third, much more speculative proposal, which
     provides the transformational-pattern construct with additional
     power to explicitly catch pattern match failure.

     We demonstrate the usefulness of the proposed extension by
     several examples, in particular, we compare our proposal with
     views, and we also discuss the use of the new patterns in
     combination with equational reasoning.}
}

@InProceedings{Esik-2000,
  author = 	 {Zolt{\'a}n {\'E}sik},
  title = {Axiomatizing the Least Fixed Point Operation and Binary Supremum},
  crossref =	 {CSL2000},
  pages =	 {302--316},
  WKloc = 	 {A-1502, doc/pap/BIB},
  abstract = {The equational properties of the least fixed point
     operation on ($\omega$-)continuous functions on
     ($\omega$-)complete partially ordered sets are captured by the
     axioms of iteration algebras, or iteration theories. We show that
     the equational laws of the binary supremum operation in
     conjunction with the least fixed point operation on
     ($\omega$-)continuous functions on ($\omega$-)complete
     semilattices have a finite axiomatization over the equations of
     iteration algebras. As a byproduct of this relative
     axiomatizability result, we obtain complete infinite equational,
     and finite implicational axiomatizations.}
}

@Article{Esik-Bernatsky-1995,
  author = 	 {Zolt{\'a}n {\'E}sik and L. Bern{\'a}tsky},
  title = 	 {Equational properties of {Kleene} algebras of relations with conversion},
  journal = 	 TCS,
  year = 	 1995,
  volume =	 137,
  pages =	 {237--251}
}

@Article{Esik-Labella-1998,
  author = 	 {Zolt{\'a}n {\'E}sik and A. Labella},
  title =        {Equational Properties of Iteration
                  in Algebraically Complete Categories},
  journal = 	 TCS,
  year = 	 1998,
  volume =	 195,
  pages =	 {61--89},
  WKloc = 	 {A-1584, doc/pap/BIB},
  abstract = {We prove the following completeness theorem: If the
     fixed point operation over a category is defined by initiality,
     then the equations satisfied by the fixed point operation are
     exactly those of iteration theories. Thus, in such categories,
     the equational axioms of iteration theories provide a sound and
     complete axiomatization of the equational properties of the fixed
     point operation.}
}

@InProceedings{Esparza-1994,
  author = {Javier Esparza},
  title = {On the decidability of Model Checking for several
		  $\mu$-Calculi and Petri Nets},
  crossref = {CAAP94},
  pages = {115--129},
  authorsAddress = {Edinburgh},
  abstract = {The decidability of the model checking problem for
		  several $\mu$-calculi and Petri nets is
		  analysed. The linear time $\mu$-calculus without
		  atomic sentences is decidable; if atomic sentences
		  are added, it becomes undecidable. A very simple
		  subset of the modal $\mu$-calculus is undecidable.}
}

@Article{Etalle-Gabbrieli-1998,
  author = {Sandro Etalle and Maurizio Gabbrieli},
  title = {Partial Evaluation of Concurrent Constraint Languages},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 11},
  WKloc = {A-0902, 41--44},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Evans-1999_pUML-panel,
  editor = {},
  author = {Evans (moderator), A.S. and S.Cook and S.Mellor and J.Warmer and A.Wills},
  title = {Panel Paper --- Advanced Methods and Tools for a Precise UML},
  crossref = {UML1999},
  WKloc = {A-0991},
  URL = {http://www.cs.york.ac.uk/puml/publications.html},
  annote = {A collection of `visions' by leading UML methodologists and developers
            on the future impact of a precise UML on tools and methods}
}

@InCollection{Evans-France-Lano-Rumpe-1999,
  author = {A.S.Evans and R.B.France and K.C.Lano and B.Rumpe},
  title = {Meta-modelling semantics of UML},
  booktitle = {Behavioural Specifications for Businesses and Systems},
  publisher = {Kluwer},
  editor = {Haim Kilov},
  chapter = 4,
  year = 1999,
  URL = {http://www.cs.york.ac.uk/puml/publications.html},
  WKloc = {A-0990},
  annote = {The pUML approach to introducing denotational semantics into the
            UML meta-model, which places emphasis on building
            a precise core semantics for the UML.}
}

@InProceedings{Evans-Kent-1999,
  author = {A.S.Evans and S.Kent},
  title = {Meta-modelling semantics of {UML}: the {pUML} approach},
  crossref = {UML1999},
  WKloc = {A-0992},
  URL = {http://www.cs.york.ac.uk/puml/publications.html},
  annote = {Further details on the pUML approach to the semantics of UML.}
}

@InProceedings{Even-DASchmidt-1990,
  author = {Susan Even and David A. Schmidt},
  title = {Type Inference for Action Semantics},
  crossref = {ESOP1990},
  pages = {118--133},
  abstract = {In a series of papers [...]\cite{Watt-1987}, Mosses
		  and Watt define
		  {\em action semantics}, a metalanguage for
		  high-level, domain-independent formulation of
		  denotational semantics definitions. Action semantics
		  was designed to support readability, abstraction,
		  modularity, and modifiability of language
		  definitions [13,15].

                  In our recent work [5], we studied a
		  combinator-based version of action semantics. We
		  developed a typing systemfor actions based on {\em
		  types} and {\em kinds}. The types within a kind are
		  partially aordered to reflect subtyping. We then
		  defined a category-sorted algebra model [18,20] for
		  the system where actions are natural transformations
		  over interpretation functors (which map the type
		  names and their ordering to predomains and coercion
		  functions). Thus, an action is a family of
		  continuous functions that behave consistently with
		  respect to the subtyping relation. Coercion
		  functions--- even nonembeddings--- disappear.

                  ...

                  This paper presents our variant of action semantics
		  and its typing system. We give the type inference
		  algorithms, state properties that action semantics
		  expressions satisfy, give soundness and completeness
		  properties and explain why they hold for type
		  inference on action semantics expressions. We also
		  present a small example.},
  annote = {see \cite{Doh-SchmidtDA-1992}}
}

@MastersThesis{Everets-2005,
  author = 	 {Kevin Everets},
  title = 	 {Assembly Language Representation and Graph Generation in a
  Pure Functional Programming Language},
  school = 	 {McMaster University, Department of Computing and Software},
  year = 	 2005,
  month =	 JAN
}

@Article{Everett-1944,
  author = {Chad J. Everett},
  title = {Closure Operators and {G}alois Theory in Lattices},
  journal = TRAMS,
  volume = 55,
  number = {},
  year = 1944,
  pages = {514--525},
  bibliographies = {RelMiCS}
}

@Article{Everett-Ulam-1946,
  author = {Chad J. Everett and Stanislaw M. Ulam},
  title = {Projective Algebra {I}},
  journal = AJM,
  volume = 68,
  year = 1946,
  pages = {77--88},
  bibliographies = {RelMiCS}
}

@InProceedings{Faconti-Bettarini-Paterno-1990,
  author = {G.P. Faconti and R.D. Bettarini and F. Paterno},
  title = {A Model of Interaction for Graphical Systems},
  crossref = {DISCO90},
  pages = {255--263},
  WKloc = {A-0354}
}

@TechReport{Fagan-1998,
  pages = 167,
  year = 1998,
  type = {Technical Report},
  number = {TR92-184},
  institution = {Rice University},
  title = {Soft Typing: An Approach to Type Checking for
                 Dynamically Typed Languages},
  bibdate = {March 31, 1998},
  author = {Mike Fagan},
  abstract = {In an effort to avoid improper use of program
                 functions, modern programming languages employ some
                 kind of preventative type system. These type systems
                 can be classified as either static or dynamic. Static
                 type systems detect ``ill-typed'' program phrases at
                 compile-time, whereas dynamic type systems detect
                 ``ill-typed'' phrases at run-time. Static typing
                 systems have two important advantages over dynamically
                 typed systems: First, they provide important feedback
                 to the programmer by detecting a large class of program
                 errors before execution. Second. they extract
                 information that a compiler can exploit to produce more
                 efficient code. The price paid for these advantages,
                 however, is a loss of expressiveness and modularity. It
                 is easy to prove that a static type system for an
                 ``interesting'' programming language necessarily
                 excludes some ``good'' programs. This paper focuses
                 on the problem of designing programming systems that
                 retain all the expressiveness of dynamic typing, but
                 still offer the early error detection and improved
                 optimization opportunities of static typing. To that
                 end, we introduce a concept called soft typing. The key
                 concept of soft typing is that a type checker need not
                 reject programs containing statically ``ill-typed''
                 phrases. Instead, the soft type checker inserts
                 explicit run-time checks. Thus, there are two issues to
                 be addressed in the design of soft typing systems.
                 First, the typing mechanism must provide reasonable
                 feedback to programmers accustomed to dynamically typed
                 languages. Current static systems fail to satisfy the
                 programmer's intuition about correctness on many
                 programs. Second, a soft typing system must sensibly
                 insert run-time checks (when necessary). This paper
                 develops a type system and checking algorithms that are
                 suitable for soft typing a significant class of
                 programming languages.},
  month = mar # { 31,}
}

@PhDThesis{Fahmy-1995,
  author = {Hoda Fahmy},
  title = {Reasoning in the Presence of Uncertainty via Graph Rewriting},
  year = 1995,
  month = MAR,
  institution = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  WKloc = {Chapter 3,4: A-0690},
  annote = {MUBEENA}
}

@InProceedings{Fahmy-Holt-Mancoridis-1997,
  author = {Hoda Fahmy and Ric Holt and Spiros Mancoridis},
  title = {Repairing Software Style using Graph Grammars},
  booktitle = {IBM Proceedings of the
         Seventh Centre for Advanced Studies Conference, Toronto, Ontario, Canada, November, 1997},
  year = 1997,
  WKloc = {A-0551}
}

@Book{Fahringer-Scholz-2003,
  author =	 {T. Fahringer and B. Scholz},
  title = 	 {Advanced Symbolic Analysis for Compilers: New Techniques and Algorithms for Symbolic Program Analysis and Optimization},
  publisher = 	 Springer,
  year = 	 2003,
  volume =	 2628,
  series =	 LNCS,
  ISBN = {3-540-01185-4},
  WKloc = 	 {owned, doc/pap/BIB},
  McMaster = 	 {QA 76.76 .C65 F34 2003},
  URL = 	 {http://link.springer.de/link/service/series/0558/tocs/t2628.htm},
  bibliographies = {Coconut, OPG}
}

@Article{Falkowski-Schmitz-1986,
  year = 1986,
  volume = 23,
  title = {A note on the Queens' problem},
  journal = {Information Processing Letters},
  author = {B.J. Falkowski and Lothar Schmitz}
}

@Misc{FanKangming-Smyth-Simpson-2004,
  author =	 {Kangming Fan and W. F. Smyth and R. J. Simpson},
  title =	 {A New Periodicity Lemma},
  month =	 OCT,
  year =	 2004,
  WKloc = 	 {A-1589}
}

@PhDThesis{Fang-1992,
  author = {Fang, Wengchang},
  title = {A Study of Splitting in Higher-Order Theorem Proving},
  keywords = {lambda calculus},
  year = 1992,
  school = {Northwestern University},
  annote = {QER92-29903},
  bibliographies = {RelMiCS}
}

@InProceedings{Fant-1993,
  author = {Karl M. Fant},
  title = {A Critial Review of the Notion of the Algorithm in Computer
          Science},
  abstract = {We first review the development of the notion of the algorithm
             as a fundamental paradigm of mathematics. We then suggest a
             definition of computer science that distinguishes it from all
             other sciences and from mathematics. Finally we argue that the
             conceptual concerns of computer science are quite different
             from the conceptual concerns of mathematics and that the notion
             of the algorithm is and has been an inappropriate and
             ineffective paradigm for computer science.},
  crossref = {ACM1993}
}

@Misc{Farina-,
  author = {Lorenzo Farina},
  title = {Positive Systems in the State Space Approach: Main Issues and Recent Results},
  OPThowpublished = {},
  OPTmonth = {},
  year = {???},
  WKloc = {A-1375, doc/pap/BIB},
  bibliographies = {PosMat},
  abstract = {A positive system is a system in which
    the state variables are always positive in value.
    In this introductory tutorial paper,
    basic results on positive systems are reviewed
    and recent developments and open problems are addressed.}
}

@Article{Farmer-1988,
  author = {William M. Farmer},
  title = {A Unification Algorithm for Second-Order Monadic Terms},
  journal = {Annals of Pure and Applied Logic},
  year = 1988,
  volume = 39,
  pages = {131--174},
  UniBwM = {MAT/Z1756-39},
  abstract = {This paper presents an algorithm that, given a
		  finite set $E$ of pairs of second-order monadic
		  terms, returns a finite set $U(E)$ of `substitution
		  schemata' such that a substitution unifies $E$ iff it
		  is an instance of sime member of $U(E)$. Moreover,
		  $E$ is unifiable precisely if $U(E)$ is not empty.
		  The algorithm terminates on all inputs, unlike the
		  unification algorithms for second-order monadic
		  terms developed by G.~Huet and G.~Winterstein.

                  The substitution schemata in $U(E)$ use expressions
		  (called `parametric terms') which represent sets of
		  terms that differ only in how many times designated
		  strings of (monadic) function constants follow
		  themselves. The substitution schemata may contain
		  unresolved `identity restrictions'; consequently,
		  the members of $U(E)$ generally do not characterize
		  all the unifiers of $E$ in a completely explicit
		  way.

                  The algorithm is particularly useful for
		  investigating the complexity of formal proofs.}
}

@Article{Farmer-1990,
  author = {William M. Farmer},
  title = {Redex Capturing in Term Graph Rewriting},
  journal = IJFCS,
  year = 1990,
  volume = 1,
  number = 4,
  pages = {369--386},
  abstract = {Term graphs are a natural generalization of terms in which
      structure sharing is allowed. Structure sharing makes term graph
      rewriting a time- and space-efficient method for implementing term
      rewrite systems. Certain structure sharing shemes can lead to a
      situation in which a term graph component is rewritten to another
      component that contains the original. This phenomenon, called
      \emph{redex capturing}, introduces cycles into the term graph which
      is being rewritten --- even when the graph and the rule themselves do
      not contain cycles. In some applications, redex capturing is
      undesirable, such as in contexts where garbage collectors require
      that graphs be acyclic. In other applications, for example in the use
      of the fixed-point combinator $Y$, redex capturing acts as a
      rewriting optimization. We show, using results about infinite
      rewritings of trees, that term graph rewriting with arbitrary
      structure sharing (including redex capturing) is sound for
      left-linear rewrite systems.}
}

@Article{Farmer-1991,
  author = {William M. Farmer},
  title = {Simple Second-Order Languages for which Unification
		  is Undecidable},
  journal = {Theoretical Computer Science},
  year = 1991,
  volume = 87,
  pages = {25--41},
  number = 1,
  abstract = {We improve Goldfarb's Theorem on the undecidability
		  of the second-order unification problems. More
		  precisely, we prove that there is a natural number
		  $n$ such that the unification problem is
		  undecideable for all second-order languages
		  containing a binary function constant and at least
		  $n$ function variables with arity $\geq1$. This
		  result allows one to draw a sharp line between
		  second-order languages for which unification is
		  decidable and second-order languages for which
		  unification is undecideable. It also answers a
		  question raised by the $k$-provability problem that
		  is not answered by Goldfarb's result. Our proof
		  utilizes term rewriting concepts and several
		  unification coding tricks.}
}

@InProceedings{Farmer-1993,
  author = {William M. Farmer},
  title = {Theory interpretation in simple type theory},
  crossref = {HOA1993},
  pages = {96--123},
  WKloc = {?},
  keywords = {?},
  contents = {?},
  abstract = {?}
}

@InProceedings{Farmer-1994,
  author = 	 {William M. Farmer},
  title = 	 {Theory Interpretation in Simple Type Theory},
  crossref =	 {HOA1993},
  pages =	 {96--123},
  bibliographies = {HHOL},
  WKloc = 	 {A-1526, doc/pap/BIB},
  abstract = {Theory interpretation is a logical technique for
     relating one axiomatic theory to another with important
     applications in mathematics and computer science as well as in
     logic itself.  This paper presents a method for theory
     interpretation in a version of simple type theory, called LUTINS,
     which admits partial functions and subtypes.  The method is
     patterned on the standard approach to theory interpretation in
     first-order logic.  Although the method is based on a
     nonclassical version of simple type theory, it is intended as a
     guide for theory interpretation in classical simple type theories
     as well as in predicate logics with partial functions.}
}

@TechReport{Farmer-1994a,
  author = {William M. Farmer},
  title = 	 {A General Method for Safely Overwriting Theories in Mechanized Mathematics Systems},
  institution =  {The MITRE Corporation},
  year = 	 1994,
  note =	 {21 pp.},
  WKloc = 	 {A-1521, doc/pap/BIB},
  bibliographies = {HHOL}
}

@InProceedings{Farmer-1996,
  author = {William M. Farmer},
  title = 	 {Perspective Switching Using Theories and Interpretations},
  crossref =	 {IntSys1996},
  pages =	 {206--207},
  volume = 	 {I},
  note =	 {Abstract},
  WKloc = {A-1520, doc/pap/BIB},
  bibliographies = {HHOL}
}

@InProceedings{Farmer-2000,
  author = {William M. Farmer},
  title = 	 {An Infrastructure for Intertheory Reasoning},
  crossref =	 {CADE2000},
  pages =	 {115--131},
  WKloc = 	 {A-1519, doc/pap/BIB},
  bibliographies = {HHOL}
}

@TechReport{Farmer-2001a,
  author = {William M. Farmer},
  title = {A Basic Extended Simple Type Theory},
  institution = {Department of Computing and Software, McMaster University},
  year = 2001,
  WKloc = {A-1486, doc/pap/BIB},
  URL = {http://imps.mcmaster.ca/wmfarmer/publications.html},
  abstract = {This paper presents an extended version of Church's simple
      type theory called Basic Extended Simple Type Theory (BESTT). By
      adding type variables and support for reasoning with tuples, lists,
      and sets to simple type theory, it is intended to be a practical
      logic for formalized mathematics.}
}

@TechReport{Farmer-2003a,
  author = {William M. Farmer},
  title = {A Basic Extended Simple Type Theory},
  institution = {Software Quality Research Laboratory, Department of Computing and Software, McMaster University},
  type = {{SQRL Report}},
  number = {14},
  year = 2003,
  month = OCT,
  note = {available from\\ \textsf{http://www.cas.mcmaster.ca/sqrl/sqrl\_reports.html}},

  abstract = {This paper presents an extended version of Church's
     simple type theory called Basic Extended Simple Type Theory
     (BESTT). By adding type variables and support for reasoning with
     tuples, lists, and sets to simple type theory, it is intended to
     be a practical logic for formalized mathematics.}

}

@TechReport{Farmer-2003b,
  author = {William M. Farmer},
  title = {The Seven Virtues of Simple Type Theory},
  institution = {Software Quality Research Laboratory, Department of Computing and Software, McMaster University},
  type = {{SQRL Report}},
  number = {18},
  year = 2003,
  month = OCT,
  WKloc = {A-1490 (draft), doc/pap/BIB},
  URL = {http://imps.mcmaster.ca/doc/seven-virtues.pdf},
  note = {available from\\ \textsf{http://www.cas.mcmaster.ca/sqrl/sqrl\_reports.html}},
  abstract = {\emph{Simple type theory}, also known as
       \emph{higher-order logic}, is a natural extension of first-order
       logic which is simple, elegant, highly expressive, and
       practical. This paper surveys the virues od simple type theory and
       attempts to show that simple type theory is an attractive
       alternative to first-order logic for practical-minded scientists,
       engineers, and mathematicians.}
}

@InProceedings{Farmer-Guttman-Thayer-1992,
  author = 	 {William M. Farmer and Joshua D. Guttman and F. Javier Thayer},
  title = 	 {Little Theories},
  crossref =	 {CADE1992},
  pages =	 {567--581},
  WKloc = 	 {A-1522},
  bibliographies = {HHOL}
}

@Article{Farmer-Guttman-Thayer-1993,
  author = {William M. Farmer and Joshua D. Guttman and F. Javier Thayer},
  title = {{IMPS}: An Interactive Mathematical Proof System},
  journal = {Journal of Automated Reasoning},
  year =  1993,
  volume = 11,
  pages = {213--248},
  URL = {http://imps.mcmaster.ca/wmfarmer/publications.html},
  WKloc = {doc/pap/BIB},
  abstract = {IMPS is an Interactive Mathematical Proof System
    intended as a general purpose tool for formulating and applying
    mathematics in a familiar fashion.  The logic of IMPS is based on
    a version of simple type theory with partial functions and
    subtypes. Mathematical specification and inference are performed
    relative to axiomatic theories, which can be related to one
    another via inclusion and theory interpretation.  IMPS provides
    relatively large primitive inference steps to facilitate human
    control of the deductive process and human comprehension of the
    resulting proofs.  An initial theory library containing almost a
    thousand repeatable proofs covers significant portions of logic,
    algebra and analysis, and provides some support for modeling
    applications in computer science.},
  keywords = {Interactive theorem proving, automated analysis,
     computing with theorems, theory interpretation, higher-order
     logic, partial functions.},
  bibliographies = {HHOL}
}

@TechReport{Farmer-Watro-1989,
  author = {W. Farmer and R. Watro},
  title = {Redex Capturing in Term Graph Rewriting},
  institution = {MITRE corporation},
  year = 1989,
  number = {M89-59},
  address = {Massachusetts}
}

@Article{Farmer-Watro-1990,
  author = {William M. Farmer and Ronald J. Watro},
  title = {Redex Capturing in Term Graph Rewriting},
  journal = FOCS,
  year = 1990,
  volume = 1,
  number = 4,
  pages = {369--386},
  WKloc = {A-1438},
  abstract = {Term graphs are a natural generalization of terms
    in which structure sharing is allowed. Structure sharing makes
    term graph rewriting a time- and space-efficient method for
    implementing term rewrite systems. Certain structure sharing schemes
    can lead to a situation in which a term graph component is rewritten
    to another component that contains the original. This phenomenon,
    called \emph{redex capturing}, introduces cycles into the term graph
    which is being rewritten---even when the graph and the rule themselves
    do not contain cycles. In some applications, redex capturing is
    undesirable, such as in contexts where garbage collectors require
    that graphs be acyclic. in other applications, for example in the use
    of the fixed-point combinator $Y$, redex capturing acts as a
    rewriting optimization. We show, using results about
    infinite rewritings of trees, that term graph rewriting
    with arbitrary structure sharing (including redex capturing)
    is sound for left-linear term rewrite systems.}
}

@Article{Farmer-vonMohrenschildt-2003,
  author = 	 {William M. Farmer and von Mohrenschildt, Martin},
  title = 	 {An overview of a Formal Framework for Managing Mathematics},
  journal = 	 {Annals of Mathematics and Artificial Intelligence},
  year = 	 2003,
  editor = 	 {B. Buchberger and G. Gonnet and M. Hazewinkel},
  volume = 	 38,
  pages = 	 {165--191},
  note =	 {special issue on Mathematical Knowledge Management},
  bibliographies = {MathScheme}
}

@TechReport{Farnum-1990,
  type = {Technical Report},
  number = {CSD-90-608},
  institution = {University of California, Berkeley},
  title = {Pattern-Based Languages for Prototyping of Compiler
                 Optimizers},
  remark = {Ph.D. Thesis},
  pages = 104,
  year = 1990,
  author = {Charles Donald Farnum},
  abstract = {Choosing an appropriate set of optimizations for a
                 particular compiler is a black art; the literature
                 abounds with unimplemented optimization algorithms.
                 This dissertation describes the core tools used in
                 Dora, an environment for exploring the design space of
                 optimizing compilers. Dora uses a lambda-calculus based
                 intermediate language schema to represent programs at
                 both high and low levels. Operations that are
                 appropriate to a particular language, machine, and/or
                 code-level are defined as necessary for a particular
                 compiler using a description language that allows the
                 implementor to state the important properties of the
                 operator with regard to optimization. ``Machine
                 independent'' optimizations, such as moving code out
                 of loops, are written to interrogate this information,
                 enabling a single specification to be applicable to
                 code at many levels for many source languages and
                 target machines. The language schema and description
                 languages provide the possibility of writing
                 optimizations in a context-independent manner, but the
                 environment must also give special support to ease this
                 writing. Dora includes attribution and transformation
                 languages based on pattern matching. The pattern
                 language is grounded in the efficient ansomata-driven
                 traditions, but with important extensions for natural
                 handling of variability operators and repetitive
                 vertical constructs such as left-associated addition
                 trees. The attribute system uses the pattern-matching
                 system so gains the descriptive advantages of an
                 attribute grammar system easy access to local contest
                 and a declarative functional specification, without
                 inheriting the difficulties of a monolithic
                 specification factored by an often irrelevant abstract
                 syntax. The transformation language uses the pattern
                 matching system for local context while relying on the
                 attribute system for global analysis. Dora has been
                 used to implement a functional prototype of Frederick
                 Chow's optimizer OPI. The example prototype
                 demonstrates the support Dora provides for building
                 actual optimizers. It also makes possible previously
                 infeasible experiments, such as rendering the
                 optimizations, adding OL bitvertar-based optimizations
                 to the suite and applying OPI languages in the LISP
                 tradition.}
}

@InProceedings{Farnum-1992,
  author = {Charles Farnum},
  title = {Pattern-Based Tree Attribution},
  crossref = {POPL1992},
  pages = {210--222},
  WKloc = {A-0171},
  abstract = {Attribute grammars have been used for many language
		  oriented tasks, including the formal description of
		  semantics and the implementation of compilation
		  tasks from simple type checking through code
		  generation. Despite their successful use, attribute
		  grammars have some disadvantages, including the
		  monolithic nature of the grammar and the fixed
		  factoring of all attribute descriptions by a single
		  set of grammar productions. {\em Attribute pattern
		  sets} provide a more expressive attribution system
		  by using pattern matching, instead of grammar
		  productions, to perform case analysis. Attribute
		  pattern sets can be implemented in terms of
		  attribute grammars in a way that maintains the
		  dependency structure of the attribute system, making
		  it straightforward to convert many of the practical
		  results from attribute grammar theory to similar
		  results for attribute pattern sets.}
}

@InProceedings{Farres-Casals-1990,
  author = {Jordi Farr\'es-Casals},
  title = {Proving correctness w.r.t.specifications with hidden parts},
  pages = {25--39},
  abstract = {The task of proving the correctness of an implementation w.r.t.
             a formal specification is sometimes complicated by the use
             of auxiliary (hidden) functions and sorts within the
             specification which are needed for the specification but are
             not meant to be implemented.

             Auxiliary sorts and functions
             are the normal way to express requirements in abstract model
             specifications. Algebraic specifications become popular as way
             to define the elements of a system without representing them in
             terms of more primitive, avoiding the definition of any extra
             structure. However, it has been shown that hidden functions are
             in general necessary for specifying computable functions [Maj
             77,TWW 79].

             In this paper we analyze general proving
             techniques for specifications with hidden parts and, in
             particular, an strategy which is complete when some side
             conditions are met.},
  crossref = {ALP1990}
}

@Article{Farrow-1986,
  author = {Rodney Farrow},
  title = {Automatic Generation of Fixed-Point-Finding Evaluators
                 for Circular, but Well-Defined, Attribute Grammars},
  booktitle = {Proceedings of the {ACM SIGPLAN '86 Symposium on
		  Compiler Construction}},
  volume = 21,
  number = 7,
  journal = SIGPLAN,
  year = 1986,
  pages = {85--98},
  organization = {Association for Computing Machinery},
  publisher = {SIGPlan},
  address = {Palo Alto, CA},
  month = jun,
  WKloc = {doc/pap/BIB},
  abstract = {In the traditional formulation of attribute grammars
                 (AGs) circularities are not allowed, that is, no
                 attribute-instance in any derivation tree may be
                 defined in terms of itself. Elsewhere, in mathematics
                 and computing, though, circular (or recursive)
                 definitions are commonplace, and even essential. Given
                 appropriate constraints, recursive definitions are
                 well-founded, and the least fixed-points they denote
                 are computable. This is also the case for circular AGs.
                 This paper presents constraints on individual
                 attributes and semantic functions of an AG that are
                 sufficient to guarantee that a circular AG specifies a
                 well-defined translation and that circularly-defined
                 attribute-instances can be computed via successive
                 approximation. AGs that satisfy these constraints are
                 called finitely recursive. An attribute evaluation
                 paradigm is presented that incorporates successive
                 approximation to evaluate circular attribute-instances,
                 along wiith an algorithm to automatically construct
                 such an evaluator. The attribute evaluators so produced
                 are static in the sense that the order of evaluation at
                 each production-instance in the derivation-tree is
                 determined at the time that each translator is
                 generated. A final algorithm is presented that tells
                 which individual attributes and functions must satisfy
                 the constraints.}
}

@Article{Farrow-1986_s,
  author = {Rodney Farrow},
  title = {Automatic Generation of Fixed-Point-Finding Evaluators
                 for Circular, but Well-Defined, Attribute Grammars},
  volume = 21,
  number = 7,
  journal = SIGPLAN,
  year = 1986,
  pages = {85--98},
  annote = {short entry for \cite{Farrow-1986}}
}

@Article{Farzan-Waller-1977,
  author = {M. Farzan and D.A. Waller},
  title = {Kronecker products and local joins of graphs},
  journal = {Can.\null{} J.\null{} Math.},
  year = 1977,
  volume = {XXIX},
  number = 2,
  pages = {255-269},
  annote = {cited by \cite{Bauderon-1995a}}
}

@InProceedings{Faure-KirchnerC-2002,
  author = {Faure, Germain and Kirchner, Claude},
  title = {Exceptions in the rewriting calculus},
  pages = {66--82},
  crossref = {RTA2002},
  WKloc = {A-1408, doc/pap/BIB},
  bibliographies = {PMC},
  DVI = {data/rhoException.dvi.gz},
  PS = {data/rhoException.ps.gz},
  PDF = {data/rhoException.pdf},
  abstract = {In the context of the rewriting calculus, we
	introduce and study an exception mechanism that allows us to
	express in a simple way rewriting strategies and that is
	therefore also useful for expressing theorem proving
	tactics. The proposed exception mechanism is expressed in a
	confluent calculus which gives the ability to simply express
	the semantics of the first tactical and to describe in full
	details the expression of conditional rewriting.},
  keywords = {Rewriting Calculus, strategy, exception, first, tactical}
}

@Article{Faxen-2001,
  author = {Karl-Filip Fax{\'en}},
  title = {Towards a Static Semantics for Haskell},
  journal = JFP,
  year = 2001,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {to appear},
  WKloc = {A-1096}
}

@InProceedings{Feather-1986,
  author = {Martin S. Feather},
  title = {A Survey and Classification of some Program Transformation Approaches and Techniques},
  authorsAddress = {Information Sciences Institute, University of Southern California},
  crossref = {IFIP-2.1-1986}
}

@InProceedings{Feeley-Dube-2003,
  author = 	 {Marc Feeley and Danny Dub{\'e}},
  title = 	 {{PICBIT}: A {Scheme} System for the {PIC} Microcontroller},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {Fourth Workshop on Scheme and Functional Programming},
  OPTpages = 	 {},
  year = 	 {2003},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1706},
  OPTannote = 	 {}
}

@Article{Fehrer-Horacek-1999,
  journal = {Information Sciences},
  volume = 116,
  number = 1,
  month = MAY,
  year = 1999,
  author = {Detlef Fehrer and Helmut Horacek},
  title = {Presenting inequations in mathematical proofs},
  pages = {3--23},
  URL = {http://www.elsevier.nl/cas/tree/store/ins/sub/1999/116/1/6193.pdf}
}

@Article{Feijs-Ommering-1997,
  author = {L. M. G. Feijs and van Ommering, R. C.},
  title = {Abstract Derivation of Transitive Closure Algorithms},
  journal = {Information Processing Letters},
  year = 1997,
  volume = 63,
  pages = {159--164},
  WKloc = {A-0469}
}

@Article{Feinerer-Salzer-2009,
   author = {Feinerer, Ingo and Salzer, Gernot},
   affiliation = {Technische Universität Wien, Institut für Computersprachen},
   title = {A comparison of tools for teaching formal software verification},
   journal = {Formal Aspects of Computing},
   publisher = {Springer London},
   issn = {0934-5043},
   keyword = {Computer Science},
   pages = {293--301},
   volume = {21},
   issue = {3},
   DOIURL = {http://dx.doi.org/10.1007/s00165-008-0084-5},
   DOI = {10.1007/s00165-008-0084-5},
   abstract = {We compare four tools regarding their suitability for
                  teaching formal software verification, namely the
                  Frege Program Prover, the Key system, Perfect
                  Developer, and the Prototype Verification System
                  (PVS). We evaluate them on a suite of small
                  programs, which are typical of courses dealing with
                  Hoare-style verification, weakest preconditions, or
                  dynamic logic. Finally we report our experiences
                  with using Perfect Developer in class.},
   year = {2009},
   WKloc = {doc/pap/BIB},
   bibliographies = {SpecTech}
}

@Misc{Felleisen-1997a,
  author = {Mathias Felleisen},
  title = {Re: Continuation Passing Style},
  howpublished = {USENET News post to \texttt{comp.lang.scheme}, 1997/12/17},
  month = DEC,
  URL = {http://groups.google.com/groups?hl=en&safe=off&selm=j7vk9d3eh1q.fsf%40new-world.cs.rice.edu&rnum=1},
  year = 1997,
  WKloc = {doc/pap/BIB},
  bibliographies = {SE3E}
}

@Article{Ferenczi1990,
  author = {M. Ferenczi},
  title = {On Inducing Homomorphisms Between Relation Set Algebras},
  journal = ALGU,
  volume = 27,
  year = 1990,
  pages = {474--479},
  bibliographies = {RelMiCS}
}

@InProceedings{Fernandes-Desharnais-2006,
  author = 	 {Therrezinha Fernandes and Jules Desharnais},
  title = 	 {Describing Data Flow Analysis Techniques with {Kleene} Algebra},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2006},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1680, doc/pap/BIB},
  OPTbibliographies = 	 {RelMiCS}
}

@InProceedings{Fernandez-Florescu-Kang-Levy-Suciu-1998,
  author = {Mary Fernandez and Daniela Florescu and Jaewoo Kang
                 and Alon Levy and Dan Suciu},
  title = {Catching the boat with {Strudel}: experiences with a
                 web-site management system},
  pages = {414--425},
  booktitle = {SIGMOD},
  year = 1998,
  URL = {http://www.research.att.com/~mff//files/sigmod98.ps.gz},
  abstract = {The Strudel system applies concepts from database
                 management systems to the process of building Web
                 sites. The key idea is separating the management of the
                 site's data, the creation and management of the site's
                 structure, and the graphical presentation of the site's
                 pages. First, the site builder creates a uniform model
                 of all the data available at the site. Second, the site
                 builder uses this model to declaratively define the Web
                 site's structure by applying a ``site-definition
                 query'' to the underlying data. The result of
                 evaluating this query is a ``site graph'', which
                 represents both the site's content and structure.
                 Third, the site builder specifies the graphical
                 presentation of pages in Strudel's HTML-template
                 language. The data model underlying Strudel is a
                 semi-structured model of labeled directed graphs.},
  WKloc = {A-1233, doc/pap/BIB}
}

@Article{Fernandez-Florescu-Levy-Suciu-2000,
  author = {Mary Fern{\'a}ndez and Daniela Florescu and Alon Levy
                 and Dan Suciu},
  title = {Declarative Specification of {Web} Sites with
                 {Strudel}},
  journal = {VLDB Journal: Very Large Data Bases},
  volume = 9,
  number = 1,
  pages = {38--55},
  month = {????},
  year = 2000,
  coden = {VLDBFR},
  ISSN = {1066-8888 (print), 0949-877X (electronic)},
  bibdate = {Wed Sep 27 10:11:55 MDT 2000},
  note = {Electronic edition.},
  URL = {http://link.springer.de/link/service/journals/00778/papers/0009001/00090038.pdf;
                 http://link.springer.de/link/service/journals/00778/bibs/0009001/00090038.htm;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/f/Fernandez:Mary_F=.html;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/f/Florescu:Daniela.html;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/l/Levy:Alon_Y=.html;
                 http://ftp.informatik.rwth-aachen.de/dblp/db/indices/a-tree/s/Suciu:Dan.html},
  acknowledgement = ack-nhfb,
  xxauthor = {Mary F. Fernandez and Daniela Florescu and Alon Y.
                 Levy and Dan Suciu},
  WKloc = {A-1232,doc/pap/BIB}
}

@InProceedings{Fernandez-Mackie-1996,
  author = {Maribel Fern\'andez and Ian Mackie},
  title = {Interaction Nets and Term Rewriting Systems},
  booktitle = {CAAP '96},
  year = 1996,
  WKloc = {A-0421}
}

@InProceedings{Fernandez-Mackie-1996a,
  author = {Maribel Fern\'andez and Ian Mackie},
  title = {From Term Rewriting to Generalised Interaction Nets},
  crossref = {PLILP1996},
  pages = {319--333},
  WKloc = {A-0560}
}

@Article{Fernandez-Mackie-1998,
  author = {Maribel Fern\'andez and Ian Mackie},
  title = {Interaction Nets and Term Rewriting Systems},
  journal = TCS,
  year = 1998,
  volume = 190,
  pages = {3--39},
  annote = {CR by WK},
  WKloc = {A-1179}
}

@InProceedings{Fernandez-Simeon-Wadler-2000,
  author = {Mary Fernandez and Jerome Simeon and Philip Wadler},
  title = {An Algebra for {XML} Query},
  crossref = {FSTTCS2000},
  pages = {11--45},
  WKloc = {A-1243, doc/pap/BIB},
  annote = {superseded by \cite{Fernandez-Simeon-Wadler-2001}}
}

@InProceedings{Fernandez-Simeon-Wadler-2001,
  author = {Mary Fernandez and Jerome Simeon and Philip Wadler},
  title = {A semi-monad for semistructured data},
  booktitle = {{ICDT, London, January 2001}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 2001,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  URL = {http://cm.bell-labs.com/cm/cs/who/wadler/topics/xml.html},
  OPTpublisher = {},
  annote = {An updated version of the paper An Algebra for XML Query. },
  WKloc = {A-1235, doc/pap/BIB}
}

@InProceedings{Fernandez-Tan-Suciu-2000,
  author = {Mary Fern\{\'a\}ndez and Wang-Chiew Tan and Dan Suciu},
  title = {{SilkRoute}: Trading between Relations and {XML}},
  booktitle = {Proceedings of Ninth International World Wide Web Conference},
  year = 2000,
  abstract = {XML is the standard format for data exchange between
     inter-enterprise applications on the Internet. To facilitate data
     exchange, industry groups define public document type definitions
     (DTDs) that specify the format of the XML data to be exchanged between
     their applications. In this work, we address the problem of automating
     the conversion of relational data into XML. SilkRoute is a general,
     dynamic, and efficient tool for viewing and querying relational data
     in XML. SilkRoute is general, because it can express mappings of
     relational data into XML that conforms to arbitrary DTDs, not just a
     canonical mapping of the relational schema. We call these mappings
     views. Applications express the data they need as an XML-QL query over
     the view. SilkRoute is dynamic, because it only materializes the
     fragment of an XML view needed by an application, and it is efficient,
     because it fully exploits the underlying RDBMs query engine whenever
     data items in an XML view need to be materialized.},
  WKloc = {A-1231, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@InProceedings{Ferragina-1994,
  author = {P. Ferragina},
  title = {Incremental Text Editing: a New Data Structure},
  crossref = {ESA94},
  authorsAddress = {Pisa}
}

@Misc{Ferrari-Montanari-199X,
  author = {GianLuigi Ferrari and Ugo Montanari},
  title = {A Tile-Based Coordination View of Asynchronous $pi$-calculus},
  year = {199?},
  WKloc = {A-0841}
}

@InProceedings{Ferrari-Montanari-Quaglia-1994,
  author = {GianLuigi Ferrari and Ugo Montanari and Paola Quaglia},
  title = {A $pi$-calculus with Explicit Substitutions: the
		  Late Semantics},
  crossref = {MFCS94},
  pages = {342--351}
}

@InProceedings{Fettig-Loechner-1996,
  author = {Roland Fettig and Bernd L{\"o}chner},
  title = {Unification of Higher-Order Patterns in a
                  Simply Typed Lambda Calculus with Finite Products and Terminal Type},
  crossref = {RTA-1996},
  pages = {347--361},
  OPTabstract = {},
  WKloc = {A-0463}
}

@Article{Fich-Ruppert-202,
  author = {Faith Fich and Eric Ruppert},
  title = {Hundreds of Impossibility Results for Distributed Computing},
  journal = {Distributed Computing},
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  WKloc = {A-1420},
  abstract = {We survey results from distributed computing that show tasks
      to be impossible, either outright or within given resource bounds, in
      various models. The parameters of the models considered include
      synchrony, fault-tolerance, different communication media, and
      randomization. The resource bounds refer to time, space, and message
      complexity. These results are useful in understanding the inherent
      difficulty of individual problems and in studying the power of
      different models of distributed computing. There is a strong emphasis
      in our presentation on explaining the wide variety of techniques that
      are used to obtain the results described.}
}

@Article{Fickert-Sudkamp-1992,
  author = {Fickert and Sudkamp},
  title = {Unification Based {FP} Interpreters},
  journal = SIGPLAN,
  year = 1992,
  volume = 27,
  number = 11,
  pages = {49--58}
}

@InProceedings{Field-1990,
  author = {John Field},
  title = {On Laziness and Optimality in Lambda Interpreters: Tools for Specification and Analysis},
  crossref = {POPL1990},
  pages = {1--15},
  OPTabstract = {},
  WKloc = {A-0556}
}

@Article{Field-Heering-Dinesh-1998,
  author = {J. Field and J. Heering and T. B. Dinesh},
  title = {Equations as a Uniform Framework for Partial Evaluation and Abstract Interpretation},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 2},
  WKloc = {A-0902, 5--12},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Field-Tip-1994,
  author = {J. Field and F. Tip},
  title = {Dynamic Dependence in Term Rewriting Systems and Its
		  Application to Program Slicing},
  crossref = {PLILP1994},
  pages = {415--431},
  WKloc = {A-0309},
  abstract = {{\em Program slicing} is a useful technique for
		  debugging, testing, and analyzing programs. A
		  program slice consistes of the parts of a program
		  which (potentially) affect the values computed at
		  some point of interest. With rare exceptions,
		  program slices have hitherto been computed and
		  defined in ad-hoc and language-specific ways. The
		  principal contribution of this paper is to show that
		  general and semantically well-founded notions of
		  slicing and {\em dependence} can be derived in a
		  simple, uniform way from {\em term rewriting
		  systems} (TRSs). Our slicing technique is applicable
		  to any language whose semantics is specified in TRS
		  form. Moreover, we show that our method admits an
		  efficient implementation.},
  annote = {Probably closely related to Partial Evaluation.}
}

@MastersThesis{Filinski-1989,
  author = {Andrezej Filinski},
  title = {Declarative Continuations and Categorical Duality},
  school = {DIKU, Computer Science Departtment, University of Copenhagen},
  year = 1989,
  month = AUG,
  note = {DIKU report 89/11}
}

@InProceedings{Filinski-1992,
  author = {Andrezej Filinski},
  title = {Linear Continuations},
  crossref = {POPL1992},
  pages = {27--38},
  abstract = {We present a functional interpretation of classical
		  linear logic based on the concept of {em linear
		  continuations}. Unlike their non-linear
		  counterparts, such continuations lead to a model of
		  control that does not inherently impose any
		  particular evaluation strategy. Instead, such
		  additional structure is expressed by admitting
		  closely controlled copying and discarding of
		  continuations. We also emphasize the importance of
		  classicality in obtaining computaionally appealing
		  categorical models of linear logic and propose a
		  simple ``coreflective subcategory'' interpretation
		  of the modality ``!''.}
}

@InProceedings{Filinski-1994,
  author = {Andrzej Filinski},
  title = {Representing Monads},
  crossref = {POPL1994},
  pages = {446--457},
  authorsAddress = {CMU},
  abstract = {We show that any monad whose unit and extension
		  operations are expressible as purely functional
		  terms can be embedded in a call-by-value language
		  with ``composable continuations''. $\ldots$}
}

@PhdThesis{Filinski-1996,
  author = 	 {Andrzej Filinski},
  title = 	 {Controlling Effects},
  school = 	 {Carnegie Mellon University},
  year = 	 1996,
  keywords = 	 {monads, monad morphisms, continuations},
  WKloc = 	 {A-1640, doc/pap/BIB}
}

@Article{Finger-1998,
  author = {Marcelo Finger},
  title = {Towards Structurally-Free Theorem Proving},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 3,
  OPTmonth = {},
  pages = {425--449},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0568},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InCollection{Finin-Labrou-Mayfield-1997,
  title = {{KQML} as an Agent Communication Language},
  author = {Tim Finin and Yannis Labrou and James Mayfield},
  pages = {291--316},
  chapter = 14,
  editor = {Jeffrey M. Bradshaw},
  booktitle = {Software Agents},
  publisher = {AAAI Press / The MIT Press},
  year = 1997,
  WKloc = {B-0101}
}

@Misc{Finin-etal-1993,
  title = {{Specification of the KQML Agent-Communication
                 Language -- plus example agent policies and
                 architectures}},
  author = {Tim Finin and Jay Weber and Gio Wiederhold and Michael
      Genesereth and Richard Fritzson and Donald McKay and James McGuire
      and Richard Pelavin and Stuart Shapiro and Chris Beck},
  year = 1993,
  month = JUN,
  organization = {The DARPA Knowledge Sharing Initiative External
                 Interfaces Working Group},
  URL = {http://www.cs.umbc.edu/kqml/papers/kqmlspec.ps},
  abstract = {This document is a draft of an initial specification
                 for the KQML agent communication language being
                 developed by the external interfaces working group of
                 the DARPA Knowledge Sharing Effort. KQML is intended to
                 be a high-level language to be used by knowledge-based
                 system to share knowledge at run time.},
  WKloc = {B-0101}
}

@Article{Finkelstein-Freyd-Lipton-2003,
  author = 	 {Stacy E. Finkelstein and Peter Freyd and James Lipton},
  title = 	 {A new framework for declarative programming},
  journal = 	 TCS,
  year = 	 2003,
  volume = 	 300,
  number = 	 {1--3},
  pages = 	 {91--160},
  month = 	 MAY,
  DOI = {http://dx.doi.org/10.1016/S0304-3975(01)00308-5},
  WKloc = {doc/pap/BIB},
  abstract = {We propose a new framework for the syntax and semantics
         of Weak Hereditarily Harrop logic programming with constraints,
         based on resolution over $\tau$-categories:
         finite product categories with canonical structure.

         Constraint information is directly built-in
         to the notion of signature via categorical syntax.
         Many-sorted equational
         are a special case of the formalism
         which combines features of uniform logic programming languages
         (modules and hypothetical implication)
         with those of constraint logic programming.
         Using the cannoical structure supplied by $\tau$-categories,
         we define a diagrammatic generalization
         of formulas, goals, programs and resolution proofs up to equality
         (rather than just up to isomorphism).

         We extend the Kowalski-van Emden fixed point interpretation,
         a cornerstone of declarative semantics,
         to an operational, non-ground, categorical semantics
         based on indexing over sorts and programs.

         We also introduce a topos-theoretic declarative semantics
         and show soundness and completeness of resolution proofs
         and of a sequent calculus over the categorical signature.
         We conclude with a discussion of semantic perspectives
         on uniform logic programming.}
}

@Manual{Finne-1996,
  title = {The {Haggis} Manual},
  author = {Sigbjorn Finne},
  month = APR,
  year = 1996,
  WKloc = {B-0106}
}

@Misc{Finne-Leijen-Meijer-PeytonJones-199X,
  author = {Sigbjorn Finne and Daan Leijen and Erik Meijer and Peyton Jones, Simon L.},
  title = {{\em H/Direct:} A Binary Foreign Language Interface for {Haskell}},
  year = {199X},
  WKloc = {A-0507}
}

@InProceedings{Finne-PeytonJones-1995,
  author = {Sigbjorn Finne and Peyton Jones, Simon},
  title = {Composing {Haggis}},
  booktitle = {Fifth Eurographics Workshop on Programming Paradigms
		  for Computer Graphics},
  month = {September},
  year = 1995,
  URL = {ftp://ftp.dcs.glasgow.ac.uk//pub/glasgow-fp/authors/Sigbjorn_Finne/composing-haggis.ps.gz},
  WKloc = {A-0705}
}

@Book{Finseth-1991,
  author =	 {Craig A. Finseth},
  title = 	 {The Craft of Text Editing --or-- Emacs for the Modern World},
  publisher = 	 Springer,
  year = 	 1991,
  URL = 	 {http://www.finseth.com/craft},
  WKloc = 	 {doc/pap/BIB}
}

@InProceedings{Fiore-Plotkin-1994,
  title = {An Axiomatization of Computationally Adequate Domain
		  Theoretic Models of {FPC}},
  author = {Marcelo P. Fiore and Gordon D. Plotkin},
  pages = {92--102},
  crossref = {LICS9},
  WKloc = {A-0367},
  abstract = {Categorical models of the metalanguage FPC (a type theory
		  with sums, products, exponentials and recursive
		  types) are defined. Then, domain-theoretic models of
		  FPC are axiomatised and a wide subclass of
		  them---the non-trivial and absolute ones---are
		  proved to be both computationally sound and
		  adequate.

                  Examples include: the category of cpos and partial
		  continuous functions and functor categories over it.}
}

@InProceedings{Fiore-Plotkin-Turi-1999,
  author = 	 { Fiore, Marcelo and Plotkin, Gordon and Turi, Daniele},
  title = 	 {Abstract syntax and variable binding},
  crossref = {LICS14},
  pages = 	 {193--202},
  year = 1999,
  DOI = {10.1109/LICS.1999.782615},
  URL = {http://ieeexplore.ieee.org/search/wrapper.jsp?arnumber=782615},
  WKloc = {A-1687, doc/pap/BIB},
  abstract = {We develop a theory of abstract syntax with variable
      binding. To every binding signature we associate a category of
      models consisting of variable sets endowed with compatible
      algebra and substitution structures. The syntax generated by the
      signature is the initial model. This gives a notion of initial
      algebra semantics encompassing the traditional one; besides
      compositionality, it automatically verifies the semantic
      substitution lemma.}
}

@Misc{Fiore-Simpson-1999,
  author = {Marcelo Fiore and Alex Simpson},
  title = {Lambda Definability with Sums via Grothendieck Logical Relations},
  year = 1999,
  URL = {http://www.cogs.susx.ac.uk/users/marcelo/papers/Types/glr.ps,
        http://citeseer.nj.nec.com/details/fiore99lambda.html},
  WKloc = {A-0917},
  abstract = {We introduce a notion of Grothendieck logical relation
              and use it to characterise the definability of morphisms
              in stable bicartesian closed categories by terms of the
              simply-typed lambda calculus with finite products and
              finite sums. Our techniques are based on concepts from
              topos theory, however our exposition is elementary.},
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Fiorentini-Ghilardi-2000,
  author = {Camillo Fiorentini and Silvio Ghilardi},
  title = {Path Rewriting and Combined Word Problems},
  institution = {Department of Computer Science, Universit\`{a} degli
                 Studi di Milano},
  year = 2000,
  number = {250-00},
  address = {Milan, Italy},
  month = may,
  URL = {http://gongolo.usr.dsi.unimi.it/~fiorenti/download/tr_250-00.ps.gz},
  WKloc = {A-1068, doc/pap/BIB}
}

@Article{Firestone-2000a,
  author = {Joseph M. Firestone},
  title = {Basic Concepts of Knowledge Management},
  journal = {DSstar},
  year = 2000,
  volume = 4,
  number = 46,
  month = NOV,
  URL = {http://www.tgc.com/dsstar/00/1114/102381.html},
  WKloc = {A-1043},
  abstract = {This paper provides an introductory conceptual framework
              for knowledge management. It treats the concepts of
              Knowledge Management System, Knowledge Base, Knowledge,
              Knowledge Process, and Knowledge Management in the abstract.
              It then develops corresponding definitions at the
              slightly lower level of abstraction of human organizations.
              Two approaches to knowledge management are identified and
              characterized. The paper then concludes with a discussion
              of some issues suggested by the framework.}
}

@Article{Fischer-Ladner-1979,
  author = {Fischer, M. and Ladner, R.},
  journal = JCOMSYS,
  pages = {194--211},
  title = {Propositional dynamic logic of regular programs},
  volume = 18,
  year = 1979,
  bibliographies = {RelMiCS}
}

@InProceedings{Fischer-Paterson-1982,
  author = {Fischer and Paterson},
  title = {The Fast Skew-Closure Algorithm},
  booktitle = {Logic and Algorithmic: An International Symposium Held
                 in Honour of Ernst Specker},
  publisher = {L'Enseignement Mathematique, Universite de Geneve},
  year = 1982
}

@TechReport{Fischer-Paterson-1992,
  author = {Fischer and Paterson},
  title = {Fishspear: {A} Priority Queue Algorithm},
  year = 1992,
  note = {an improved version is to be published in JACM in 1993}
}

@InProceedings{Fisher-Dietz-1999,
  author = {Randall J. Fisher and Henry G. Dietz},
  title = {The {Scc} Compiler: {SWARing at MMX and 3DNow!}},
  crossref = {LCPC1999},
  pages = {3999-414},
  WKloc = {A-1472, doc/pap/BIB},
  bibliographies = {Anand},
  abstract = {Last year, we discussed the issues surrounding the
      development of languages and compilers for a general, portable,
      high-level SIMD Within A Register (SWAR) execution model. In a first
      effort to provide such a language and a framework for further
      research on this form of parallel processing, we proposed the
      vector-based language SWARC, and an experimental module compiler for
      this language, called Scc, which targeted IA32+MMX-based
      architectures.

      Since that time, we have worked to expand the types of targets that
      Scc supports and to include optimizations based on both vector
      processing and enhanced hardware support for SWAR. This paper
      provides a more formal description of the SWARC language, describes
      the organization of the current version of the Scc compiler, and
      discusses the implementation of optimizations within this framework.}
}

@Book{Fitch-52,
  author = {Fitch, F. B.},
  title = {Symbolic Logic: An Introduction},
  publisher = {Ronald Press},
  year = 1952,
  address = {New York}
}

@Misc{Fitzpatrick-2014,
  author = 	 {Brad Fitzpatrick},
  title = 	 {Go: 90\% Perfect, 100\% of the time},
  howpublished = {``GoCon Tokyo'' slides, \url{http://talks.golang.org/2014/gocon-tokyo.slide}},
  month = 	 MAY,
  year = 	 2014
}

@InProceedings{Flanagan-Felleisen-1997,
  author = {Cormac Flanagan and Matthias Felleisen},
  title = {Componential Set-Based Analysis},
  pages = {235--248},
  crossref = {PLDI1997},
  WKloc = {A-1305, doc/pap/BIB}
}

@Article{Flanagan-Felleisen-1999,
  author = {Cormac Flanagan and Matthias Felleisen},
  title = {Componential Set-Based Analysis},
  journal = ACM-TOPLAS,
  volume = 21,
  number = 2,
  pages = {370--416},
  month = MAR,
  year = 1999,
  ISSN = {0164-0925},
  URL = {http://www.acm.org:80/pubs/citations/journals/toplas/1999-21-2/p370-flanagan/},
  abstract = {Set-based analysis (SBA) produces good predictions
                 about the behavior of functional and object-oriented
                 programs. The analysis proceeds by inferring {\em
                 constraints\/} that characterize the data flow
                 relationships of the analyzed program. Experiences with
                 MrSpidey, a static debugger based on SBA, indicate that
                 SBA can adequately deal with programs of up to a couple
                 of thousand lines of code. SBA fails, however, to cope
                 with larger programs because it generates systems of
                 constraints that are at least linear, and possibility
                 quadratic, in the size of the analyzed program. This
                 article presents theoretical and practical results
                 concerning methods for reducing the size of constraint
                 systems. The theoretical results include of
                 proof-theoretic characterization of the {\em observable
                 behavior\/} of constraint systems for program
                 components, and a complete algorithm for deciding the
                 observable equivalence of constraint systems. In the
                 course of this development we establish a close
                 connection between the observable equivalence of
                 constraint systems and the equivalence of regular-tree
                 grammars. We then exploit this connection to adapt a
                 variety of algorithms for simplifying grammars to the
                 problem of simplifying constraint systems. Based on the
                 resulting algorithms, we have developed {\em
                 componential set-based analysis\/}, a modular and
                 polymorphic variant of SBA. Experimental results verify
                 the effectiveness of the simplification algorithms and
                 the componential analysis. The simplified constraint
                 systems are typically an order of magnitude smaller
                 than the original systems. These reductions in size
                 produce significant gains in the speed of the
                 analysis.},
  keywords = {algorithms; languages; performance; theory},
  subject = {{\bf D.2.5} Software, SOFTWARE ENGINEERING, Testing
                 and Debugging, Debugging aids. {\bf D.2.6} Software,
                 SOFTWARE ENGINEERING, Programming Environments. {\bf
                 D.3.4} Software, PROGRAMMING LANGUAGES, Processors,
                 Debuggers. {\bf D.3.4} Software, PROGRAMMING LANGUAGES,
                 Processors, Optimization. {\bf F.3.1} Theory of
                 Computation, LOGICS AND MEANINGS OF PROGRAMS,
                 Specifying and Verifying and Reasoning about Programs,
                 Mechanical verification. {\bf F.3.3} Theory of
                 Computation, LOGICS AND MEANINGS OF PROGRAMS, Studies
                 of Program Constructs, Type structure. {\bf D.2.5}
                 Software, SOFTWARE ENGINEERING, Testing and Debugging,
                 Symbolic execution.}
}

@Book{Fletcher-Rossing-2000,
  author = {Fletcher, N.H. and Rossing, T.D.},
  title = {The Physics of Musical Instruments},
  publisher = Springer,
  year = 2000,
  edition = {2nd ed. 1998. Corr. 3rd printing 2000},
  ISBN = {0-387-98374-0},
  note = {XIX, 756 pp. 485 figs.},
  URL = {http://www.springer.de/cgi-bin/search_book.pl?isbn=0-387-98374-0},
  bibliographies = {Vati},
  abstract = {While the history of musical instruments is nearly as old as
      civilisation itself, the science of acoustics is quite recent. By
      understanding the physical basis of how instruments are used to make
      music, one hopes ultimately to be able to give physical criteria to
      distinguish a fine instrument from a mediocre one. At that point
      science may be able to come to the aid of art in improving the design
      and performance of musical instruments. As yet, many of the
      subtleties in musical sounds of which instrument makers and musicians
      are aware remain beyond the reach of modern acoustic measurements.
      This book describes the results of such acoustical investigations -
      fascinating intellectual and practical exercises. Addressed to
      readers with a reasonable grasp of physics who are not put off by a
      little mathematics, this book discusses most of the traditional
      instruments currently in use in Western music. A guide for all who
      have an interest in music and how it is produced, as well as serving
      as a comprehensive reference for those undertaking research in the
      field.

      "Essentially everything you have ever wanted to know about the
      physics of musical instruments" PHYSICS TODAY "a rigor, graphical
      detail, and verbal description." AUDIO},
  contents = {Part I : Vibrating Systems. Free and Forced Vibrations of Simple Systems. Continuous Systems in One Dimension: Strings and Bars. Two-Dimensional Systems: Membranes, Plates, and Shells. Coupled Vibrating Systems. Nonlinear Systems.- Part II: Sound Waves. Sound Waves in Air. Sound Radiation. Pipes, Horns, and Cavities.- Part III: String Instruments. Guitars and Lutes. Bowed String Instruments. Harps, Harpsicords, Clavicords, and Dulcimers. The Piano.- Part IV : Wind Instruments. Sound generation by Reed and Lip Vibrations. Lip-driven Brass Instruments. Woodwind Reed Instruments. Flutes and Flue Organ Pipes. Pipe Organs.- Part V: Percussion Instruments. Drums. Mallet Percussion Instruments. Cymbals, Gongs, Plates, and Steel Drums. Bells.- Part VI: Materials. Materials for Musical Instruments.- Name Index.- Subject Index.}
}

@Article{Flocchini-Mans-Santoro-2002,
  author = {Paola Flocchini and Bernard Mans and Nicola Santoro},
  title = {Sense of direction in distributed computing},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {to appear},
  WKloc = {doc/pap/BIB},
  bibliographies = {concurrency},
  abstract = {Sense of direction is a property of labeled graphs which has
      been shown to have a definite impact on computability and complexity
      in systems of communicating entities, and whose applicability ranges
      from the analysis of graph classes to distributed object systems. The
      full consequences of this property are still not known; in fact, the
      ongoing investigations continue to bring new (often surprising)
      results, to establish unsuspected links with other research and/or
      application areas, and to pose more questions than they answer. The
      aim of this paper is to provide a view of the current status of
      research, describing some of the relevant results, and providing
      pointers to future research directions.},
  OPTannote = {}
}

@Misc{Floyd-,
  author = {Paul Floyd},
  title = {A Discussion of {Oberon}},
  howpublished = {EDM/2},
  WKloc = {A-1431}
}

@InProceedings{Floyd-1967,
  author = {R. W. Floyd},
  title = {Assigning meaning to programs},
  editor = {J. T. Schwartz},
  series = {Proc.\null{} Sympos.\null{} in Appl.\null{} Math.},
  pages = {19--32},
  booktitle = {Mathematical Aspects of Computer Science},
  year = 1967,
  organization = AMS,
  bibliographies = {RelMiCS}
}

@InProceedings{Fokker-1995,
  author = {Jeroen Fokker},
  title = {Functional Specification of {JPEG} Decompression, and an Implementation for Free},
  pages = {102--120},
  URL = {http://www.cs.uu.nl/people/jeroen/article/jpeg/},
  bibliographies = {FP},
  WKloc = {A-0467},
  booktitle = {Programming Paradigms in Graphics, {Proceedings of the Eurographics workshop in Maastricht, the Netherlands, September 1995}},
  year = 1995,
  editor = {R.C. Veltkamp and E.H.Blake},
  address = {Wien},
  publisher = Springer
}

@PhDThesis{Fokkinga-1992,
  author = {M.M. Fokkinga},
  title = {Law and order in algorithmics},
  school = {Twente University},
  year = 1992,
  bibliographies = {RelMiCS}
}

@TechReport{Fokkinga-Meijer-1991,
  year = 1991,
  title = {Program Calculation Properties of Continuous Algebras},
  number = {91-4},
  institution = {CWI Amsterdam},
  author = {Maarten Fokkinga and Erik Meijer},
  bibliographies = {RelMiCS}
}

@TechReport{Fokkink-Verhoef-1997,
  author = {Wan Fokkink and Chris Verhoef},
  title = {An {SOS} Message: Conservative Extension in
           Higher-Order Positive/Negative Conditional Term Rewriting},
  number = {P9715},
  year = 1997,
  month = SEP,
  institution = {University of Amsterdam, Programming Research Group},
  WKloc = {A-0426}
}

@Misc{Fokkink-Verhoef-199X,
  author = {Wan Fokkink and Chris Verhoef},
  title = {A Conservative Look at Operational Semantics with Variable Binding},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {199?},
  OPTnote = {},
  WKloc = {A-1324}
}

@InProceedings{Ford-2004,
  author = 	 {Bryan Ford},
  title = 	 {Parsing Expression Grammars: A Recognition-Based Syntactic Foundation},
  crossref =  {POPL2004},
  pages =	 {\unfinished},
  WKloc = 	 {A-1678, doc/pap/BIB}
}

@InProceedings{Forest-2002,
  author = {Julien Forest},
  title = {A Weak Calculus with Explicit Operators for Pattern Matching and Substitution},
  crossref = {RTA2002},
  pages = {174--191},
  WKloc = {doc/pap/BIB},
  bibliographies = {PMC},
  abstract = {In this paper we propose a Weak Lambda Calculus called
   ${\lambda P_w}$ having explicit operators for Pattern Matching and
   Substitution. This formalism is able to specify functions defined by
   cases via pattern matching constructors as done by most modern
   functional programming languages such as OCAML. We show the main
   property enjoyed by ${\lambda P_w}$, namely subject reduction,
   confluence and strong normalization.}
}

@InProceedings{Forest-Kesner-2003,
  author = {Julien Forest and Delia Kesner},
  title = {Expression Reduction Systems with Patterns},
  crossref = {RTA2003},
  pages = {107--122},
  bibliographies = {PMC},
  WKloc = {A-1403, doc/pap/BIB}
}

@Article{Forguson-1993,
  author = {Iain Forguson},
  title = {Functional programming: More Fundamental than BASIC?},
  journal = {LISP POINTERS},
  volume = 6,
  number = 2,
  year = 1993
}

@Unpublished{Formisano-Omodeo-Simeoni-2000,
  author = {Andrea Formisano and Eugenio G. Omodeo and Marta Simeoni},
  title = {A Graphical Approach to Map Reasoning},
  note = {unpublished draft (as of 2000-10-17)},
  month = OCT,
  year = 2000,
  abstract = {Map reasoning is concerned with relations over an unspecified
              domain of discourse. Two limitations w.r.t. first-order
              reasoning are: only dyadic relations are taken into account;
              all map formulas are equations, having the same expressive power
              as first-order sentences in three variables. The map formalism
              inherits from the Peirce-Schr\"oder tradition, through
              contributions of Tarski and many others.

              Algebraic manipulation of map expressions
              (equations in particular) is much less natural than
              developing inferences in first-order logic; it may in fact
              appear to be overly machine-oriented for direct hand-based
              exploitation.

              The situation radically changes when one resorts to a
              convenient representation of map expressions based on
              labeled graphs. The paper provides details of this
              representation, which abstracts w.r.t. inessential features
              of expressions.

              Formal techniques illustrating three uses of the
              graph representation of map expressions are discussed:
              one technique deals with translating first-order specifications
              into map algebra; another one, with inferring equalities
              within map calculus with the aid of convenient
              diagram-rewriting rules; a third one with checking,
              in the specialized framework of set theory, the definability
              of particular set operations. Examples of use of these
              techniques are produced; moreover, a possible approach to
              mechanization of graphical map-reasoning is outlined.},
  keywords = {Formalized reasoning, algebra of dyadic relations, labeled multi-graphs, Peircean existential diagrams, graph transformation.},
  bibliographies = {RelMiCS, GraphCalc},
  WKloc = {A-1036, doc/pap/BIB/Formisano-Omodeo-Simeoni-2000.ps.gz}
}

@Article{Formisano-Omodeo-Temperini-2000,
  author = {Andrea Formisano and Eugenio G. Omodeo and Marco Temperini},
  title = {Goals and Benchmarking for Automated Map Reasoning},
  journal = JSYCO,
  year = 2000,
  volume = 29,
  number = 2,
  pages = {259--297},
  WKloc = {A-1037},
  bibliographies = {RelMiCS},
  abstract = {Tarski-Givant's map calculus is briefly reviewed,
              and a plan of research is outlined aimed at
              investigating applications of this ground equational formalism
              in the theorem-proving field. The main goal is to create
              synergy between first-order predicate calculus and the
              map calculus. Techniques for translating isolated sentences,
              as well as entire theories, from first-order logic into
              map calculus are designed, or in some cases simply
              brought nearer through the exercise of specifying properties
              of a few familiar structures (natural numbers, nested lists,
              finite sets, lattices). It is also highlighted to what extent
              a state-of-the-art theorem-prover for first-order logic,
              namely Otter, can be exploited not only to emulate,
              but also to reason about, map calculus. Issues regarding
              ``safe'' forms of map reasoning are singled out,
              in sight of possible generalizations to the database area.}
}

@InProceedings{Forsberg-Ranta-2004,
  author = 	 {Markus Forsberg and Aarne Ranta},
  title = 	 {Functional Morphology},
  crossref =	 {ICFP2004},
  URL = {http://www.cs.chalmers.se/~markus/},
  URL_PDF = 	 {http://www.cs.chalmers.se/~markus/FM/FM_ICFP2004.pdf},
  WKloc = 	 {A-1611}
}

@Article{Forslund-1995,
  author = {Robert R. Forslund},
  title = {A Logical Alternative to the Existing Positional Number System},
  journal = {},
  year = {},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0702},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Fortenbacher-1990,
  author = {A. Fortenbacher},
  title = {Efficient Type Inference and Coercion in Computer Algebra},
  crossref = {DISCO90},
  pages = {56--60},
  keywords = {{\sc Scratchpad}}
}

@InProceedings{Foster-Struth-2011,
  author =       {Simon Foster and Georg Struth},
  title =        {Integrating an Automated Theorem Prover into {Agda}},
  crossref =  {NASAFM2011},
  pages =     {116--130},
  DOI =      {10.1007/978-3-642-20398-5_10},
  SpringerURL =    {http://www.springerlink.com/content/f561604gw221g4x7/},
  abstract = {Agda is a dependently typed functional programming language
     and a proof assistant in which
     developing programs and proving their correctness is one activity.
     We show how this process can be enhanced
     by integrating external automated theorem provers,
     provide a prototypical integration
     of the equational theorem prover Waldmeister,
     and give examples of how this proof automation works in practice.}
}

@TechReport{Foubister-1995,
  author = {Sandra P. Foubister},
  title = {Graphical Application and Visualization of Lazy Functional Computation},
  year = 1995,
  month = MAY,
  school = {The University of York, Department of Computer Science},
  WKloc = {A-0826}
}

@Misc{Foubister-Runciman-199X,
  author = {Sandra P. Foubister and Colin Runciman},
  title = {Techniques for Simplifying the Visualization of Graph Reduction},
  year = {199?},
  WKloc = {A-0816}
}

@Book{Foulds-1981,
  author =	 {L. R. Foulds},
  title = 	 {Optimization Techniques},
  publisher = 	 Springer,
  year = 	 1981,
  keywords = 	 {integer programming}
}

@Book{Foulds-1984,
  author =	 {L. R. Foulds},
  title = 	 {Combinatorial Optimization for Undergraduates},
  publisher = 	 Springer,
  year = 	 1984,
  keywords = 	 {integer programming},
  McMaster = 	 {QA 164 .F68 1984}
}

@Article{Fourer-Gay-Kernighan-1990,
  author = {R. Fourer and D. M. Gay and B. W. Kernighan},
  title = {A Modeling Language for Mathematical Programming},
  journal = {Management Science},
  volume = 36,
  number = 5,
  pages = {519--554},
  year = 1990,
  coden = {MSCIAM},
  ISSN = {0025-1909},
  WKloc = {A-1373},
  note = {(The URL is for the longer technical report cited in
                 the {\em Management Science} paper.)},
  URL = {http://cm.bell-labs.com/cm/cs/what/ampl/REFS/amplmod.ps.gz}
}

@InProceedings{Fradet-1994,
  author = {P. Fradet},
  title = {Compilation of Head and Strong Reduction},
  crossref = {ESOP1994},
  pages = {211--224},
  abstract = {Functional language compilers implement only
		  weak-head reduction. However, there are cases where
		  head normal forms or full normal forms are
		  needed. Here, we study how to use cps conversion for
		  the compilation of head and strong reductions. We
		  apply cps expressions to a special continuation so
		  that their head or strong normal form can be
		  obtained by the usual weak-head reduction. We remain
		  within the functional framework and no special
		  abstract machine is needed. Used as a preliminary
		  step our method allows a standard compiler to
		  evaluate under $\lambda$'s.},
  annote = {Uses ``Reducing Reflexive Types'' from
		  \cite{Astesiano-Costa-1980}}
}

@Article{Fradet-Metayer-1991,
  author = {Pascal Fradet and le M\'etayer, Daniel},
  title = {Compilation of Functional Languages by Program
		  Transformation},
  year = 1991,
  volume = 13,
  pages = {21--51},
  number = 1,
  month = JAN,
  journal = ACM-TOPLAS,
  WKloc = {A-0014},
  ACMCAT = {D11 (Programming Techniques):Functional Programming; D24
      (Software Engineering): Program Verification --- correctness proofs;
      D34 (Programming Languages): Processors --- compilers, optimization},
  abstract = {One of the most important issues concerning functional
      languages is the efficiency and the correctness of their
      imlementation. We focus on sequential implementations for
      conventional von Neumann computers. The compilation process is
      described in terms of program transformations in the functional
      framework. The original functional expression is transformed into a
      fuctional term that can be seen as a traditional machine code. The
      two main steps are the compilation of the computation rule by the
      introduction of continuation fuctions and the compilation of the
      environment management using combinators. The advantage of this
      approach is that we do not have to introduce an abstract machine,
      which makes correctness proofs much simpler. As far as efficiency is
      concerned, this approach is promising since many optimizations can be
      described and formally justified in the functional framework.},
  annote = {{--- HOPSnotes ---}}
}

@Article{Fraenzle-vonStengel-Wittmuess-1995,
  year = 1995,
  volume = 53,
  title = {A generalized notion of semantic independence},
  pages = {5--9},
  journal = IPLET,
  author = {Martin Fr{\"a}nzle and Bernhard von Stengel
                          and Arne Wittm{\"u}ss},
  bibliographies = {RelMiCS}
}

@InProceedings{Francesco-Montanari-Ristori-1994,
  author = {de Francesco, N. and U. Montanari and G. Ristori},
  title = {Modelling Concurrent Accesses to Shared Data via Petri Nets},
  crossref = {PROCOMET94},
  pages = {397--416},
  keywords = {Concurrent Programming; Program Verification;
		  Databesed Management Systems; Semantics of
		  Nondeterminism and Concurrency}
}

@InProceedings{Franchetti-Pueschel-2003,
  author = {Franz Franchetti and Markus P{\"u}schel},
  title = {Short Vector Code Generation and Adaptation for DSP Algorithms},
  crossref = {ICASSP2003},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  URL = {http://www.ece.cmu.edu/~spiral/abstracts.html#simdov},
  WKloc = {doc/pap/BIB},
  bibliographies = {Anand},
  abstract = {Most recent general purpose processors feature short
       vector SIMD instructions, like SSE on Pentium III/4. In this paper we
       automatically generate platform-adapted short vector code for DSP
       transform algorithms using SPIRAL. SPIRAL represents and generates fast
       algorithms as mathematical formulas, and translates them into
       code. Adaptation is achieved by searching in the space of algorithmic
       and coding alternatives for the fastest implementation on the given
       platform. We explain the mathematical foundation that relates formula
       constructs to vector code, and overview the vector code generator
       within SPIRAL. Experimental results show excellent speed-ups compared
       to ordinary C code for a variety of transforms and computing
       platforms. For the DFT on Pentium 4, our automatically generated code
       compares favorably with the hand-tuned Intel MKL vendor library.}
}

@Article{Franciosa-Frigionia-Giaccio-2001,
  author = {Paolo Giulio Franciosa and Daniele Frigionia and Roberto Giaccio},
  title = {Semi-Dynamic Breadth-First Search in Digraphs},
  journal = TCS,
  year = 2001,
  volume = 250,
  number = {1--2},
  pages = {201--217},
  month = JAN,
  WKloc = {doc/pap/BIB},
  bibliographies = {HOPS},
  abstract = {In this paper we propose dynamic algorithms for maintaining
      a breadth-first search tree from a given source vertex of a directed
      graph G in either an incremental or a decremental setting. During a
      sequence of q edge insertions or a sequence of q edge deletions the
      total time required is O(m?min{q,n}), where n is the number of
      vertices of G, and m is the final number of edges of G in the case of
      insertions or the initial number of edges of G in the case of
      deletions. This gives O(n) amortized time for each operation if the
      sequence has length (m). Our algorithms require O(n+m) space. These
      are the first results in the literature concerning the dynamic
      maintenance of a breadth-first search tree for directed graphs. As a
      straightforward application of such algorithms we can maintain a
      shortest path tree for a directed graph in the case of unit edge
      weights within the same time bounds. In this case distance queries
      can be answered in constant time, while shortest path queries can be
      answered in time linear in the length of the retrieved path.},
  keywords = {Breadth-first search tree; Incremental algorithms; Decremental algorithms; Shortest paths; Amortized analysis}
}

@InProceedings{Franciosa-Gambosi-Nanni-1994,
  author = {P.G. Franciosa and G. Gambosi and U. Nanni},
  title = {On the Structure of DFS-Forests on Directed Graphs
		  and the Dynamic Maintenance of DFS on DAG's},
  crossref = {ESA94},
  authorsAddress = {Roma}
}

@PhDThesis{Frappier-1995,
  year = 1995,
  title = {A Relational Basis for Program Construction by Parts},
  school = {University of Ottawa},
  author = {Marc Frappier},
  address = {Computer Science Department, 150 Louis Pasteur,
                  Ottawa, ON, K1N 6N5, Canada},
  bibliographies = {RelMiCS},
  CiteSeer = {http://citeseer.nj.nec.com/frappier95relational.html},
  URL = {http://www.csi.uottawa.ca/~mfrappie/these.ps.gz},
  abstract = {Program construction by parts consists in tackling a complex
      specification one component at a time, developing a partially defined
      solution for each component, then combining the partial solutions
      into a global solution for the aggregate specification. This method
      is desirable whenever the specification at hand is too complex to be
      grasped in all its detail. It is feasible whenever the specification
      at hand is structured as an aggregate of clearly defined
      subspecifications --- where each [...]},
  WKloc = {A-0987, B-0062}
}

@InProceedings{Frappier-Mili-Desharnais-1995,
  author = {Marc Frappier and Ali Mili and Jules Desharnais},
  title = {Program Construction by Parts},
  pages = {257--281},
  crossref = {MPC1995},
  year = 1995,
  WKloc = {A-0437},
  bibliographies = {RelMiCS, RelMiS}
}

@Article{Frappier-Mili-Desharnais-1996,
  title = {A Relational Calculus for Program Construction by Parts},
  author = {Marc Frappier and Ali Mili and Jules Desharnais},
  pages = {237--254},
  journal = SCICOP,
  month = MAY,
  year = 1996,
  volume = 26,
  number = 3,
  WKloc = {A-0628},
  bibliographies = {RelMiCS}
}

@Article{Frappier-Mili-Desharnais-1998,
  pages = {317-340},
  year = 1998,
  title = {Unifying Program Construction and Modification},
  author = {Marc Frappier and Ali Mili and Jules Desharnais},
  month = MAR,
  volume = 6,
  number = 2,
  journal = {Logic Journal of the IGPL},
  abstract = {We propose a method which integrates program
                  modification to the refinement calculus style of
                  program development. Given a program developed
                  through stepwise refinement of a specification, we
                  propose an approach to specify modifications and to
                  derive a new program from the existing refinement
                  steps. This approach is based on the refinement
                  lattice operator meet. A modification to a
                  specification is represented by taking the meet of
                  the old specification and the new feature to add. A
                  solution to the new specification is constructed by
                  {\em coercing\/} the new feature to match the
                  structure of the existing refinement steps. The
                  method fosters reuse of refinement steps and their
                  proofs. We also show that program construction is
                  streamlined by using coercion.},
  WKloc = {A-0629},
  bibliographies = {RelMiCS}
}

@Article{Frappier-Mili-Desharnais-199X,
  author = {Marc Frappier and Ali Mili and Jules Desharnais},
  title = {Program Construction by Parts},
  journal = TCS,
  year = {199?},
  OPTbibliographies = {RelMiCS}
}

@InProceedings{Frederickson-1993,
  WKloc = {?},
  keywords = {?},
  authorsAddress = {Purdue University, West Lafayette},
  abstract = {?},
  title = {A Data Structure for Dynamically Maintaining Rooted Trees},
  pages = {?},
  crossref = {SODA1993},
  author = {Greg N. Frederickson}
}

@Article{Frege-1895,
  author = {Gottlob Frege},
  title = {{Kritische Beleuchtung einiger Punkte in E.\null{} Schr\"oders
		Vorlesungen \"uber die Algebra der Logik}},
  journal = ARSYST,
  volume = 1,
  pages = {433--456},
  note = {Engl.\null{} translation in {Frege} \cite{Frege1952}},
  year = 1895,
  bibliographies = {RelMiCS}
}

@Book{Frege-1952,
  author = {Gottlob Frege},
  title = {{Translations from the Philosophical Writings of Gottlob
		Frege}},
  note = {Peter Geach and Max Black, editors},
  publisher = Blackwell,
  address = {Oxford},
  year = 1952,
  bibliographies = {RelMiCS}
}

@Article{Freuder-1978,
  author = {Eugene C. Freuder},
  title = {Synthesizing Constraint Expressions},
  journal = CACM,
  volume = 21,
  number = 11,
  month = NOV,
  year = 1978,
  pages = {958--966},
  bibliographies = {RelMiCS}
}

@Book{Freyd-1964,
  author = 	 {Peter Freyd},
  title = 	 {Abelian Categories},
  publisher = 	 {Harper and Row},
  year = 	 1964,
  address = 	 {New York}
}

@Unpublished{Freyd-Hoogendijk-deMoor-1993,
  author = {Peter Freyd and Paul Hoogendijk and de Moor, Oege},
  title = {Membership of Datatypes},
  note = {Unpublished manuscript, see also \cite[Sect.~6.5]{Bird-deMoor-1997}.},
  annote = {extended abstract, obtained from PH at Dagstuhl seminar 9403},
  year = 1993,
  month = DEC,
  WKloc = {A-0219},
  abstract = {Using the first author's theory of allegories, we
		  investigate the concept of {\sl membership} of
		  datatypes. Datatypes are modelled as monotonic
		  endofunctors on an allegory. It is shown that under
		  some mild conditions on the allegory, the existence
		  of membership implies that the functor is strong,
		  and that its strength is unique. By contrast, not
		  every strong functor has membership.Furthermore, the
		  unique strength constructed from membership
		  satisfies the coherence conditions of a strong monad
		  for whatever monad structure the functor may
		  possess. As a more concrete application of
		  membership, it is shown how one may reason about the
		  implementation of sets by an inductive datatype.}
}

@Article{Freyd-Kelly-1972,
  author = {P. Freyd and G. M. Kelly},
  title = {Categories of Continuous Functors {I}},
  journal = {Journal of Pure and Applied Algebra},
  volume = 2,
  number = 3,
  year = 1972,
  pages = {169--191},
  WKloc = {A-0405},
  annote = {\S 2: Prefactorizations and Factorizations}
}

@Article{Freyd-OHearn-Power-Takeyama-Street-Tennent-1996,
  author = {Peter J. Freyd and Peter W. O'Hearn and A. John Power
                 and M. Takeyama and Ross Howard Street and Robert D.
                 Tennent},
  title = {Bireflectivity},
  journal = {Electronic Notes in Theoretical Computer Science},
  volume = 1,
  note = {to appear},
  year = 1996
}

@InProceedings{Freyd-Robinson-Rosolini-1991,
  author = {P.J. Freyd and E.P. Robinson and G. Rosolini},
  title = {Dinaturality for Free},
  pages = {107--118},
  booktitle = {Applications of Categories in Computer Science.
                 Proceedings of the {LMS} Symposium, {Durham} July 1991},
  year = 1992,
  editor = {M.P. Fourman and P.T. Johnstone and A.M. Pitts},
  publisher = {Cambridge University Press},
  address = {New York},
  note = {London Mathematical Society Lecture Note Series 177}
}

@InProceedings{Freyd-Robinson-Rosolini-1992,
  title = {Functorial Parametricity},
  author = {P. J. Freyd and E. P. Robinson and G. Rosolini},
  pages = {444--452},
  crossref = {LICS7},
  abstract = {In this paper we consider the idea of treating a
		  parameterized type as an arbitrary functor from some
		  parametrizing category to a category of types, and
		  giving elements semantics as natural
		  transformations.  We show that under reasonable
		  hypotheses this is only possible when the
		  parametrizing category is a groupoid. This suggests
		  a semantics for a ``semi-parametric'' form of
		  polymorphism.  In the final section we discuss the
		  interpretation of this form of parametricity in a
		  PER model, and show that it coincides with the
		  ostensibly stronger form derived from dinaturality.}
}

@Book{Freyd-Scedrov-1990,
  title = {Categories, Allegories},
  author = {Freyd, Peter J. and Scedrov, Andre},
  year = 1990,
  series = {North-Holland Mathematical Library},
  volume = 39,
  pages = {xviii+296},
  publisher = NoHo,
  address = {Amsterdam},
  ISBN = {0-444-70368-3 and 0-444-70367-5 (pbk)},
  UniBwM = {MAT200/V8901},
  WKloc = {Q-012},
  bibliographies = {RelMiCS}
}

@InProceedings{Frias-Aguayo-1994,
  author = {Frias, Marcelo F. and Aguayo, N.G.},
  title = {Natural Specifications vs.\null{} Abstract
		  Specifications. {A} Relational Approach},
  booktitle = {Proc.\null{} of {SOFSEM '94, Milovy, Czech Republic}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1994,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  month = NOV,
  pages = {17--22},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Frias-Aguayo-Novak-1993,
  author = {Frias, Marcelo F. and Aguayo, N.G. and Novak, B.},
  title = {Development of Graph Algorithms with Fork Algebras},
  booktitle = {Proc.\null{} of the {XIX  Latin\-american Conf.\null{} on
		  Informatics}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1993,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  pages = {529--554},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Frias-Baum-1995,
  author = {Frias, Marcelo F. and  Baum, Gabriel A.},
  title = {On the Exact Expressiveness and Probability of Fork Algebras},
  booktitle = {{Abstracts of the {$10^{th}$} Latinamerican Sympos.\null{}  on Mathematical Logic,
		  Colombia}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1995,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Unpublished{Frias-Baum-Aguirre-2000,
  author = {Marcelo Frias and Gabriel Baum and Nazareno M. Aguirre},
  title = {A Relational Characterization of Exhaustive Search and Backtracking},
  note = {Unpublished draft},
  month = APR,
  year = 2000,
  annote = {submitted to RelMiCS 5 proceedings},
  WKloc = {A-1024}
}

@Article{Frias-Baum-Haeberer-1997,
  title = {Fork Algebras in Algebra, Logic and Computer Science},
  author = {M. F. Frias and G. A. Baum and A. M. Haeberer},
  pages = {1--25},
  volume = 32,
  OPTnumber = {},
  year = 1997,
  journal = FUNDI,
  bibliographies = {RelMiCS}
}

@Article{Frias-Baum-Haeberer-1998,
  title = {Representability and Program Construction within Fork Algebras},
  author = {M.~F.~Frias and G.~A.~Baum and A.~M.~Haeberer},
  pages = {227--257},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {The representation theorem for fork algebras was always
      misunderstood regarding its applications in program construction. Its
      application was always described as ``the portability of properties
      of the problem domain into the abstract calculus of fork algebras''.
      In this paper we show that the results provided by the representation
      theorem are by far more important. We show that not only the
      heuristic power coming from concrete binary relations is captured
      inside the abstract calculus, but also design strategies for program
      development can be successfully expressed. This result makes fork
      algebras a programming calculus by far more powerful than it was
      previously thought.},
  bibliographies = {RelMiCS}
}

@TechReport{Frias-Baum-Haeberer-Veloso-1993,
  author = {Frias, Marcelo F. and Baum, Gabriel A.
      and Haeberer, Armando Mart\'{\i}n and Paulo A.S. Veloso},
  title = {A Representation Theorem for Fork Algebras},
  institution = {PUC-RJ},
  year = 1993,
  OPTkey = {},
  OPTtype = {},
  number = {MCC.\null{} 29/93},
  OPTaddress = {},
  month = AUG,
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Frias-Baum-Haeberer-Veloso-1995,
  author = {Frias, Marcelo F. and Baum, Gabriel A. and Haeberer, Armando
      Mart\'{\i}n and Paulo A.S. Veloso},
  title = {Fork Algebras are Representable},
  journal = Bulletin_of_the_Section_of_Logic_University_of_Lodz,
  year = 1995,
  volume = 24,
  number = 2,
  pages = {64--75},
  bibliographies = {RelMiCS}
}

@InProceedings{Frias-Gordillo-1995,
  author = {Frias, Marcelo F. and Gordillo, S.E.},
  title = {Semantical Optimization of Queries in Deductive
		  Object--Oriented Databases},
  booktitle = {{Proc.\null{} of ADBIS'95, Moscow}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1995,
  OPTorganization = {},
  publisher = Springer,
  OPTaddress = {},
  OPTmonth = {},
  pages = {55--72},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Frias-Haeberer-Veloso-1995,
  author = {Frias, Marcelo F. and Haeberer, Armando Mart\'{\i}n and Paulo
      A.S. Veloso},
  title = {On  the Metalogical Properties of Fork Algebras},
  journal = Bulletin_of_Symbolic_Logic,
  year = 1995,
  volume = 1,
  number = 3,
  pages = {364--365},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Frias-Haeberer-Veloso-1995a,
  author = {Frias, Marcelo F. and Haeberer, Armando Mart\'{\i}n and Paulo
      A.S. Veloso},
  title = {A Finite Axiomatization for Fork Algebras},
  journal = Bulletin_of_the_Section_of_Logic_University_of_Lodz,
  year = 1995,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Frias-Haeberer-Veloso-Baum-1995,
  author = {Frias, Marcelo F. and Haeberer, Armando Mart\'{\i}n and Paulo
      A.S. Veloso and Baum, Gabriel A.},
  title = {Representability of Fork Algebras},
  journal = Bulletin_of_Symbolic_Logic,
  year = 1995,
  volume = 1,
  number = 2,
  pages = {234--235},
  bibliographies = {RelMiCS}
}

@Misc{Frias-LopezPombo-Baum-2002,
  author =	 {Frias, Marcelo F. and L{\'o}pez Pombo, Carlos and Baum, Gabriel A.},
  title =	 {The Specification Language {Ag}},
  year =	 2002,
  WKloc = 	 {A-1497, doc/pap/BIB},
  abstract = {In this paper we present Ag, a specification language
     with simple syntax and easy to understand semantics in terms of
     binary relations. We present results on the expressiveness of Ag,
     a complete deductive system for Ag, and compare Ag with Alloy, a
     well known and attractive (due to its simplicity) specification
     language.}
}

@Article{Frias-Maddux-2001,
  author = {Marcelo F. Frias and Roger D. Maddux},
  title = {Completeness of a relational calculus for program schemes},
  journal = TCS,
  volume = 254,
  number = {1--2},
  pages = {543--556},
  day = 6,
  month = MAR,
  year = 2001,
  ISSN = {0304-3975},
  URL = {http://www.elsevier.nl/gej-ng/10/41/16/191/20/37/abstract.html},
  pdf = {http://www.elsevier.nl/gej-ng/10/41/16/191/20/37/article.pdf},
  bibliographies = {RelMiCS},
  WKloc = {A-1286, doc/pap/BIB}
}

@Misc{Frias-Wachenchauzer-1992,
  year = 1992,
  title = {Optimization of Queries with {Haeberer-Veloso's}
		  Relational Algebra},
  howpublished = {Document 688-Aug-2, 44th Meeting of the IFIP
		  Working Group 2.1. ``Programming Languages and Calculi''},
  author = {M. Frias and Rosa Wachenchauzer},
  address = {Augsburg, Germany},
  bibliographies = {RelMiCS}
}

@Book{Friedman-Wand-Haynes-,
  author = {Friedman and Wand and Haynes},
  title = {Essentials of Programming Languages},
  publisher = {MIT Press},
  year = 2001,
  edition = {2nd edition},
  abstract = {The goal of this book is to give students a deep, hands-on
      understanding of the essential concepts of programming languages,
      using Scheme as an executable metalanguage. Because Scheme is a
      wide-spectrum language, it enables us to write both at the very high
      level needed to produce a concise, comprehensible interpreter and at
      the much lower level needed to understand how that interpreter might
      be coded in assembly language, or transformed into a compiler.
      Because of Scheme's excellent abstraction facilities, we can write
      substantial language-processing systems that are nevertheless compact
      enough for students to understand and manipulate with a reasonable
      amount of effort. This is a hands-on book: everything discussed in
      the book may be programmed by students.},
  URL = {http://www.cs.indiana.edu/eopl/},
  bibliographies = {SE3E}
}

@InProceedings{Friedman-Wise-1976,
  author = 	 {Friedman, Daniel P. and Wise, David S.},
  title = 	 {{CONS} Should Not Evaluate its Arguments},
  crossref =	 {ICALP1976},
  pages =	 {257--284}
}

@InProceedings{Fritchie-etal-2000,
  author = {Scott Lystig Fritchie and others},
  title = {Sendmail Meets {Erlang}: Experiences Using {Erlang} for Email Applications},
  booktitle = {Sixth International Erlang/OTP User Conference, Oct. 3, 2000, Stockholm, Sweden},
  year = 2000,
  WKloc = {A-1368}
}

@INPROCEEDINGS{Fronk-2004,
  author = {Alexander Fronk},
  title = {Using relation algebra for the analysis of {Petri} nets in a {CASE} tool based approach},
  booktitle = {2nd IEEE Intl. Conference on Software Engineering and Formal Methods (SEFM 2004)},
  pages = {396--405},
  year = {2004}
}

@Article{Fruchterman-Reingold-1991,
  author = 	 {Thomas M. J. Fruchterman and Edward M. Reingold},
  title = 	 {Graph drawing by force-directed placement},
  journal = 	 {Software Practice and Experience},
  year = 	 1991,
  volume = 	 21,
  number = 	 11,
  pages = 	 {1129-1164},
  annote = 	 {Reference for spring embedder},
  WKloc =        {doc/pap/BIB},
  CiteSeer = 	 {http://citeseer.ist.psu.edu/fruchterman91graph.html},
  abstract = {In this paper, we introduce an algorithm that attempts to produce
      aesthetically-pleasing, two-dimensional pictures of graphs
      by doing simplified simulations of physical systems.
      We are concerned with drawing undirected graphs
      according to some generally accepted aesthetic criteria:
      1. Distribute the vertices evenly in the frame.
      2. Minimize edge crossings.
      3. Make edge lengths uniform.
      4. Reflect inherent symmetry.
      5. Conform to the frame.
      Our algorithm does not explicitly strive for these goals,
      but does well at distributing vertices evenly,
      making edge lengths uniform, and reflecting symmetry.
      Our goals for the implementation are speed and simplicity.
      PREVIOUS WORK Our algorithm for drawing undirected graphs
      is based on the work of Eades which, in turn, evolved
      from a VLSI technique called force-directed placement.}
}

@InProceedings{Fruehwirth-1995,
  author =       {Thom Fr{\"u}hwirth},
  title =        {Constraint handling rules },
  crossref =  {CP1994},
  pages =     {90--107},
  DOI =     {10.1007/3-540-59155-9_6},
  URL = {http://www.springerlink.com/content/p329j2243h1481m6/},
  abstract =    {We are investigating the use of a class of logical
                  formulas to define constraint theories and implement
                  constraint solvers at the same time. The
                  representation of constraint evaluation in a
                  declarative formalism greatly facilitates the
                  prototyping, extension, specialization and
                  combination of constraint solvers. In our approach,
                  constraint evaluation is specified using
                  multi-headed guarded clauses called constraint
                  handling rules (CHRs). CHRs define determinate
                  conditional rewrite systems that express how
                  conjunctions of constraints propagate and simplify.
                  In this paper we concentrate on CHRs as an extension
                  for constraint logic programming languages. Into
                  such languages, the CHRs can be tightly
                  integrated. They can make use of any hard-wired
                  solvers already built into the host
                  language. Program clauses can be used to specify the
                  non deterministic behavior of constraints, i.e. to
                  introduce search by constraints. In this way our
                  approach merges the advantages of constraints (eager
                  simplification by CHRs) and predicates (lazy choices
                  by clauses).}
}

@article{Fruehwirth-1998,
  author = {Thom Fr{\"u}hwirth},
  title = {Theory and practice of constraint handling rules},
  journal = {Special issue on constraint logic programming.
    Journal of Logic Programming},
  volume = {37},
  number = {1--3},
  pages = {95--138},
  year = 1998
}

@TechReport{Fu-1993a,
  author = {Fu, Yuxi},
  title = {Categorical Properties of Logical Frameworks},
  institution = {Manchester University, Department of Computer Science},
  year = 1993,
  number = {UMCS-93-6.3},
  URL = {ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-93-6-3.ps.Z},
  directory = {~kahl/doc/pap/manchester},
  abstract = {In this paper we give a new presentation of ELF which is
		  well-suited for semantic analysis.  We introduce the
		  notions of internal codability, internal
		  definability, internal typed calculi and frame
		  languages.  These notions are central to our
		  perspective of logical frameworks.  We will argue
		  that a logical framework is a typed calculus which
		  formalizes the relationship between internal typed
		  languages and frame languages.  In the second half
		  of the paper, we demonstrate the advantage of our
		  logical framework by showing some categorical
		  properties of it and of encodings in it.  By doing
		  so we hope to indicate a sensible model theory of
		  encodings.}
}

@TechReport{Fu-1993b,
  author = {Fu, Yuxi},
  title = {Encodings in Polymorphism, Revisited},
  institution = {Manchester University, Department of Computer Science},
  year = 1993,
  number = {UMCS-93-6.4},
  URL = {ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-93-6-4.ps.Z},
  directory = {~kahl/doc/pap/manchester},
  abstract = {We consider encodings in polymorphism with finite
		  product types.  These encodings are given in terms
		  of I-algebras.  They have the property that all
		  canonical terms (ground terms) are normal terms.  We
		  transplant the proof of a well-known result to our
		  setting and show why weak recursion is admissible.
		  We also show how to carry out the dual encodings
		  using the existential quantifier.}
}

@TechReport{Fu-1993c,
  author = {Fu, Yuxi},
  title = {Understanding Inductive Types in Constructions},
  institution = {Manchester University, Department of Computer Science},
  year = 1993,
  number = {UMCS-93-6.5},
  URL = {ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-93-6-5.ps.Z},
  directory = {~kahl/doc/pap/manchester},
  abstract = {In this paper we extend the Calculus of Construction
		  with generalized inductive types.  The extension is
		  justified by showing that the usual set theoretical
		  model can be effectivized.  It is also pointed out
		  that the model given in a published paper for a
		  collection of inductive types in a different style
		  is wrong.}
}

@TechReport{Fu-1993d,
  author = {Fu, Yuxi},
  title = {Recursive Models of General Inductive Types},
  institution = {Manchester University, Department of Computer Science},
  year = 1993,
  number = {UMCS-93-6.6},
  URL = {ftp://ftp.cs.man.ac.uk/pub/TR/UMCS-93-6-6.ps.Z},
  directory = {~kahl/doc/pap/manchester},
  abstract = {We give an interpretation of Martin-Lof's type theory
		  (with universes) extended with generalized inductive
		  types.  The model is an extension of the recursive
		  model given by Beeson.  By restricting our attention
		  to PER model, we show that the strictness of
		  positivity condition in the definition of
		  generalized inductive types can be dropped.  It
		  therefore gives an interpretation of general
		  inductive types in Martin-Lof's type theory.}
}

@InProceedings{Fu-1998,
  author = {Yuxi Fu},
  title = {Bisimulation Lattice of {Chi} Processes},
  crossref = {ASIAN1998},
  pages = {245--262},
  WKloc = {doc/pap/BIB/Fu-1998.pdf},
  abstract = {Chi calculus was proposed as a process algebra that has a
      uniform treatment of names. The paper carries out a systematic study
      of bisimilarities for chi processes. The notion of L-bisimilarity is
      introduced to give a possible classification of bisimilarities on chi
      processes. It is shown that the set of L-bisimilarities forms a four
      element lattice and that well-known bisimilarities for chi processes fit
      into the lattice hierarchy. The four distinct L-bisimilarities give rise to
      four congruence relations. Complete axiomatization system is given for
      each of the four relations. The bisimulation lattice of asynchronous chi
      processes and that of asymmetric chi processes are also investigated. It
      turns out that the former consists of two elements while the latter
      twelve elements. Finally it is pointed out that the asynchronous
      asymmetric chi calculus has a bisimulation lattice of eight elements.}
}

@MastersThesis{FuGuangrui-1999,
  author = {Guangrui Fu},
  title = {Design and Implementation of an Operating System in {Standard ML}},
  school = {Iniversity of Hawaii},
  year = 1999,
  URL = {http://www2.ics.hawaii.edu/~esb/prof/proj/hello/index.html},
  keywords = {Hello, Fox},
  WKloc = {A-1278},
  abstract = {The Hello project aims to design and implement an operating
      system in Standard ML of New Jersey(SML/NJ). We give an empirical
      study on the interaction between advanced programming languages and
      systems programming. This thesis presents the design and
      implementation of the Hello operating system, including porting the
      runtime system of SML/NJ, constructing the kernel image on a bare PC,
      and writing SML software to access and manage the hardware and system
      resources.}
}

@InProceedings{Fuchi-Furukawa-1991,
  title = {Role of Logic Programming in the {FGCS} Project},
  author = {Kazuhiro Fuchi and Koichi Furukawa},
  pages = {311--325},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {The research of the Fifth Generation Computer Project was
		  conducted based on a single principle: Logic
		  Programming. Logic programming unifies the ideas of
		  retrieval and computation. Both of these can be
		  regarded as forms of deduction. Research shows that
		  logic programming plays a central role in the
		  project. This role is as the foundation of a very
		  high level programming language based on constraint
		  logic programming, and as a formalization of a very
		  powerful concurrent programming language, which also
		  gives specifications for multi-processor architecture.}
}

@InProceedings{Fuchs-1998,
  author = {Marc Fuchs},
  title = {System Description: Similarity-Based Lemma Generation for Model Elimination},
  crossref = {CADE1998},
  pages = {33--37},
  OPTabstract = {},
  WKloc = {A-0600}
}

@InCollection{Fuchss-Reif-Schellhorn-Stenzel-1995,
  author = {T. Fuch{\ss} and W. Reif and G. Schellhorn and K. Stenzel},
  title = {{Three Selected Case Studies in Verification}},
  crossref = {KORSO-1995},
  bibliographies = {SpecTech},
  WKloc = {A-1061}
}

@TechReport{Fuehrmann-1998,
  author = 	 {Carsten F{\"u}hrmann},
  title = 	 {Direct models for the computational lambda-calculus},
  institution =  {University of Edinburgh, Laboratory for Foundations of Computer Science},
  year = 	 1998,
  number =	 {ECS-LFCS-98-400},
  URL = 	 {http://www.lfcs.inf.ed.ac.uk/reports/98/ECS-LFCS-98-400/},
  abstract = {We give direct categorical models for the
    computational lambda-calculus. By `direct' I mean that the model
    consists of one category together with operators on objects and
    morphisms for modelling type and program constructors,
    respectively. Moggi's $\lambda_C$-models, for example, are not
    direct, because the category of program denotations is constructed
    as the Kleisli category of a monad. We call our models `direct
    $\lambda_C$-models'. The main result is, loosely speaking, that each
    $\lambda_C$-model generates a direct $\lambda_C$-model, and each direct
    $\lambda_C$-model arises in this way. We shall make this precise by
    showing that the category of direct $\lambda_C$-models is reflective
    in the category of $\lambda_C$-models. From this we shall deduce that
    we can replace $\lambda_C$-models by direct $\lambda_C$-models without
    losing or gaining generality. We shall also see that the category
    of direct $\lambda_C$-models is equivalent to the category of
    $\lambda_C$-models that fulfil the equalizing requirement. Moreover,
    we shall see that we can describe our direct $\lambda_C$-models with
    universally quantified equations, which helps reasoning about
    programs and models. Finally, we shall see that direct
    $\lambda_C$-models reveal two kinds of well-behaved programs which
    are not obvious from $\lambda_C$-models.}
}

@Article{Fuentes-Quintana-Llorens-Genova-PrietoDiaz-2003,
 author = {Jos{\'e} M. Fuentes and V{\'\i}ctor Quintana and Juan Llorens and Gonzalo G{\'e}nova and Rub{\'e}n Prieto-D{\'\i}az},
 title = {Errors in the {UML} metamodel?},
 journal = {SIGSOFT Softw. Eng. Notes},
 volume = {28},
 number = {6},
 year = {2003},
 issn = {0163-5948},
 pages = {3--3},
 doi = {http://doi.acm.org/10.1145/966221.966236},
 publisher = {ACM Press},
 WKloca = {doc/pap/BIB}
}

@InProceedings{Fukunaga-Pree-Kimura-1993,
  author = {Alex Fukunaga and Wolfgang Pree and Takayuki Dan Kimura},
  title = {Functions as Data Objects in a Data Flow Based Visual Language},
  abstract = {Data flow based visual programming languages are an active
             area of research in visual programming languages. Some recent
             data flow visual programming languages have implemented higher
             order functions, allowing functions to be passed to/from
             functions. This paper describes a data flow visual programming
             language in which the first class citizenship of programs have
             been taken a further, and programs can be manipulated as data
             with the same kind of flexibility that LISP offers in
             manipulating programs as data.},
  crossref = {ACM1993},
  WKloc = {A-0167}
}

@TechReport{Furbach-Schmitz-1983,
  year = 1983,
  title = {{Rigorous derivation of a sophisticated algorithm:
                    {Smoothsort}}},
  number = 8302,
  institution = {Hochschule der Bundeswehr M\"unchen, Fachbereich Informatik},
  author = {U. Furbach and Lothar Schmitz}
}

@InProceedings{Furbach-Schmitz-1987,
  authorsAddress = {inf2},
  year = 1986,
  title = {Rigorous Derivation of a sophisticated algorithm:
                  {Smoothsort}},
  pages = {49--72},
  editor = {L.G.L.T. Meertens},
  booktitle = {Program specification and transformation, IFIP
		  TC2/WG 2.1 Working Conference},
  author = {Furbach, U. and Schmitz, L.},
  address = {Bad T\"olz}
}

@MastersThesis{Furey-1992,
  year = 1992,
  title = {A Computer-Aided Comparison of Walter Jerrold's
		  ``Goldsmith'' Poem to the Goldsmith Canon},
  school = {Michigan State University},
  author = {Martin Joseph Furey},
  annote = {QER13-47709},
  bibliographies = {RelMiCS}
}

@Article{Furs-1987,
  author = {S. N. Furs},
  title = {Syllogistics of some Theories},
  journal = ZMALOG,
  volume = 33,
  year = 1987,
  pages = {31--42},
  bibliographies = {RelMiCS}
}

@InProceedings{Furukawa-Okumura-Murukami-1987,
  author = {Koichi Furukawa and Akira Okumura and Masaki Murukami},
  title = {Unfolding rules for {GHC} Programs},
  pages = {149-161},
  crossref = {ProgFutGenCompII},
  abstract = {This paper presents a set of rules for the transformation of
      GHC (Guarded Horn Clauses) programs based on unfolding. The proposed
      set of rules, called UR-set, is shown to preserve freedom from
      deadlock and to preserve the set of solutions to be derived. UR-set
      is expected to give a basis for various program transformations,
      especially partial evaluation of GHC programs.},
  bibliographies = {RelMiCS}
}

@PhDThesis{Furusawa-1998,
  author = {Hitoshi Furusawa},
  title = {Algebraic Formalisations of Fuzzy Relations and Their Representation Theorems},
  year = 1998,
  month = MAR,
  school = {Department of Informatics, Kyushu University},
  bibliographies = {RelMiCS}
}

@Unpublished{Furusawa-2003z,
  author = {Hitoshi Furusawa},
  title = {The FL-Sketch for Kleene Algebras},
  note = {Notes produced at RelMiCS 7},
  month = MAY,
  year = 2003,
  WKloc = {A-1452}
}

@InProceedings{Furusawa-2003z,
  author = {Hitoshi Furusawa},
  title = {A Free Construction of {Kleene} Algebras with Tests},
  crossref = {MPC2004},
  month = FEB,
  year = 2004,
  WKloc = {A-1533}
}

@TechReport{Furusawa-Kahl-1998,
  author = {Hitoshi Furusawa and Wolfram Kahl},
  title = {A Study on Symmetric Quotients},
  OPTinstitution = {{Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen}},
  institution = {{Fakult\"at f\"ur Informatik, Univ.\null{} der Bundeswehr M\"unchen}},
  year = 1998,
  month = DEC,
  number = {1998-06},
  pages = 28,
  DirectURL = {http://relmics.mcmaster.ca/~kahl/Publications/TR/Furusawa-Kahl-1998.html},
  abstract = {Symmetric quotients, introduced in the context of
      heterogeneous relation algebras, have proven useful for applications
      comprising for example program semantics and databases.

      Recently, the increased interest in fuzzy relations has fostered a
      lot of work concerning relation-like structures with weaker
      axiomatisations.

      In this paper, we study symmetric quotients in such settings and
      provide many new proofs for properties previously only shown in the
      strong theory of heterogeneous relation algebras. Thus we hope to
      make both the weaker axiomatisations and the many applications of
      symmetric quotients more accessible to people working on problems in
      some specific part of the wide spectrum of relation categories.},
  bibliographies = {RelMiCS}
}

@TechReport{Furusawa-Kahl-2004,
  author = {Hitoshi Furusawa and Wolfram Kahl},
  title = {Table Algebras:  Algebraic Structure for Tabular Notation,
           Including Nested Headers},
  institution = {Research Center for Verification and Semantics, CVS,
    National Institute of Advanced Industrial Science and Technology, AIST},
  year = 2004,
  month = DEC,
  type = {Programming Science Technical Report},
  number = {PS-2004-009},
  pages = 15,
  URL = {http://unit.aist.go.jp/cvs/English/TR-en/04tr-en.html},
  PDFURL = {http://unit.aist.go.jp/cvs/tr-data/ps04-009.pdf},
  abstract = {We define a class of algebraic structures
      for the interpretation of tabular notations
      common in software requirements documentation
      and show how to use table algebra homomorphisms
      to define staged table semantics,
      translation between different kinds of tables,
      and table transformation.
      An important advantage of this approach is that it covers
      the practically important use of \emph{nested headers} in a natural way.},
  bibliographies = {WK, RelMiCS}
}

@Unpublished{Furusawa-Kameyama-Kinoshita-2001,
  author = {Hitoshi Furusawa and Yukiyoshi Kameyama and Yoshiki Kinoshita},
  title = {Axiomatics for the {Hoare-He-Sanders'} setting in refinement},
  note = {draft},
  WKloc = {A-1504},
  year = 2001,
  annote = {p.2 l.1: complecated --> complicated

            $\Bigwedge$ is the greatest lower bound ...
            $\Bigwedge [ \omega \inC i \mapsto C_i \leq c$ --> $\geq c$}
}

@Article{Furusawa-Nishizawa-Tsumagari-2009,
  author = 	 {Furusawa, H. and Nishizawa, K. and Tsumagari, N.},
  title = 	 {Multirelational Models of Lazy, Monodic Tree, and Probabilistic Kleene Algebras},
  journal = 	 {Bulletin of Informatics and Cybernetics},
  year = 	 2009,
  volume = 	 41,
  pages = 	 {11--24},
  bibliographies = {RelMiCS},
  abstract = {This paper studies basic properties of multirelations,
    and then shows that classes of multirelations provides models
    of three weaker variants of Kleene algebras, namely,
    lazy, monodic tree, and probabilistic Kleene algebras.
    Also it is shown that these classes of up-closed multirelations
    need not be models of Kozen's Kleene algebras
    unlike the case of ordinary binary relations.},
  InstURL = {http://catalog.lib.kyushu-u.ac.jp/recordID/21043}
}

@Misc{Furusawa-Sanda-Tsumagari-2009a,
  author = 	 {Hitoshi Furusawa and Fumiya Sanda and Norihiro Tsumagari},
  title = 	 {On 2 of Lemma 4 and 4 of Proposition 2 in ``$*$-Continuous Idempotent Semirings and Their Ideal Completion''},
  WKloc = {A-1752},
  OPTyear = 	 {2009 or 2010}
}

@InProceedings{Furuse-2001,
  author = {Jun Furuse},
  title = {Generic Polymorphism in {ML}},
  booktitle = {Journ{\'e}es Francophones des Langages Applicatifs --- {JFLA01}},
  year = 2001,
  WKloc = {A-1274},
  keywords = {polytypic programming}
}

@InProceedings{Furuse-Weis-2000,
  author = {Jun Furuse and Pierre Weis},
  title = {Entr{\'e}es/Sorties de valeurs en Caml},
  booktitle = {Journ{\'e}es Francophones des Langages Applicatifs --- {JFLA00}},
  year = 2000,
  month = JAN,
  pages = {???},
  OPTabstract = {},
  WKloc = {A-0904},
  URL = {http://pauillac.inria.fr/~weis/articles/jfla2000.ps.Z},
  annote = {see also \cite{Dubois-Rouaix-Weis-1995} and Jun Furuse's forthcoming PhD thesis}
}

@InProceedings{Futamura-Konishi-Glueck-2000,
  author = {Y. Futamura and Z. Konishi and R Gl\"uck},
  title = {Implementation of an experimental system for automatic program transformation based on generalized partial computation},
  booktitle = {Third International Workshop on Intelligent Software Engineering, {University of Limerick. Limerick, Ireland}},
  pages = {39--48},
  year = 2000,
  editor = {T. Menzies},
  note = {DIKU TOPPS publication D-418},
  keywords = {partial evaluation, program transformation, theorem proving, program optimization, recursion removal, algebraic manipulation},
  abstract = {Generalized Partial Computation (GPC) is a program
      transformation method utilizing partial information about input data,
      abstract data types of auxiliary functions and the logical structure
      of a source program. GPC uses both an inference engine such as a
      theorem prover and a classical partial evaluator to optimize
      programs. Therefore, GPC is more powerful than classical partial
      evaluators but harder to implement and control. We have implemented
      an experimental GPC system called WSDFU (Waseda
      Simplify-Distribute-Fold-Unfold). This paper discusses the power of
      the program transformation system, its theorem prover and future
      work.},
  URL = {http://www.diku.dk/topps/bibliography/2000.html#D-418},
  WKloc = {A-1284, doc/pap/BIB},
  bibliographies = {MathScheme}
}

@Manual{GHC.0.26,
  title = {The {Glorious Glasgow Haskell Compilation System Version 0.26, User's Guide}},
  author = {{AQUA Team}},
  organization = {Department of Computing Science, University of Glasgow},
  year = 1995,
  WKloc = {B-0081}
}

@Misc{GXL-FAQ,
  key =  {GXL~FAQ},
  author = {Ric Holt and Andy Sch{\"u}rr and Susan Elliott Sim and Andreas Winter},
  title =     "{G}raph e{X}change {L}anguage Frequently Asked Questions",
  month =     JUL,
  year =      {2002},
  note =      {Available at \textsf{http://www.gupro.de/GXL/faq/faq.html} (2004-10-10)},
  bibliographies = {GXL}
}

@Misc{GXL1.0,
  key =  {GXL~1.0},
  author = {Ric Holt and Andy Sch{\"u}rr and Susan Elliott Sim and Andreas Winter},
  title =     "{G}raph e{X}change {L}anguage",
  OPThowpublished = {},
  month =     APR,
  year =      {2002},
  note =      {Available at \textsf{http://www.gupro.de/GXL/dtd/gxl-1.0.html} (2004-10-10);
apparently updated since its official date.},
  bibliographies = {GXL}
}

@Misc{GXL1.1a,
  key = {{GXL~1.1$\alpha$}},
  author = {Ric Holt and Andy Sch{\"u}rr and Susan Elliott Sim and Andreas Winte},
  title = {{Graph eXchange Language GXL 1.1, Document Type Definition (alpha release)}},
  OPThowpublished = {},
  month =        DEC,
  year =         {2002},
  note =         {Available at \textsf{http://www.gupro.de/GXL/dtd/gxl-1.1.html} (2004-10-10)},
  bibliographies = {GXL}
}

@Manual{GXLValidator,
  title = 	 {{GXL Validator} short manual},
  author = 	 {Alexander Kaczmarek},
  organization = {University of Koblenz-Landau, Institute for Software Technology},
  month =	 OCT,
  year =	 2003,
  note = {Available from \textsf{http://www.gupro.de/GXL} since Jan.~2004},
  bibliographies = {GXL}
}

@InCollection{Gabbay-1992,
  author = {Dov M. Gabbay},
  title = {Elements of Algorithmic Proof},
  pages = {311--413},
  crossref = {HBLCS-II},
  WKloc = {A-1358},
  bibliographies = {FP, MathScheme}
}

@InProceedings{Gabbay-Pitts-1999,
  author = {Murdoch J. Gabbay and Andrew M. Pitts},
  title = {A New Approach to Abstract Syntax Involving Binders},
  crossref = {LICS14},
  pages = {214--224},
  note = {superseeded by \cite{Gabbay-Pitts-2000a}},
  URL = {http://www.cl.cam.ac.uk/users/amp12/freshml/}
}

@Article{Gabbay-Pitts-2000a,
  author = {Murdoch J. Gabbay and Andrew M. Pitts},
  title = {Abstract Syntax with Variable Binding},
  journal = FACOMP,
  OPTkey = {},
  OPTpages = {},
  year = 2000,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  note = {submitted},
  WKloc = {A-1039},
  URL = {http://www.cl.cam.ac.uk/users/amp12/freshml/}
}

@Article{GabbayM-2011a,
  author =       {Murdoch J. Gabbay},
  title =        {Foundations of nominal techniques: logic and semantics of variables in abstract syntax},
  journal =      Bulletin_of_Symbolic_Logic,
  year =         {2011},
  OPTkey =       {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTpages =     {},
  OPTmonth =     {},
  note =      {(to appear)},
  WKloc = {doc/pap/BIB},
  DirectURL = {http://www.gabbay.org.uk/papers.html#fountl},
  abstract =    {A survey and update paper describing a view of the logical and semantic foundations of nominal techniques applied to nominal abstract syntax.}
}

@InCollection{GabbayM-2011b,
  author =       {Murdoch J. Gabbay},
  title =        {Nominal terms and nominal logics: from foundations to meta-mathematics},
  booktitle =    {Handbook of Philosophical Logic, Volume 17},
  OPTcrossref =  {},
  OPTkey =       {},
  OPTpages =     {},
  OPTpublisher = {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTtype =      {},
  OPTchapter =   {},
  OPTaddress =   {},
  OPTedition =   {},
  DirectURL =     {http://www.gabbay.org.uk/papers.html#nomtnl},
  OPTnote =      {},
  WKloc = {doc/pap/BIB},
  abstract =    {A survey and update chapter describing a journey from (permissive-)nominal sets, via nominal terms, rewriting and algebra, to permissive-nominal logic and a finite first-order nominal axiomatisation of arithmetic.}
}

@InProceedings{Gabbrielli-Levi-Martelli-1992,
  author = {Maurizio Gabbrielli and Giorgio Levi and Maurizio Martelli},
  title = {New Semantic Tools for Logic Programming},
  crossref = {REX92},
  pages = {204--235},
  abstract = {The paper is a general overview of our approach to
		  the semantics of logic programs. We introduce new
		  notions and modify the classical semantics, i.e.\ we
		  use new {\em tools} to model those logic program
		  aspects which really capture their operational
		  semantics, and are therefore useful for defining
		  program equivalences and for semantics-based program
		  analysis. The approach leads to the introduction of
		  extended interpretations wwhich are more expressive
		  than Herbrand interpretations. The semantics in
		  terms of extended interpretations can be obtaind as
		  a result of both an operational and a fixpoint
		  construction. It can also be characterized from the
		  model-theoretic viewpoint, by defining a set of
		  extended models which contains standard Herbrand
		  models. The original construction for positive
		  programs is shown to be applicable to other classes
		  of logic programs, such as {\em constraint logic
		  programs, open programs} and {\em general
		  programs}. Different extensions are also shown
		  capable to handle higher order constructs such as
		  those used to compute sets of solutions.},
  keywords = {logic programming, operational semantics,
		  declarative semantics},
  contents = {0. Introduction
		  0.1. Why a new semantics
		  0.2. Program equivalences and observables
		  0.3. LP extensions
		  0.4. Our approach
		  0.5. Plan of the paper

		  1. The general approach
		  1.1. Preliminaries
		  1.2. Observable properties and $\pi$-interpretations
		  1.3. Fixpoint semantics and unfolding
		  1.4. Model-theoretic semantics

		  2. Positive logic programs
		  2.1. $\pi$-interpretations
		  2.2. Fixpoint semantics
		  2.3. Model-theoretic semantics

		  3. Constraint logic programs
		  3.1. The answer constraint semantics

		  4. Open programs
		  4.1. Computed answer substitution semantics for
		       $\Omega$-open programs

		  5. The treatment of negation and general logic programs
		  5.1. Finite Failure
		  5.2. The NAI-rule
		  5.3. SLDNF-resolution

		  6. Sets of solutions
		  6.1. The ground case
		  6.2. The non-ground case
		  6.3. A restricted case of Setof

		  7. Concludions}
}

@PhDThesis{Gadducci-1996,
  author = {Fabio Gadducci},
  title = {On the Algebraic Approach to Concurrent Term Rewriting},
  school = {Universit\`a degli studi di Pisa, Dipartimento di Informatica},
  year = 1996,
  OPTkey = {},
  OPTaddress = {},
  OPTtype = {},
  month = MAR,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Gadducci-Heckel-1998,
  author = {Gadducci, F. and Heckel, R.},
  title = {An Inductive View of Graph Transformation},
  editor = {Parisi-Presicce, F.},
  booktitle = {Recent Trends in Algebraic Development Techniques},
  year = 1998,
  publisher = {Springer Verlag},
  series = {LNCS},
  volume = 1376,
  pages = {219--233}
}

@Misc{Gadducci-Heckel-Llabres-1998,
  author = {Fabio Gadducci and Reiko Heckel and Merc{\'e} Llabr{\'e}s},
  title = {A Bi-Categorical Axiomatisation of Concurrent Graph Rewriting},
  year = 1998,
  WKloc = {A-0747}
}

@InProceedings{Gadducci-Montanari-1995,
  author = {Gadducci, Fabio and Montanari, Ugo},
  title = {Enriched Categories as Models of Computation},
  editor = {De Santis, Alfredo},
  booktitle = {Fifth Italian Conference on Theoretical Computer Science},
  pages = {20--42},
  publisher = {World Scientific},
  year = 1995,
  WKloc = {A-0962},
  abstract = {In this paper we discuss a general methodology to provide
              a categorical semantics for a wide class of
              computational systems, whose behaviour can be described
              by a suitable set of transition steps. We open our survey
              presenting some results on the semantics of Petri Nets.
              Starting from this, we elaborate a two-steps procedure
              allowing for the description of all the sequences of transitions
              performed by a given system, and equipping them with a
              suitable equivalence relation. This relation provides
              the system under analisys with a concurrent semantics:
              equivalence classes denote families of
              ``computationally equivalent'' behaviours, corresponding
              to the execution of the same set of
              (causally) independent transition steps.},
  annote = {Contains definitions of 2-categories, computads, and double categories.}
}

@InProceedings{Gadducci-Montanari-1998,
  author = {Gadducci, Fabio and Montanari, Ugo},
  title = 	 {Axioms for Contextual Net Processes},
  crossref =  {ICALP1998},
  pages = 	 {296--308}
}

@InCollection{Gadducci-Montanari-1999,
  author = {Fabio Gadducci and Ugo Montanari},
  title = {The Tile Model},
  booktitle = {Essays in Honour of Robin Milner},
  publisher = {MIT Press ??},
  year = {1999 ??},
  note = {to appear},
  annote = {semifinal ?? version, with WK annotations},
  WKloc = {A-0807}
}

@Misc{Gadducci-Montanari-1999a,
  author = {Fabio Gadducci and Ugo Montanari},
  title = {Comparing Logics for Rewriting: Rewriting Logic, Action Calculi and Tile Logic},
  year = 1999,
  month = MAR,
  WKloc = {A-0840}
}

@Article{Gaede-Guenther-1998,
  author = {Volker Gaede and Oliver G{\"u}nther},
  title = {Multidimensional access methods},
  journal = ACMCS,
  year = 1998,
  volume = 30,
  number = 2,
  pages = {170--231},
  WKloc = {doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/280277.280279},
  abstract = {Search operations in databases require special support at
      the physical level. This is true for conventional databases as well
      as spatial databases, where typical search operations include the
      point query (find all objects that contain a given search point) and
      the region query (find all objects that overlap a given search
      region). More than ten years of spatial database research have
      resulted in a great variety of multidimensional access methods to
      support such operations. We give an overview of that work. After a
      brief survey of spatial data management in general, we first present
      the class of point access methods, which are used to search sets of
      points in two or more dimensions. The second part of the paper is
      devoted to spatial access methods to handle extended objects, such as
      rectangles or polyhedra. We conclude with a discussion of theoretical
      and experimental results concerning the relative performance of
      various approaches.}
}

@Article{Gaehler-2000,
  author = 	 {Werner G{\"a}hler},
  title = 	 {General Topology --- The Monadic Case, Examples, Applications},
  journal = 	 {Acta Math.\null{} Hungar.\null{}},
  year = 	 2000,
  volume =	 88,
  number =	 4,
  pages =	 {279--290},
  URL = 	 {http://resolver.scholarsportal.info/resolve/02365294/v88i0004/279_gttmcea&form=pdf&file=file.pdf},
  WKloc = {doc/pap/BIB},
  abstract = {The paper deals with monadic as well as monadic-free
     topological notions. For defining these monadic-free notions the
     notion of basic triple F is introduced. A lot of monadic-free
     topological notions are presented, for instance that of
     F-convergence structure, F-hull operator and F-uniform
     structure. By means of a generalized metric, e.g. a probabilistic
     metric, and the general notion of F-zero approach introduced in
     this paper, a F-uniform structure is generated. In case of a
     fuzzy metric the related F-uniform structure defines in a canonic
     way a fuzzy topology which is used for developing a fuzzy
     analysis and fuzzy calculus.}
}

@InProceedings{Gaehler-Eklund-2000,
  author = 	 {W. G{\"a}hler and P. Eklund},
  title = 	 {Extension Structures and Compactifications},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {Categorical Methods in Algebra and Topology (CatMAT 2000)},
  pages = 	 {181--205},
  year = 	 {2000},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Gaerdenfors-1988,
  author = {G{\"{a}}rdenfors, P.},
  address = {Cambridge, MA},
  publisher = MIT_P,
  title = {Knowledge in Flux},
  year = 1988,
  bibliographies = {RelMiCS}
}

@InProceedings{Gaertner-Kimms-Kluge-1992,
  author = {D. G{\"a}rtner and A. Kimms and W. Kluge},
  title = {{$\pi$-${\sc Red}^+$} --- A Compiling Graph
		  Reduction System for a Full-Fledged $\lambda$-Calculus},
  crossref = {Kuchen-Loogen-1992},
  WKloc = {A-0270},
  abstract = {This paper describes a compiling graph reduction system
		  which performs high-level program transformations
		  governed by the reduction rules of an untyped
		  applied $\lambda$-calculus. In contrast to other
		  well-known graph reducers, our system completely
		  implements the reduction semantics of a full-fledged
		  $\lambda$-calculus. This renders it possible to
		  support higher-order functions without any
		  restrictions. Functions may be computed by
		  applications of functions to other functions or by
		  partial applications and are returned to the user as
		  high-level programs. Also, programs may be executed
		  in a step-by-step manner under interactive
		  control. All intermediate results are presented to
		  the user as high-level programs. Stepwise program
		  execution provides a convenient high-level debugging
		  facility.

                  In ordert to execute programs with competitive
		  speed, we have developed an abstract stack machine
		  ASP, which gives special support to some key
		  techniques for the execution of untyped functional
		  languages: function calls, parameter passing,
		  dynamic type-checking and heap-management.

                  The second part of the paper describes an extension
		  to the original version of {$\pi$-${\sc Red}^+$},
		  which terminates reduction whenever weak normal
		  forms (WNF{}s) are reached. The extension makes it
		  possible to reduce programs to their mormal forms by
		  using the basic mechanisms of the abstract stack
		  machine in conjunction with a novel indexing scheme
		  which resolves naming-conflicts that may occur in
		  the presence of free variables.}
}

@Article{Gaines-2009,
  author =       {Brian R. Gaines},
  title =        {Designing visual languages for description logics},
  journal =      {Journal of Logic, Language and Information},
  year =         2009,
  volume =    18,
  pages =     {217--250},
  annote =    {syllogism ``combinators'' instead of Boolean junctors, see cg message of 2010-12-07}
}

@Article{Gaines-2010,
  author =       {Brian R. Gaines},
  title =        {Human rationality challenges universal logic},
  journal =      {Logica Universalis},
  year =         2010,
  volume =    4,
  pages =     {163--205},
  annote =    {syllogism ``combinators'' instead of Boolean junctors, see cg message of 2010-12-07}
}

@Manual{Galassi-1998,
  author = {Mark Galassi},
  title = {Get Going with {DocBook}},
  year = 1998,
  institution = {Cygnus Solutions},
  WKloc = {A-0896}
}

@Book{Gall-1986,
  UniBwM = {KYB100/X1157},
  year = 1988,
  title = {Systemantics, The Underground Text of Systems Lore,
		  How Systems Really Work And Especially How They Fail},
  publisher = {The General Systemantics Press},
  edition = {2nd},
  author = {John Gall},
  address = {3200 West Liberty, Ann Arbor, MI 48103, USA}
}

@Article{Gallagher-Lafave-1998,
  author = {J. P. Gallagher and L. Lafave},
  title = {The Role of Trace Abstractions in Program Specialization Algorithms},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 12},
  WKloc = {A-0902, 45--48},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{GallegoArias-Lipton-Marino-Nogueira-2011,
  author = {Arias, Emilio Jesús Gallego and Lipton, James and Mariño, Julio and Nogueira, Pablo},
  title = {First-order unification using variable-free relational algebra},
  year = {2010},
  doi = {10.1093/jigpal/jzq011},
  abstract ={We present a framework for the representation and resolution
    of first-order unification problems and their abstract syntax
    in a variable-free relational formalism which is an executable variant
    of the Tarski-Givant relation algebra and of Freyd’s allegories
    restricted to the fragment necessary to compile and run logic programs.
    We develop a decision procedure for validity of relational terms,
    which corresponds to solving the original unification problem.
    The decision procedure is carried out
    by a conditional relational-term rewriting system.
    There are advantages over classical unification approaches.
    First, cumbersome and underspecified meta-logical procedures
    (name clashes, substitution, etc.) and their properties
    (invariance under substitution of ground terms,
    equality’s congruence with respect to term forming, etc.)
    are captured algebraically within the framework.
    Second, other unification problems can be accommodated, for example,
    existential quantification in the logic can be interpreted
    as a new operation whose effect is to formalize
    the costly and error prone handling of fresh names (renaming apart).},
  URL = {http://jigpal.oxfordjournals.org/content/early/2010/05/06/jigpal.jzq011.abstract},
  eprint = {http://jigpal.oxfordjournals.org/content/early/2010/05/06/jigpal.jzq011.full.pdf+html},
  journal = {Logic Journal of IGPL}
}

@Article{Gallier-1993,
  author = {Gallier},
  title = {Proof and Typed $\lambda$-calculus},
  journal = {Theoretical Computer Science},
  year = 1993,
  volume = 110,
  number = 2
}

@InProceedings{Gallier-Ratz-1985,
  title = {Logic Programming and Graph Rewriting},
  pages = {208--219},
  crossref = {SLP85},
  author = {Gallier, J. H. and Raatz, S.}
}

@InProceedings{Galligo-Grimm-Pottier-1990,
  author = {A. Gallig\`o and J. Grimm and L. Pottier},
  title = {The Design of {SISYPHE}: A System for Doing Symbolic
		  and Algebraic Computations},
  crossref = {DISCO90},
  pages = {30--39},
  annote = {implemented in Le-Lisp}
}

@TechReport{Gallo-Scutella-1999,
  author = {Giorgio Gallo and Maria Grazia Scutell{\`a}},
  title = {Directed Hypergraphs as a Modelling Paradigm},
  year = 1999,
  month = FEB,
  institution = {Universit{\`a} di Pisa, Dipartimento di Informatica},
  OPTtype = {},
  number = {TR-99-02},
  OPTaddress = {},
  WKloc = {A-0836}
}

@InProceedings{Galmiche-Hermann-1993,
  author = {D. Galmiche and O. Hermann},
  title = {{SKIL}: A System for Programming with Proofs},
  crossref = {LPAR93},
  pages = {348--350},
  note = {system description},
  WKloc = {A-0124},
  abstract = {SKIL (Synthesizing Knowledge in Intuitionistic Logic) is an
             interactive theorem prover dedicated to program synthesis and
             implemented in Quintus-Prolog. The object-level logic is a
             second-order constructive logic denoted AF2 which includes
             induction through the data type definitions [3]. Since the
             object-level is constructive, terms of $\lambda$-calculus are
             constructed during the proof search. These extracted terms are
             programs following the programming with proofs paradigm, having
             the two properties of correctness and termination [3]. That is,
             the system is in fact a program synthesis environment, since a
             specification can be regarded as a proposition and its
             extracted term as a program which meets this specification.}
}

@Book{Gamma-Helm-Johnson-Vlissides-1994,
  author = {Erich Gamma and Richard Helm and Ralph Johnson and
                 John Vlissides},
  title = {Design Patterns: Elements of Reusable Object-Oriented
                 Software},
  publisher = {Addison Wesley},
  address = {Massachusetts},
  year = 1994,
  WKLloc = {A-1634},
  ISBN = {0-201-63361-2}
}

@InProceedings{Gammie-vanderMeyden-2004,
  author = 	 {P. Gammie and R. van der Meyden},
  title = 	 {{MCK}: Model Checking the Logic of Knowledge},
  booktitle =	 {{16th International conference on Computer Aided Verification, CAV 2004}},
  pages =	 {479--483},
  year =	 2004,
  series =	 LNCS,
  WKloc = {doc/pap/BIB},
  publisher =	 Springer,
  URL = {http://www.cse.unsw.edu.au/~mck/},
  note =	 {\textsf{http://www.cse.unsw.edu.au/\~{}mck/}}
}

@Article{Gansner-Koutsofios-North-Vo-1993,
  author = {Emden R. Gansner and Eleftherios Koutsofios and Stephen C. North and Kiem-Phong Vo},
  title = {A Technique for Drawing Directed Graphs},
  journal = {IEEE-TSE},
  year = 1993,
  volume = 19,
  pages = {214--230},
  month = MAR
}


@InProceedings{GarciaPardo-Cabrera-Cordero-OjedaAciego-2013,
  author = {F. Garcia-Pardo and I. P. Cabrera and P. Cordero and Manuel Ojeda-Aciego},
  title = {On {Galois} Connections and Soft Computing},
  pages = {224--235},
  abstract = {After recalling the different interpretations usually assigned to the term
    Galois connection, both in the crisp and in the fuzzy case, we survey on several of their
    applications in Computer Science and specifically, in Soft Computing.},
  booktitle = {Advances in Computational Intelligence},
  year = 2013,
  volume = 7903,
  series = LNCS,
  DOI={10.1007/978-3-642-38682-4},
  DOIURL={https://dx.doi.org/10.1007/978-3-642-38682-4},
  publisher = {Springer-Verlag Berlin Heidelberg}
}


@InProceedings{Gardiner-Martin-deMoor-1992,
  author = {Paul Gardiner and Clare Martin and de Moor, Oege},
  title = {An Algebraic Construction of Predicate Transformers},
  crossref = {MPC1992},
  pages = {100--121},
  WKloc = {A-0236},
  bibliographies = {RelMiCS},
  abstract = {In this paper we present an algebraic construction
		  of the category of monotonic predicate transformaers
		  from the category of relations which is similar to
		  the standard algebraic construction of the integers
		  from the natural numbers. The same construction
		  yields the category of relations from the category
		  of total functions. This provides a mechanism
		  through which the rich type structure of the
		  category of total functions can be promoted to
		  successively weaker ones in the categories of
		  relations and predicate transformers. In addition,
		  it has exposed two complete rules for the refinement
		  and composition of specifications in Morgan's
		  refinement calculus.}
}

@TechReport{Gardiner-Morgan-1989,
  author = {P.H.B. Gardiner and Carroll Morgan},
  title = {A single complete rule for data refinement},
  year = 1989,
  number = {PRG-TR-7-89},
  month = NOV,
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0082},
  keywords = {design, theory, verification, bounded
		  nondeterminism, weakest precondition},
  ACMCAT = {D.2.2 Software Engineering: Tools and Techniques --
		  modules and interfaces;
                  D.2.2 Software Engineering: Program Verification --
		  correctness proofs;
                  D.3.3 Programming Languages: Language Constructs --
		  abstract data types;
                  F.3.1 Logics and Meaning of Programs: Specifying and
		  Verifying and Reasoning about Programs --
		  specification techniques},
  abstract = {One module is said to be refined by a second if no
        program using the second module can detect that it
        is not using the first; in that case the second
        module can replace the first in any program. Data
        refinement transformas the interior of a module ---
        its state and consequentially its operations --- in
        order to refine the module overall.

        A method for data refinement is sound if applying it
        does refine the module; a (sound) method is complete
        if any refinement of modules can be realised by its
        application.

        It has been known for some time that there are two
        methods of data refinement which are jointly
        complete for boundedly-nondeterministic programs:
        any refinement can be realised by applying one
        method then the other. Here it is shown that both of
        these methods are special cases of a single one
        which, therefore, is complete in itself.}
}

@Article{Gardiner-Morgan-1993,
  author = {P.H.B. Gardiner and Carroll Morgan},
  title = {A single complete rule for data refinement},
  journal = FACOMP,
  year = 1993,
  volume = 5,
  pages = {367--382}
}

@Misc{Gardner-,
  OPTkey = {},
  OPTauthor = {Philippa Gardner},
  OPTtitle = {Graphical Presentations of Interactive Systems},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  WKloc = {A-1264},
  keywords = {interaction net, sharing graphs, simple action graphs, nested graphs},
  annote = {p. 12: ``Our example is similar in spirit to Jeffrey's
      motivating example of his flow graphs \cite[Jeffrey-1997] based on
      the premonoidal categories of Power and Robinson
      \cite{Power-Robinson-1996}. Our approach has ann associative tensor
      operator, and includes the node \textbf{do} : $1 \tfun 1$ which
      controls which command is activated next. Jeffrey's approach regards
      the tensor as non-associative, and does not have the node
      \textbf{do}. Instead, the top wire connecting the assign and return
      nodes is a control line, which indicates that we must reduce the
      first command node attached to the control line.''

      Multiple \textbf{do} nodes for concurrency?}
}

@InProceedings{Gardner-1993,
  author = {Philippa Gardner},
  title = {A New Type theory for Representing Logics},
  crossref = {LPAR93},
  pages = {146--157},
  WKloc = {A-0123},
  abstract = {We propose a new type theory for representing logics, called
             $\LF^+$ and based on the Edinburgh Logical Framework. The new
             framework allows us to give, apparently for the first time,
             general definitions which capture how well a logic has been
             represented. Using our definitions, we show that, for example,
             first-order logic can be wellrepresented in $\LF^+$, whereas
             linear and relevant logics cannot. These syntactic definitions
             of representation have a simple formulation as indexed
             isomorphisms, which both confirms that our approach is a
             natural one, and provides a link between type-theoretic and
             approaches to frameworks.}
}

@InProceedings{Gardy-Louchard-1995,
  author = {D. Gardy and G. Louchard},
  title = {Dynamic Analysis of the Sizes of Relations},
  crossref = {STACS1995},
  pages = {433--444},
  authorsAddress = {Versailles Saint-Quentin; Bruxelles},
  abstract = {We present a dynamic modelization of a database when
		  submitted to a sequence of queries and updates, that
		  allows us to study the evolution of the sizes of
		  relations. While the problem of estimating the sizes
		  of derived relations at a given time (``static''
		  case) has been the subject of several studies, to
		  the best of our knowledge the evolution of the
		  relation sizes under queries and updates
		  (``dynamic'' cases) has not been studied so far. We
		  consider the size of a relation as a random
		  variable, and we study its probability distribution
		  when the database is submitted to a sequence of
		  insertions, deletions and queries. We show that it
		  behaves asymptotically as a Gaussian process, whose
		  expectation and covariance are proportional to the
		  time. This approach also allows us to analyze the
		  maximum of the size of the derived relation.},
  bibliographies = {RelMiCS}
}

@Unpublished{Garel-Olivier-1994,
  author = {Emmanuelle Garel and Jean-Pierre Olivier},
  title = {The Opoid Generated by Transitive Closure and
                  Interior and Symmetric Closure and Interior, A
                  Charaterization Using Generators and Relations},
  note = {?},
  OPTkey = {},
  year = 1994,
  month = OCT,
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Book{Garey-Johnson-1979,
  author = {Michael R. Garey and David S. Johnson},
  title = {Computers and Intractibility,
		A Guide to the Theory of NP-Completeness},
  publisher = Freeman,
  address = {New York},
  year = 1979,
  bibliographies = {RelMiCS}
}

@InProceedings{Gargov-Passy-Tinchev-1987,
  author = {Gargov, G. and Passy, S. and Tinchev, T.},
  title = {Modal Environment for {Boolean} Speculations},
  booktitle = {Mathematical Logic and Applications},
  editor = {Skordev, D.},
  year = 1987,
  publisher = Plenum,
  address = {New York},
  pages = {253--263},
  bibliographies = {RelMiCS}
}

@InCollection{Garillot-WernerB-2007,
  author={Garillot, François and Werner, Benjamin},
  title={Simple Types in Type Theory: Deep and Shallow Encodings},
  pages={368--382},
  LNCSbooktitle={TPHOLs 2007},
  booktitle={Theorem Proving in Higher Order Logics, {TPHOLs 2007}},
  year={2007},
  isbn={978-3-540-74590-7},
  volume={4732},
  series= LNCS,
  editor={Schneider, Klaus and Brandt, Jens},
  DOI={10.1007/978-3-540-74591-4_27},
  DOIURL={http://dx.doi.org/10.1007/978-3-540-74591-4_27},
  publisher = Springer,
  WKloc = {doc/pap/BIB},
  abstract = {We present a formal treatment of normalization by evaluation
    in type theory. The involved semantics of simply-typed λ-calculus
    is exactly the simply typed fragment of the type theory.
    This means we have constructed and proved correct a decompilation function
    which recovers the syntax of a program, provided it belongs to the
    simply typed fragment. The development runs and is checked in Coq.
    Possible applications include the formal treatment of languages with binders.}
}

@InProceedings{Garlan-Delisle-1990,
  author = {David Garland and Norman Delisle},
  title = {Formal Specifications As Reusable Frameworks},
  crossref = {VDM1990},
  pages = {150--163},
  WKloc = {A-1336},
  abstract = {We use our experience in applying formal methods to the
      development of electronic instrumtentation systems to argue the value
      of developing formal, domain-specific models that serve as reusable
      frameworks for a family of software products. To illustrate what we
      mean by ``framework'' we present a non-trivial specification for a
      family of instruments, and show how certain properties of that
      specification lead to its potential for reusability. Finally, we
      outline the important research issues that are raised by this
      approach. In particular, we examine the suitability of existing
      formal specification notations for explicitly characterizing and
      instantiating such frameworks.}
}

@InProceedings{Garrigue-1998,
  author = {Jacques Garrigue},
  title = {Programming with Polymorphic Variants},
  booktitle = {ML Workshop},
  year = 1998,
  month = SEP,
  URL = {http://wwwfun.kurims.kyoto-u.ac.jp/~garrigue/papers/variants.ps.gz},
  WKloc = {A-1194}
}

@Manual{Garrigue-1998a,
  title = {{Objective Label 2.00}: User's Manual},
  author = {Jacques Garrigue},
  month = SEP,
  year = 1998,
  WKloc = {A-1195}
}

@InProceedings{Garrigue-AitKaci-1994,
  author = {Jaques Garrigue and A{\"{\i}}t Kaci, Hassan},
  title = {The Typed Polymorphic Label-Selective $\lambda$-Calculus},
  crossref = {POPL1994},
  pages = {35--47},
  WKloc = {A-0397},
  abstract = {Formal calculi of record structures have recently
		  been a focus of active research. However, scarcely
		  anyone has studied formally the dual notion---{\sl
		  i.e.}, argument-passing to functions by keywords,
		  and its harmonization with currying. We have. $\ldots$}
}

@Article{Garrigue-Remy-1999,
  author = {Jaques Garrigue and Didier R{\'e}my},
  title = {Extending {ML} with Semi-Explicit Higher-Order Polymorphism},
  year = 1999,
  journal = iandc,
  volume = 155,
  month = DEC,
  pages = {134--171},
  WKloc = {A-0726}
}

@Article{Gates-Manganotti-Sabadini-Walters-1993,
  author = {R. Gates and F. Manganotti and N. Sabadini and R.F.C. Walters},
  title = {Distributive automata and {\sc unity}},
  OPTjournal = {},
  year = 1993,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  month = MAR,
  note = {Submitted. Available by anonymous {\tt ftp} at {\tt
      ghost.sm.dsi.unimi.it} in the directory {\tt pub2/papers/sabadini}
      and at {\tt maths.su.oz.au} in the directory {\tt
      sydcat/papers/walters}}
}

@Book{Gecseg-Steinby-1984,
  author = {F. G{\'e}cseg and M. Steinby},
  title = {Tree Automata},
  publisher = {Akad\'emiai Kiad\'o},
  year = 1984,
  address = {Budapest}
}

@Book{Geddes-Czapor-Labahn-1992,
  author = {Keith O. Geddes and S. R. Czapor and George Labahn},
  title = {Algorithms for Computer Algebra},
  publisher = {Kluwer},
  year = 1992,
  address = {Boston},
  pages = 585
}

@Book{Gediga-Duentsch-2000,
  author = {G{\"u}nther Gediga and Ivo D{\"u}ntsch},
  title = {Rough Set Data Analysis --- A Road to Non-Invasive Knowledge Discovery},
  publisher = {Methodos Publishers},
  year = 2000,
  address = {UK},
  URL = {http://methodos.saxen.net/books/primer2.htm},
  ISBN = {190328001X},
  note = {additional resources at URL: http://www.methodos.de/noninv/resources.html},
  bibliographies = {RelMiCS},
  abstract = {This is not the first book on rough set analysis and
              certainly not the first book on knowledge discovery algorithms,
              but it is the first attempt to do this in a non-invasive way.

              The term "non-invasive" in connection with knowledge discovery
              or data analysis is new and needs some introductory remarks.
              We have worked from about 1993 on topics of knowledge discovery
              and/or data analysis (both topics are sometimes hard to
              distinguish), and we felt that most of the common work on this
              topics was based on at least discussable assumptions.
              We regarded the invention of Rough Set Data Analysis (RSDA)
              as one of the big events in those days, because, at the start,
              RSDA was clearly structured, simple, and straightforward from
              basic principles to effective data analysis. It is our conviction
              that a model builder who uses a structural and/or statistical
              system should be clear about the basic assumptions of the model.
              Furthermore, it seems to be a wise strategy to use models with
              only a few (pre-)assumptions about the data. If both
              characteristics are fulfilled, we call a modelling process
              non-invasive. This idea is not really new, because the
              non-parametric statistics approach based on the motto of
              R.A. Fisher ``Let the data speak for themselves'' can be
              transferred to the context of knowledge discovery. It is no
              wonder that e.g. the randomisation procedure (one of the
              flagships of non-parametric statistics) is part of the
              non-invasive knowledge discovery approach.

              In this book we present an overview of the work we have done
              in the past seven years on the foundations and details of
              data analysis. During this time, we have learned to look at
              data analysis from many different angles, and we have tried
              not to be biased for --- or against --- any particular method,
              although our ideas take a prominent part of this book.
              In addition, we have included many citations of papers on RSDA
              in knowledge discovery by other research groups as well to
              somewhat alleviate the emphasis on our own work. We hope that
              the presentation is neither too rough nor too fuzzy, so that
              the reader can discover some knowledge in this book},
  contents = {1. Introduction
2. Data models and model assumptions
3. Basic rough set data analysis
3.1 Fundamentals
3.2 Approximation quality
3.3 Information systems
3.4 Indiscernability relations
3.5 Feature selection
3.6 Discernability matrices and Boolean reasoning
3.7 Rules
3.8 Approximation quality of attribute sets
4. Rule significance
4.1 Significant and casual rules
4.2 Conditional significance
4.3 Sequential randomisation
5. Data discretisation
5.1 Classificatory discretisation
5.2 Discretisation of real valued attributes
6. Model selection
6.1 Dynamic reducts
6.2 Rough entropy measures
6.3 Entropy measures and approximation quality
7. Probabilistic granule analysis
7.1 The variable precision model
7.2 Replicated decision systems
7.3 An algorithm to find probabilistic rules
7.4 Unsupervised learning and nonparametric distribution estimates
8. Imputation
8.1 Statistical procedures
8.2 Imputation from known values
9.0 Beyond rough sets
9.1 Relational attribute systems
9.2 Non-invasive test theory
10. Epilogue}
}

@Book{Gehani-McGettrick-1986,
  editor = {Narain Gehani and Andrew McGettrick},
  title = {Software Specification Techniques},
  publisher = {Addison-Wesley},
  year = 1986,
  series = {International Computer Science Series},
  UniBwM = {INF460/P6751}
}

@Book{Gehani-McGettrick-1988,
  editor = {Narain Gehani and Andrew McGettrick},
  title = {Concurrent Programming},
  publisher = {Addison-Wesley},
  year = 1988,
  series = {International Computer Science Series},
  McMaster = {QA 76.6 .C643 1988},
  bibliographies = {SE3B}
}

@Article{Gehrke-WalkerC-WalkerE-1996,
  author = 	 {Mai Gehrke and Carol Walker and Elbert Walker},
  title = 	 {De Morgan Systems on the Unit Interval},
  journal = 	 {International Journal of Intelligent Systems},
  year = 	 {1996},
  OPTkey = 	 {},
  OPTvolume = 	 {11},
  OPTnumber = 	 {},
  OPTpages = 	 {751--759},
  OPTmonth = 	 {},
  WKloc = 	 {A-1618, doc/pap/BIB},
  abstract = {Logical connectives on fuzzy sets arise from those on
     the unit interval. The basic theory of these connectives is cast
     in an algebraic spirit with an emphasis on equivalence between
     the various systems that arise. Special attention is given to De
     Morgan systems with strict Archimedean t-norms and strong
     negations. A typical result is that any De Morgan system with
     strict t-norm and strong negation is isomorphic to one whose
     t-norm is multiplication.}
}

@Article{Geiger-1990,
  author = {David Geiger},
  title = {Algebras of Binary Relations},
  journal = ALGU,
  volume = 27,
  year = 1990,
  pages = {319--332},
  bibliographies = {RelMiCS}
}

@Misc{Geist-Kohl-Papadopoulos-1996,
  author = {G. A. Geist and J. A. Kohl and P. M. Papadopoulos},
  title = {{PVM} and {MPI}: a Comparison of Features},
  year = 1996,
  month = MAY,
  WKloc = {A-0779}
}

@InProceedings{Gemis-Paredaens-Peelman-VanDenBussche-1993,
  author = {Marc Gemis and Jan Paredaens and Peter Peelman and van den
		  Bussche, Jan},
  title = {A Computational Model for Generic Graph Functions},
  crossref = {GTCS93},
  pages = {170--187},
  abstract = {The {\em generic graph machine}, a Turing
		  machine-like computation model for generic graph
		  functions, is introduced. A configuration of this
		  machine consists of a number of machine instances
		  that each are in a state and point to two nodes of a
		  graph. Durng the execution of a step, the machine
		  instances perform in parallel a local transformation
		  on the graph and are each replaced by a number of
		  other machine instances. It is proved that the
		  generic graph machines express a large and natural
		  class of generic graph functions.},
  keywords = {GOOD},
  bibliographies = {RelMiCS}
}

@Manual{Genet-VietTriemTong-,
  title = {Timbuk, A Tree Automata Library},
  OPTkey = {},
  OPTauthor = {Thomas Genet and Viet Triem Tong, Val{\'}rie},
  OPTorganization = {IRISA / Universit{\'e} de Rennes 1},
  OPTaddress = {},
  URL = {http://www.irisa.fr/lande/genet/timbuk/},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1105}
}

@Book{Gericke-1963,
  author = {H. Gericke},
  title = {{Theorie der Verb\"ande}},
  address = {Mannheim},
  publisher = BI,
  year = 1963,
  bibliographies = {RelMiCS}
}

@Article{Germano-Mazzanti-1989,
  author = {G. M. Germano and S. Mazzanti},
  title = {Closure Functions and General Iterates as Reflectors},
  journal = {Theoretical Computer Science},
  year = 1989,
  volume = 82,
  OPTnumber = {},
  OPTmonth = {},
  pages = {215--252},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0828},
  abstract = {Closure functions and the semantics of {\bf while-do}
               statements prove to have the same algebraic structure;
               both are reflectors, i.e.\null{} left adjoints
               for the inclusion functor.
               So reflectors appear to be basic elements
               of constructive mathematics.},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Germano-Mazzanti-1991,
  author = {G. M. Germano and S. Mazzanti},
  title = {General Iteration and Unary Functions},
  journal = {Annals of Pure and Applied Logic},
  year = 1991,
  volume = 54,
  OPTnumber = {},
  OPTmonth = {},
  pages = {137--178},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0835},
  abstract = {Programming practice suggests a notion of
               general iteration corresponding to the {\bf while-do} construct.
               This leads to new characterizations of
               general computable unary functions usable in computer science.},
  OPTcontents = {},
  OPTannote = {}
}

@Book{Gerritzen-1994,
  author = {Lothar Gerritzen},
  title = {Grundbegriffe der Algebra: Eine Einf\"uhrung unter
		  Ber\"ucksichtigung funktorieller Aspekte},
  publisher = {Vieweg},
  year = 1994,
  address = {Braunschweig}
}

@Misc{Gerth-Peled-Vardi-Wolper-,
  author = {R. Gerth and D. Peled and M. Y. Vardi and P. Wolper},
  title = {Simple On-the-fly Automatic Verification of Linear Temporal Logic},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  abstract = {We present a tableau-based algorithm for obtaining an automaton from a temporal logic formula. ...},
  WKloc = {A-1201},
  bibliographies = {SpecTech}
}

@TechReport{Geser-1992,
  DIRECTORY = {ftp.informatik.uni-ulm.de:/pub/uib},
  abstract = {The semantic path ordering $\preceq_{spo}$ is an ordering
      that allows to prove termination of term rewriting systems. Unlike
      other such orderings, it is not monotonic. We construct two monotonic
      suborderings $\preceq_{cspo}$, $\preceq_{mspo}$, of $\preceq_{spo}$.
      Both orderings rely on reasonable assumptions on the underlying
      semantic ordering, and mirror Kamin/L\'evy's termination proof
      method. Moreover, $\preceq_{mspo}$ is shown to cover $\preceq_{spo}$
      up to the subterm property. In the case of the semantic ordering
      being a simplification quasiordering, the three orderings even
      coincide. Thus the Knuth/Bendix ordering turns out to be a special
      case of the semantic path ordering.},
  year = 1992,
  type = {Ulmer Informatik-Berichte},
  title = {On a monotonic semantic path ordering},
  number = {UIB-92-13},
  month = {November},
  institution = {Universit{\"a}t Ulm},
  author = {Alfons Geser}
}

@InProceedings{Geser-1994,
  author = {Alfons Geser},
  title = {Mechanized Inductive Proof of Properties of a Simple
		  Code Optimizer},
  crossref = {KielTool94},
  pages = {91--115}
}

@TechReport{Geser-1998,
  author = {Alfons Geser},
  title = {Termination of one-rule string rewriting systems $\ell\to r$
                  where $|r|\leq 9$},
  institution = {Wilhelm-Schickard-Institut, Universit\"at T\"ubingen, Germany},
  note = {URL: \textsf{http://www-sr.informatik.uni-tuebingen.de/$\sim$geser/papers/srs9.ps.gz}},
  year = 1998,
  month = JAN,
  WKloc = {doc/pap/BIB},
  abstract = {It is an open problem whether termination of one-rule string
      rewriting systems is decidable. This paper contributes a few new
      criteria for termination of one-rule string rewriting systems. Using
      these results we give a complete classification of all $\ell\to r$
      where the length of $r$ does not exceed $9$.}
}

@Book{Geurts-1998,
  author = {Fr\'ed\'eric Geurts},
  title = {Abstract Compositional Analysis of Iterated Relations. A Structural Approach to Complex Transition Systems},
  year = 1998,
  publisher = Springer,
  series = LNCS,
  volume = 1426,
  UniBwM = {INF400/YE1228},
  bibliographies = {RelMiCS}
}

@Article{Geuvers-Pollack-Wiedijk-Zwanenburg-2002,
  author = 	 {H. Geuvers and R. Pollack and F. Wiedijk and J. Zwanenburg},
  title = 	 {A Constructive Algebraic Hierarchy in {C}oq},
  journal = 	 {Journal of Symbolic Computation},
  year = 	 2002,
  volume =	 34,
  number =	 4,
  pages =	 {271--286},
  note = {Special Issue on the Integration of Automated Reasoning
          and Computer Algebra Systems},
  annote = {From: Randy Pollack $<$rap@inf.ed.ac.uk$>$
            Date: Tue, 23 Mar 2004 12:27:54 +0000
            To: Lawrence Paulson $<$lp15@cam.ac.uk$>$
            Cc: pvs@csl.sri.com, hol-info@lists.sourceforge.net,
               coq-club@pauillac.inria.fr, isabelle-users@cl.cam.ac.uk,

            In our Coq constructive proof of the
            fundamental theorem of algebra
            we use setoids for quotienting.

            The library of algebraic structures developed for this project has
            been considerably improved and developed by people at Nijmegen.
            See \url{http://www.cs.kun.nl/fnds/ccorn}.}
}

@InProceedings{Geuvers-Werner-1994,
  title = {On the {Church-Rosser} Property for Expressive Type Systems
		  and its Consequences for their Metatheoretic Study},
  author = {Herman Geuvers and Benjarnin Werner},
  pages = {320--329},
  crossref = {LICS9},
  WKloc = {A-0372},
  abstract = {We consider two alternatives definitions for the
		  conversion rule in Pure Type Systems.  We study the
		  consequences of this choice for the metatheory and
		  point out the related implementation issues. We
		  relate two open problems by showing that if a {\sf
		  PTS} allows the construction of a fixed point
		  combinator, then Church-Rosser for
		  $\beta\eta$-reduction fails.  We present a new
		  formalization of Russell's paradox in a slight
		  extension of Martin-L\"of's inconsistent theory with
		  Type:Type and show that the resulting term leads to
		  a fix-poing construction.  The main consequence is
		  that the corresponding system is non-confluent.
		  This example shows that in some typed
		  $\lambda$-calculi, the Church-Rosser proof for the
		  $\beta\eta$-reduction is not purely combinatorial
		  anymore, as in pure $\lambda$-calculus, but relies
		  on the normalization and thus the logical
		  consistency of the system.}
}

@InProceedings{Ghani-1995,
  author = {Neil Ghani},
  title = {$\beta\eta$-Equality for Coproducts},
  crossref = {TLCA95},
  pages = {171--185},
  OPTabstract = {},
  WKloc = {A-0570}
}

@Misc{Ghani-1996u,
  author = {Neil Ghani},
  title = {Adjoint Rewriting and the !-Type Constructor },
  year = 1996,
  WKloc = {A-0867},
  note = {unpublished draft}
}

@InProceedings{Ghani-Lueth-1997,
  author = {Neil Ghani and Christoph L{\"u}th},
  title = {Monads and Modular Term Rewriting},
  booktitle = {Category Theory and Computer Science 1997},
  series = {LNCS},
  volume = 1290,
  publisher = {Springer-Verlag},
  year = 1997,
  pages = {69--86},
  OPTabstract = {},
  WKloc = {A-0870}
}

@Misc{Ghani-Lueth-199x,
  author = {Neil Ghani and Christoph L{\"u}th},
  title = {An Introduction to Categorical Rewriting},
  year = {199?},
  WKloc = {A-0869}
}

@InProceedings{Ghani-Lueth-Kahrs-1999,
  author = {Neil Ghani and Christoph L{\"u}th},
  title = {Rewriting the Conditions in Conditional Rewriting},
  booktitle = {Category Theory and Computer Science 1999},
  series = {LNCS},
  volume = {???},
  publisher = {Springer-Verlag},
  year = 1999,
  pages = {???},
  OPTabstract = {},
  WKloc = {A-0871}
}

@Article{Ghani-Lueth-DeMarchi-2005,
  author =       {Neil Ghani and Christoph L{\"u}th and De Marchi, Federico},
  title =        {Monads of Coalgebras: Rational Terms and Term Graphs},
  journal =      MSCS,
  year =         2005,
  volume =    15,
  pages =     {433--451},
  DOI =     {10.1017/S0960129505004743},
  JournalURL =   {http://journals.cambridge.org/action/displayAbstract?fromPage=online&aid=304566},
  WKloc = {doc/pap/BIB},
  abstract = {This paper introduces guarded and strongly guarded monads
     as a unified model of a variety of different term algebras
     covering fundamental examples such as
     initial algebras, final coalgebras, rational terms and term graphs.
     We develop a general method for obtaining finitary guarded monads
     that allows us to define and prove properties
     of the rational and term graph monads.
     Furthermore, our treatment of rational equations extends the traditional approach
     to allow right-hand sides of equations to be
     infinite terms, term graphs or other such coalgebraic structures.
     As an application, we use these generalised rational equations
     to sketch part of the
     correctness of the term graph implementation of functional programming languages.}
}

@Article{Ghani-Uustalu-2004,
  author = 	 {Neil Ghani and Tarmo Uustalu},
  title = 	 {Coproducts of Ideal Monads},
  journal = 	 {Journal of Theoretical Informatics and Applications},
  number  =      {38},
  pages =        {321--342},
  year = 	 {2004},
  WKloc = {doc/pap/BIB}
}

@Book{Ghezzi-Jazayeri-Mandrioli-2003,
  author =    {Carlo Ghezzi and Mehdi Jazayeri and Dino Mandrioli},
  title =     {Fundamentals of Software Engineering},
  publisher = {Prentice Hall},
  year =      2003,
  edition =   {2nd ed.}
}

@InProceedings{Ghica-2007,
  author = {Dan R. Ghica},
  title = {Geometry of synthesis: a structured approach to VLSI design},
  doi = {http://doi.acm.org/10.1145/1190216.1190269},
  crossref = {POPL2007},
  pages = {363--375},
  bibliographies = {Coconut},
  abstract = {We propose a new technique for hardware synthesis from
                  higher-order functional languages with imperative
                  features based on Reynolds's Syntactic Control of
                  Interference. The restriction on contraction in the
                  type system is useful for managing the thorny issue
                  of sharing of physical circuits. We use a semantic
                  model inspired by game semantics and the geometry of
                  interaction, and express it directly as a certain
                  class of digital circuits that form a cartesian,
                  monoidal-closed category. A soundness result is
                  given, which is also a correctness result for the
                  compilation technique.}
}

@Article{Ghilardi-1992,
  author = {Silvio Ghilardi},
  title = {Free {Heyting algebras} as {bi-Heyting algebras}},
  journal = {C. R. Math. Rep. Acad. Sci. Canada},
  year = 1992,
  OPTkey = {},
  OPTvolume = 14,
  OPTnumber = {},
  OPTpages = {240-244},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {}
}

@Article{Ghilardi-1997,
  title = {Unification through Projectivity},
  author = {Silvio Ghilardi},
  pages = {733--752},
  journal = {Journal of Logic and Computation},
  year = 1997,
  month = dec,
  volume = 7,
  number = 6,
  references = {\cite{JSYMC::Baader1989} \cite{JSYMC::MartinN1989}
                 \cite{JACM::Nipkow1990}}
}

@Article{Ghilardi-1999,
  author = {Silvio Ghilardi},
  title = {Unification in Intuitionistic Logic},
  journal = JSYLO,
  year = 1999,
  number = 2,
  volume = 64,
  pages = {859--880}
}

@Misc{Ghilardi-2001,
  author = {Silvio Ghilardi},
  title = {Unification in Propositional Logic},
  howpublished = {Talk at Tarski Centenary Conference,
    Stefan Banach Center, Warsaw},
  OPTmonth = {},
  year = 2001,
  WKloc = {doc/pap/BIB/Tarski-centenary},
  OPTnote = {},
  keywords = {Heyting algebra},
  annote = {Dipartamento delle Science dell'Informazione,
                   Universit\'a degli Studi di Milano}
}

@Article{Ghilardi-Meloni-1997,
  title = {Constructive canonicity in non-classical logics},
  author = {Silvio Ghilardi and Giancarlo Meloni},
  pages = {1--32},
  journal = {Annals of Pure and Applied Logic},
  month = {13~} # jun,
  year = 1997,
  volume = 86,
  number = 1
}

@Article{Ghilardi-Zawadowski-1995,
  author = {Silvio Ghilardi and M. Zawadowski},
  title = {A Sheaf Representation and Duality for Finitely Presented {Heyting} Algebras},
  journal = JSYLO,
  year = 1995,
  volume = 60,
  pages = {911--939}
}

@InProceedings{Ghosh-Kanhere-Krishnaiyer-Kulkarni-LiWei-LimCC-NgJohn-2003,
  author = {Somnath Ghosh and Abhay Kanhere and Rakesh Krishnaiyer and Dattatraya Kulkarni and Wei Li and Chu-Cheow Lim and John Ng},
  title = {Integrating High-Level Optimizations in a Production Compiler: Design and Implementation Experience},
  crossref = {CC2003},
  pages = {303--319},
  bibliographies = {Anand},
  WKloc = {A-1470, doc/pap/BIB},
  abstract = {The High-Level Optimizer (HLO) is a key part of the compiler
      technology that enabled Itanium$^{\mathrm{TM}}$ and
      Itanium$^{\mathrm{TM}}$2 processors deliver leading floating-point
      performance at their introduction. In this paper, we discuss the
      design and implementation experience in integrating diverse
      optimizations in the HLO module. In particular, we describe decisions
      made in the design of HLO targeting Itanium processor family. We
      provide empirical data to validate the design decisions. Since HLO
      was implemented in a production compiler, we made certain engineering
      trade-offs. We discuss these trade-offs and outline key learning
      derived from our experience.}
}

@InProceedings{Giammaresi-Montalbano-1995,
  author = {D. Giammaresi and R. Montalbano},
  title = {Deterministic Generalized Automata},
  crossref = {STACS1995},
  pages = {325--336},
  authorsAddress = {Palermo},
  abstract = {A generalized automaton (GA) is a finite automaton
		  where the single transitions are defined on words
		  rather than on single letters. Generalized automata
		  were considered by K. Hashiguchi who proved that the
		  problem of calculating the size of a minimal GA is
		  decidable.

                  We define the model of deterministic generalized
		  automaton (DGA) and study the problem of its
		  minimalization. A DGA has the restriction that, for
		  each state, the sets of words corresponding to the
		  transitions of that state are prefix sets. We solve
		  the problem of calculating the number of states of a
		  minimal DGA for a given language, by giving a
		  procedure that effectively constructs it starting
		  from the minimal (conventional) deterministic automaton.}
}

@InProceedings{Giancarlo-Mignosi-1994,
  author = {R. Giancarlo and F. Mignosi},
  title = {Generalizations of the Periodicity Theorem of Fine
		  and Wilf},
  crossref = {CAAP94},
  pages = {130--141}
}

@PhDThesis{Giannakopoulou-1999,
  author = {Dimitra Giannakopoulou},
  title = {Model Checking for Concurrent Software Architectures},
  school = {Department of Computing, Imperial College of Science, Technology and Medicine},
  year = 1999,
  address = {London},
  WKloc = {A-0905}
}

@Article{Giannini-Longo-1984,
  author = {Paola Giannini and Giuseppe Longo},
  title = {Effectively Given Domains and Lambda-Calculus Models},
  year = 1984,
  volume = 62,
  pages = {36--63},
  number = 1,
  month = JUL,
  journal = INFCON,
  keywords = {generalized Rice--Shapiro theorem, generalized
      Myhill--Shepherdson theorem, neighborhood systems, f-spaces (Ershov)},
  abstract = {The syntax of a formal language is effectively given. This
      is not immediately so for the semantics. This paper deals with the
      simple but sufficiently powerful applicative language
      ($\lambda$-calculus) and studies effectiveness properties of its
      semantics. In particular it analyses the effectiveness of the
      interpretation of $\lambda$-terms as well as different notions of
      computability over models.},
  bibliographies = {RelMiCS}
}

@InProceedings{Giannini-RonchiDellaRocca-1991,
  title = {Type Inference in Polymorphic Type Discipline},
  author = {Paola Giannini and Simona Ronchi Della Rocca},
  pages = {18--37},
  crossref = {TACS1991},
  abstract = {A hierarchy of type assignment systems is defined, which
		  is a complete stratification of the polymorphic type
		  assignment system. For each of such systems a type
		  inference algorithm is given.}
}

@PhDThesis{Gibbons-1991,
  year = 1991,
  title = {Algebras for Tree Algorithms},
  school = {Oxford University Computing Laboratory PRG},
  authorsAddress = {e-mail: jeremy\@cs.aukuni.ac.nz},
  author = {Jeremy Gibbons},
  address = {11 Keble Road, Oxford OX1 3QD England},
  bibliographies = {RelMiCS}
}

@InProceedings{Gibbons-1992,
  author = {Jeremy Gibbons},
  title = {Upwards and downwards accumulations on trees},
  pages = {122--138},
  abstract = {An accumulation is a higher-order operation over structured
             objects of some type; it leaves the shape of an object
             unchanged, but replaces each element of that object with some
             accumulated information about the other elements. Upwards and
             downwards accumulations on trees are two instances of this
             scheme; they replace each element of a tree with some
             function--in fact, some homomorphism--of that element's
             descendants and of its ancestors, respectively. These two
             operations can be thought of as passing information up and down
             the tree.

             We introduce these two accumulations, and show
             how together they solve the so-called prefix sums problem.},
  crossref = {MPC1992},
  WKloc = {A-0237}
}

@Misc{Gibbons-1994,
  author = {Jeremy Gibbons},
  title = {An Introduction to the {Bird-Meertens} Formalism},
  month = NOV,
  year = 1994,
  howpublished = {Presented at New Zealand Formal Program Development Colloquium Seminar, {Hamilton, Nov.~1994}},
  note = {Available at \textsf{URL: http://www.brookes.ac.uk/\~p0071749/},
            \textsf{http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html\#squiggolintro}},
  WKloc = {A-1454, doc/pap/BIB}
}

@InProceedings{Gibbons-1995,
  author = {Jeremy Gibbons},
  title = {An Initial-Algebra Approach to Directed Acyclic Graphs},
  crossref = {MPC1995},
  pages = {122--138},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#damgs},
  WKloc = {A-1455, doc/pap/BIB}
}

@InProceedings{Gibbons-1999,
  author = 	 {Jeremy Gibbons},
  title = 	 {Lecture Notes on Algebraic and Co-algebraic Methods fo Calculatinf Functional Programs},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {1999},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month = 	 MAR,
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1644, doc/pap/BIB}
}

@InProceedings{Gibbons-2001,
  author = {Jeremy Gibbons},
  title = {Towards a colimit-based semantics for visual programming},
  crossref = {COORD2001},
  pages = {166--173},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#superposition},
  WKloc = {A-1460, doc/pap/BIB},
  abstract = {Software architects such as Garlan and Katz promote the
    separation of computation from coordination. They encourage the study
    of connectors as first-class entities, and superposition of connectors
    onto components as a paradigm for component-oriented programming.
    We demonstrate that this is a good model for what visual programming tools
    like IBM's VisualAge actually do. Moreover, Fiadeiro and Maibaum's
    categorical semantics of parallel programs is applicable to this model,
    so we can make progress towards a formal semantics of visual programming.}
}

@InProceedings{Gibbons-2002,
  author = {Jeremy Gibbons},
  title = {Calculating Functional Programs},
  crossref = {ACMMPC2002},
  pages = {148--203},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#calcfp},
  WKloc = {A-1457, doc/pap/BIB},
  abstract = {Functional programs are merely equations; they may be
      manipulated by straightforward equational reasoning. In particular,
      one can use this style of reasoning to calculate programs, in the
      same way that one calculates numeric values in arithmetic. Many
      useful theorems for such reasoning derive from an algebraic view of
      programs, built around datatypes and their operations. Traditional
      algebraic methods concentrate on initial algebras, constructors, and
      values; dual co-algebraic methods concentrate on final co-algebras,
      destructors, and processes. Both methods are elegant and powerful;
      they deserve to be combined.}
}

@InProceedings{Giegerich-Kurtz-1994,
  author = {Robert Giegerich and Stefan Kurtz},
  title = {Suffix Trees in the Functional Programming Paradigm},
  crossref = {ESOP1994},
  pages = {225--240},
  WKloc = {A-0330},
  abstract = {We explore the design space of implementig suffix
		  tree algorithms in the functional paradigm. We
		  review the linear time and space algorithms of
		  McCreight and Ukkonen. Based on a new terminology of
		  nested suffixes and nested prefixes, we give a
		  simpler and more declarative explanation of these
		  algorithms than was previously known. We design two
		  ``naive'' versions of these algorithms which are not
		  linear time, but use simpler data structures, and
		  can be implemented in a purely functional
		  style. Furthermore, we present a new, ``lazy''
		  suffixes tree construction which is even simpler. We
		  evaluate both imperative and functional
		  implementations of these algorithms. Our results
		  show that the naive algorithms perform very
		  favorably, and in particular, the ``lazy''
		  construction compares very well to all the others.}
}

@TechReport{Gilham-Juellig-Ladkin-Polak-1987,
  author = {L-M. Gilham and R. J\"ullig and P. B. Ladkin and W. Polak},
  title = {Knowledge-based Project Management},
  institution = {Kestrel Inst.},
  number = {KES.U.87.3},
  bibliographies = {RelMiCS},
  year = 1987
}

@InProceedings{Gill-2000,
  author = {Andy Gill},
  title = {Debugging {Haskell} by Observing Intermediate Data Structures},
  crossref = {Haskell2000},
  WKloc = {A-1122},
  keywords = {HOOD}
}

@InProceedings{Gill-Launchbury-PeytonJones-1993,
  author = {Andrew Gill and John Launchbury and Peyton Jones, Simon},
  title = {A Short Cut to Deforestation},
  crossref = {FPCA-1993},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0792}
}

@InProceedings{Gill-PeytonJones-1994,
  author = {Andrew J. Gill and Peyton Jones, Simon L.},
  title = {Cheap Deforestation in Practice: An Optimizer for {Haskell}},
  crossref = {IFIP94A},
  pages = {581--586},
  abstract = {We present a simple, automatic transformation ---
		  the {\tt foldr/build} transformation --- which
		  successfully removes any intermediate lists from
		  programs written in non-strict functional
		  programming languages. While the idea is simple and
		  elegent, it turns out that some care is needed in
		  the compiler to set up the right conditions for the
		  {\tt foldr/build} transformation to be
		  applicable. We report on this practical experience,
		  and present results which quantify the benefits that
		  can in practice be achieved.},
  acmcodes = {D.1.1; I.1.4},
  ACMcats = {Programming Techniques, Applicative (Functional)
		  Programming; Algebraic Manipulation, Applications},
  annote = {see also {http://research.microsoft.com/Users/simonpj/Papers/deforestation-short-cut.ps.Z}}
}

@InCollection{Gilman1883,
  author = {Benjamin Ives Gilman},
  title = {Operations in Relative Number with Applications to the
		Theory of Probabilities},
  booktitle = {Studies in Logic by Members of the
		Johns Hopkins University},
  publisher = Little,
  pages = {107--125},
  year = 1883,
  bibliographies = {RelMiCS}
}

@Article{Gilman1892,
  author = {Benjamin Ives Gilman},
  title = {On the Properties of a one-dimensional Manifold},
  journal = MIND,
  volume = {1, New Series},
  year = 1892,
  pages = {518--526},
  bibliographies = {RelMiCS}
}

@Book{Gimbel-1993,
  year = 1993,
  title = {Quo vadis, graph theory? A source book for
		  challenges and directions},
  series = Annals,
  publisher = NoHo,
  number = 55,
  author = {John Gimbel},
  annote = {tubibmue},
  address = {Amsterdam},
  bibliographies = {RelMiCS}
}

@Book{Ginzburg-Moss-deRijke-1999,
  editor = {Jonathan Ginzburg and Lawrence Moss and de Rijke, Maarten},
  title = {Logic, Language and Computation},
  year = 1999,
  publisher = {Cambridge University Press},
  ISBN = 1575861801,
  URL = {http://www.cup.cam.ac.uk/scripts/webbook.asp?isbn=1575861801},
  bibliographies = {RelMiCS}
}

@Book{GinzburgVL-2001,
  author = {Ginzburg, V.L., Russian Academy of Science, Moscow, Russia},
  title = {The Physics of a Lifetime --- Reflections on the Problems and
      Personalities of 20th Century Physics},
  publisher = Springer,
  year = 2001,
  URL = {http://www.springer.de/cgi-bin/search_book.pl?isbn=3-540-67534-5},
  pages = {XIII, 513 pp.},
  ISBN = {3-540-67534-5},
  abstract = {Every reader interested in understanding the important
      problems in physics and astrophysics and their historic development
      over the past 60 years will enjoy this book immensely. The
      philosophy, history and the individual views of famous scientists of
      the 20th century known personally to the author, make this book
      fascinating for non-physicists, too. The book consists of three parts
      on (I) major problems of physics and astrophysics, (II) the
      philosophy and history of science and (III) memorial essays on famous
      physicists. The author is an internationally renowned scientist, who
      summarises here his life-long experience.},
  keywords = {Astrophysics, Solid-State Physics, High-Temperature Superconductivity, SocialAspects of Science Development, History of Science}
}

@InProceedings{Giorgino-Strecker-2012,
  author    = {Giorgino, Mathieu and Strecker, Martin},
  title     = {Correctness of Pointer Manipulating Algorithms Illustrated by a Verified
               {BDD} Construction},
  year      = {2012},
  editor    = {Dimitra Giannakopoulou and Dominique M{\'e}ry},
  booktitle = {{FM 2012: Formal Methods --- 18th International Symposium,
               Paris, France, August 27-31, 2012. Proceedings}},
  pages     = {202--216},
  publisher = Springer,
  series    = LNCS,
  volume    = {7436},
  language  = {english},
  WKloc = {doc/pap/BIB},
  keywords = {Isabelle}
}

@InProceedings{Giovini-Niesi-1990,
  author = {A. Giovini and G. Niesi},
  title = {{CoCoA}: A User-Friendly System for Commutative Algebra},
  crossref = {DISCO90},
  pages = {20--29},
  annote = {Macintosh only}
}

@Article{Girard-1986,
  author = {Jean-Yves Girard},
  title = {The System {\em {F}} of Variable Types, Fifteen Years Later},
  WKloc = {A-0048},
  year = 1986,
  volume = 45,
  pages = {159--192},
  journal = TCS
}

@Article{Girard-1987,
  author = 	 {Jean-Yves Girard},
  title = 	 {Linear Logic},
  journal = 	 TCS,
  year = 	 1987,
  volume =	 50,
  pages =	 {1--101},
  annote =	 {includes relational model},
  bibliographies = {RelMiCS}
}

@Book{Girard-1989,
  author = {Jean-Yves Girard},
  title = 	 {Proofs and Types},
  publisher = 	 CambridgeUP,
  year = 	 1989,
  volume =	 7,
  series =	 Cambridge,
  McMaster = 	 {QA 9.54 .G57}
}

@InCollection{Girard-1989a,
  author = {Jean-Yves Girard},
  title = {Towards a Geometry of Interaction},
  booktitle = {Categories in Computer Science and Logic},
  publisher = AMS,
  year = 1989,
  editor = {J. W. Gray and A. Scedrov},
  volume = 92,
  series = {Contemporary Mathematics},
  pages = {69--108},
  bibliographies = {RelMiCS}
}

@InProceedings{Girard-1995,
  author = {Jean-Yves Girard},
  title = {Linear Logic: Its Syntax and Semantics},
  booktitle = {Advances in Linear Logic},
  year = 1995,
  editor = {J.-Y. Girard and Y. Lafont and L. Regnier},
  series = {Workshop on Linear Logic, 1993},
  pages = {1--42},
  publisher = CambridgeUP,
  OPTnote = {Available by ftp from {\tt lmd.univ-mrs.fr}, directory {\tt pub/girard},
 		  file {\tt Synsem.dvi.Z} or {\tt Synsem.ps.Z}},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@InProceedings{Girard-Lafont-1987,
  author = {Jean-Yves Girard and Yves Lafont},
  title = {Linear Logic and Lazy Computation},
  pages = {52--66},
  crossref = {TAPSOFT1987},
  WKloc = {A-0037},
  abstract = {Recently, J.Y.~Girard discoverd that usual logic connectors
	such as $\zimp$ (implication) could be broken up into more
	elementary {\em linear} connectors. This provided a new {\em linear}
	logic [Girard86] where hypothesis are (in some sense) used {\em once}
	and {\em only once}. The most surprising is that all the power of
	the usual logic can be recovered by means of recursive logical
	operators (connector ``of course'').

	There are two versions of the {\em linear logic}: the
	{\em intuitionistic} one and the {\em classical} one. It seems that
	the second provides a appropriate formalism for {\em parallelism}
	and {\em communication}. This approach is entirely new and requires
	further development. Here we restrict out attention to the
	{\em intuitionistic} version and to the consequences os the
	{\em linear} constraint to the computation process.

	\medskip
	We give two equivalent presentations of the (propositional part of)
	{\em linear} logic: a sequent calculus and a (categorical)
	combinator system.

	Then we introduce {\em inductive} and {\em projective} connectors,
	in particular the connector ! (read ``of course''). It plays a
	fundamental role in the encoding of usual intuitionistic logic into
	{\em linear} logic.

	There is a {\em cut elimination theorem} for the sequent calculus
	that corresponds to an {\em evaluation mechanism} for the combinator
	system. We present a very simple (abstract) machine that performs
	{\em linear} computations with the following features:
	\begin{itemize}
	\item A {\em very natural} lazy evaluation mechanism.
	\item No need of {\em garbage collector}.
	\end{itemize}
	Finally, we discuss the relevance of {\em linear} logic to implement
	functional languages.},
  bibliographies = {RelMiCS}
}

@InProceedings{Girard-Scedrov-Scott-1992,
  author = {Jean-Yves Girard and Andre Scedrov and Philip J. Scott},
  title = {Normal Forms and Cut-Free Proofs as Natural Transformations},
  booktitle = {Logic from Computer Science},
  series = {MSRI Publications},
  volume = 21,
  editor = {Y. Moschovakis},
  year = 1992,
  publisher = {Springer-Verlag},
  pages = {217--241},
  OPTabstract = {},
  WKloc = {A-0913},
  note = {Dedicated to the Memory of E.~S.~Bainbridge}
}

@Book{Girard-Taylor-Lafont-1989,
  title = {Proofs and Types},
  author = {Girard, Jean-Yves and Taylor, Paul and Lafont, Yves},
  year = 1989,
  series = Cambridge,
  volume = 7,
  pages = {xi+176},
  publisher = CambridgeUP,
  ISBN = {0-521-37181-3},
  UniBwM = {MAT010/T1303},
  WKloc = {Q-009},
  bibliographies = {RelMiCS}
}

@Article{Girard1987,
  author = {Jean-Yves Girard},
  title = {Linear Logic},
  journal = TCS,
  year = 1987,
  volume = 50,
  pages = {1--102},
  bibliographies = {RelMiCS}
}

@Book{Girault-Valk-2003,
  author =    {Claude Girault and R{\"u}diger Valk},
  title =        {{Petri} Nets for Systems Engineering: A Guide to Modeling, Verification, and Applications},
  publisher =    {\unfinished},
  year =         {2003},
  McMaster =     {QA 76.9 .F67 G57 2003},
  McMasterID =   {39005024119012},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Girbal-Vasilache-Bastoul-Cohen-Parello-Sigler-Temam-2006,
  author = {Sylvain Girbal and Nicolas Vasilache and C{\'e}dric Bastoul and Albert Cohen and David Parello and Marc Sigler and Olivier Temam},
  title = 	 {Semi-Automatic Composition of Loop Transformations for Deep Parallelism and Memory Hierarchies},
  journal = 	 {International Journal of Parallel Programming},
  year = 	 2006,
  volume =	 34,
  number =	 3,
  pages =	 {261--317},
  month =	 JUN,
  URL = {http://www.springerlink.com/content/v7g3q18122591365/},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {Coconut},
  keywords = 	 {Compiler optimization, semi-automatic program transformation,
       polyhedral model, automatic parallelization},
  abstract = {Modern compilers are responsible for translating
       the idealistic operational semantics of the source program into
       a form that makes efficient use of a highly complex
       heterogeneous machine. Since optimization problems are
       associated with huge and unstructured search spaces, this
       combinational task is poorly achieved in general, resulting in
       weak scalability and disappointing sustained performance. We
       address this challenge by working on the program representation
       itself, using a semi-automatic optimization approach to
       demonstrate that current compilers offen suffer from
       unnecessary constraints and intricacies that can be avoided in
       a semantically richer transformation framework. Technically,
       the purpose of this paper is threefold: (1) to show that
       syntactic code representations close to the operational
       semantics lead to rigid phase ordering and cumbersome
       expression of architecture-aware loop transformations, (2) to
       illustrate how complex transformation sequences may be needed
       to achieve significant performance benefits, (3) to facilitate
       the automatic search for program transformation sequences,
       improving on classical polyhedral representations to better
       support operation research strategies in a simpler, structured
       search space. The proposed framework relies on a unified
       polyhedral representation of loops and statements, using
       normalization rules to allow flexible and expressive
       transformation sequencing. Thisrepresentation allows to extend
       the scalability of polyhedral dependence analysis, and to delay
       the (automatic) legality checks until the end of a
       transformation sequence. Our work leverages on algorithmic
       advances in polyhedral code generation and has been implemented
       in a modern research compiler.}
}

@Article{Gischer-1988,
  author = {J.L. Gischer},
  title = {The equational theory of pomsets},
  journal = TCS,
  year = 1988,
  volume = 61,
  pages = {199--224}
}

@InProceedings{Giunchilia-Cimatti-1994,
  author = {Fausto Giunchilia and Alessandro Cimatti},
  title = {Introspective Metatheoretic Reasoning},
  crossref = {LOPSTR-META-94},
  pages = {425--439},
  OPTabstract = {},
  WKloc = {A-0713}
}

@Booklet{Givant-1970,
  author = {Steven Givant},
  title = {Group Relation Algebras},
  note = {Report for a seminar on relation algebras conducted by
		A.\null{} Tarski, mimeographed, Spring, 1970},
  address = {Berkeley},
  bibliographies = {RelMiCS}
}

@InProceedings{Givant-1991,
  author = {Steven Givant},
  title = {Tarski's Development of Logic and Mathematics based
		  on the Calculus of Relations},
  crossref = {Andreka-Monk-Nemeti-1991},
  pages = {189--216},
  WKloc = {A-0127},
  bibliographies = {RelMiCS}
}

@Book{Givant-1994,
  author = {Steven Roger Givant},
  title = {The Structure of Relation Algebras Generated by Relativizations},
  publisher = AMS,
  series = {Contemporary Mathematics},
  address = {Providence},
  volume = 156,
  year = 1994,
  pages = {xvi+134},
  bibliographies = {RelMiCS}
}

@Article{Givant-1999,
  author = {Steven Roger Givant},
  title = {Unifying Threads in {Alfred Tarski's} Work},
  journal = {The Mathematical Intelligencer},
  year = 1999,
  volume = 21,
  number = 1,
  pages = {47-58},
  bibliographies = {RelMiCS}
}

@Article{Givant-2006,
  author =       {Steven Givant},
  title =        {The Calculus of Relations as a Foundation for Mathematics},
  journal =      JAR,
  year =         2006,
  DOI =       {10.1007/s10817-006-9062-x},
  volume =    37,
  number =    4,
  pages =     {277--322},
  URL = {http://www.springerlink.com/content/m3967658508477m2/},
  bibliographies =     {RelMiCS},
  keywords =      {calculus of relations,
     authomated reasoning,
     algebraic logic,
     set theory,
     mathematical foundation},
  abstract =    {A variable-free, equational logic $\mathcal{L}^\times$
    based on the calculus of relations
    (a theory of binary relations developed by De Morgan, Peirce, and Schröder
     during the period 1864--1895)
    is shown to provide an adequate framework
    for the development of all of mathematics.
    The expressive and deductive powers of $\mathcal{L}^\times$ are equivalent
    to those of a system of first-order logic with just three variables.
    Therefore, three-variable first-order logic also provides
    an adequate framework for mathematics.
    Finally, it is shown that a variant of $\mathcal{L}^\times$
    may be viewed as a subsystem of sentential logic.
    Hence, there are subsystems of sentential logic
    that are adequate to the task of formalizing mathematics.

   This paper is an expanded version of a talk given by the author
   at the Special Session on Automated Reasoning in Mathematics and Logic,
   held March 8--10, 2002, at the Georgia Institute of Technology,
   during the Joint Southeastern Section MAA/Southeast Regional AMS Meeting.
   The session was organized by Johan G. F. Belinfante.}
}


@Article{Giveon1952,
  author = {Y. Give'on},
  title = {Lattice Matrices},
  journal = INFCON,
  volume = 7,
  year = 1952,
  pages = {477--484},
  bibliographies = {RelMiCS}
}

@InProceedings{Glanfield-Winter-2006,
  author = 	 {Joel Glanfield and Michael Winter},
  title = 	 {{RelAPS}: A Proof System for Relational Categories},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {RelMiCS 9 Doctoral Symposium},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1670, doc/pap/BIB},
  OPTannote = 	 {}
}

@InProceedings{Glauert-Kennaway-Sleep-1990,
  author = {J. R. W. Glauert and J. R. Kennaway and M. R. Sleep},
  title = {Dactl: An Experimental Graph Rewriting Language},
  pages = {378--395},
  crossref = {GG1990},
  keywords = {Dactl, graph rewriting, computational model,
		  parallel architecture, reduction machine, compiler
		  target language, declaratice language, categorical semantics},
  abstract = {Dactl is an experimental language programming
		  language based on fine grain graph
		  transformations. It was developed in the context of
		  a large parallel reduction machine project. The
		  design of the language is outlined, and examples
		  given of its use both as a compiler target language
		  and as a programming language. Dactl has a formal
		  semantics and stable implementations on a number of
		  platforms.}
}

@Book{Gloger-1993,
  UniBwM = {MAG X8310},
  WKloc = {HB},
  PRICE = {DM 74.-},
  keywords = {SAMPLE, SAMP$\lambda$E},
  ISBN = {3-8244-2037-6},
  year = 1993,
  title = {{Implementierung funktionaler Programmiersprachen:
      Codegenerierung, Speicherverwaltung und Testsysteme f\"ur Sprachen
      mit verz\"ogerter Auswertung}},
  publisher = {Dt. Univ.-Verl.},
  author = {Michael Gloger},
  annote = {zugl. TH Darmstadt, Diss.},
  address = {Wiesbaden}
}

@InProceedings{Gluck-Jorgensen-1994,
  author = {Robert Gluck and Jesper Jorgensen},
  title = {Generating Transformers for Deforestation and
		  Supercompilation},
  crossref = {SAS94},
  authorsAddress = {University of Copenhagen}
}

@TechReport{Glynn-Sulzmann-Stuckey-2000,
  author = {K. Glynn and M. Sulzmann and P.J. Stuckey},
  year = 2000,
  title = {Type Classes and Constraint Handling Rules},
  institution = {Department of Computer Science and Software Engineering, University of Melbourne},
  numberofpages = 14,
  subcat = {G4},
  authorcode = {433,46,00,FU,MD,M and 433,13,00,FU,MD,M and 433,13,00,FU,MD,M},
  URL = {http://www.cs.mu.oz.au/tr_submit/test/cover_db/mu_TR_2000_7.html},
  WKloc = {A-1106, doc/pap/BIB}
}

@InProceedings{Gnaedig-1992,
  author = {Isabelle Gnaedig},
  title = {{ELIOS-OBJ} Theorem Proving in a Specification
		  Language},
  authorsadress = {gnaedig\@loria.fr},
  crossref = {ESOP1992},
  pages = {182--199},
  authorsAddress = {INRIA Lorraine - CRIN, gnaedig\@loria.fr},
  abstract = {In the context of the executable specification
		  language {\em OBJ3}, an order-sorted completion
		  procedure is implemented, providing automatically
		  convergent specifications from user-given ones. This
		  feature is of first importance to insure unambiguity
		  and termination of the rewriting execution process.
		  We describe here how we specified a modular
		  completion design in terms of inference rules and
		  control language, using {\em OBJ3} itself. On
		  another hand, the specific problems encountered to
		  integrate a completion process in an already
		  reduction-oriented environment are pointed out.},
  bibliographies = {RelMiCS}
}

@Misc{Go-2014,
  key = 	 {Go 2014},
  OPTauthor = 	 {},
  title = 	 {The {Go} Programming Language Specification, Version of November 11, 2014},
  howpublished = {\url{http://www.golang.org}},
  OPTmonth = 	 {},
  year = 	 {2014},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Godsil-Royle-2001,
  author = {Chris Godsil and Gordon Royle},
  title = {Algebraic Graph Theory},
  publisher = Springer,
  year = 2001,
  volume = 207,
  series = {Graduate Texts in Mathematics},
  UniBwM = {MAT880/YG3567},
  ISBN = {0-387-95241 hc.; 0-387-95220-9 pbk.},
  URL = {http://quoll.uwaterloo.ca/agt/}
}

@InProceedings{Goettler-1983,
  author = {H. G{\"o}ttler},
  title = {Attribute Graph Grammars for Graphics},
  crossref = {GG1982}
}

@TechReport{Gogolla-1998,
  author = {Martin Gogolla},
  title = {{UML} for the Impatient},
  year = 1998,
  institution = {Universit\"at Bremen},
  number = {3/98},
  WKloc = {A-0736}
}

@Article{Goguen-1967,
  author = {J.A. Goguen},
  title = {L-Fuzzy Sets},
  pages = {145--174},
  year = 1967,
  OPTnumber = {},
  volume = 18,
  journal = {J. Math. Anal. Appl.}
}

@PhDThesis{Goguen-1968,
  author = {Joseph Amadee Goguen},
  title = {Categories of Fuzzy Sets: Applications of Non-{Cantorian} Set Theory},
  school = {University of California, Berkeley},
  year = 1968,
  WKloc = {A-1394}
}

@Article{Goguen-1974,
  author = {Joseph A. Goguen},
  title = {Concept Representation in Natural and Artificial Languages: Axioms, Extensions and Applications for Fuzzy Sets},
  journal = {Int.~J.~Man-Machine Studies},
  year = 1974,
  volume = 6,
  pages = {513--561},
  WKloc = {A-1393},
  abstract = {This paper reports research related to mathematics,
      philosophy, computer science and linguistics. It gives a system of
      axioms for a relatively simple form of fuzzy set theory, and uses
      these axioms to consider the accuracy of representing concepts in
      various ways by fuzzy sets. By-products of this approach include a
      number of new operations an laws for fuzzy sets., parallel to thos
      for ordinary sets, and a demonstration hat all the basic operations
      are intrinsically determined. In addition, the paper explores both
      hierarchical and algorithmic extensions of fuzzy sets, and then
      applications to problems in natural language semantics and
      combinatorics. Finally, the paper returns to the problem of
      representing concepts, and discusses some implications for artificial
      intelligence.}
}

@InCollection{Goguen-1989,
  author = {Joseph A. Goguen},
  title = 	 {What is Unification?},
  crossref =  {AitKaci-Nivat-1989a},
  pages = 	 {217--261},
  annote = {Substitutions as morphisms ``the wrong way around'':
            $\sigma : \CalX \tfun \Caly$
            substitutes terms over $\CalX$
           into variable over $\CalY$.}
}

@InProceedings{Goguen-1990,
  author = {Joseph A. Goguen},
  title = {Proving and Rewriting},
  pages = {1--24},
  abstract = {This paper presents some ways to prove theorems in first and
             second order logic, such that rewriting does the routine work
             automatically, and pratically successful proofs often return
             information that suggestes what to try next. The theoretical
             framework makes extensive use of general algebra, and main
             results include an extension of many-sorted equational logic to
             universal quantification over functions, some techniques for
             handling first order logic, and some structural induction
             principles. The OBJ language is used for illustration, and
             initiality is recurrent theme.},
  crossref = {ALP1990}
}

@InCollection{Goguen-1999,
  author = {Joseph Goguen},
  title = {Tossing Algebraic Flowers down to the Great Divide},
  booktitle = {People \& Ideas in Theoretical Computer Science},
  publisher = Springer,
  year = 1999,
  WKloc = {A-1074, doc/pap/BIB}
}

@Misc{Goguen-199X,
  author = {Joseph Goguen},
  title = {Social and Semiotic Analyses for Theorem Prover User interface Design},
  OPThowpublished = {},
  OPTmonth = {},
  year = {199?},
  OPTnote = {},
  WKloc = {A-1101}
}

@Article{Goguen-Burstall-1992,
  author = {J. A. Goguen and R. M. Burstall},
  title = {Institutions: Abstract Model Theory for Specification and Programming},
  journal = JACM,
  year = 1992,
  volume = 39,
  number = 1,
  pages = {95--146},
  URL = {http://www.acm.org/pubs/citations/journals/jacm/1992-39-1/p95-goguen/},
  WKloc = {A-1047, doc/pap/BIB},
  abstract = {There is a population explosion among the logical systems used in
       computing science. Examples include first-order logic, equational logic,
       Horn-clause logic, higher-order logic, infinitary logic, dynamic logic,
       intuitionistic logic, order-sorted logic, and temporal logic; moreover, there is
       a tendency for each theorem prover to have its own idiosyncratic logical
       system. The concept of institution is introduced to formalize the informal
       notion of ``logical system''. The major requirement is that there is a
       satisfaction relation between models and sentences that is consistent under
       change of notation. Institutions enable abstracting away from syntactic and
       semantic detail when working on language structure ``in-the-large''; for
       example, we can define language features for building large logical system.
       This applies to both specification languages and programming languages.
       Institutions also have applications to such areas as database theory and the
       semantics of artificial and natural languages. A first main result of this paper
       says that any institution such that signatures (which define notation) can be
       glued together, also allows gluing together theories (which are just
       collections of sentences over a fixed signature). A second main result
       considers when theory structuring is preserved by institution morphisms. A
       third main result gives conditions under which it is sound to use a theorem
       prover for one institution on theories from another. A fourth main result
       shows how to extend institutions so that their theories may include, in
       addition to the original sentences, various kinds of constraint that are useful
       for defining abstract data types, including both ``data'' and ``hierarchy''
       constraints. Further results show how to define institutions that allow
       sentences and constraints from two or more institutions. All our general
       results apply to such ``duplex'' and ``multiplex'' institutions.}
}

@InProceedings{Goguen-Dianescu-1992,
  author = {J.A. Goguen and R. Dianescu},
  title = {Towards an Algebraic Semantics for the Object Paradigm},
  crossref = {SADT92},
  pages = {1--29},
  note = {invited paper},
  keywords = {OBJ3}
}

@InProceedings{Goguen-Dianescu-1995,
  author = {J.A. Goguen and R. Dianescu},
  title = {An Introduction to Category-Based Equational Logic},
  crossref = {AMAST1995},
  pages = {91--126},
  WKloc = {A-0620},
  keywords = {CBEL, constraint logic programming}
}

@InCollection{Goguen-Malcolm-1994,
  author = {Joseph A. Goguen and Grant Malcolm},
  title = {Proof of Correctness of Object Representations},
  crossref = {Roscoe-1994},
  pages = {119--142},
  chapter = 8,
  OPTnote = {},
  OPTannote = {}
}

@Book{Goguen-Malcolm-1996,
  author =	 {Joseph A. Goguen and Grant Malcolm},
  title = 	 {Algebraic Semantics of Imperative Programs},
  publisher = 	 {MIT Press},
  year = 	 1996,
  series =	 {Foundations of Computing},
  McMaster = 	 {QA 76.7 .G62 1996},
  bibliographies = {SE3E}
}

@Article{Goguen-Malcolm-1999,
  author = 	 {Joseph A. Goguen and Grant Malcolm},
  title = 	 {Hidden coinduction: behavioural correctness proofs for objects},
  journal = 	 MSCS,
  year = 	 1999,
  volume = 	 9,
  pages = 	 {287-319 },
  DOI = 	 {10.1017/S0960129599002777},
  URL = 	 {http://journals.cambridge.org/action/displayAbstract?aid=44821},
  WKloc = {doc/pap/BIB},
  abstract = 	 {This paper unveils and motivates an ambitious
                  programme of hidden algebraic research in software
                  engineering. We begin with an outline of our general
                  goals, continue with an overview of results, and
                  conclude with a discussion of some future plans. The
                  main contribution is powerful hidden coinduction
                  techniques for proving behavioural correctness of
                  concurrent systems, and several mechanical proofs
                  are given using OBJ3. We also show how
                  modularization, bisimulation, transition systems,
                  concurrency and combinations of the functional,
                  constraint, logic and object paradigms fit into
                  hidden algebra.}
}

@Article{Goguen-Malcolm-2000,
 author = {Goguen, Joseph and Malcolm, Grant},
 title = {A hidden agenda},
 journal = TCS,
 volume = {245},
 number = {1},
 year = {2000},
 issn = {0304-3975},
 pages = {55--101},
 doi = {http://dx.doi.org/10.1016/S0304-3975(99)00275-3},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
}

@InCollection{Goguen-Meseguer-1987,
  year={1987},
  isbn={978-3-540-17611-4},
  booktitle = {TAPSOFT '87},
  volume={250},
  series = LNCS,
  editor={Ehrig, Hartmut and Kowalski, Robert and Levi, Giorgio and Montanari, Ugo},
  DOI={10.1007/BFb0014969},
  title={Models and equality for logical programming},
  DOIURL={http://dx.doi.org/10.1007/BFb0014969},
  publisher = Springer,
  author={Goguen, Joseph A. and Meseguer, Jos{\'e}},
  pages={1--22},
  WKloc = {doc/pap/BIB},
  abstract = {We argue that some standard tools from model theory
    provide a better semantic foundation
    than the more syntactic and operational approaches
    usually used in logic programming.
    In particular, we show how initial models capture the intended semantics
    of both functional and logic programming, as well as their combination,
    with existential queries having logical variables
    (for both functions and relations)
    in the presence of arbitrary user-defined abstract data types,
    and with the full power of constraint languages,
    having any desired built-in (computable) relations and functions,
    including disequality (the negation of the equality relation)
    as well as the usual ordering relations on the usual built-in types,
    such as numbers and strings.
    These results are based on a new completeness theorem
    for order-sorted Horn clause logic with equality,
    plus the use of standard interpretations
    for fixed sorts, functions and relations.
    Finally, we define ``logical programming'',
    based on the concept of institution,
    and show how it yields a general framework for discussions of this kind.
    For example, this viewpoint suggests that the natural way
    to combine functional and logic programming is simply to combine their logics,
    getting Horn clause logic with equality.}
}

@InProceedings{GoguenH-McBride-McKinna-2005,
  author = 	 {Healfdene Goguen and Conor McBride and James McKinna},
  title = 	 {Eliminating Dependent Pattern Matching},
  crossref =  {GoguenFestschrift2005},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1656, doc/pap/BIB},
  bibliographies = {PMC},
  abstract = {This paper gives a reduction-preserving translation
      from Coquand's dependent pattern matching \cite{Coquand-1992}
      into a traditional type theory \cite{LuoZhaohui-1994} with
      universes, inductive types and relations and the axiom K
      \cite{Streicher-1993}. This translation serves as a proof of
      termination for structurally recursive pattern matching
      programs, provides an implementable compilation technique in the
      style of functional programming languages, and demonstrates the
      equivalence with a more easily understood type theory.}
}

@Book{Golas-2011,
  author =    {Golas, Ulrike},
  title =        {Analysis and Correctness of Algebraic Graph and Model Transformations},
  publisher =    {Springer Vieweg},
  year =         {2011},
  SpringerURL =  {\url{http://link.springer.com/book/10.1007/978-3-8348-9934-7}},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTedition =   {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@InCollection{Golas-2012,
  author={Golas, Ulrike},
  title={A General Attribution Concept for Models in {$\mathcal{M}$}-Adhesive Transformation Systems},
  crossref={ICGT2012},
  pages={187--202},
  DOI={10.1007/978-3-642-33654-6_13},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-33654-6_13},
  abstract = {Attributes are an important concept
    for modeling data in practical applications.
    Up to now there is no adequate way to define attributes for different
    kinds of models used in $\CalM$-adhesive transformation systems,
    which are a special kind of graph transformation system
    based on $\CalM$-adhesive categories.
    Especially a proper representation and definition of attributes
    and their values as well as a suitable handling of the data
    does not fit well with other graph transformation formalisms.

    In this paper, we propose a new method to define attributes in a natural,
    but still formally precise and widely applicable way.
    We define a new kind of adhesive category, called $\CalW$-adhesive,
    that can be used for transformations of attributes,
    while the underlying models are still $\CalM$-adhesive ones.
    As a result, attributed models can be used as they are intended to be,
    but with a formal background and proven well-behavior.}
}

@Book{Goldblatt-1984,
  title = {Topoi, The Categorical Analysis of Logic},
  author = {Goldblatt, Robert},
  year = 1984,
  series = {Studies in Logic and the Foundations of Mathematics},
  edition = {Revised},
  volume = 98,
  pages = {xvi+551},
  publisher = {North-Holland},
  address = {Amsterdam},
  annote = {The first edition was 1979},
  ISBN = {0-444-86711-2},
  WKloc = {Q-008},
  AMAZON = {2008-05-11 \$21.74 http://www.amazon.ca/dp/0486450260/ref=pe_ar_x6},
  keywords = {topos}
}

@Book{Goldblatt-1987,
  author = {Goldblatt, R.},
  address = {Stanford},
  publisher = CSLI_P,
  title = {Logics of Time and Computation},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Book{Goldblatt-1993,
  author = {Robert Goldblatt},
  title = {Mathematics of Modality},
  publisher = {CSLI Publications, Stanford University},
  year = 1993,
  ISBN = {$22.95 (paper) 1-881526-23-2, $45.95 (cloth) 1-881526-24-0},
  volume = 43,
  series = {CSLI Lecture Notes},
  pages = 273,
  abstract = {This volume collects together a number of the
		  author's papers on modal logic, beginning with his
		  doctoral thesis about the duality between algebraic
		  and set-theoretic models, and including two
		  completely new articles, one on infinitary rules of
		  inference, and the other about recent results on the
		  relationship between modal logic and first-order
		  logic. Another paper on the "Henkin method" in
		  completeness proofs has been substantially extended
		  to give new applications. Other articles are
		  concerned with quantum logic, provability logic, the
		  temporal logic of relativistic spacetime, modalities
		  in topos theory, and the logic of programs.}
}

@Article{Goldfarb-1981,
  abstract = {It is shown that there is no effective procedure for
		  determining whether or not two terms of the language
		  of second-order logic have a common instance.},
  year = 1981,
  volume = 13,
  title = {The Undecidability of the Second-Order Unification Problem},
  pages = {225--230},
  journal = {Theoretical Computer Science},
  author = {Warren D. Goldfarb}
}

@Book{Goldfarb-1990,
  author = {Goldfarb, Charles F.},
  title = {The SGML handbook},
  year = 1990,
  publisher = {Oxford University Press},
  pages = {xxiv, 663 p. ; 24 cm},
  keywords = {Electronic publishing},
  contents = {Includes bibliographical references and index},
  ISBN = {0-19-853737-9},
  editor = {Yuri Rubinsky}
}

@Article{Goldfarb1979,
  author = {Warren D. Goldfarb},
  title = {Logic in the Twenties: the Nature of the Quantifier},
  journal = JSYLO,
  volume = 44,
  year = 1979,
  pages = {351--368},
  OPTnote = {MR 82f:03062.},
  bibliographies = {RelMiCS}
}

@Article{Goldson-Reeves-Bornat-1993,
  author =       {Doug Goldson and Steve Reeves and Richard Bornat},
  title =        {A Review of Several Programs for the Teaching of Logic},
  journal =      {The Computer Journal},
  year =         1993,
  volume =    36,
  pages =     {373--386}
}

@Article{Gomard-1990,
  author = {Carsten K. Gomard},
  email = {gomard@diku.dk},
  title = {Partial type inference for untyped functional programs
                 (extended abstract).},
  journal = {1990 {ACM} Conf. on {LISP} and Functional Programming},
  pages = {282--287},
  year = 1990,
  keywords = {FP, functional programming, type, types, error,
                 errors, message, LAFP, LFP,
                 type inference, untyped lambda calculus,
                 binding time analysis, partial evaluation},
  abstract = {This extended abstract decribes a way of inferring as
                 much type information as possible about programs
                 written in an untyped programming language. We present
                 an algorithm that underlines the untypable parts of a
                 program and assigns types to the rest. The algorithm is
                 derived in a very simple manner from the well- known
                 algorithm W of Damas and Milner~\cite{Damas-Milner-1982}.
                 Our algorithm provides
                 us with an easy solution to the problem of doing
                 {\em binding time analysis} of the untyped higher order
                 lambda calculus, and thereby of the wide range of
                 programming languages based upon the lambda calculus.
                 The techniques can also be used to eliminate
                 superfluous run-time type checking in untyped
                 functional languages, to produce better error messages
                 from the type analyzers for strongly typed languages,
                 and to analyze feasibility of arity raising.},
  reffrom = {Jensen:fplca:1991},
  reffrom = {Wang:Hilfinger:acm:lfp:1992},
  reffrom = {Henglein:acm:lfp:1992},
  reffrom = {Wright:Cartwright:acm:lfp:1994}
}

@Article{Gomard-1991,
  author = {Carsten K. Gomard},
  title = {A Self-Applicable Partial Evaluator for the Lambda
		  Calculus: Correctness and Pragmatics},
  year = 1992,
  volume = 14,
  pages = {147--172},
  number = 2,
  month = APR,
  journal = ACM-TOPLAS,
  email = {gomard@diku.dk},
  WKloc = {A-0017},
  ACMCAT = {D31 (Programming Languages): Formal Definitions and
		  Theories;
                  D34 (Programming Languages): Processors;
                  F32 (Logics and Meanings of Programs): Semantics of
		  Programming Languages
                  F41 (Mathematical Logic and Formal Languages):
		  Mathematical Logic},
  abstract = {We describe theoretical and a few practical aspects
		  of an implemented self-applicable partial evaluator
		  for the untyped lambda calculus with constants,
		  conditionals, and a fixed point operator.

                  The purpose of this paper is first to announce the
		  existence of (and to describe) a partial evaluator
		  that is both higher-order and self-applicable;
		  second to describe a surprisingly simple solution to
		  the central problem of binding time analysis, and
		  third to prove that the partial evaluator yields
		  correct answers.

		  While $\lambda$-mix (the name of our system) seems
		  to have been the first higher-order self-applicable
		  partial evaluator to run on a computer, it was
		  developed mainly for research purposes. Two recently
		  developed systems are much more powerful for
		  practical use, but also much more complex: Similix
		  [3,5] and Schism [7].

		  Our partial evaluator is surprisingly simple,
		  completely automatic, and has been implemented in a
		  side effect-free subset of Scheme. It has been used
		  to compile, generate compilers and generate a
		  compiler generator.}
}

@TechReport{Gondow-Katayama-1995,
  author = {Katsuhiko Gondow and Takuya Katayama},
  institution = {Tokyo Institute of Technology},
  title = {On formalization of object oriented attribute grammars
                 {OOAG} and higher order attribute grammars using record
                 calculus},
  year = 1995,
  number = {95TR-0020},
  URL = {http://www.cs.titech.ac.jp/TR/tr-abst95.html#95TR-0020},
  abstract = {The purpose of this paper is twofold. First we present
                 a denotational semantics of attribute grammars(AGs) by
                 using Cardelli's record calculus. The denotational
                 semantics is simple and natural. In our semantics, an
                 attributed tree is represented by nested records to
                 preserve the structural information of the attributed
                 tree, while in traditional denotational semantics AGs
                 are formalized by either valuation mapping from
                 attributes (often in the root) to their values or
                 mapping from inherited attributes to synthesized
                 attributes in the root. It is a positive characteristic
                 of our semantics to deal with attributed tree as AG's
                 semantics rather than attribute valuation. For the
                 purpose of describing structure-oriented software
                 development environments, many computational models
                 based on AGs have already been proposed. These
                 computational models are usually extended so as to deal
                 with tree transformation. We believe that our semantics
                 can be a good formal basis to define these
                 computational models. To show this, we formalize
                 OOAG(Object-Oriented AGs) and higher order AGs(HAGs) by
                 extending our denotational semantics of AGs. This is
                 the second purpose of this paper. Both of them are
                 computational models to deal with tree transformation
                 depending upon attribute values. As the result of these
                 formalizations, we can formally discuss the differences
                 between OOAG and HAGs. For example, we show that tree
                 transformation in OOAG is modeled as a function to
                 determine the next state, while that in HAGs is just a
                 static tree construction. This paper is the revised
                 English version of ``Attribute Grammars as Record
                 Calculus'' (Technical Report No.93TR-0047), which is
                 written in Japanese.}
}

@InProceedings{Gonnet-1994,
  author = {Gaston H. Gonnet},
  title = {Wanna Buy an Algorithm? Cheap! or: Algorithms for
		  Text Searching which could have Commercial Value},
  crossref = {ESA94},
  authorsAddress = {ETH Z\"urich}
}

@InProceedings{Gonthier-Abadi-Levy-1992,
  author = {Georges Gonthier and Mart\'{\i}n Abadi and
		  Jean-Jacques L\'evy},
  title = {The Geometry of Optimal Lambda Reduction},
  abstract = {Lamping discovered an optimal graph-reduction
		  implementation of the $\lambda$-calculus.
		  Simultaneously, Girard invented the geometry of
		  interaction, a mathematical foundation for
		  operational semantics. In this paper, we connect and
		  explain the geometry of interaction and Lamping's
		  graphs. The geometry of interaction provides a
		  suitable semantic basis for explaining and improving
		  Lamping's system. On the other hand, graphs similar
		  to Lamping's provide a concrete representation of
		  the geometry of interaction. Together, they offer a
		  new understanding of computation, as well as ideas
		  for efficient and correct implementations.},
  pages = {15--26},
  WKloc = {A-0109},
  crossref = {POPL1992}
}

@InProceedings{Gonthier-Abadi-Levy-1992-x,
  author = {Georges Gonthier and Mart\'{\i}n Abadi and
		  Jean-Jacques L\'evy},
  title = {The Geometry of Optimal Lambda Reduction},
  abstract = {Lamping discovered an optimal graph-reduction
		  implementation of the $\lambda$-calculus.
		  Simultaneously, Girard invented the geometry of
		  interaction, a mathematical foundation for
		  operational semantics. In this paper, we connect and
		  explain the geometry of interaction and Lamping's
		  graphs. The geometry of interaction provides a
		  suitable semantic basis for explaining and improving
		  Lamping's system. On the other hand, graphs similar
		  to Lamping's provide a concrete representation of
		  the geometry of interaction. Together, they offer a
		  new understanding of computation, as well as ideas
		  for efficient and correct implementations.},
  pages = {15--26},
  WKloc = {A-0109},
  UniBwM = {INF400/Z3061-20},
  year = 1993,
  booktitle = {20th Annual ACM SIGPLAN-SIGACT Symposium on Principles of
		  Programming Languages},
  publisher = {acm press},
  month = JAN,
  address = {Charleston, South Carolina}
}

@Article{GonzalezMoreno-HortalaGonzalez-LopezFraguas-RodriguezArtalejo-1999,
  author = {Gonz{\'{a}}lez-Moreno, J.C. and Hortal{\'{a}}-Gonz{\'{a}}lez, M.T. and
            L{\'{o}}pez-Fraguas, F.J. and Rodr{\'{\i}}guez-Artalejo, M.},
  title = {An Approach to Declarative Programming Based on a Rewriting Logic},
  journal = 	 {The Journal of Logic Programming},
  year = 	 {1999},
  volume = 	 {41},
  number = 	 {1},
  pages = 	 {47--87},
  WKloc = {doc/pap/BIB},
  bibliographies = {PMC},
  DOI = 	 {10.1016/S0743-1066(98)10029-8},
  URL = {http://www.ingentaconnect.com/content/els/07431066/1999/00000040/00000001/art10029},
  keywords = 	 {Declarative programming; Non-deterministic functions;
                  Constructor-based rewriting logic; Lazy narrowing},
  annote = {Curry report: Such non-deterministic functions can be given a perfect declarative semantics \cite{GonzalezMoreno-HortalaGonzalez-LopezFraguas-RodriguezArtalejo-1999}},
  abstract = {We propose an approach to declarative programming
     which integrates the functional and relational paradigms by
     taking possibly non-deterministic lazy functions as the
     fundamental notion. Classical equational logic does not supply a
     suitable semantics in a natural way. Therefore, we suggest to
     view programs as theories in a constructor-based conditional
     rewriting logic. We present proof calculi and a model theory for
     this logic, and we prove the existence of free term models which
     provide an adequate intended semantics for programs. We develop a
     sound and strongly complete lazy narrowing calculus, which is
     able to support sharing without the technical overhead of graph
     rewriting and to identify safe cases for eager variable
     elimination. Moreover, we give some illustrative programming
     examples, and we discuss the implementability of our approach.}
}

@InProceedings{Gonzalia-2000,
  author = {Carlos Gonzal{\'{\i}}a},
  title = {The Allegory of E-Relations in Constructive Type Theory},
  crossref = {RelMiCS2000-M},
  pages = {19--38},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {This paper presents a first step in the formalisation
    of a relational calculus inside a constructive type-theoretic
    framework.  The calculus is the one based on allegories, and its
    formalisation is carried out with the help of a software assistant
    for Martin-L\"of's monomorphic type theory. The goal of this
    effort is allowing us to import relational calculus inside type
    theory, and take advantage of the wealth of results and methods
    for program construction and verification that are well established
    in relational formalisms.}
}

@MastersThesis{Gonzalia-2002,
  author = {Carlos Gonzal{\'{\i}}a},
  title = {Relation Calculus in Martin-L{\"o}f Type Theory},
  school = {Department of Computing Science, Chalmers University of Technology, G\"oteborg University},
  year = 2002,
  type = {Licenciate Thesis},
  WKloc = {C-0018},
  bibliographies = {RelMiCS}
}

@PhDThesis{Gonzalia-2006,
  author = {Carlos Gonzal{\'{\i}}a},
  title = {Relations in Dependent Type Theory},
  type = {{Ph.D.\null{} thesis, also as Technical Report No.~14D}},
  school = {Department of Computer Science and Engineering, Chalmers University of Technology, G\"oteborg University},
  year = 2006,
  PDFURL = {http://www.cs.chalmers.se/~gonzalia/phdthesis.pdf},
  OPTnote = {{Technical Report No.~14D}},
  bibliographies = {RelMiCS}
}

@InProceedings{Gopinath-Hennessy-1989,
  author = {K. Gopinath and John L. Hennessy},
  title = {Copy Elimination in Functional Languages},
  crossref = {POPL1989},
  pages = {303--314},
  WKloc = {A-0210},
  abstract = {Copy elimination is an important optimization for
		  compiling functional languages. Copies arise because
		  these languages lack the concepts of state and
		  variable; hence updating an object involves a copy
		  in a naive implementation. Copies are also possible
		  if proper targeting has not been carried out inside
		  functions and across function calls. Targeting is
		  the proper selection of a storage area for
		  evaluating an expression. By abstracting a
		  collection of functionsby a target operator, we
		  compute the targetsof function bodies that can be
		  used to define an optimized interpreter to eliminate
		  copies due to updates and copies across function
		  calls. The language we consider is typed lambda
		  calculus with higher-order functions and special
		  constructs for array operations. Our apprach can
		  eliminate copies in divide and conquer problems like
		  quicksort and bitonic sort that previous approaches
		  could not handle.

                  We also present some results of implementing a
		  compiler for a single assignment language called SAL
		  on some small but tough programs. Our results
		  indicate that it is possible to appraoch a
		  performance comparable to imerative languages like Pascal.}
}

@InProceedings{Goranko-1987,
  author = {Goranko, V.},
  title = {Completeness and incompleteness
                  in the bimodal base {L($R,-R$)}},
  booktitle = {Proc.\null{} of the Conf.\null{} on Mathematical Logic
                  {``Heyting '88'', Chaika, Bulgaria}},
  year = 1987,
  publisher = Plenum,
  address = {New York},
  bibliographies = {RelMiCS}
}

@Article{Goranko-1990,
  author = {Goranko, V.},
  title = {Modal definability in enriched languages},
  journal = NOTRE,
  year = 1990,
  volume = 31,
  pages = {81--105},
  bibliographies = {RelMiCS}
}

@InProceedings{Gordeev-2000,
  author = {Lew Gordeev},
  title = {Combinatorial Principles in Finite Variable Logic},
  crossref = {RelMiCS2000-M},
  pages = {39--64},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {For any $n > 0$ and a suitable $m > n$ we expose a
    simple three-variable equality-free combinatorial sentence $F_{m,n}$
    of 1-order logic, which is provable in the corresponding
    $(n+3)$-variable equality-free Hilbert-Bernays formalism,
    but not in the corresponding $(n+2)$-variable equality-free
    Hilbert-Bernays formalism. The crucial negative part of the proof
    is based on a suitable nested finite-variable variant of the
    familiar Herbrand Lemma. The canonical translation of $F_{m,n}$
    into the language of relation algebras yields the
    identity-free algebraic equation $T_{m,n} = 1$
    having the same proof theoretical feature.}
}

@InProceedings{Gordon-1993a,
  author = {Andrew Gordon},
  title = {An Operational Semantics for I/O in a Lazy
		  Functional Language},
  crossref = {FPCA-1993},
  WKloc = {A-0378},
  URL = {ftp://ftp.cl.cam.ac.uk/papers/adg/fpca93.ps.gz}
}

@InProceedings{Gordon-1993a-x,
  author = {Andrew Gordon},
  title = {An Operational Semantics for I/O in a Lazy
		  Functional Language},
  URL = {ftp://ftp.cl.cam.ac.uk/papers/adg/fpca93.ps.gz},
  UniBwM = {INF400/Z7648-7},
  year = 1993,
  booktitle = {Functional Programming Languages and Computer Architecture, 7th {ACM} Conference},
  organization = {ACM},
  note = {LNCS ??},
  editor = {??}
}

@InProceedings{Gordon-1993b,
  author = {Andrew D. Gordon},
  title = {A Mechanisation of Name-carrying Syntax up to Alpha-conversion},
  crossref = {HUG93},
  pages = {413--425},
  authorsAddress = {Gothenburg: {\tt gordon\@cs.chalmers.se}},
  WKloc = {A-0319},
  abstract = {We present a new strategy for representing syntax in
		  a mechanised logic. We define an underlying type of
		  de Bruijn terms, define an operation of named
		  lambda-abstraction, and hence inductively define a
		  set of conventional name-carrying terms. The result
		  is a mechanisation of the practice of most authors
		  studying formal calculi: to work with conventional
		  name-carrying notation and substitution, but to
		  identify terms up to alpha-conversion. This strategy
		  falls between most previous works, which either
		  treat bound variable names literally or dispense
		  with them altogether. The theory has been
		  implemented in the Cambridge HOL system and used in
		  an experimental application.}
}

@InCollection{Gordon-1994,
  author = {Mike Gordon},
  title = {A Mechanized {Hoare} Logic of State Transitions},
  crossref = {Roscoe-1994},
  pages = {143--159},
  chapter = 9,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Gordon-1995,
  author = {Andrew D. Gordon},
  title = {A Tutorial on Co-induction and Functional Programming},
  booktitle = {Proceedings of the 1994 {Glasgow Workshop on Functional Programming, September 8--10, 1994, Ayr, Scotland}},
  series = {Workshops in Computing},
  publisher = {Springer Verlag},
  year = 1995,
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0577}
}

@Article{Gordon-Jeffrey-2002,
  author = {Andrew D. Gordon and Alan Jeffrey},
  title = {Typing Correspondence Assertions for Communication Protocols},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  bibliographies = {SQRL},
  note = {to appear},
  abstract = {Woo and Lam propose correspondence assertions for specifying
      authenticity properties of security protocols. Prior work on checking
      correspondence assertions depends on model-checking and is limited to
      finite-state systems. We propose a dependent type and effect system
      for checking correspondence assertions. Since it is based on
      type-checking, our method is not limited to finite-state systems.
      This paper presents our system in the simple and general setting of
      the $\pi$-calculus. We show how to type-check correctness properties
      of example communication protocols based on secure channels. In a
      related paper, we extend our system to the more complex and specific
      setting of checking cryptographic protocols based on encrypted
      messages sent over insecure channels.}
}

@Book{Gordon-Melham-1993,
  editor = {Michael J. C. Gordon and T. F. Melham},
  title = {Introduction to {HOL}: A Theorem Proving Environment
		  for Higher Order Logic},
  publisher = {Cambridge University Press},
  year = 1993
}

@TechReport{Gore-1997,
  author = {Rajeev Gor{\'e}},
  title = {Gaggles, {Gentzen}, and {Galois}: {Cut-free} Display Calculi and Relational Semantics for Algebraizable Logics},
  institution = {Automated Reasoning Project, Australian National University},
  year = 1997,
  number = {TR-ARP-07-97},
  bibliographies = {RelMiCS}
}

@Article{Gore-1998,
  author = {Rajeev Gor{\'e}},
  title = {Substructural Logics on Display},
  journal = LJIGPL,
  year = 1998,
  volume = 6,
  number = 3,
  pages = {451-504},
  WKloc = {A-1073},
  bibliographies = {RelMiCS}
}

@Book{Gosling-Joy-Steele-Bracha-2000,
  author = {James Gosling and Bill Joy and Guy L. Steele and Gilad
                 Bracha},
  title = {The {Java} language specification},
  publisher = {Ad{\-d}i{\-s}on-Wes{\-l}ey},
  address = {Reading, MA, USA},
  edition = {Second},
  pages = {xxv + 505},
  year = 2000,
  ISBN = {0-201-31008-2},
  LCCN = {QA76.73.J38 G68 2000},
  bibdate = {Tue Feb 20 18:39:03 MST 2001},
  series = {Java series},
  URL = {http://java.sun.com/people/jag/},
  keywords = {java (computer program language)},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Goubault-1994,
  title = {Rigid {$\vec{E}$}-Unifiability is {DEXPTIME}-Complete},
  author = {Jean Goubault},
  pages = {498--506},
  crossref = {LICS9},
  abstract = {We prove that rigid $\vec{E}$-unifiability, a decision
		  problem invented by Gallier {\em et al.}~to extend
		  first-order tableaux-like proof procedures to
		  first-order logic with equality, is
		  DEXPTIME-complete; and that, when restricted to
		  monadic terms, it is PSPACE-complete.}
}

@InProceedings{Goubault-1994a,
  author = {Jean Goubault},
  title = {Higher-Order Rigid E-Unification},
  crossref = {LPAR94},
  authorsAddress = {Bull Corporate Research, Les Clayes sous Bois},
  pages = {129--143}
}

@InProceedings{Goyal-Paige-1997,
  author = {Deepak Goyal and Robert Paige},
  title = {The Formal Reconstruction and Speedup of the Linear Time Fragment of {Willard's} Relational Calculus Subset},
  crossref = {IFIP-WG2-WC-1997},
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0486},
  bibliographies = {RelMiCS}
}

@TechReport{Grabau-Mast-Obermayer-Schmitz-1987,
  year = 1987,
  title = {{COSY-Handbuch (Beschreibung eines Compiler-erzeugenden Systems)}},
  number = 8803,
  month = SEP,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {S. Grabau and A. Mast and R. Obermayer and Lothar Schmitz}
}

@Unpublished{Grabmeyer-2005,
  author = 	 {Clemens Grabmeyer},
  title = 	 {A Duality between Proof Systems for Recursive Types, and Cyclic Term Graphs},
  note = 	 {unpublished draft},
  WKloc = 	 {A-1637}
}

@Article{Grabowski-Kornilowicz-Naumowicz-2010,
  author =       {Grabowski, Adam and Korni{\l}owicz, Artur and Naumowicz, Adam},
  title =        {Mizar in a Nutshell},
  journal =      {J.~Formalized Reasoning},
  year =         2010,
  volume =    3,
  number =    2,
  pages =     {153--245}
}

@InProceedings{Grabska-1993,
  author = {E. Grabska},
  title = {Graphs and Designing},
  crossref = {GTCS93},
  pages = {188--202},
  abstract = {In this paper the idea of defining graphical models
		  or pictures by specifying their logical structures
		  (graphs) and the possible ways of realization of
		  such structures (realization schemes) is
		  presented. The fact that the structure of the object
		  is independent of its realization may be useful in
		  the process of designing.},
  keywords = {Pythagoras Tree}
}

@InProceedings{Graedel-Gurevich-1994,
  author = {E. Gr\"adel and Y. Gurevich},
  title = {Tailoring Recursion for Complexity},
  pages = {118--129},
  crossref = {ICALP1994},
  authorsAddress = {EG: RWTH Aachen; YG: U. Michigan},
  abstract = {In this paper, a global function is a function that
		  computes a (local) function in each ordered
		  structure of a specified vocabulary. We design
		  algebras of global functions for a number of
		  complexity classes for which such algebras have not
		  been known, e.g.\null{} for the functions computable
		  in nondeterministic logarithmic space, or in
		  nondeterministic polynomial time. In addition, we
		  present a fiunctional analogue of first-order logic
		  and give a new functional characterization of
		  polynomial time.}
}

@Book{Graetzer-1968,
  author = {George Gr\"atzer},
  title = {Universal Algebra},
  series = {The University Series in Higher Mathematics},
  publisher = {D.~Van Nostrand Company, Inc.},
  address = {Princeton, New Jersey},
  year = 1968,
  UniBwM = {MAT200/C11667}
}

@Book{Graetzer-1978,
  author = {George Gr{\"a}tzer},
  title = {General Lattice Theory},
  publisher = {Birkh\"auser},
  address = {Basel},
  year = 1978,
  UniBwM = {MAT160/F26776},
  bibliographies = {RelMiCS}
}

@Book{Graetzer-1979,
  author = {George Gr{\"a}tzer},
  title = {Universal Algebra},
  publisher = Springer,
  address = {New York},
  edition = {second edition},
  year = 1979,
  UniBwM = {Mag/J16375},
  note = {(first edition 1968)},
  bibliographies = {RelMiCS}
}

@Book{Graf-1996,
  author = {P. Graf},
  title = {Term Indexing},
  publisher = Springer,
  year = 1996,
  volume = 1053,
  series = LNCS,
  annote = {defines ``compiled non-deterministic perfect AC discrimination net'' for efficient term rewriting}
}

@Book{Graham-Knuth-Patashnik-1989,
  author = {Ronald Lewis Graham and Donald Erwin Knuth and Oren
		  Patashnik},
  title = {Concrete Mathematics: A Foundation for Computer Science},
  publisher = Addison,
  year = 1989,
  UniBwM = {MAT100/V15736},
  bibliographies = {RelMiCS},
  contents = {1 Recurrent Problems
	1.1 The Tower of Hanoi
	1.2 Lines in the Plane
	1.3 The Josephus Problem
	2 Sums
	2.1 Notation
	2.2 Sums and Recurrences
	2.3 Manipulations of Sums
	2.4 Multiple Sums
	2.5 General Methods
	2.6 Finite and Infinite Calculus
	2.7 Infinite Sums
	3 Integer Functions
	3.1 Floors and Ceilings
	3.2 Floor/Ceiling Applications
	3.3 Floor/Ceiling Recurrences
	3.4 `mod': The Binary Operation
	3.5 Floor/Ceiling Sums
	4 Number Theory
	4.1 Divisibility
	4.2 Primes
	4.3 Prime Examples
	4.4 Factorial Factors
	4.5 Relative Primality
	4.6 `mod': The Congruence Relation
	4.7 Independent Residues
	4.8 Additional Applications
	4.9 Phi and Mu
	5 Binomial Coefficients
	5.1 Basic Identities
	5.2 Basic Practice
	5.3 Tricks of the Trade
	5.4 Generating Functions
	5.5 Hypergeometric Functions
	5.6 Hypergeometric Transformations
	5.7 Partial Hypergeometric Sums
	6 Special Numbers
	6.1 Stirling Numbers
	6.2 Eulerian Numbers
	6.3 Harmonic Numbers
	6.4 Harmonic Summation
	6.5 Bernoulli Numbers
	6.6 Fibonacci Numbers
	6.7 Continuants
	7 Generating Functions
	7.1 Domino Theory and Change
	7.2 Basic Maneuvers
	7.3 Solving Recurrences
	7.4 Special Generating Functions
	7.5 Convolutions
	7.6 Exponential Generating Functions
	7.7 Dirichlet Generating Functions
	8 Discrete Probability
	8.1 Definitions
	8.2 Mean and Variance
	8.3 Probability Generating Functions
	8.4 Flipping Coins
	8.5 Hashing
	9 Asymptotics
	9.1 A Hierarchy
	9.2 O Notation
	9.3 O Manipulation
	9.4 Two Asymptotic Tricks
	9.5 Euler's Summation Formula
	9.6 Final Summations
	A Answers to Exercises
	B Bibliography
	C Credits for Exercises
	Index
	List of Tables}
}

@InProceedings{Gramlich-1994,
  author = {Bernhard Gramlich},
  title = {On modularity of Termination and Confluence
		  Properties of Conditional Rewrite Systems},
  crossref = {ALP1994},
  pages = {186--203},
  abstract = {We investigate the modularity behaviour of
		  termination and confluence properties of conditional
		  term rewriting systems. In particular, we show how
		  to obtain sufficient conditions for the modularity
		  of weak termination, weak innermost termination,
		  (strong) innermost termination, (strong)
		  termination, confluence and completeness of
		  conditional rewrite systems.}
}

@InProceedings{Granlund-Kenner-1992,
  author = 	 {Torbjorn Granlund and Richard Kenner},
  title = 	 {Eliminating Branches
      Using a Superoptimizer and the {GNU C} Compiler},
  crossref =	 {PLDI1992},
  pages =	 {341--352},
  CiteSeer = 	 {http://citeseer.ist.psu.edu/granlund92eliminating.html},
  WKloc = 	 {A-1624, doc/pap/BIB}
}

@Article{Grattand-Guinness-1975,
  author = {I. Grattan-Guinness},
  title = {Wiener on the Logics of {Russell} and {Schr\"oder}:
		an Account of his Doctoral Thesis,
		and of his Discussion of it with {Russell}},
  journal = ANSCI,
  volume = 32,
  year = 1975,
  pages = {103--132},
  bibliographies = {RelMiCS}
}

@InProceedings{Graunke-Findler-Krishnamurthi-Felleisen-2001,
  author = {Paul Graunke and Robert Bruce Findler and Shriram Krishnamurthi and Matthias Felleisen},
  title = {Automatically Restructuring Programs for the Web},
  crossref = {ASE2001},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  abstract = {The construction of interactive server-side Web applications
    differs substantially from the construction of traditional
    interactive programs. In contrast, existing Web programming paradigms
    force programmers to save and restore control state between
    user interactions. We present an automated transformation that converts
    traditional interactive programs into standard CGI programs.
    This enables reuse of existing software development methodologies.
    Furthermore, an adaptation of existing programming environments
    supports the development of Web programs.}
}

@InProceedings{Gray-1987,
  author = {John W. Gray},
  title = {A Categorical Treatment of Polymorphic Operations},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  pages = {2--22},
  WKloc = {A-0070},
  keywords = {Cartesian Closed Category, CCC, Functor, natural
		  transformation, category theory, polymorphism,
                  lax modification},
  bibliographies = {RelMiCS}
}

@article{Gray-2003,
 author = {Gray, Jim},
 title = {What next?: A dozen information-technology research goals},
 journal = JACM,
 issue_date = {January 2003},
 volume = {50},
 number = {1},
 month = jan,
 year = {2003},
 issn = {0004-5411},
 pages = {41--57},
 numpages = {17},
 url = {http://doi.acm.org.libaccess.lib.mcmaster.ca/10.1145/602382.602401},
 doi = {10.1145/602382.602401},
 acmid = {602401},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@InProceedings{Green-1989,
  author = 	 {Green, T. R. G.},
  title = 	 {Cognitive Dimensions of Notations},
  pages = 	 {443--460},
  URL = {http://www.cl.cam.ac.uk/~afb21/CognitiveDimensions/papers/Green1989.pdf},
  WKloc = 	 {doc/pap/BIB},
  booktitle =	 {People and Computers V},
  year =	 1989,
  editor =	 {A. Sutcliffe and L. Macaulay},
  publisher =	 CambridgeUP
}

@Article{Greenwood-Gleadon-1955,
  author = {R. E. Greenwood and Andrew M. Gleason},
  title = {Combinatorial Relations and Chromatic Graphs},
  journal = CANAD,
  volume = 7,
  year = 1955,
  pages = {1--7},
  bibliographies = {RelMiCS}
}

@Article{Gregoire-Leroy-2002,
  author = {Gr\'{e}goire, Benjamin and Leroy, Xavier},
  title = {A compiled implementation of strong reduction},
  journal = {SIGPLAN Not.},
  volume = {37},
  number = {9},
  year = {2002},
  issn = {0362-1340},
  pages = {235--246},
  doi = {http://doi.acm.org/10.1145/583852.581501},
  publisher = {ACM},
  address = {New York, NY, USA},
  anstract = {Motivated by applications to proof assistants based on dependent types,
    we develop and prove correct a strong reducer and $\beta$--equivalence checker
    for the $\lambda$-calculus with products, sums, and guarded fixpoints.
    Our approach is based on compilation to the bytecode
    of an abstract machine performing weak reductions on non-closed terms,
    derived with minimal modifications
    from the ZAM machine used in the Objective Caml bytecode interpreter,
    and complemented by a recursive ``read back'' procedure.
    An implementation in the Coq proof assistant demonstrates important speed-ups
    compared with the original interpreter-based implementation of strong reduction in Coq.}
}

@InProceedings{Gribomont-Hagelstein-1994,
  author = {E.P. Gribomont and J. Hagelstein},
  title = {Reactive Variables for System Specification and Design},
  crossref = {STACS1994},
  pages = {275--286}
}

@Article{Grieder-1980,
  author = {Alfons Grieder},
  title = {On the Logic of Relations},
  journal = DIAL,
  volume = 34,
  year = 1980,
  pages = {167--182},
  OPTnote = {MR 82f:03062.},
  bibliographies = {RelMiCS}
}

@Book{Gries-1981,
  author = {D. Gries},
  title = {The Science of Programming},
  publisher = Springer,
  address = {New York},
  year = 1981,
  bibliographies = {RelMiCS}
}

@article{Gries-1991,
  author = {Gries, David},
  title = {Teaching calculation and discrimination: a more effective curriculum},
  journal = CACM,
  volume = {34},
  issue = {3},
  month = MAR,
  year = {1991},
  pages = {44--55},
  numpages = {12},
  DOIURL = {http://doi.acm.org/10.1145/102868.102870},
  DOI = {10.1145/102868.102870},
  publisher = {ACM},
  address = {New York, NY, USA},
  WKloc = {doc/pap/BIB}
}

@InCollection{Gries-1994,
  author = {David Gries},
  title = {Constant-space Quicksort},
  crossref = {Roscoe-1994},
  pages = {161--170},
  chapter = 10
}

@Book{Gries-Schneider-1993,
  author = {David Gries and Fred B. Schneider},
  title = {A Logical Approach to Discrete Math},
  publisher = Springer,
  year = 1993,
  series = {Monographs in Computer Science},
  ISBN = {3-540-94115-0},
  URL = {http://www.springer.de/cgi-bin/search_book.pl?isbn=3-540-94115-0},
  abstract = {Here, the authors strive to change the way
     logic and discrete math are taught in computer science and mathematics:
     while many books treat logic simply as another topic of study,
     this one is unique in its willingness to go one step further.
     The book treats logic as a basic tool which may be applied in
     essentially every other area.

     "This is a rather extraordinary book, and deserves to be read
     by everyone involved in computer science and ---
     perhaps more importantly --- software engineering.
     I recommend it highly $\ldots$ .
     If the book is taken seriously, the rigor that it unfolds
     and the clarity of its concepts could have a significant impact
     on the way in which software is conceived and developed."

                         --- Peter G. Neumann}
}

@InProceedings{Griffin-1990,
 author = {Timothy G. Griffin},
 title = {A formulae-as-type notion of control},
 booktitle = {POPL '90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages},
 year = {1990},
 isbn = {0-89791-343-4},
 pages = {47--58},
 WKloc = {doc/pap/BIB},
 bibliographies = {OPG},
 location = {San Francisco, California, United States},
 doi = {http://doi.acm.org/10.1145/96709.96714},
 publisher = {ACM Press},
 address = {New York, NY, USA},
}

@TechReport{Griswold-Notkin-1991,
  author = {William G. Griswold and David Notkin},
  title = {Semantic Manipulation of Program Source},
  year = 1991,
  number = {TR 91-08-03},
  institution = {University of Washigton},
  abstract = {Tool-assisted, semantically-based manipulation of
		  program source benefits from a program
		  representation that facilitates semantic queries,
		  such as the Program Dependence Graph (PDG).
		  However, if the manipulation requires, for instance,
		  scoping information, or the result is to be source
		  readable by humans, then information from other
		  representations such as the Abstract Syntax Tree
		  (AST) are also needed, requiring simultaneous,
		  coordinated manipulation of multiple
		  representations.  This paper describes a model for
		  meaning-preserving program transformation that aids
		  correct and efficientp manipulation of multiple
		  program representations.  The model is based on
		  relating not only the data of the representations,
		  but also the transformations on them.  An equation
		  describes a global source transformation as a
		  composition of local transformations related through
		  relationships naturally represented in the PDG, and
		  a set of PDG substitution rules is used to map the
		  result to a meaning-preserving PDG transformation.
		  The result, when successful, is that the source
		  transformation's implementation is shown to preserve
		  meaning.  Additionally, the resulting PDG
		  transformation can be applied efficiently to the PDG
		  to keep it up-to-date with the source, allowing
		  interactive use of the tool.  The model has been
		  successfully used to implement a prototype program
		  restructuring tool for use in software maintenance.}
}

@PhDThesis{Gritzner-1989,
  author = {Thomas F. Gritzner},
  title = {{Die Axiomatik abstrakter Relationenalgebren: Darstellung der
      Grundlagen und Anwendung auf das Unsch\"arfeproblem relationaler
      Produkte}},
  school = U_TUM,
  type = {Diplomarbeit},
  year = 1989,
  bibliographies = {RelMiCS}
}

@TechReport{Gritzner-1991,
  author = {Thomas F. Gritzner},
  title = {{Die Axiomatik abstrakter Relationenalgebren: Darstellung der
      Grundlagen und Anwendung auf das Unsch\"arfeproblem relationaler
      Produkte}},
  institution = U_TUM,
  type = {Internal Report},
  number = {TUM-INFO-04-91-I00},
  month = APR,
  year = 1991,
  bibliographies = {RelMiCS}
}

@TechReport{Gritzner-1992,
  author = {Thomas F. Gritzner},
  title = {The Action Graph Model as a Link Between Abstract Relation
      Algebras and Process-Algebraic Specifications},
  institution = U_TUM,
  number = {{SFB}~342/6/92~A},
  year = 1992,
  bibliographies = {RelMiCS}
}

@TechReport{Gritzner-Berghammer-1993,
  year = 1993,
  title = {A Relation Algebraic Model of Robust Correctness},
  type = {Bericht},
  number = 9301,
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen},
  month = JAN,
  author = {Thomas F. Gritzner and Rudolf Berghammer},
  note = {Accepted for publication in TCS \unfinished},
  bibliographies = {RelMiCS}
}

@Unpublished{Grobauer-1994,
  author = {Bernd Grobauer},
  title = {{Pattern matching \"uber abstrakten Datentypen}},
  note = {Handout zum Seminar ``Funktionale Programmierung''
		  SS94, T. Nipkow; Zusammenfassung von
		  \cite{Burton-Cameron-1993}},
  year = 1994,
  month = JUN,
  WKloc = {A-0261}
}

@MastersThesis{Grobauer-1997,
  author = {Bernd Grobauer},
  title = {A Verified Unification Algorithm for Higher-Order Patterns},
  school = {Fakult\"at f\"ur Informatik, TU M\"unchen},
  year = 1997,
  WKloc = {B-0114}
}

@InProceedings{Groenboom-Hendriks-Polak-Terlouw-Udding-1995,
  author = {Rix Groenboom and Chris Hendriks and Indra Polak and Jan Terlouw and Jan Tijmen Udding},
  title = {Algebraic Proof Assistants in {HOL}},
  crossref = {MPC1995},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0714}
}

@Article{Groenendijk-Stokhof-1991,
  author = {Groenendijk, J. and Stokhof, M.},
  journal = Linguistics_and_Philosophy,
  pages = {39--100},
  title = {Dynamic Predicate Logic},
  volume = 14,
  year = 1991,
  bibliographies = {RelMiCS}
}

@Misc{Groeneveld-Veltman-1994,
  author = {W. Groeneveld and F. Veltman},
  howpublished = {Manuscript, ILLC, Amsterdam},
  title = {Inference Systems for Update Semantics},
  year = 1994,
  bibliographies = {RelMiCS}
}

@unpublished{Gross-Chlipala-SpivakDI-2014,
  author = {Jason Gross and Adam Chlipala and David I. Spivak},
  title = {Experience Implementing a Performant Category-Theory Library in {C}oq},
  note = {Draft; accepted at ITP 2014; The final publication is available at link.springer.com.},
  bibliographies = {RATH-Agda},
  month = {July},
  year = {2014},
  abstract = {We describe our experience implementing a broad category-theory library
	in Coq. Category theory and computational performance are not usually
	mentioned in the same breath, but we have needed substantial engineering
	effort to teach Coq to cope with large categorical constructions
	without slowing proof script processing unacceptably. In this paper,
	we share the lessons we have learned about how to represent very
	abstract mathematical objects and arguments in Coq and how future
	proof assistants might be designed to better support such reasoning.
	One particular encoding trick to which we draw attention allows category-theoretic
	arguments involving \emph{duality} to be internalized in Coq's logic
	with definitional equality. Ours may be the largest Coq development
	to date that uses the relatively new Coq version developed by homotopy
	type theorists, and we reflect on which new features were especially
	helpful.},
  eprint = {1401.7694},
  full-bibliography = {http://people.csail.mit.edu/jgross/personal-website/papers/category-coq-experience.html},
  owner = {Jason},
  reviews = {http://people.csail.mit.edu/jgross/personal-website/papers/category-coq-experience-itp-2014-reviews.txt},
  timestamp = {2014.01.19},
  original-url = {http://people.csail.mit.edu/jgross/personal-website/papers/category-coq-experience-itp-submission.pdf},
  url = {http://people.csail.mit.edu/jgross/personal-website/papers/category-coq-experience-itp-submission-final.pdf}
}

@InProceedings{GrosseRhode-Wolter-1992,
  author = {M. Gro\ss{}e-Rhode and U. Wolter},
  title = {2-Categorical Specification of Partial Algebras},
  crossref = {SADT92},
  pages = {207--220},
  authorsAddress = {TU Berlin},
  WKloc = {A-0338},
  abstract = {$\ldots$}
}

@Article{Grossi-1991,
  author = {R. Grossi},
  title = {A Note on the Subtree Isomorphism for Ordered Trees and
           Related Problems},
  year = 1991,
  volume = 39,
  pages = {81--84},
  journal = IPLET,
  nutshell = {Result for sequential complexity: If $P$ matches at $v
		  \Rightarrow |tree(v)| = |P|$, then $O(|T|)$ total
		  time. Something of parallel complexity.},
  bibliographies = {RelMiCS}
}

@Book{Grune-Bal-Jacobs-Langendoen-2000,
  author =	 {Dick Grune and Henri E. Bal and Ceriel J.H. Jacobs and Koen G. Langendoen},
  title = 	 {Modern Compiler Design},
  publisher = 	 {Wiley},
  year = 	 2000,
  URL = 	 {http://www.cs.vu.nl/~dick/MCD.html}
}

@InProceedings{ Gschwind-Altman-2002,
    author = "Michael Gschwind and Erik R. Altman",
    title = "Precise Exception Semantics in Dynamic Compilation",
    booktitle = "Proc. of 2002 Symposium on Compiler Construction",
    pages = "95--110",
    year = "2002",
    CiteSeer = "http://citeseer.ist.psu.edu/gschwind02precise.html",
  bibliographies = {OPG}
}

@Misc{Gtk2Hs,
  key =		 {Gtk2Hs},
  author =	 {Axel Simon},
  title =	 {{Gtk2Hs}: A {Gtk2} binding for {Haskell}},
  year =	 2002,
  note =	 {\textsf{https://sourceforge.net/projects/gtk2hs/}}
}

@InProceedings{Guenot-2011,
  author    = {Nicolas Guenot},
  title     = {Nested Proof Search as Reduction in the $\lambda$-calculus},
  URL = {http://www.lix.polytechnique.fr/~nguenot/},
  crossref = {PPDP2011},
  year      = 2011,
  WKloc = {doc/pap/BIB}
}

@Misc{Guentzer-Schmidt-Kempf-Moeller-1989,
  year = 1989,
  title = {Mathematische Logik},
  pages = 136,
  note = {1991 Neuauflage an der Universit\"at Stuttgart},
  howpublished = {Vorlesungsskript, Institut f\"ur Informatik,  Technische Universit\"at M\"unchen},
  author = {U. G\"untzer and G. Schmidt and M. Kempf and B. M\"oller}
}

@InProceedings{Gui-Okada-1993,
  author = {Yexuan Gui and Mitsuhiro Okada},
  title = {{LAMBDALG}: Higher Order Algebraic Specification Language},
  crossref = {RTA93},
  pages = {462--466},
  WKloc = {A-0135},
  abstract = {LAMBDALG offers the followings; \begin{enumerate}
            \item The usual higher order rewrite definition of recursive
		  functionals (of higher types) is extended to a
		  non-recursive scheme of functionals defined on
		  algebraic data types. Mixed terms composed of lambda
		  terms, first order algebraic terms and higher order
		  functional constants can be used.
            \item The order-sorted structure of the algebraic base
		  types is extended to the higher type structure,
		  which accommodates higher order rewriting with
		  sub-typing, based on the underlying sub-typing
		  mechanism of \cite{Cardelli-Mitchell-1989}.
            \item The first order polymorphic type ranges over the
		  algebraic base types (the basic data types), while
		  the second order polymorphic type ranges over higher
		  types. A first order and a higher order sub-typing
		  are defined in terms of a bounded first order and a
		  bounded second order polymorphism, respectively.
            \end{enumerate}}
}

@InProceedings{Gui-Okada-1993-x,
  author = {Yexuan Gui and Mitsuhiro Okada},
  title = {{LAMBDALG}: Higher Order Algebraic Specification Language},
  pages = {462--467},
  WKloc = {A-0135},
  ISBN = {3-540-56868-9},
  year = 1993,
  volume = 690,
  booktitle = {Rewriting Techniques and Applications: 5th
		  international conference, {RTA 5, Montreal, Canada,
		  June 16--18, 1993}},
  series = {LNCS},
  publisher = {Springer},
  UniBwM = {INF700/Z9362-5},
  editor = {Claude Kirchner},
  address = {Berlin},
  refsfrom = { <(ifip94:8)> }
}

@InProceedings{Gui-Okada-1993a,
  author = {Yexuan Gui and Mitsuhiro Okada},
  title = {System Description of {LAMBDALG} -- A Higher Order
		  Specification Language},
  crossref = {LPAR93},
  pages = {354--356},
  WKloc = {A-0125},
  abstract = {Typed functional languages like Standard ML or Haskel do not
             allow algebraic definitions of abstract data tapes and
             operators although they may employ a very rich machinery for
             defining polymorphic recursive functions of a higher type. On
             the other hand, equational languages like OBJ allow arbitrary
             (first order) algebraic definitions, but they do not have the
             full-power of parametricity given by ML style polymorphism nor
             functional definitions of higher types. LAMBDALG, an
             experimental higher-orderr algebraic specification programmin
             system, integrates the OBJ style module based algebraic
             specification language [5] and the ML style polymorphic typed
             functional languag [6] in a simple yet expressive language.
             Algebraic definition of abstract data types and
             context-sensitive definitions of operators can be directly
             executed, in contrast to the traditional ML-style languages or
             their extensions (eg.[8]). In particular, LAMBDALG supports
             both subsort polymorphism and parametric polymorphism for
             algebraic specifications. The computation mechanism of LAMBDALG
             is based on the combined system of polymorphic typed lambda
             calculus and the first order and restricted higher order term
             rewriting in [7].}
}

@Article{Gumm-HughesJesse-SchroederT-2003,
  author = {H. Peter Gumm and Jesse Hughes and Tobias Schr{\"o}der},
  title = "Distributivity of categories of coalgebras",
  journal = TCS,
  volume = 308,
  number = "1--3",
  pages = "131--143",
  year = 2003,
  ISSN = {0304-3975},
  DOI = {http://dx.doi.org/10.1016/S0304-3975(02)00582-0},
  DirectURL = {http://www.sciencedirect.com/science/article/pii/S0304397502005820},
  abstract = {For any $Set$-endofunctor $F$,
    the category $Set_F$ of $F$-coalgebras has preimages,
    i.e. pullbacks along an injective map.
    If $F$ preserves preimages, then $Set_F$ is distributive,
    and the converse holds, whenever $Set_F$ has finite products.},
}

@TechReport{Gunter-1985,
  year = 1985,
  title = {Profinite Solutions for Recursive Domain Equations},
  number = {CMU-CS-85-107},
  institution = {Computer Science Department, Carnegie-Mellon University},
  author = {C. Gunter},
  address = {Pittsburg}
}

@InProceedings{Gunter-1985a,
  author = 	 {Carl A. Gunter},
  title = 	 {Comparing Categories of Domains},
  crossref =  {MFPS1985},
  pages = 	 {101--121},
  PDF = {http://www.cis.upenn.edu/~gunter/publications/documents/Gunter85.pdf},
  WKloc = 	 {doc/pap/BIB}
}

@Book{Gunter-1992,
  year = 1992,
  title = {Semantics of Programming Languages: Structures and Techniques},
  series = {Foundations of Computing},
  publisher = {MIT Press},
  pages = 419,
  author = {Carl A. Gunter},
  address = {Cambridge, Mass.}
}

@Article{Gunther-1992a,
  abstract = {This paper introduces an operator called $\cal M$
		  called the {\em mixed powerdomain} which generalizes
		  the convex (Plotkin) powerdomain. The construction
		  is based on the idea of presenting partial
		  information about a set of data items using a {\em pair}
		  of sets, one representing partial information in the
		  manner of the upper (Smyth) powerdomain and the
		  other in the manner of the lower (Hoare) powerdomain
		  where the components of such pairs are required to
		  satisfy a consistency condition. This provides a
		  richer family of meaningful partial descriptions
		  than are available in the convex powerdomain and
		  also makes it possible to include the {\em empty set}
		  in a satisfactory way. The new construct is
		  given a rigorous mathematical treatment like that
		  which has been applied to the known powerdomains. It
		  is proved that $\cal M$ is a continuous functor on
		  bifinite domains which is left adjoint to the
		  forgetful functor from a category of continuous
		  structures called {\em mix algebras}. For a domain
		  $D$ with coherent Scott topology, elements of ${\cal
		  M}D$ can be represented as pairs $(U,V)$ where $U
		  \subseq D$ is a compact upper set, $V \subseq D$ is
		  a closed set and the downward closure of $U \inter V$
		  is equal to $V$. A Stone dual characterization of
		  $\cal M$ is also provided.},
  year = 1992,
  volume = 103,
  title = {The Mixed Powerdomain},
  pages = {311--334},
  journal = {Theoretical Computer Science},
  author = {Carl A. Gunther}
}

@InProceedings{Gunter-Gunter-MacQueen-1991,
  title = {An Abstract Interpretation for {ML} Equality Kinds},
  author = {Carl A. Gunter and Elsa L. Gunter and David B. MacQueen},
  pages = {112--130},
  crossref = {TACS1991},
  abstract = {The definition of Standard ML provides a form of generic
		  equality which is inferred for certain types, called
		  {\em equality types,} on which it is possible to
		  define a computable equality relation.  However, the
		  standard definition is incomplete in the sense that
		  there are interesting and useful types which are
		  {\em not} inferred to be equality types but which
		  nevertheless have a computable equality relation.
		  In this paper, a refinement of the Standard ML
		  system of equality types is introduced and is proven
		  sound and {\em complete} with respect to the
		  existence of a computable equality.  The technique
		  used here is based on an abstract interpretation of
		  ML operators as monotone functions over a three
		  point lattice.  It is shown how the equality
		  relation can be defined (as an ML program) from the
		  definition of a type with our equality property.
		  Finally, a sound, efficient algorithm for inferring
		  the equality property which corrects the limitations
		  of the standard definition in all cases of practical
		  interest is demonstrated.}
}

@TechReport{Gupta-1991a,
  author = {S. A. Gupta},
  title = {Functional Encapsulation and Type Reconstruction in a Strongly-typed, Polymorphic Language},
  institution = {Massachusetts Institute of Technology, Laboratory for
                 Computer Science},
  type = {Technical Report},
  number = {MIT-LCS//MIT/LCS/TR-647},
  pages = 186,
  month = FEB,
  year = 1995,
  WKloc = {B-0091}
}

@TechReport{Gupta-1991b,
  author = {S. A. Gupta},
  title = {An Incremental Type Inference System for
                 the Programming Language {Id}},
  institution = {Massachusetts Institute of Technology, Laboratory for
                 Computer Science},
  type = {Technical Report},
  number = {MIT-LCS//MIT/LCS/TR-488},
  pages = 173,
  month = may,
  year = 1991,
  WKloc = {B-0090},
  abstract = {Modern computing environments strive to be robust and
                 reliable, and at the same time, aim at providing enough
                 flexibility to an interactive user to edit, debug, and
                 test programs easily and efficiently. Strongly typed
                 languages satisfactorily meet the former goal by
                 guaranteeing that ``type-consistent'' programs will
                 not incur run-time type-errors. But most programming
                 environments for such languages have to sacrifice the
                 flexibility of interactive and incremental program
                 development in order to achieve this consistency over
                 the whole program. The problem is further complicated
                 by the presence of polymorphism in many of these
                 languages, where the definition of
                 ``type-consistency'' is one of inclusion, rather than
                 equality. In this thesis, we address the latter goal of
                 providing an interactive and incremental program
                 development environment in context of Id, which is a
                 polymorphic, strongly typed, incrementally compiled,
                 parallel programming language, developed at the
                 Laboratory for Computer Science, MIT. Id's typing
                 mechanism is based on the Hindley/Milner type inference
                 system. We describe modifications and extensions to its
                 inference algorithm to allow the building and testing
                 of programs in an incremental, definition by definition
                 basis, and in arbitrary order. We also prove the
                 correctness (soundness and completeness) of our
                 incremental typing scheme with respect to the original
                 type system. We also examine the problem of
                 systematically identifying overloaded identifiers in a
                 program using type information, and translating them
                 into efficient actual code. Finally, we describe the
                 problems encountered while typing non-functional,
                 polymorphic objects in our type system and discuss some
                 possible solutions.},
  note = {Cost is \$20. Date: Nov. 1990 Keywords: Hindley/Milner
                 type system, Id, incremental compilation, incremental
                 type inference, I-structures, overloading,
                 polymorphism, polymorphic references, static semantics,
                 type checking, type inference}
}

@InProceedings{Gupta-1992,
  author = {Rajiv Gupta},
  title = {Generalized Dominators and Post-dominators},
  crossref = {POPL1992},
  pages = {246--257},
  abstract = {The notion of dominators is generalized to include
		  multiple-vertex dominators in addition to
		  single-vertex dominators. A multiple-vertex
		  dominator is a group of vertices that collectively
		  dominate the vertex. Existing algorithms compute
		  immediate single-vertex dominators, and an algorithm
		  for comuting immediate multiple-vertex dominators is
		  presented in this paper. The immediate dominator
		  information is expressed in the form of a directed
		  acyclic graph referred to as the dominator DAG or
		  the DDAG. The generalized dominator set of any
		  vertex, which consists of all single-vertex and
		  multiple-vertex dominators of the vertex, can be
		  computed from the DDAG. The single-vertex dominator
		  information restricts the proagation of loop
		  invariant statements and array bound checks out of
		  loops. Generalized dominator information avoids
		  these restrictions. In addition, it can be used to
		  identify natural loops and improve the existing
		  optimization algorithm for code hoisting. The dual
		  notion of generalized post-dominators can be used
		  for computing control dependencies and automatic
		  generation of compact test suites for programs.}
}

@PhDThesis{Gupta-1994,
  author = {Vineet Gupta},
  title = {Chu Spaces: A Model of Concurrency},
  school = {Stanford University},
  month = SEP,
  year = 1994,
  URL = {http://boole.stanford.edu/pub/gupthes.ps.gz},
  WKloc = {doc/pap/BIB/Gupta-1994\_thesis.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {A Chu space is a binary relation between two sets. In this
      thesis we show that Chu spaces form a non-interleaving model of
      concurrency which extends event structures while endowing them with
      an algebraic structure whose natural logic is linear logic.

      We provide several equivalent definitions of Chu spaces, including
      two pictorial representations. Chu spaces represent processes as
      automata or schedules, and Chu duality gives a simple way of
      converting between schedules and automata. We show that Chu spaces
      can represent various concurrency concepts like conflict, temporal
      precedence and internal and external choice, and they distinguish
      between causing and enabling events.

      We present a process algebra for Chu spaces including the standard
      combinators like parallel composition, sequential composition,
      choice, interaction, restriction, and show that the various
      operational identities between these hold for Chu spaces. The
      solution of recursive domain equations is possible for most of these
      operations, giving us an expressive specification and programming
      language. We define a history preserving equivalence between Chu
      spaces, and show that it preserves the causal structure of a process.}
}

@InProceedings{Gupta-1998,
  author = {Gopal Gupta},
  title = {Horn Logic Denotations and Their Applications},
  OPTcrossref = {},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1485, doc/pap/BIB}
}

@InProceedings{Gupta-Pratt-1993,
  author = {Gupta, V. and Pratt, V.R.},
  title = {Gates Accept Concurrent Behavior},
  booktitle = {Proc. 34th Ann. IEEE Symp. on Foundations of Comp. Sci.},
  pages = {62--71},
  month = NOV,
  year = 1993,
  URL = {http://boole.stanford.edu/pub/gates.ps.gz},
  WKloc = {doc/pap/BIB/Gupta-Pratt-1993\_gates.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We represent concurrent processes as Boolean propositions or
      gates, cast in the role of acceptors of concurrent behavior. This
      properly extends other mainstream representations of concurrent
      behavior such as event structures, yet is defined more simply. It
      admits an intrinsic notion of duality that permits processes to be
      viewed as either schedules or automata. Its algebraic structure is
      essentially that of linear logic, with its morphisms being
      consequence-preserving renamings of propositions, and with its
      operations forming the core of a natural concurrent programming
      language.}
}

@InProceedings{Gupta-Rao-2001,
  author = {Neelam Gupta and Praveen Rao},
  title = {Program Execution Based Module Cohesion Measurement},
  crossref = {ASE2001},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  abstract = {Module cohesion describes the degree to which
    different actions performed by a module contribute towards a
    unified function. High module cohesion is a desirable property
    of a program. The program modifications during
    successive maintenance interventions can have negative effect
    on the structure of the program resulting in less cohesive modules.
    Therefore, metrics that measure module cohesion are important
    for software restructuring during maintenance.
    The existing static slice based module cohesion metrics
    significantly overestimate cohesion due to
    the limitations of static slicing.

    In this paper, we present a novel program execution based approach
    to measure module cohesion of legacy software. We define
    cohesion metrics based on \emph{definition-use} pairs
    in the dynamic slices of the outputs. Our approach
    significantly improves the accuracy of cohesion measurement.
    We implemented our technique and measured module cohesion
    for several programs. Cohesion measurements using our technique
    were found to be more insightful than static slice based measurements.}
}

@Article{Gurevich-1991,
  author = {Gurevich, Yuri},
  journal = Bulletin_of_the_European_Association_for_Theoretical_Computer_Science,
  pages = {264--286},
  title = {Evolving algebras: A tutorial introduction},
  volume = 43,
  year = 1991,
  bibliographies = {RelMiCS}
}

@InProceedings{Gurevich-1993,
  author = {Yuri Gurevich},
  title = {{Evolving Algebras 1993: Lipari Guide}},
  booktitle = {Specification and Validation Methods},
  pages = {231--243},
  year = 1995,
  editor = {E. B{\"o}rger},
  publisher = OUP,
  WKloc = {A-1093}
}

@InProceedings{Gurevich-1993a,
  author = {Yuri Gurevich},
  title = {{Evolving Algebras}: An Attempt to Discover Semantics},
  booktitle = {Current Trends in Theoretical Computer Science},
  pages = {266--292},
  year = 1993,
  editor = {G. Rozenberg and A. Salomaa},
  publisher = {World Scientific},
  WKloc = {A-1098}
}

@Misc{Gurevich-1997,
  author = {Yuri Gurevich},
  title = {{May 1997 Draft of the ASM Guide}},
  month = MAY,
  year = 1997,
  WKloc = {A-1099}
}

@Article{Gurevich-2000,
  author =       {Yuri Gurevich},
  title =        {Sequential Abstract State Machines Capture Sequential Algorithms},
  journal =      {ACM Transactions on Computational Logic},
  year =         2000,
  volume =    1,
  number =    1,
  pages =     {77-111},
  month =     JUL,
  DOIURL =    {http://dx.doi.org/10.1145/343369.343384}
}

@InProceedings{Gurevich-Immerman-Shelah-1994,
  title = {McColm's Conjecture},
  author = {Yuri Gurevich and Neil Immerman and Saharon Shelah},
  pages = {10--19},
  crossref = {LICS9},
  abstract = {Gregory McColm conjectured that positive elementary
		  inductions are bounded in a class K of finite
		  structures if every (FO + LFP) formula is equivalent
		  to a first-order formula in K.  Here (FO + LFP) is
		  the extension of first-order logic with the least
		  fixed point operator.  We disprove the conjecture.
		  Our main results are two model-theoretic
		  constructions, one deterministic and the other
		  randomized, each of which refutes McColm's conjecture.}
}

@InProceedings{Gurr-Tourlas-2000,
  author = {Corin Gurr and Konstantinos Tourlas},
  title = {Towards the Principled Design of Software Engineering Diagrams},
  crossref = {ICSE2000},
  URL = {http://www.dcs.ed.ac.uk/home/kxt/gurr.ps.gz},
  WKloc = {A-1198}
}

@PhDThesis{Gutierrez-1999,
  author = {Claudio Guti{\'e}rrez},
  title = {The Arithmetic and Geometry of Allegories: Normal Forms and Complexity of a Fragment of the Theory of Relations},
  school = {Wesleyan University},
  year = 1999,
  bibliographies = {RelMiCS, GraphCalc}
}

@Article{Gutierrez-2000,
  author = {Claudio Guti{\'e}rrez},
  title = {Normal Forms for Connectedness in Categories},
  journal = ANPURE,
  volume = {Special issue devoted to the XI Simposio Latinoamericano de Logica Matematica, Venezuela, July 1998},
  year = 2000,
  note = {to appear},
  bibliographies = {RelMiCS, GraphCalc}
}

@Book{Habel-1992,
  author = {Annegret Habel},
  title = {Hyperedge Replacement: Grammars and Languages},
  publisher = Springer,
  year = 1992,
  volume = 643,
  series = LNCS
}

@Article{Habel-Kreowski-Lautemann-1991,
  author = {Annegret Habel and Hans-J\"org Kreowski and C. Lautemann},
  title = {A comparison of compatible, finite and inductive
		  graph properties},
  journal = {Theoretical Computer Science},
  year = 1991,
  volume = 89,
  pages = {33--62}
}

@InProceedings{Habel-Kreowski-Plump-1987,
  author = {Annegret Habel and Hans-J\"org Kreowski and Detlef Plump},
  title = {Jungle Evaluation},
  booktitle = {Recent Trends in Data Type Specification: 5th Workshop
		on Specification of Abstract Data Types},
  publisher = Springer,
  address = {New York},
  pages = {92--112},
  year = 1987,
  series = LNCS,
  volume = 332
}

@TechReport{Habel-Mueller-Plump-2000,
  author = {Annegret Habel and J\"urgen M\"uller and Detlef Plump},
  title = {Double-Pushout Graph Transformation Revisited},
  institution = {Fachbereich Mathematik und Informatik, Universit\"at Bremen},
  year = 2000,
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  WKloc = {A-0980}
}

@incollection {Habel-Mueller-Plump-2000_injective,
   author = {Habel, Annegret and M{\"u}ller, J{\"u}rgen and Plump, Detlef},
   title = {Double-Pushout Approach with Injective Matching},
   crossref = {TAGT1998},
   pages = {431--444},
   DOIURL = {http://dx.doi.org/10.1007/978-3-540-46464-8_8},
   DOI = {10.1007/978-3-540-46464-8_8},
   WKloc = {doc/pap/BIB},
   abstract = {We investigate and compare four variants of the double-pushout approach to graph transformation. Besides the traditional approach with arbitrary matching and injective right-hand morphisms, we consider three variations by employing injective matching and/or arbitrary right-hand morphisms in rules. For each of the three variations, we clarify whether the well-known commutativity theorems are still valid and-where this is not the case-give modified results. In particular, for the most general approach with injective matching and arbitrary right-hand morphisms, we establish sequential and parallel commutativity by appropriately strengthening sequential and parallel independence. We also show that injective matching provides additional expressiveness in two respects, viz. for generating graph languages by grammars without nonterminals and for computing graph functions by convergent graph transformation systems.},
   year = {2000}
}

@Article{Habel-Mueller-Plump-2001,
  author = {Annegret Habel and J\"urgen M\"uller and Detlef Plump},
  title = {Double-Pushout Graph Transformation Revisited},
  journal = {Mathematical Structures in Computer Science},
  year = 2001,
  volume = 11,
  number = 4,
  OPTpages = {},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Habel-Plump-1995,
  author = {Annegret Habel and Detlef Plump},
  title = {Unification, Rewriting, and Narrowing on Term Graphs},
  crossref = {SEGRAGRA-1995}
}

@InProceedings{Habel-Plump-1998,
  author = {Annegret Habel and Detlef Plump},
  title = {Complete Strategies for Term Graph Narrowing},
  crossref = {WADT1998},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0653}
}

@InCollection{Habel-Plump-2002,
  author = {Habel, Annegret and Plump, Detlef},
  title = {Relabelling in Graph Transformation},
  crossref = {ICGT2002},
  pages = {135-147},
  DOIURL = {http://dx.doi.org/10.1007/3-540-45832-8_12},
  DOI = {10.1007/3-540-45832-8_12},
  WKloc = {doc/pap/BIB},
  abstract = {The traditional double-pushout approach to graph transformation
     does not allow to change node labels in an arbitrary context.
     We propose a simple solution to this problem,
     namely to use rules with partially labelled interface graphs
     and to match rules injectively.
     In [8] we have shown that injective matching
     makes the double-pushout approach more expressive,
     and here we further generalise that approach.
     Besides solving the relabelling problem,
     our framework allows to write rules with partially labelled left-hand sides
     which are equivalent to
     (possibly infinite) sets of rules in the traditional setting.
     Unlike previous work on rules with partially labelled graphs,
     we do not need any labelling condition on matching morphisms,
     nor do we exclude node merging rules.}
}

@InCollection{Habel-Plump-2012,
  author={Habel, Annegret and Plump, Detlef},
  title={{$\mathcal{M},\mathcal{N}$}-Adhesive Transformation Systems},
  pages={218--233},
  DOI={10.1007/978-3-642-33654-6_15},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-33654-6_15},
  crossref={ICGT2012},
  abstract = {The categorical framework of $\CalM$-adhesive transformation systems does not cover graph transformation with relabelling. Rules that relabel nodes are natural for computing with graphs, however, and are commonly used in graph transformation languages. In this paper, we generalise $\CalM$-adhesive transformation systems to $\CalM,\CalN$-adhesive transformation systems, where $\CalN$ is a class of morphisms containing the vertical morphisms in double-pushouts. We show that the category of partially labelled graphs is $\CalM,\CalN$-adhesive, where $\CalM$ and $\CalN$ are the classes of injective and injective, undefinedness-preserving graph morphisms, respectively. We obtain the Local Church-Rosser Theorem and the Parallelism Theorem for graph transformation with relabelling and application conditions as instances of results which we prove at the abstract level of $\CalM,\CalN$-adhesive systems.}
}

@Misc{Haeberer-Baum-Duran-1992,
  title = {Some Examples of Program Derivation and Heuristics
		  Whithin an Extended Relational Algebra},
  howpublished = {Document 697-Aug-11, 44st Meeting of the IFIP
		  Working Group 2.1. ``Algorithmic Languges and
		  Calculi''},
  author = {Armando Mart\'{\i}n Haeberer and Gabriel A. Baum and Ju\'{a}n E. Dur{\'a}n},
  bibliographies = {RelMiCS}
}

@InProceedings{Haeberer-Baum-Schmidt-1994,
  author = {Haeberer, Armando Mart\'{\i}n and Baum, Gabriel A. and Gunther Schmidt},
  title = {On the smooth calculation of relational recursive expressions
      out of first-order non-constructive specifications involving
      quantifiers},
  crossref = {Bjorner-Broy-Pottosin-1994},
  pages = {403--420},
  WKloc = {A-0996},
  bibliographies = {RelMiCS}
}

@TechReport{Haeberer-Baum-Veloso-1987,
  author = {Armando Mart\'{\i}n Haeberer and Gabriel A. Baum and Paulo A. S. Veloso},
  title = {On an Algebraic Theory of Problems and Software Development},
  year = 1987,
  type = {Res.\null{} Rept.},
  number = {MCC 2},
  institution = RiodeJaneiro,
  address = {Rio de Janeiro},
  bibliographies = {RelMiCS}
}

@InCollection{Haeberer-Frias-Baum-Veloso-1997,
  author = {Armando Haeberer and Marcelo Frias and Gabriel Baum and Paulo Veloso},
  title = {Fork Algebras},
  chapter = 4,
  pages = {54--69},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@InProceedings{Haeberer-Veloso-1991a,
  title = {Program Derivation Calculi Cannot Preserve
		  Termination},
  crossref = {LMPS1991},
  author = {Armando Mart\'{\i}n Haeberer and Paulo A. S. Veloso},
  bibliographies = {RelMiCS}
}

@InProceedings{Haeberer-Veloso-1991b,
  author = {Haeberer, Armando Mart\'{\i}n and Paulo A.S. Veloso},
  title = {Partial Relations for Program Derivation: Adequacy,
		  Inevitability and Expressiveness},
  crossref = {IFIP1991},
  pages = {319--371},
  bibliographies = {RelMiCS}
}

@Misc{Haeberer-Veloso-Elustondo-1990,
  year = 1990,
  title = {Towards a Relational Calculus for Software Construction},
  howpublished = {Document 640-BUR-5, 41st Meeting of the IFIP
		  Working Group 2.1. ``Programming Languges and Calculi''},
  author = {Armando Mart\'{\i}n Haeberer and Paulo A. S. Veloso and P. Elustondo},
  address = {Chester, England},
  bibliographies = {RelMiCS}
}

@PhDThesis{Hagino-1987,
  author = {Tatsuya Hagino},
  title = {A Categorical Programming Language},
  school = {Edinburgh University},
  year = 1987,
  keywords = {dialgebras},
  WKloc = {A-1398, doc/pap/BIB},
  PDFURL = {http://www.tom.sfc.keio.ac.jp/~hagino/thesis.pdf}
}

@InProceedings{Hagiya-1991,
  title = {From Programming-by-Example to Proving-by-Example},
  author = {Masami Hagiya},
  pages = {387--419},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {In the machine learning community, it is widely recognized
		  that there are two fundamentally different
		  approaches to learning: {\em inductive learning\/}
		  and {\em deductive learning}. This paper formalizes
		  both of the approaches to learning under a uniform
		  framework of type theory and investigates the use of
		  higher-order unification to solve learning problems
		  in both.

                  I first introduce the simply typed
		  $\lambda$-calculus with inductive definitions and
		  give a unification procedure for the calculus.  I
		  then formalize a problem of {\em
		  programming-by-example\/} (i.e., inductive learning)
		  as a system of equations in the calculus and
		  reformulate existing methods for
		  programming-by-example as restricted versions of the
		  unification procedure.

                  It is a new attempt to formalize a problem of {\em
		  proving-by-example\/} (i.e., deductive learning) as
		  an equation in a typed $\lambda$-calculus.  For that
		  purpose, I extend {\bf LF} with inductive
		  definitions and consider a unification procedure for
		  it.}
}

@InProceedings{Haiman-1985,
  author = {Mark D. Haiman},
  title = {Linear Lattice Proof Theory: an Overview},
  booktitle = {Universal Algebra and Lattice Theory},
  note = {Proc. of the Southeastern Conf. in Universal
		Algebra and Lattice Theory, July 11-14, 1984},
  series = LNM,
  volume = 1149,
  publisher = Springer,
  year = 1985,
  pages = {129--141},
  bibliographies = {RelMiCS}
}

@Booklet{Haiman-1986,
  author = {Mark D. Haiman},
  title = {Arguesian Lattices which are not linear},
  note = {Preprint, March 1986, pp. 4},
  bibliographies = {RelMiCS}
}

@Misc{Hajek-a,
  author =	 {H{\'a}jek, Petr},
  title = 	 {Deductive Systems of Fuzzy Logic},
  WKloc = {A-1619, doc/pap/BIB}
}

@Misc{Hajek-1998a,
  author =	 {H{\'a}jek, Petr},
  title = 	 {Metamathematical Fuzzy Logic --- state of art},
  WKloc = {A-1620, doc/pap/BIB}
}

@Misc{Hajek-1998b,
  author =	 {H{\'a}jek, Petr},
  title = 	 {Ten Questions and One Probelm on Fuzzy Logic},
  WKloc = {A-1621, doc/pap/BIB}
}

@Book{Hajek-2002,
  author =	 {H{\'a}jek, Petr},
  title = 	 {Metamathematics of Fuzzy Logic},
  publisher = 	 {Springer},
  year = 	 2002,
  volume =	 4,
  series =	 {Trends in Logic},
  ISBN = 	 {1-4020-0370-6},
  URL = 	 {http://www.springeronline.com/sgw/cda/frontpage/0,11855,4-40109-22-33733735-0,00.html}
}

@Book{Hajek-Havranek-1978,
  author = {Petr H{\'a}jek and  T. Havr{\'a}nek},
  title = {Mechanizing Hypothesis Formation --- Mathematical Foundations for a General Theory},
  publisher = Springer,
  year = 1978,
  note = {soon on \textsf{http://www..cs.cas.cz/\~{}hajek/}},
  bibliographies = {RelMiCS},
  keywords = {GUHA, data mining}
}

@Article{Halbwachs-Caspi-Pilaud-Pilaud-1991,
  author = {N.  Halbwachs and P. Caspi and P. Raymond and D. Pilaud},
  title = {The synchronous dataflow programming language {\sc Lustre}},
  journal = {Proceedings of the IEEE},
  volume = 79,
  number = 9,
  month = {September},
  pages = {1305--1320},
  year = 1991
}

@InProceedings{Hale-1987,
  author = {Roger Hale},
  title = {Using Temporal Logic for Prototyping:
           The Design of a Lift Controller},
  pages = {375--408},
  year = 1987,
  booktitle = {Temporal Logic in Specification},
  editor = {B. Banieqbal and H. Barringer and A. Pnueli},
  series = {LNCS},
  volume = 398,
  publisher = {Springer-Verlag},
  UniBwM = {MAT006/S13083}
}

@InProceedings{Hall-Hammond-PeytonJones-Wadler-1994,
  author = {C. Hall and K. Hammond and Peyton Jones, Simon
		  L. and Phil Wadler},
  title = {Type Classes in {Haskell}},
  crossref = {ESOP1994},
  pages = {241--256},
  URL = {ftp://ftp.dcs.glasgow.ac.uk/pub/papers?},
  WKloc = {A-0324},
  abstract = {This paper defines a set of type inference rules for
		  resolving overloading introduced by type
		  classes. Programs including type classes are
		  transformaed into ones which may be typed by the
		  Hindley-Milner inference rules. In contrast to other
		  work on type classes, the rules presented here
		  relate directly to user programs. An innovative
		  aspect of this work is the use of second-order
		  lambda calculus to record type information in the program.}
}

@Article{Hall-Padua-Pingali-2010,
  author = {Hall, Mary and Padua, David and Pingali, Keshav},
  title = {Compiler research: the next 50 years},
  journal = {Commun. ACM},
  volume = {52},
  number = {2},
  year = {2009},
  issn = {0001-0782},
  pages = {60--67},
  doi = {http://doi.acm.org/10.1145/1461928.1461946},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {Coconut},
  abstract = {Research and education in compiler technology is more important than ever.}
}

@InProceedings{Hallem-Chelf-Xie-Engler-2002,
  author = {Seth Hallem and Benjamin Chelf and Yichen Xie and Dawson Engler},
  title = {A System and Language for Building System-Specific, Static Analyses},
  crossref = {PLDI2002},
  WKloc = {A-1322, doc/pap/BIB},
  URL = {http://www.stanford.edu/~engler/p27-hallem.ps},
  annote = {Engler: ``The best description of our MC system. Focuses on interprocedural analysis, ranking, and simple path sensitivity to suppress false paths.''},
  abstract = {This paper presents a novel approach to bug-finding analysis
      and an implementation of that approach. Our goal is to find as many
      serious bugs as possible. To do so, we designed a flexible,
      easy-to-use extension language for specifying analyses and an
      efficiant algorithm for executing these estensions. The language,
      \emph{metal}, allows users of our system to specify a braod class of
      analyses in terms that resemble the intuitive description of the
      rules that they check. The system, \emph{xgcc}, executes these
      analyses efficiently using a context-sensitive, interprocedural
      analysis.

      Our prior work has shown that the approach described in this paper is
      effective: it has successfully found thousands of bugs in real
      systems code. This paper describes the underlying system used to
      achieve these results. We believe that our system is an effective
      framework for deploying new bug-finding analyses quickly and easily.}
}

@InProceedings{Hallgren-Carlsson-1995,
  author = {Thomas Hallgren and Magnus Carlsson},
  title = {Programming with {Fudgets}},
  crossref = {AFP1995},
  WKloc = {A-0466}
}

@InProceedings{Hallgren-Hook-Jones-Kieburtz-2004,
  author = {Thomas Hallgren and James Hook and Mark P. Jones and Richard B. Kieburtz},
  title = 	 {An Overview of the Programatica Toolset},
  booktitle =  {HCSS '04},
  URL = 	 {http://www.cse.ogi.edu/PacSoft/projects/programatica/},
  bibliographies = {HHOL},
  WKloc = 	 {A-1542, doc/pap/BIB},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@Book{Halmos-1962,
  author = {Paul R. Halmos},
  title = {Algebraic Logic},
  publisher = Chelsea,
  address = {New York},
  year = 1962,
  bibliographies = {RelMiCS}
}

@Book{Halmos-1974,
  author = {Paul R. Halmos},
  title = {Lectures on Boolean Algebras},
  publisher = Springer,
  year = 1974,
  bibliographies = {RelMiCS}
}

@Article{Halpern-Moses-1992,
  author = {J.Y. Halpern and Y. Moses},
  title = {A Guide to Completeness and Complexity for Modal
		  Logics of Knowledge and Belief},
  journal = {Artificial Intelligence},
  year = 1992,
  volume = 54,
  pages = {319--379},
  annote = {cited in \cite{Arnborg-1993} as source for the logics
		  \def\LO#1{${\rm #1}_n$}\LO{K}, \LO{T}, \LO{S4},
		  \LO{S5} and \LO{KD45} with and without multiple agents}
}

@InProceedings{Halpern-Shoham-1986,
  author = {J. Y. Halpern and Yoav Shoham},
  title = {A Propositional Modal Logic of Time Intervals},
  booktitle = {Proc.\null{} of the Sympos.\null{}  on Logic in Computer
		Science 1986},
  pages = {279--292},
  publisher = IEEE,
  year = 1986,
  bibliographies = {RelMiCS}
}

@Inbook{Hamana-2001,
  author="Hamana, Makoto",
  editor="Kobayashi, Naoki and Pierce, Benjamin C.",
  chapter="A Logic Programming Language Based on Binding Algebras",
  title={{Theoretical Aspects of Computer Software: 4th International Symposium, TACS 2001 Sendai, Japan, October 29--31, 2001 Proceedings}},
  LNCSbooktitle = {TACS 2001},
  year="2001",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="243--262",
  ISBN="978-3-540-45500-4",
  DOI="10.1007/3-540-45500-0_12",
  DOIURL="http://dx.doi.org/10.1007/3-540-45500-0_12",
  WKloc = {A-1763 (pre), doc/pap/BIB},
  abstract = {We give a logic programming language
     based on Fiore, Plotkin and Turi’s binding algebras.
     In this language, we can use not only first-order terms
     but also terms involving variable binding.
     The aim of this language is similar to Nadathur and Miller’s $\lambda$-Prolog,
     which can also deal with binding structure
     by introducing $\lambda$-terms in higher-order logic.
     But the notion of binding used here is finer in a sense
     than the usual $\lambda$-binding.
     We explicitly manage names used for binding
     and treat $\alpha$-conversion with respect to them.
     Also an important difference is the form of application related to $\beta$-conversion,
     i.e. we only allow the form $(M x)$, where $x$ is a (object) variable,
     instead of usual application $(M N)$.
     This notion of binding comes from the semantics of binding by the category of presheaves.
     We firstly give a type theory which reflects this categorical semantics.
     Then we proceed along the line of first-order logic programming language,
     namely, we give a logic of this language,
     an operational semantics by SLD-resolution and unification algorithm for binding terms.}
}

@Article{Hamblin-1971,
  author = {C. L. Hamblin},
  title = {Instants and Intervals},
  journal = STUDGEN,
  volume = 27,
  year = 1971,
  pages = {127--134},
  bibliographies = {RelMiCS}
}

@InProceedings{Hamel-Goguen-1994,
  author = {L. H. Hamel and J. A. Goguen},
  title = {Towards a Provably Correct Compiler for {OBJ3}},
  crossref = {PLILP1994},
  pages = {132--146},
  WKloc = {A-0304},
  keywords = {abstract machine}
}

@InProceedings{Hamilton-1996,
  author = {G. W. Hamilton},
  title = {Higher-Order Deforestation},
  crossref = {PLILP1996},
  pages = {213--227},
  OPTabstract = {},
  WKloc = {A-0553}
}

@Misc{Hammond-Hall-1992,
  author = {Kevin Hammond and Cordelia Hall},
  title = {A Dynamic Semantics for {Haskell} (Draft)},
  year = 1992,
  month = FEB,
  WKloc = {A-0863}
}

@Article{Han-Tsuda-1997,
  author = {Dony-Soo Han and Takao Tsuda},
  title = {Non-Graph Based Approach on the Analysis of Pointers
		  and Structures},
  journal = {IEICE Transactions on Information and Systems},
  year = 1997,
  volume = {E80-D},
  number = 4,
  month = APR,
  pages = {480--488},
  publisher = {Institute of Electronics, Information and Communication Engineers},
  abstract = {In high performance compilers to process pointer-handling
        programs, precise pointer alias analysis is useful for the compilers
        to generate efficient object code. It is well known that most
        compiler techniques such as data flow analysis. dependence analysis,
        side effect analysis and optimization are related to the alias
        problem. However, without data structure information,  there is a
        limit on the precision of the alias analysis. Even  though the
        automatic data structure detectior problem is complex, when pointer
        manipulation satisfies some restrtctions, some data structures can be
        detected automatically by compilers with some knowledge of aliases.
        In this paper, we propose an automatic data structure detection
        method for Pascal and Fortran 90. Linear list, tree and dag data
        structures are detected. Detected data structure information can be
        used not only for raising the precision of alias analysis but also
        for some optimizing techniques for pointer handling programs
        directly.},
  keywords = {record handling programs, pointer alias analysis, data
        structure detection, linear list, tree, dag},
  annote = {polytypic programming, second-order sharing}
}

@MastersThesis{HanJinrong-2008,
  author = 	 {Jinrong Han},
  title = 	 {Proofs of Relational Semigroupoids in {Isabelle/Isar}},
  school = 	 {McMaster University, Department of Computing and Software},
  year = 	 2008,
  type = 	 {{M.Sc.~thesis}}
}

@Book{Handbook-LAILP,
  title = {Handbook of Logic in Artificial Intelligence and Logic Programming},
  publisher = {Oxford University Press},
  year = 1993,
  editor = {Dov Gabbay and Jorg Siekmann},
  volume = 1
}

@InProceedings{Hankin-1991,
  author = {Chris Hankin},
  title = {Static Analysis of Term Graph Rewriting Systems},
  crossref = {PARLE91b},
  pages = {367--384},
  abstract = {In this paper we present a framework for the abstract
               imterpretation of term graph rewriting system.The
               framework is based on the approach taken by the Cousots
               for flowchart progrm.We give an exampe of the use of the
               framework by presenting an interpretation which performs
               a type inference.}
}

@InProceedings{Hankin-Hunt-1992,
  author = {Chris Hankin and Sebastian Hunt},
  title = {Approximate Fixed Points in Abstract Interpretation},
  crossref = {ESOP1992},
  pages = {219--232},
  authorsAddress = {Imperial College of Science, Technology and
		  Medicine, London},
  abstract = {Much of the earlier development of abstract
		  interpretation, and its application to imparative
		  programming languages, has concerned techniques for
		  finding fixed points in large (often infinite)
		  lattices. The standard approach in the abstract
		  interpretation of functional languages has been to
		  work with small, finite lattices and this supposedly
		  circumvents the need for such techniques. However,
		  practical experience has shown that, in the presence
		  of higher order functions, the lattices soon become
		  too large (although still finite) for the
		  fixed-point finding problem to be tractable. This
		  paper develops some approximation techniques which
		  were first proposed by Hunt and shows how these
		  techniques relate to the earlier use of widening and
		  narrowing operations by the Cousots.},
  bibliographies = {RelMiCS}
}

@InProceedings{Hankin-LeMetayer-1994,
  author = {Chris Hankin and Le M{\'e}tayer, Daniel},
  title = {Deriving Algorithms From Type Inference Systems:
		  Application to Strictness Analysis},
  crossref = {POPL1994},
  pages = {202--212},
  keywords = {frontiers machine},
  annote = {see also \cite{Hankin-LeMetayer-1994a}!},
  abstract = {The r\^ole of non-standard tpe inference in static
		  program analysis has been much studied
		  recently. Early work emphasised the efficiency of
		  type inference algorithms and paid little attention
		  to the correctness of the inference system. Recently
		  more powerful inference systems have been
		  investigated but the connection with efficient
		  inference algorithms has been obscured. The
		  contribution of theis paper is twofold: first we
		  show how to transform a program logic into an
		  algorithm and, second, we introduce the notion of
		  lazy types and show how to derive an efficient
		  algorithm for strictness analysis.}
}

@InProceedings{Hankin-LeMetayer-1994a,
  author = {Chris Hankin and Le M{\'e}tayer, Daniel},
  title = {Lazy Type Inference for the Strictness Analysis of Lists},
  crossref = {ESOP1994},
  pages = {257--271},
  annote = {see also \cite{Hankin-LeMetayer-1994}!},
  WKloc = {A-0329},
  abstract = {We present a type inference system for the
		  strictness analysis of lists and we show that it can
		  be used as the basis for an efficient algorithm. The
		  algorithm is as accurate as the usual abstract
		  interpretation technique. One distinctive advantage
		  of this approach is that it is not necessary to
		  impose an abstract domain of a particular depth
		  prior to the analysis: the lazy algorithm will
		  instead explore the part of a potentially infinite
		  domain required to prove the strictness property.}
}

@Misc{Hankin-LeMetayer-Sands-199X,
  author = {Chris Hankin and Le M{\'e}tayer, Daniel and David Sands},
  title = {Refining Multiset Transformers},
  year = {199?},
  WKloc = {A-0749}
}

@Article{Hanschke-Wuertz-1993,
  author = {P. Hanschke and J. W{\"u}rtz},
  title = {Satisfiability of the smallest binary program},
  pages = {237--241},
  year = 1993,
  volume = 45,
  number = 5,
  month = APR,
  journal = IPLET,
  filename = {IPL93-Wuertz.dvi},
  abstract = {Recursivity is well known to be a crucial and
		  important  concept in programming theory. The
		  simplest scheme of recursion in the context of logic
		  programming is the binary Horn clause
		  $\,P(l_1,\ldots,l_n) \!\gets\! P(r_1,\ldots,r_n)\,$.
		  The decidability of the satisfiability problem  of
		  programs consisting of such a rule,  a fact and a
		  goal -- called {\it smallest  binary program} -- has
		  been a goal of research for some time. In this
		  paper the undecidability of the smallest binary
		  program is  shown by a   simple reduction of the
		  Post  Correspondence Problem.},
  bibliographies = {RelMiCS}
}

@Misc{Hansen-1999,
  author = {Wilfred J. Hansen},
  title = {Deployment Descriptions in a World of {COTS} and {Open Source}},
  month = AUG,
  year = 1999,
  URL = {http://www.sei.cmu.edu/staff/wjh/DeployDesc.html},
  note = {URL: {\sf http://www.sei.cmu.edu/staff/wjh/DeployDesc.html}},
  WKloc = {A-1177}
}

@Article{Hansoul-1983,
  author = {G. Hansoul},
  title = {A Duality for Boolean Algebras with Operators},
  journal = ALGU,
  volume = 17,
  year = 1983,
  pages = {34--49},
  bibliographies = {RelMiCS}
}

@InProceedings{Hanus-1990,
  author = {Michael Hanus},
  title = {Logic Programs with Equational Type Specifications},
  crossref = {ALP1990},
  pages = {70--85},
  abstract = {This paper proposes a framework for logic programming with
             different type systems. In this framework a typed logic program
             consists of a type specification and a Horn clause program
             which is well-typed with respect to the specification. The type
             specification defines all types which can be in the logic
             program. Relations between types are expressed by equations on
             the level of types. This permits the specification of
             many-sorted, order-sorted, polymorphic and polymorphically
             order-sorted type system.

             We present the declarative
             semantics of our framework and a resolution procedure for typed
             logic programs. Resolution requires a unification procedure for
             typed terms which is based on a unification procedure for the
             type theory. An interesting application is a type system that
             combines parametric polymorphism with order-sorted typing and
             permits higher-order logic programming. Moreover, our framework
             sheds some new light on the r\^ole of types in logic
             programming.}
}

@InProceedings{Hanus-1990a,
  author = {Michael Hanus},
  title = {A Functional and Logic Language with Polymorphic Types},
  crossref = {DISCO90},
  pages = {215--224}
}

@Article{Hanus-1991,
  author = {Michael Hanus},
  title = {Horn Clause Programs with Polymorphic Types: Semantics
                 and Resolution},
  journal = {Theoretical Computer Science},
  volume = 89,
  year = 1991
}

@InProceedings{Hanus-1992,
  keywords = {ALF, SLD-resolution, innermost basic narrowing},
  authorsAddress = {MPI f\"ur Informatik, Saarbr\"ucken},
  abstract = {This paper shows the advantages of amalgamating
		  functional and logic programming languages. In
		  comparison with pure functional languages, an
		  amalgamated functional logic language has more
		  expressive power. In comparison with pure logic
		  languages, functional logic languages have a better
		  control behaviour. The latter will be shown by
		  presenting methods to translate logic programs into
		  a functional logic language with a
		  narrowing/rewriting semantics. The translated
		  programs produce the same set of answers and have at
		  least the same efficiency as the original programs.
		  But in many cases the control behaviour of the
		  translated programs is improved. This requires the
		  addition of further knowledge to the programs. We
		  discuss methods for this and show the gain in
		  efficiency by means of several examples.},
  title = {Improving Control of Logic Programs by Using
		  Functional Logic Languages},
  pages = {1--23},
  crossref = {PLILP1992},
  author = {Michael Hanus}
}

@InProceedings{Hanus-1994,
  author = {Michael Hanus},
  title = {Combining Lazy Narrowing and Simplification},
  crossref = {PLILP1994},
  pages = {370--384}
}

@InProceedings{Hanus-1994a,
  author = {Michael Hanus},
  title = {Lazy Unification with Simplification},
  crossref = {ESOP1994},
  pages = {272--287},
  keywords = {E-unification},
  abstract = {unification in the presence of an equational theory
		  is an iportant problem in theorem-proving and in hte
		  integration of functional and logic programming
		  languages. This paper presents an improvement of the
		  proposed lazy unification methods by incorporating
		  simplification into the unification process. Since
		  simplification is a deterministic computation
		  process, more efficient unification algorithms can
		  be ahieved. Moreover, simplification reduces the
		  search space so that in some case infinite search
		  spaces are reduced to finite ones. We show soundness
		  and completeness of our method for equational
		  theories represented by ground confluent and
		  terminating rewrite systems which is a reasonable
		  class w.r.t.\null{} functional logic programming.},
  annote = {compare \cite{Hanus-1994}}
}

@Article{Hanus-1994b,
  author = {Michael Hanus},
  title = {The Integration of Functions into Logic Programming:
		  From Theory to Practice},
  journal = {J. Logic Programming},
  year = 1994,
  volume = {19,20},
  pages = {583--628},
  WKloc = {A-0404},
  annote = {Functional Logic Programming (FLP) survey}
}

@InProceedings{Hanus-1995,
  author = {Michael Hanus},
  title = {efficient Translation of Lazy Functional Logic Programs into Prolog},
  crossref = {LOPSTR-1995},
  pages = {252--266},
  OPTabstract = {},
  WKloc = {A-0453}
}

@InProceedings{Hanus-1997,
  author = {Hanus, M.},
  title = {A Unified Computation Model for Functional and Logic Programming},
  year = 1997,
  booktitle = {Proc. 24st ACM Symposium on Principles of Programming
  		    Languages (POPL'97)},
  pages = {80-93},
  abstract = {We propose a new computation model which combines the operational
        principles of functional languages (reduction), logic languages
        (non-deterministic search for solutions), and integrated functional
        logic languages (residuation and narrowing).  This computation model
        combines efficient evaluation principles of functional languages with
        the problem-solving capabilities of logic programming.  Since the
        model allows the delay of function calls which are not sufficiently
        instantiated, it also supports a concurrent style of programming. We
        provide soundness and completeness results and show that known
        evaluation principles of functional logic languages are particular
        instances of this model.  Thus, our model is a suitable basis for
        future declarative programming languages.},
  URL = {http://www-i2.informatik.rwth-aachen.de/~hanus/publications/papers/POPL97.html},
  WKloc = {A-0944}
}

@InProceedings{Hanus-1997a,
  author = {Hanus, Michael},
  title = {A Unified Computation Model for Declarative Programming},
  year = 1997,
  booktitle = {Proc.\null{} of the 1997 Joint Conference on
               Declarative Programming {(APPIA-GULP-PRODE'97)}},
  pages = {9--24},
  abstract = {We propose a new computation model which combines the operational
       principles of functional languages (reduction), logic languages
       (non-deterministic search for solutions), and integrated functional
       logic languages (residuation and narrowing).  This computation model
       combines efficient evaluation principles of functional languages with
       the problem-solving capabilities of logic programming.  Since the
       model allows the delay of insufficiently instantiated function calls,
       it also supports a concurrent style of programming. We show that many
       known evaluation principles of declarative languages are particular
       instances of our model.  This computation model is the basis of Curry,
       a multi-paradigm language which combines functional, logic and
       concurrent programming styles. We conclude with a description of some
       features of Curry.},
  WKloc = {A-0943},
  URL = {http://www-i2.informatik.rwth-aachen.de/~hanus/publications/papers/AGP97.html}
}

@InProceedings{Hanus-1999,
  author = {Hanus, M.},
  title = {Distributed Programming in a Multi-Paradigm Declarative Language},
  year = 1999,
  pages = {376--395 \unfinished},
  publisher = Springer,
  series = LNCS,
  volume = 1702,
  booktitle = {Proc.\null{} of the International Conference on Principles
      and Practice of Declarative Programming (PPDP'99)},
  abstract = {
       Curry is a multi-paradigm declarative language covering
       functional, logic, and concurrent programming paradigms.
       Curry's operational semantics is based on lazy reduction of expressions
       extended by a possibly non-deterministic binding of free variables
       occurring in expressions. Moreover, constraints
       can be executed concurrently which provides for
       concurrent computation threads that are synchronized
       on logical variables.
       In this paper, we extend Curry's basic computational model by
       a few primitives to support distributed applications where
       a dynamically changing number of different program units must
       be coordinated. We develop these primitives as a special case
       of the existing basic model so that the new
       primitives interact smoothly with the existing features for
       search and concurrent computations. Moreover, programs
       with local concurrency can be easily transformed into
       distributed applications. This supports a simple development
       of distributed systems that are executable on local networks
       as well as on the Internet. In particular, sending
       partially instantiated messages containing logical variables
       is quite useful to implement reply messages.
       We demonstrate the power of these primitives
       by various programming examples.},
  URL = {http://www-i2.informatik.rwth-aachen.de/~hanus/publications/papers/PPDP99.html},
  WKloc = {A-0946}
}

@Manual{Hanus-1999b,
  author = {Michael Hanus and Sergio Antoy and Herbert Kuchen and
            Francisco J. L{\'o}pez-Fraguas and Wolfgang Lux and
            Moreno Navarro, Juan Jos{\'e} and Frank Steiner},
  editor = {Michael Hanus},
  title = {Curry --- An Integrated Functional Logic Language, Version 0.6},
  year = 1999,
  month = OCT,
  WKloc = {A-0950}
}

@Manual{Hanus-2006a,
  author = {Michael Hanus and others},
  editor = {Michael Hanus},
  title = {Curry --- An Integrated Functional Logic Language, Version 0.8.2},
  year = 2006,
  month = MAR,
  note = {\url{http://www.informatik.uni-kiel.de/~curry/report.html}},
  URL = {http://www.informatik.uni-kiel.de/~curry/report.html}
}

@InProceedings{Hanus-2000,
  author = {Hanus, M.},
  title = {A Functional Logic Programming Approach to Graphical User Interfaces},
  year = 2000,
  pages = {47-62},
  publisher = Springer,
  series = LNCS,
  volume = 1753,
  booktitle = {Proc.\null{} of the Second International Workshop on
      Practical Aspects of Declarative Languages (PADL'00)},
  abstract = {We show how the features of modern integrated functional logic
       programming languages can be exploited to implement graphical user
       interfaces (GUIs) in a high-level declarative style.  For this
       purpose, we have developed a GUI library in Curry, a multi-paradigm
       language amalgamating functional, logic, and concurrent programming
       principles.  The functional features of Curry are exploited to define
       the graphical structure of an interface and to implement new graphical
       abstractions, and the logic features of Curry are used to specify the
       logical dependencies of an interface.  Moreover, the concurrent and
       distributed features of Curry support the easy implementation of GUIs
       to distributed systems.},
  URL = {http://www-i2.informatik.rwth-aachen.de/~hanus/publications/papers/PADL00.html}
}

@Article{Hanus-Prehofer-1999,
  author = {Hanus, M. and Prehofer, C.},
  title = {Higher-Order Narrowing with Definitional Trees},
  journal = {Journal of Functional Programming (to appear)},
  volume = 9,
  number = 1,
  pages = {33-75},
  year = 1999,
  abstract = {Functional logic languages with a sound and complete operational
         semantics are mainly based on an inference rule called narrowing.
         Narrowing extends functional evaluation by goal solving capabilities
         as in logic programming. Due to the huge search space of simple
         narrowing, steadily improved narrowing strategies have been
         developed in the past. Needed narrowing is currently the best
         narrowing strategy for first-order functional logic programs due to
         its optimality properties w.r.t.\ the length of derivations and the
         number of computed solutions. In this paper, we extend the needed
         narrowing strategy to higher-order functions and $\lambda$-terms as
         data structures. By the use of definitional trees, our strategy
         computes only independent solutions. Thus, it is the first calculus
         for higher-order functional logic programming which provides for
         such an optimality result. Since we allow higher-order logical
         variables denoting $\lambda$-terms, applications go beyond current
         functional and logic programming languages.
         We show soundness and completeness of our strategy with respect to
         LNT reductions, a particular form of higher-order reductions defined via
         definitional trees. A general completeness result is only provided
         for terminating rewrite systems due to the lack of an overall
         theory of higher-order reduction which is outside the scope
         of this paper.},
  URL = {http://www-i2.informatik.rwth-aachen.de/~hanus/publications/papers/JFP99.html},
  WKloc = {A-0945}
}

@Article{Harary-1950,
  author = {Frank Harary},
  title = {On Complete Atomic Proper Relation Algebras},
  journal = JSYLO,
  volume = 15,
  year = 1950,
  pages = {197--198},
  bibliographies = {RelMiCS}
}

@Article{Hardin-1989,
  contents = {Introduction and summary
   1. Preliminaries
   1.1 Rewriting systems
   1.2 $\lambda$-calculi
   1.3 The Strong Categorical Combinatory Logic
   1.4 Subsystems of $CCL\beta\eta SP$: weak confluence, termination
   2. Confluence properties for subsystems of $CCL\beta\eta SP$
   2.1 Statement of the problems
   2.2 The Interpretation Method
   2.3 The subsystem $SL$ is confluent on the subset $\cal D$
   2.4 The subsystem $SL\beta$ is confluent on the subset $\cal D$
   2.5 $SL\eta$ is confluent on the subset $\cal D$
   3. $({\cal D},SL\beta\eta)$: a confluent conservative extension of
   $\lambda$-calculi
   3.1 $\cal D$ and $\Lambda$
   3.2 $\Lambda_{c,f}$ and CCL
   4. $CCL\beta SP$ is not confluent
   4.1 Yet another counter-example for $\Lambda_{c,a}$
   4.2 The relation $\beta SP$ is not confluent
   4.3 $CCL\beta SP$ is not confluent
   Conclusion
   References},
  abstract = {The Strong Categorial Combinatory Logic $(CCL,CCL\beta\eta SP)$,
   developed by Curien (1986) is, when typed and augmented with a rule
   defining a terminal object, a presentation of Cartesian Closed Categories.
   Furthermore, it is equationally equivalent to the Lambda-calculus with
   explicit couples and Surjective Pairing. Here we study the confluence
   properties of $(CCL, CCL\beta\eta SP)$ and of several of its subsystems,
   and the relationship between untyped Lambda-calculi and
   $(CCL, CCL\beta\eta SP)$ as rewriting systems. We prove that there exists
   a subset $\cal D$ of $CCL$, and a subsystem $SL\beta$ of $CCL\beta\eta SP$
   confluent on $\cal D$, a very simple isomorphism between $\Lambda$,
   the classical Lambda-calculus, and a subset ${\cal SD}_\lambda$ of
   $\cal D$, which is extended between $\beta$-derivations of $\Lambda$
   and a class of derivations of $SL\beta$. Substitution, which is a
   one-step operation belonging to the meta-language of $\Lambda$, is
   now described by rewritings with $SL\beta$ and calculations between
   several substitutions launched at the same time may be performed by
   $SL\beta$. This point is a real increase in the calculation capacities
   of Lambda-calculus (same results for $\cal D$).
   The same result holds for the Lambda-calculus with couples and
   projection rules (without Surjective Pairing).
   The locally confluent subsystem $CCL\beta SP$ (that is
   $SL\beta + (SP))$ is not confluent. This result is obtained by
   firstly designing a new counter-example (different from J.W.~Klop's one)
   for confluence of the Lambda-calculus with couples and Surjective Pairing
   and then translating it into CCL. However, $CCL\beta SP$ is shown
   to be confluent on the set derived from ${\cal SD}_\lambda$.
   These results cannot be obtained with classical methods of confluence
   and we designed a new method called Interpretation Method based on this
   trick: a given relation $R$ is confluent on a set $X$ if and only if
   a relation ${\cal E}(R)$ induced by $R$ on a set of regularized terms
   ${\cal E}(R)$ is confluent.},
  year = 1989,
  volume = 65,
  title = {Confluence Results for the Pure Strong Categorical Logic {CCL}. $\lambda$-Calculi as Subsystems of {CCL}},
  pages = {291--342},
  journal = {Theoretical Computer Science},
  author = {Th\'er\`ese Hardin}
}

@InProceedings{Hardin-1992,
  authorsAddress = {Conservatoire National des Art et M\'tiers,
		  Paris, hardin\@margaux.inria.fr},
  abstract = {Some new calculi [1,12,8], referred to by the
		  collective name of $\lambda\sigma$-calculus, have
		  been recently introduced to provide an explicit
		  treatment of substitutions in the
		  $\lambda$-calculus. They are term rewriting systems,
		  with two sorts: {\tt substitution} and {\tt term}.
		  The $\lambda$-terms are exactly the ground
		  $\llambda\sigma$-terms of sort {\tt term} containing
		  no substitutions and the $\bet$-reduction is
		  decomposed in these calculi, into a starting
		  reduction with a rule called (Beta) followed by a
		  derivation computing explicitly the substitution.
		  These calculi differ by their treatment of
		  substitution. In this paper, we extend the
		  $\lambda\sigma$-calculi with a conditional rewriting
		  relation, called $c\eta$. This relation coincides,
		  on $\lambda$-terms, with the classical
		  $\eta$-reduction of $\lambda$-calculus. We prove
		  that the confluent $\lambda\sigma$-calculus,
		  augmented by $c\eta$, remains confluent and that the
		  ground confluent version [1], extended by $c\eta$,
		  is still ground confluent. The proof is done by the
		  interpretation method introduced in [10].},
  title = {Eta-conversion for the languages of explicit substitutions},
  pages = {306--321},
  crossref = {ALP1992},
  author = {Th\'er\`ese Hardin}
}

@InProceedings{Hardin-Levy-1989,
  year = 1989,
  title = {A Confluent Calculus of Substitutions},
  note = {$\lambda Env$ rewriting system implementing
		  $\lambda$-calculus.},
  month = NOV,
  booktitle = {Proceedings of the France Japan Artificial
		  Intelligence and Computer Science Symposium},
  author = {T. Hardin and J.-J. L\'evy},
  address = {Izu, Japan}
}

@TechReport{Hardwick-Sipelstein-1996,
  author = {Jonathan C. Hardwick and Jay Sipelstein},
  institution = {School of Computer Science, Carnegie Mellon
                 University},
  title = {Java as an Intermediate Language},
  year = 1996,
  abstract-url = {http://www.cs.cmu.edu/~scandal/html-papers/javanesl/},
  URL = {http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/CMU-CS-96-161.ps.Z},
  keywords = {Java, NESL, VCODE, compiler, intermediate language,
                 performance, prototyping},
  month = aug,
  number = {CMU-CS-96-161},
  scope = {lang},
  WKloc = {B-0111}
}

@TechReport{Hardy-Schwartz-1994,
  author = {Darren R. Hardy and Michael F. Schwartz},
  title = {Customized Information Extraction as a Basis for Resource Discovery},
  year = 1994,
  month = MAR,
  institution = {Department of Computer Science, University of Colorado at Boulder},
  OPTtype = {},
  number = {CU-CS-707-94},
  OPTaddress = {},
  WKloc = {A-0697},
  annote = {Essence}
}

@InCollection{Harel-1984,
  author = {Harel, D.},
  address = {Dordrecht},
  booktitle = {Handbook of Philosophical Logic, Vol.\ II},
  editor = {Gabbay, D.M. and Guenthner, F.},
  pages = {497--604},
  publisher = Reidel,
  title = {Dynamic Logic},
  year = 1984,
  bibliographies = {RelMiCS}
}

@InProceedings{Harel-1994,
  author = {David Harel},
  title = {Towards a Theory of Recursive Structures},
  crossref = {STACS1994},
  pages = {633--645},
  authorsAddress = {Weizmann Institute},
  abstract = {In computer science, one is interested mainly in
		  finite objects. Insofar as {\em infinite} objects
		  are of interest, they must be computable, i.e.,
		  recursive, thus admitting an effective finite
		  representation. This leads to the notion of a
		  recursive graph, or, more generally, a recursive
		  structure or data base. In this paper we summarize
		  our recent work on recursive structures and data
		  bases, including (i) the high undecidability of many
		  problems on recursive graphs, (ii) somewhat
		  surprising ways of deducing results on the
		  classification of NP optimization problems from
		  results on the degree of undecidability of their
		  infinitary analogues, and (ii) completeness results
		  for query languages on recursive data bases.},
  annote = {summary of three papers},
  bibliographies = {RelMiCS}
}

@Manual{Haridi-Franzen-2001,
  title = {Tutorial of {Oz}, Version 1.2.3},
  OPTkey = {},
  OPTauthor = {Seif Haridi and Nils Franz{\'e}n},
  OPTorganization = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = DEC,
  OPTyear = 2001,
  WKloc = {A-1323},
  abstract = {This tutorial introduces the Oz programming language and the Mozart programming system. [...]}
}

@Book{Harland-1986,
  author = {David M. Harland},
  title = {Concurrency and Programming Languages},
  publisher = {Ellis Horwood},
  annote = {Ellis Horwood seems to belong to Wiley},
  year = 1986,
  bibliographies = {SE3B},
  McMaster = {QA 76.7 .H367 1986}
}

@Article{Harper-Honsell-Plotkin-19XX,
  author = {Robert Harper and Furio Honsell and Gordon Plotkin},
  title = {A Framework for Defining Logics},
  journal = {},
  year = {},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0693},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {LF, LCF}
}

@article{Harper-Licata-2007,
 author = {Harper, Robert and Licata, Daniel R.},
 title = {Mechanizing metatheory in a logical framework},
 journal = {J. Funct. Program.},
 volume = {17},
 issue = {4-5},
 month = {July},
 year = {2007},
 issn = {0956-7968},
 pages = {613--673},
 numpages = {61},
 url = {http://dl.acm.org/citation.cfm?id=1296837.1296842},
 doi = {10.1017/S0956796807006430},
 acmid = {1296842},
 publisher = {Cambridge University Press},
 address = {New York, NY, USA},
}

@InProceedings{Harper-Lillibridge-1994,
  author = {Robert Harper and Mark Lillibridge},
  title = {A Type-Theoretic Approach to Higher-Order Modules
		  with Sharing},
  crossref = {POPL1994},
  pages = {123--137},
  authorsAddress = {CMU},
  WKloc = {A-0393},
  abstract = {The design of a module system for constructiong and
		  maintaining large programs is a difficult task that
		  raises a number of theoretical and practical
		  issues. A fundamental issue is the management of the
		  flow of information between program units at compile
		  time via the notion of an interface. Experience has
		  shown that fully opaque interfaces are akward to use
		  in practice since too much information is hidden,
		  and that fully transparent interfaces lead to
		  excessive interdependencies, creating problems for
		  maintenance and separate compilation. The
		  ``sharing'' specifications of Standard ML address
		  this issue by allowing the programmer to specify
		  equational relationships between types in separate
		  modules, but are not expressive enough to allow the
		  programmer complete control over the propagation of
		  type information between modules.

                  These problems are addressed from a type-theoretic
		  viewpoint by considering a calculus based on
		  Girard's system ${\sf F}_\omega$. The calculus
		  differs from those considered in previous studies by
		  relying exclusively on a new form of weak sum type
		  to propagate information at compile-time, in
		  contrast to approaches based on strong sums which
		  rely on substitution. The new form of sum type
		  allows for the specification of equational, as well
		  as type and kind, information in interfaces. This
		  provides complete control over the propagation of
		  compile-time information between program units and
		  is sufficient to encode in a straightforward way
		  most uses of type sharing specifications in Standard
		  ML. Modules are treated as ``first-class'' citizens,
		  and therefore the system supports higher-order
		  modules and some object-oriented programming idioms;
		  the language may be easily restricted to
		  ``second-class'' modules found in ML-like languages.}
}

@TechReport{Harper-Morriset-1994,
  author = {Robert Harper and Greg Morriset},
  title = {Compiling Polymorphism Using Intensional Type Analysis},
  institution = {School of Computer Science, Carnegie Mellon University},
  year = 1994,
  number = {CMU-CS-94-185},
  address = {Pittsburgh, PA 15213},
  note = {also published as Fox Memorandum CMU-CS-FOX-94-07},
  WKloc = {A-0361}
}

@InProceedings{Harper-Pierce-1991,
  author = {Robert Harper and Benjamin Pierce},
  title = {A Record Calculus Based on Symmetric Concatenation},
  pages = {131--142},
  abstract = {Type systems for operations extensible records form a
             foundation for statically typed languages addressing some
             aspects of object oriented programming and database
             applications. A number of primitive operations have been
             proposed: extending a record with a new field, overwriting an
             existing field, removing a field, and various kinds of
             concatenation. We show here that a record calculus based on a
             symmetric concatenation operator, where two records may be
             concatenated only if they have no overlapping fields, also
             captures the types of many other useful primitive record
             operations. ``Mergeability constraints'' are expressed directly
             using explicit annotations on type variables and constrained
             second-order type quantification instead of a rule of
             subsumption; we argue that the resulting system is more
             straightforward than subsumption-based alternatives.},
  crossref = {POPL1991},
  WKloc = {A-0174}
}

@InProceedings{Harris-Marlow-PeytonJones-2005,
 author = {Harris, Tim and Marlow, Simon and Jones, Simon Peyton},
 title = {Haskell on a shared-memory multiprocessor},
 booktitle = {Proceedings of the 2005 ACM SIGPLAN workshop on Haskell},
 series = {Haskell '05},
 year = {2005},
 isbn = {1-59593-071-X},
 location = {Tallinn, Estonia},
 pages = {49--61},
 numpages = {13},
 url = {http://doi.acm.org/10.1145/1088348.1088354},
 doi = {10.1145/1088348.1088354},
 acmid = {1088354},
 publisher = {ACM},
 address = {New York, NY, USA},
 WKloc = {doc/pap/BIB},
}

@InProceedings{Harris-Marlow-PeytonJones-Herlihy-2005,
  author = {Tim Harris and Simon Marlow and Peyton Jones, Simon and Maurice Herlihy},
  title =  {Composable Memory Transactions},
  crossref = {PPoPP2005},
  OPTpages = {},
  WKloc = {A-1600, doc/pap/BIB},
  URL = {http://research.microsoft.com/~simonpj/papers/stm/index.htm},
  abstract = {Writing concurrent programs is notoriously difficult,
     and is of increasing practical importance. A particular source of
     concern is that even correctly-implemented concurrency
     abstractions cannot be composed together to form larger
     abstractions. In this paper we present a new concurrency model,
     based on transactional memory, that offers far richer
     composition. All the usual benefits of transactional memory are
     present (e.g. freedom from deadlock), but in addition we describe
     new modular forms of blocking and choice that have been
     inaccessible in earlier work.}
}

@Article{Harrison-1979,
  author = {D. K. Harrison},
  title = {Double Coset and Orbit Spaces},
  journal = PACIF,
  volume = 80,
  year = 1979,
  pages = {451--491},
  bibliographies = {RelMiCS}
}

@Misc{ Harrison-1995,
  author = "J. Harrison",
  title = "Metatheory and reflection in theorem proving: A survey and critique",
  text = "J. Harrison. Metatheory and reflection in theorem proving: A survey and
    critique, 1995.",
  year = "1995",
  url = "citeseer.ist.psu.edu/harrison95metatheory.html",
  WKloc = {A-1568, doc/pap/BIB},
  abstract = {One way to ensure correctness of the inference
     performed by computer theorem provers is to force all proofs to
     be done step by step in a simple, more or less traditional,
     deductive system. Using techniques pioneered in Edinburgh LCF,
     this can be made palatable. However, some believe such an
     approach will never be efficient enough for large, complex
     proofs. One alternative, commonly called reflection, is to
     analyze proofs using a second layer of logic, a metalogic, and so
     justify abbreviating...}
}

@InProceedings{Harrison-1999,
  author = {John Harrison},
  title = {A Machine-Checked Theory of Floating Point Arithmetic},
  crossref = {TPHOL1999},
  pages = {113--130},
  WKloc = {A-1477},
  bibliographies = {Anand}
}

@Manual{Harrison-2000m,
  title = {The {HOL Light} Manual (1.1)},
  author = {John Harrison},
  organization = {University of Cambridge Computer Laboratory},
  URL = {http://www.cl.cam.ac.uk/users/jrh/},
  month = APR,
  year = 2000,
  WKloc = {A-1478, doc/pap/BIB},
  typos = {
    p. 65: CONS --> COND
   .}
}

@InProceedings{Harrison-Kieburtz-2002,
  author = {William L. Harrison and Richard B. Kieburtz},
  title = {Pattern-driven Reduction in Haskell},
  crossref = {WRS2002},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = {doc/pap/BIB},
  abstract = {Haskell is a functional programming language with nominally
      non-strict semantics, implying that evaluation of a Haskell
      expression proceeds by demand-driven reduc- tion. However, Haskell
      also provides pattern matching on arguments of functions, in let
      expressions and in the match clauses of case expressions.
      Pattern-matching requires data-driven reduction to the extent
      necessary to evaluate a pattern match or to bind variables introduced
      in a pattern. In this paper we provide both an ab- stract semantics
      and a logical characterization of pattern-matching in Haskell and the
      reduction order that it entails.}
}

@Article{Harrison-Kieburtz-2005,
  author = 	 {William L. Harrison and Richard B. Kieburtz},
  title = 	 {The Logic of Demand in {Haskell}},
  journal = 	 JFP,
  year = 	 2005,
  volume =	 15,
  number =	 6,
  pages =	 {837--891},
  URL = 	 {http://resolver.scholarsportal.info/resolve/09567968/v15i0006/837_tlodih},
  WKloc = 	 {A-1695, doc/pap/BIB},
  bibliographies = {PMC},
  abstract = {Haskell is a functional programming language whose
     evaluation is lazy by default. However, Haskell also provides
     pattern matching facilities which add a modicum of eagerness to
     its otherwise lazy default evaluation. This mixed or
     ``non-strict'' semantics can be quite difficult to reason
     with. This paper introduces a programming logic, P-logic, which
     neatly formalizes the mixed evaluation in Haskell
     pattern-matching as a logic, thereby simplifying the task of
     specifying and verifying Haskell programs. In p-logic, aspects of
     demand are reflected or represented within both the predicate
     language and its model theory, allowing for expressive and
     comprehensible program verification.}

}

@InProceedings{Harrison-Sheard-Hook-2002,
  author = {William L. Harrison and Timothy Sheard and James Hook},
  title = {Fine Control of Demand in {Haskell}},
  crossref = {MPC2002},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {doc/pap/BIB},
  OPTannote = {}
}

@InProceedings{Hartel-Langendoen-1992,
  author = {P. H. Hartel and K. G. Langendoen},
  title = {Benchmarking implementations of lazy functional languages},
  booktitle = {6th Functional programming languages
                     and computer architecture},
  publisher = {acm},
  pages = {341-349},
  address = {Copenhagen, Denmark},
  month = JUN,
  year = 1993
}

@InProceedings{Hasegawa-1991,
  title = {Parametricity of Extensionally Collapsed Term Models of
		  Polymorphism and Their Categorical Properties},
  author = {Ryu Hasegawa},
  pages = {495--512},
  crossref = {TACS1991},
  abstract = {In the preceding paper, the author proved that parametric
		  natural models have many categorical data types:
		  finite products, finite coproducts, initial and
		  terminal fixed points.  In this paper, we show the
		  second order minimum model is parametric, and thus
		  enjoys the property.  In addition to that, we give
		  representation of internal right and left Kan
		  extensions.  We also show that extensionally
		  collapsed models of closed types/terms collection
		  are partially parametric, and that they have a part
		  of the categorical data types above.}
}

@PhDThesis{Hasegawa-1997,
  author = {Masahito Hasegawa},
  title = {Models of Sharing Graphs, A Categorical Semantics of {\sf let} and {\sf letrec}},
  school = {University of Edinburgh},
  year = 1997,
  WKloc = {A-0878}
}

@InProceedings{Hasegawa-1997a,
  author = {Masahito Hasegawa},
  title = {Recursion from Cyclic Sharing: Traced Monoidal Categories and Models of Cyclic Lambda Calculi},
  crossref = {TLCA-1997},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0777},
  bibliographies = {Coconut}
}

@TechReport{Hasegawa-1999,
  author = {Masahito Hasegawa},
  title = {Categorical glueing and logical predicates for models of linear logic},
  institution = {Kyoto University},
  type = {Preprint},
  number = {RIMS-1223},
  year = 1999,
  WKloc = {A-0916, doc/pap/BIB},
  abstract = {We give a series of glueing constructions for categorical
              models of fragments of linear logic. Specifically,
              we consider the glueing of
              (i) symmetric monoidal closed categories
                  (models of Multiplicative Intuitionistic Linear Logic),
              (ii) symmetric monoidal adjunctions
                   (for interpreting the modality !) and
              (iii) $*$-autonomous categories
                    (models of Multiplicative Linear Logic);
              the glueing construction for $*$-autonomous categories
              is a mild generalization of the double glueing construction
              due to Hyland and Tan. Each of the glueing techniques
              can be used for creating interesting models of linear logic.
              In particular, we use them, together with the free symmetric
              monoidal cocompletion, for deriving Kripke-like
              parameterized logical predicates (logical relations)
              for the fragments of linear logic. As an application,
              we show full completeness results for translations
              between linear type theories.},
  URL = {http://www.kurims.kyoto-u.ac.jp/~hassei/papers/full.ps.gz,
         http://citeseer.nj.nec.com/details/hasegawa99categorical.html},
  bibliographies = {LogRel}
}

@Article{Hasegawa-2000,
  author = {Masahito Hasegawa},
  title = {Girard translation and logical predicates},
  journal = JFP,
  year = 2000,
  volume = 10,
  number = 1,
  pages = {77--89},
  month = JAN,
  URL = {http://www.kurims.kyoto-u.ac.jp/~hassei/papers/girard.html},
  abstract = {We present a short proof of a folklore result: the Girard
      translation from the simply typed lambda calculus to the linear
      lambda calculus is fully complete. The proof makes use of a notion of
      logical predicates for intuitionistic linear logic. While the main
      result is of independent interest, this paper can be read as a
      tutorial on this proof technique for reasoning about relations
      between type theories.},
  annote = {A-1112, doc/pap/BIB},
  bibliographies = {LogRel}
}

@InProceedings{Hasegawa-Kakutani-2001,
  author = {Masahito Hasegawa and Yoshihiko Kakutani},
  title = {Axioms for Recursion in Call-by-Value (Extended Abstract)},
  booktitle = {Proc.\null{} Foundations of Software Science and Computation Structures {(FoSSaCS 2001)}, Genova},
  pages = {246--260},
  year = 2001,
  volume = 2030,
  series = LNCS,
  month = APR,
  WKloc = {A-1113}
}

@Misc{Haskell-1.3-Monads-Proposal,
  author = {{Haskell 1.3 Committe}},
  title = {{A Proposal for Monadic I/O in Haskell 1.3}},
  howpublished = {WWW page URL: http://www.cl.cam.ac.uk/users/adg/io.html},
  year = 1994,
  month = AUG
}

@Article{Haskell-Report-1.2,
  author = {Paul Hudak and  Peyton Jones, Simon L. and Philip Wadler
		  and others},
  title = {Report on the Programming Language {Haskell}, A
		  Non-Strict Purely Functional Language, Version 1.2},
  journal = SIGPLAN,
  year = 1992,
  volume = 27,
  number = 5,
  bibliographies = {FP},
  note = {See also \url{http://haskell.org/}}
}

@Misc{Haskell-Report-1.3,
  author = {John Peterson and Kevin Hammond and Lennart Augustsson and
      Brian Boutel and Warren Burton and Joseph Fasel and Andrew D. Gordon
      and John Hughes and Paul Hudak and Thomas Johnsson and Marek Jones
      and Peyton Jones, Simon and Alastair Reid and Philip Wadler},
  title = {Report on the Programming Language {Haskell}, A
		  Non-Strict Purely Functional Language, Version 1.3},
  month = MAY,
  year = 1996,
  note = {see \textsf{URL: http://www.haskell.org/definition/}},
  WKloc = {B-0066}
}

@Book{Haskell98,
  author = {Peyton Jones, Simon and others},
  title = {The Revised Haskell 98 Report},
  publisher = CambridgeUP,
  year = 2003,
  note = {Also on \url{http://haskell.org/}},
  bibliographies = {FP},
  ISBN = {0521826144},
  OPTURL = {http://titles.cambridge.org/catalogue.asp?isbn=0521826144}
}

@Manual{Haskell98FFI,
  author =	 {Manuel Chakravarty and others},
  title =	 {{The Haskell 98 Foreign Function Interface 1.0}:
An Addendum to the {Haskell} 98 Report},
  note = {\textsf{http://www.cse.unsw.edu.au/\~{}chak/haskell/ffi/}},
  OPTnote = {\url{http://www.cse.unsw.edu.au/~chak/haskell/ffi/}},
  month =	 APR,
  year =	 2005
}

@InProceedings{Hassan-Jiresch-SatoShinya-2010,
  author =       {Abubakar Hassan and Eugen Jiresch and Shinya Sato},
  title =        {An Implementation of Nested Pattern Matching in Interaction Nets},
  booktitle = {Proceedings Tenth International Workshop on Rule-Based Programming,
               {RULE} 2009, Bras{\'{\i}}lia, Brazil, 28th June 2009.},
  pages     = {13--25},
  year      = {2009},
  crossref  = {RULE2009},
  DOIURL    = {http://dx.doi.org/10.4204/EPTCS.21.2},
  DOI       = {10.4204/EPTCS.21.2},
  timestamp = {Mon, 28 Oct 2013 16:56:55 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/abs-1003-4562},
  bibsource = {dblp computer science bibliography, http://dblp.org},
  abstract = {Reduction rules in interaction nets are constrained to
    pattern match exactly one argument at a time.
    Consequently, a programmer has to introduce auxiliary rules
    to perform more sophisticated matches.
    In this paper, we describe the design and implementation
    of a system for interaction nets
    which allows nested pattern matching on interaction rules.
    We achieve a system that provides convenient ways
    to express interaction net programs without defining auxiliary rules.}
}

@Article{Hassan-Mackie-Sato-2009,
  author =       {Abubakar Hassan and Ian Mackie and Shinya Sato},
  title =        {Compilation of Interaction Nets},
  journal =      ENTCS,
  year =         2009,
  DOI =          {10.1016/j.entcs.2009.10.018},
  DOIURL =       {http://dx.doi.org/10.1016/j.entcs.2009.10.018},
  volume =    253,
  number =    4,
  pages =     {73--90},
  OPTnote =      {Proc.~Fifth International Workshop on Computing with Terms and Graphs (TERMGRAPH 2009)},
  note =      {Proc.~TERMGRAPH 2009},
  abstract =    {This paper is about a new implementation technique for interaction nets --- a visual programming language based on graph rewriting. We compile interaction nets to C, which offers a robust and efficient implementation, in addition to portability. In the presentation of this work we extend the interaction net programming paradigm to introduce a number of features which make it a practical programming language.}
}

@InProceedings{Hassan-Mackie-Sato-2010,
  author =       {Abubakar Hassan and Ian Mackie and Shinya Sato},
  title =        {A lightweight abstract machine for interaction nets},
  crossref =      {GTVMT2010},
  ee = {http://journal.ub.tu-berlin.de/eceasst/article/view/416},
  pages =     {9.1--9.12},
  bibliographies = {INet},
  abstract = {Abstract: We present a new abstract machine for interaction nets and demonstrate
that an implementation based on the ideas is significantly more efficient than existing
interaction net evaluators. The machine, which is founded on a chemical abstract
machine formulation of interaction nets, is a simplification of a previous abstract
machine for interaction nets. This machine, together with an implementation, is
at the heart of current work on using interaction nets as a new foundation as an
intermediate language for compiler technology.}
}

@InProceedings{Hassan-Mackie-Sato-2015_TermGraph2014,
  author    = {Abubakar Hassan and
               Ian Mackie and
               Shinya Sato},
  title     = {An Implementation Model for Interaction Nets},
  pages     = {66--80},
  crossref  = {TERMGRAPH2014_EPTCS},
  DOIURL    = {http://dx.doi.org/10.4204/EPTCS.183.5},
  DOI       = {10.4204/EPTCS.183.5},
}

@Article{Hasselbring-2000,
  author = {Wilhelm Hasselbring},
  title = {Programming languages and systems for prototyping concurrent applications},
  journal = ACMCS,
  year = 2000,
  volume = 32,
  number = 1,
  pages = {43--79},
  month = MAR,
  URL = {http://portal.acm.org/citation.cfm?id=349199&coll=portal&dl=ACM&idx=J204&part=journal&WantType=Journals&title=CSUR&CFID=380924&CFTOKEN=73907084},
  WKloc = {A-1240, doc/pap/BIB}
}

@Article{Hatcliff-1998,
  author = {John Hatcliff},
  title = {Foundations for Partial Evaluation of Functional Programs
                  with Computational Effects},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 13},
  WKloc = {A-0902, 49--56},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Hatcliff-Danvy-1994,
  author = {John Hatcliff and Oliver Danvy},
  title = {A Generic Accouint of Continuation-Passing Styles},
  crossref = {POPL1994},
  pages = {458--471},
  abstract = {We unify previous work on the continuation-passing
		  style (CPS) transformations in a generic framework
		  based on Moggi's computational meta-language. $\ldots$}
}

@Book{Hattensperger-1997,
  author = {Claudia Hattensperger},
  title = {{Rechnergest\"utztes Beweisen in heterogenen Relationenalgebren}},
  publisher = {Dissertationsverlag NG Kopierladen, M\"unchen},
  note = {ISBN 3-928536-99-0; zugl.\null{} Dissertation an der Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur Informatik},
  year = 1997,
  month = DEC,
  bibliographies = {RelMiCS}
}

@InProceedings{Hattensperger-Berghammer-Schmidt-1993,
  author = {Claudia Hattensperger and Rudolf Berghammer and Gunther Schmidt},
  title = {{RALF} --- {A} relation-algebraic formula manipulation system
      and proof checker. {Notes} to a system demonstration},
  crossref = {AMAST1993},
  pages = {405--406},
  bibliographies = {RelMiCS},
  directory = {/home/tools/papers}
}

@Book{Hausser-1999,
  author = {Roland Hausser},
  title = {Foundation of Computational Linguistics. Man-Machine Communication in Natural Language},
  year = 1999,
  publisher = Springer,
  UniBwM = {KYB800/YE7877},
  ISBN = {3-540-66015-1},
  pages = {xii+534}
}

@Article{Havelund-Shankar-1996,
  author = {K. Havelund and N. Shankar},
  title = {Experiments in Theorem Proving and Model Checking for
                 Protocol Verification},
  journal = {Lecture Notes in Computer Science},
  volume = 1051,
  pages = {662--??},
  year = 1996,
  coden = {LNCSD9},
  ISSN = {0302-9743},
  bibdate = {Sat May 11 13:45:32 MDT 1996},
  WKloc = {A-0936},
  bibliographies = {SpecTech}
}

@Article{Hawkins-1975,
  author = {Benjamin S. Hawkins},
  title = {A Compendium of {C.\null{} S.\null{} Peirce's} 1866--1885 Work},
  journal = NOTRE,
  year = 1975,
  volume = 16,
  pages = {109--115},
  bibliographies = {RelMiCS}
}

@Article{Hawkins-1979,
  author = {Benjamin S. Hawkins},
  title = {A Reassessment of {De Morgan, Augustus's} Logic of
		Relations: a Documentary Reconstruction},
  journal = INTLOG,
  year = 1979,
  volume = 10,
  pages = {32--61},
  bibliographies = {RelMiCS}
}

@Article{Hawkins-1986,
  author = {Benjamin S. Hawkins},
  title = {{J.\null{} W.\null{} Dauben on C.\null{} S.\null{} Peirce's}
      Place in Mathematics: some Reflections},
  journal = INTLOG,
  year = 1986,
  volume = 17,
  pages = {62--69},
  bibliographies = {RelMiCS}
}

@Book{Hay-2001,
  author =	 {Jonathan Hay},
  title = 	 {{Shitao}: Painting and Modernity in Early {Qing} China},
  publisher = 	 {Cambridge University Press},
  year = 	 2001,
  address =	 {New York},
  bibliographies = {China}
}

@InProceedings{Hayashi-1991,
  title = {Singleton, Union and Intersection Types for Program Extraction},
  author = {Susumu Hayashi},
  pages = {701--730},
  crossref = {TACS1991},
  abstract = {Two type theories, ATT and ATTT, are introduced.  ATT is
		  an impredicative type theory closely related to the
		  polymorphic type theory of implicit typing of
		  MacQueen et al. [MPS86].  ATTT is another version of
		  ATT that extends the Girard-Reynolds second order
		  lambda calculus.  ATT has notions of intersection,
		  union and singelton types. ATTT has a notion of
		  refinement types as in the type system for ML by
		  Freeman and Pfenning [FP91], plus intersection and
		  union of refinement types and singleton refinement
		  types.  We will show how singleton, union and
		  intersection types serve for development of programs
		  without unnecessary codes via a variant of the
		  Curry-Howard isomorphism.  More exactly, they give a
		  way to write types as specifications of programs
		  without unnecessary codes which is inevitable in the
		  usual Curry-Howard isomorphism.}
}

@TechReport{Hayden-1997t,
  author = {Mark Hayden},
  title = {Ensemble Tutorial},
  year = 1997,
  month = MAY,
  institution = {Cornell University},
  WKloc = {A-0780}
}

@Manual{Hayden-2001,
  author = {Mark Hayden},
  title = {Ensemble Tutorial},
  organization = {Cornell University},
  month = JUN,
  year = 2001,
  WKloc = {B-0125}
}

@Book{Hayes-1993,
  editor = {I. J. Hayes},
  title = {Specification Case Studies},
  publisher = {Prentice Hall International Series in Computer Science},
  edition = {2nd},
  length = 350,
  year = 1993,
  ISBN = {13-832544-8},
  price = {\pounds21.99 (\$37.00) paperback},
  annote = {This is a revised edition of the first ever book on Z,
                  originally published in 1987; it contains substantial
                  changes to every chapter. The notation has been revised to
                  be consistent with {\em The Z Notation: A Reference Manual}
                  by Mike Spivey \cite{z:spiv92}. The CAVIAR chapter has been
                  extensively changed to make use of a form of
                  modularization. \par Divided into four sections, the first
                  provides tutorial examples of specifications, the second is
                  devoted to the area of software engineering, the third
                  covers distributed computing, analyzing the role of
                  mathematical specification, and the fourth part covers the
                  IBM CICS transaction processing system. Appendices include
                  comprehensive glossaries of the Z mathematical and schema
                  notation. The book will be of interest to the professional
                  software engineer involved in designing and specifying
                  large software projects. \par The other contributors are
                  W.~Flinn, R.~B.\ Gimson, S.~King, C.~C.\ Morgan, I.~H.\
                  S{\o}rensen and B.~A.\ Sufrin.},
  UniBwM = {INF460/Q4839}
}

@TechReport{Hayes-Jones-Nicholls-1993,
  author = {Ian J. Hayes and Cliff B. Jones and J. E. Nicholls},
  title = {Understanding the differences between {VDM} and {Z}},
  year = 1993,
  institution = {University of Manchester, Department of Computer Science},
  number = {UMCS-93-8-1},
  WKloc = {A-0647}
}

@InCollection{He-1994,
  author = {He, Jifeng},
  title = {From {CSP} to Hybrid Systems},
  crossref = {Roscoe-1994},
  pages = {171--189},
  chapter = 11,
  OPTnote = {},
  OPTannote = {}
}

@Article{He-Hoare-1999,
  author = {He, Jifeng and C.A.R. Hoare},
  title = {Linking theories in probabilistic programming},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {205--218},
  bibliographies = {RelMiCS},
  abstract = {This paper presents a theory of probabilistic programming
      based on relational calculus through a series of stages; each stage
      concentrates on a different and smaller class of program, defined by
      the healthiness conditions of increasing strength. At each stage we
      show that the notations of the probabilistic language conserve the
      healthiness conditions of their operands, and that every theory
      conserves the definition of recursion.},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/30/abstract.html},
  WKloc = {A-0806}
}

@InProceedings{He-Hoare-Sanders-1986,
  title = {Data Refinement Refined},
  author = {Jifeng He and C. A. R. Hoare and J. W. Sanders},
  crossref = {ESOP1986},
  pages = {187--196},
  bibliographies = {RelMiCS},
  WKloc = {A-1325}
}

@InProceedings{He-Josephs-Hoare-1990,
  title = {A Theory of Synchrony and Asynchrony},
  pages = {459-478},
  crossref = {IFIP1990},
  author = {He, Jifeng and M. B. Josephs and C. A. R. Hoare},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@InProceedings{Hearn-Meinke-1993,
  author = {B.M. Hearn and K. Meinke},
  title = {{\it ATLAS}: A typed language for algebraic specification},
  crossref = {HOA1993},
  pages = {146--168},
  WKloc = {?},
  abstract = {We introduce an implementation of rewriting for
		  type and combinator terms called {\it ATLAS}. This
		  system implements the algebraic and term rewriting
		  theory for abstract types and combinators developed
		  in \cite{Meinke-1991,Meinke-1992b}. The system is intended to
		  support the execution of equational specifications
		  of abstract types and combinators. The type checking
		  algorithms of the system also allow it to function
		  as a framework for defining logics and proof
		  checking. We present a short tutorial introduction
		  to {\it ATLAS} by means of examples taken from first
		  and higher order algebraic specifications and logics.},
  bibliographies = {RelMiCS}
}

@Book{Heath-1966,
  author = {Heath, Peter},
  title = {On the Syllogism, and Other Logical Writings.},
  publisher = {Routledge and Kegan Paul},
  year = 1966,
  bibliographies = {RelMiCS}
}

@PhDThesis{Heckel-1998,
  author = {Reiko Heckel},
  title = {Open Graph Transformation Systems, A New Approach to the Compositional Modelling of Concurrent and Reactive Systems},
  school = {Fachbereich 13 --- Informatik der Technischen Universit\"at Berlin},
  year = 1998,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {}
}

@Article{Heckel-Corradini-Ehrig-Loewe-1996,
  author = {Reiko Heckel and Andrea Corradini and Hartmut Ehrig and Michael L{\"o}we},
  title = {Horizontal and Vertical Structuring of Typed Graph Transformation Systems},
  journal = MSCS,
  volume = 6,
  number = 6,
  pages = {613--648},
  year = {1996},
  WKloc = {doc/pap/BIB}
}

@InCollection{Heckel-Ehrig-Engels-Taentzer-1999,
  author = {Reiko Heckel and Hartmut Ehrig and Gregor Engels and Gabi Taentzer},
  title = {Classification and Comparison of Module Concepts for Graph Transformation Systems},
  crossref = {HBGraTraII},
  chapter = 17,
  pages = {669--689}
}

@InCollection{Heckel-Ehrig-Engels-Taentzer-1999a,
  author = {Reiko Heckel and Hartmut Ehrig and Gregor Engels and Gabi Taentzer},
  title = {A View-Based Approach to System Modeling Based on Open Graph Transformation Systems},
  crossref = {HBGraTraII},
  chapter = 16,
  pages = {639--668},
  WKloc = {A-1355, doc/pap/BIB}
}

@TechReport{Heckel-Ehrig-Wolter-Corradini-1997,
  author = {Heckel, Reiko and Ehrig, Hartmut and Wolter, Uwe and Corradini, Andrea},
  title = {Loose Semantics and Constraints for Graph Transformation Systems},
  institution = {Fachbereich Informatik, TU Berlin},
  year = 1997,
  number = {97/7},
  WKloc = {A-1183, doc/pap/BIB},
  keywords = {double-pullback transitions}
}

@InProceedings{Heckel-Kuester-Taentzer-2002,
  author =       {Reiko Heckel and Jochen Malte K{\"u}ster and Gabriele Taentzer},
  title =        {Confluence of Typed Attributed Graph Transformation Systems},
  crossref =  {ICGT2002},
  DOI = {10.1007/3-540-45832-8_14},
  pages =     {161--176},
  abstract =    {The issue of confluence is of major importance for the successful application of attributed graph transformation, such as automated translation of UML models into semantic domains. Whereas termination is undecidable in general and must be established by carefully designing the rules, local confluence can be shown for term rewriting and graph rewriting using the concept of critical pairs. In this paper, we discuss typed attributed graph transformation using a new simplified notion of attribution. For this kind of attributed graph transformation systems we establish a definition of critical pairs and prove a critical pair lemma, stating that local confluence follows from confluence of all critical pairs.}
}

@Article{Heckel-Llabres-Ehrig-Orejas-2000,
  author = {Reiko Heckel and Merc{\`e} Llabre{\'e}s and Hartmut Ehrig and Fernando Orejas},
  title = {Concurrency of Double-Pullback Graph Transitions},
  journal = {Applied Categorical Structures},
  year = 2000,
  OPTpages = {85--102},
  WKloc = {A-1187, doc/pap/BIB (Extended Abstract)}
}

@InProceedings{Heckmann-1992,
  author = {Reinhold Heckmann},
  title = {Power Domains Supporting Recursion and Failure},
  pages = {165--181},
  crossref = {CAAP92},
  abstract = {Following the program of Moggi, the semantics of a
		  simple non-deterministic functional language with
		  recursion and failure is described by a monad. We
		  show that this monad cannot be any of the known
		  power domain constructions, because they do not
		  handle non-termination properly. Instead, a novel
		  construction is proposed and investigated. It
		  embodies both non-determinism (choice and failure)
		  and possible non-termination caused by recursion.}
}

@PhdThesis{Heeren-2005,
  author =       {Bastiaan Heeren},
  title =        {Top Quality Type Error Messages},
  school =       {Universiteit Utrecht, The Netherlands},
  year =         2005,
  month  = sep,
  url    = {http://www.cs.uu.nl/people/bastiaan/phdthesis}
}

@InProceedings{Heeren-Hage-Swierstra-2003,
  author =       {Bastiaan Heeren and Jurriaan Hage and S. Doaitse Swierstra},
  title =        {Constraint Based Type Inferencing in Helium},
  booktitle = {Workshop on Immediate Applications of Constraint Programming, {CP-03}},
  year =      2003,
  abstract =    {The Helium compiler implements a significant subset of the functional programming language Haskell. One of the major motivations for developing it was to yield understandable and appropriate type error messages. The separation between the generation, the ordering and the solving of constraints on types has led to a flexible framework which has been employed successfully in a classroom setting. Among its many advantages are the possibility to plug in heuristics for deciding the most likely source of a type error, the generation of multiple type error messages during a single compilation, and the possibility for programmers to tune the type inference process and resulting messages to their own needs without having to know any of the details of the implementation.}
}

@TechReport{Heering-1992b,
  author = {Jan Heering},
  email = {jan@cwi.nl},
  title = {Second-Order Algebraic Specification of Static
                 Semantics},
  institution = {CWI},
  year = 1992,
  month = dec,
  number = {CS-R9254},
  URL = {ftp://ftp.cwi.nl/pub/gipe/reports/Hee92b.ps.Z},
  pages = 16,
  source = {dept. library / copy},
  checked = 19940208,
  abstract = {Higher-order algebraic specification is a synthesis of
                 first-order algebraic specification and higher-order
                 functional programming which is both logically
                 appealing as well as considerably more expressive than
                 its two predecessors. To illustrate this, we describe
                 the static semantics of a simple block-structured
                 programming language using second-order equations in
                 addition to first order ones. The specification has a
                 highly non-deterministic character and does not use a
                 type environment. Furthermore, it supports error
                 recovery and early detection of errors in incomplete
                 programs.}
}

@Unpublished{Heering-1994,
  author = {Jan Heering},
  title = {Second-Order Algebraic Specification of Static Semantics},
  WKloc = {A-0527},
  year = 1994,
  month = NOV,
  note = {presented at HOA '93},
  bibliographies = {RelMiCS}
}

@Article{Hehner-1984,
  author = {Eric C. R. Hehner},
  title = {Predicative Programming, {Parts I and II}},
  journal = CACM,
  year = 1984,
  volume = 27,
  number = 2,
  pages = {134--151},
  month = FEB,
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {A-1290, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/69610.357988},
  abstract = {Programs are given a new semantics with the merit that a
      specification written as a first-order predicate can be refined, step
      by step, to a program via the rules of Predicate Calculus. The
      semantics allows a free mixture of predicate and programming
      notations, and manipulation of programs.

      Part I of this two-part paper presented a new semantics of programs.
      Each program is considered to be a predicate, in a restricted
      notation, that specifies the observable behavior of a computer
      executing the program. We considered a variety of notations,
      including assignment, composition (semicolon), deterministic choice
      (if), nondeterministic choice, definition (nonrecursive and
      recursive), and variable declaration. We did not consider any input
      or output notations, or concurrency; that is the subject of Part II.
      We assume the reader is familiar with Part I, so that we can build on
      ideas presented there.}
}

@Article{Hehner-1984a,
  author = {Eric C. R. Hehner},
  title = {Predicative Programming, {Part I}},
  journal = CACM,
  year = 1984,
  volume = 27,
  number = 2,
  pages = {134--143},
  month = FEB,
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {A-1290, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/69610.357988},
  abstract = {Programs are given a new semantics with the merit that a
      specification written as a first-order predicate can be refined, step
      by step, to a program via the rules of Predicate Calculus. The
      semantics allows a free mixture of predicate and programming
      notations, and manipulation of programs.}
}

@Article{Hehner-1984b,
  author = {Eric C. R. Hehner},
  title = {Predicative Programming, {Parts I and II}},
  journal = CACM,
  year = 1984,
  volume = 27,
  number = 2,
  pages = {134--151},
  month = FEB,
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {A-1290, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/69610.357990},
  abstract = {Part I of this two-part paper presented a new semantics of
      programs. Each program is considered to be a predicate, in a
      restricted notation, that specifies the observable behavior of a
      computer executing the program. We considered a variety of notations,
      including assignment, composition (semicolon), deterministic choice
      (if), nondeterministic choice, definition (nonrecursive and
      recursive), and variable declaration. We did not consider any input
      or output notations, or concurrency; that is the subject of Part II.
      We assume the reader is familiar with Part I, so that we can build on
      ideas presented there.}
}

@InCollection{Hehner-1994,
  author = {Eric C. R. Hehner},
  title = {Abstractions of Time},
  crossref = {Roscoe-1994},
  pages = {191--210},
  chapter = 12,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Hehner-1999,
  author = {Eric C.R. Hehner},
  title = {Formalism and the Variable},
  booktitle = {Symposium on the occasion of the retirement of C.A.R.Hoare, Oxford},
  year = 1999,
  month = September,
  WKloc = {A-0874}
}

@Book{Hehner-2005,
  ALTauthor = 	 {},
  ALTeditor = 	 {},
  title = 	 {},
  publisher = 	 {},
  year = 	 {},
  URL = 	 {http://www.cs.toronto.edu/~hehner/aPToP/},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@TechReport{Heilbrunner-Mast-Schmitz-1987,
  year = 1987,
  title = {{Exploring Ada Concepts and Techniques}},
  number = 8706,
  month = SEP,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {S. Heilbrunner and A. Mast and Lothar Schmitz}
}

@Article{Heilbrunner-Schmitz-1981,
  year = 1981,
  volume = 2,
  title = {{Zur attributierten Grammatik von PEARL}},
  number = 3,
  month = SEP,
  journal = {PEARL Rundschau},
  author = {S. Heilbrunner and Lothar Schmitz}
}

@Article{Heilbrunner-Schmitz-1991,
  year = 1991,
  volume = 80,
  title = {An Efficient Recognizer for the Boolean Closure of Context-free
      Languages},
  journal = {Theoretical Computer Science},
  author = {S. Heilbrunner and Lothar Schmitz}
}

@InCollection{Heim-1983,
  author = {Heim, I.},
  address = {Berlin},
  booktitle = {Meaning, Use and Interpretation of Language},
  editor = {B{\"{a}}uerle, R. and Schwarze, C. and Stechow, A.~von},
  publisher = GRUYTER,
  title = {File change semantics and the familiarity theory of definites},
  year = 1983,
  bibliographies = {RelMiCS}
}

@Inbook{Heindel2010,
  author="Heindel, Tobias",
  title="Hereditary Pushouts Reconsidered",
  crossref = {ICGT2010},
  pages="250--265",
  WKloc = {doc/pap/BIB},
  DOI="10.1007/978-3-642-15928-2_17",
  DOIURL="http://dx.doi.org/10.1007/978-3-642-15928-2_17",
  abstract = {The introduction of adhesive categories revived interest in the study of properties of pushouts with respect to pullbacks, which started over thirty years ago in the category of graphs. Adhesive categories provide a single property of pushouts that suffices to derive lemmas that are essential for central theorems of double pushout rewriting such as the local Church-Rosser Theorem.

  The present paper shows that the same lemmas already hold for pushouts that are hereditary, i.e. those pushouts that remain pushouts when they are embedded into the associated category of partial maps. Hereditary pushouts – a twenty year old concept – induce a generalization of adhesive categories, which will be dubbed partial map adhesive. An application relevant category that does not fit the framework of adhesive categories and its variations in the literature will serve as an illustrating example of a partial map adhesive category.}
}

@InProceedings{Heindel-2010_hereditary,
  title={Hereditary pushouts reconsidered},
  author={Heindel, Tobias},
  crossref={ICGT2010},
  OPTbooktitle={International Conference on Graph Transformation},
  pages={250--265},
  OPTyear={2010},
  OPTorganization={Springer Berlin Heidelberg},
  abstract = {The introduction of adhesive categories revived interest
     in the study of properties of pushouts with respect to pullbacks,
     which started over thirty years ago in the category of graphs.
     Adhesive categories provide a single property of pushouts that suffices
     to derive lemmas that are essential for central theorems
     of double pushout rewriting such as the local Church-Rosser Theorem.

    The present paper shows that the same lemmas already hold
     for pushouts that are hereditary,
     i.e. those pushouts that remain pushouts
     when they are embedded into the associated category of partial maps.
     Hereditary pushouts – a twenty year old concept –
     induce a generalization of adhesive categories,
     which will be dubbed partial map adhesive.
     An application relevant category
     that does not fit the framework of adhesive categories
     and its variations in the literature
     will serve as an illustrating example of a partial map adhesive category.}
}

@InProceedings{Heindel-Sobocinski-2009,
  author =       {Tobias Heindel and Pawe{\l} Soboci{\'n}ski},
  title =        {{Van Kampen} Colimits as Bicolimits in {Span}},
  crossref =     {CALCO2009},
  pages =     {335--349},
  WKloc =        {A-1759, doc/pap/BIB},
  DOI =    {10.1007/978-3-642-03741-2_23},
  abstract =    {The exactness properties of coproducts in extensive categories
    and pushouts along monos in adhesive categories
    have found various applications in theoretical computer science,
    e.g. in program semantics, data type theory and rewriting.
    We show that these properties can be understood
    as a single universal property in the associated bicategory of spans.
    To this end, we first provide a general notion of Van Kampen cocone
    that specialises to the above colimits.
    The main result states that Van Kampen cocones can be characterised
    as exactly those diagrams in $\mathbb{C}$
    that induce bicolimit diagrams
    in the bicategory of spans $\mathcal{S}pan_{\mathbb{C}}$,
    provided that $\mathbb{C}$ has pullbacks and enough colimits.}
}

@TechReport{Heinle-1991,
  author = {Wolfgang Heinle},
  title = {A Relation Algebraic Approach to Modal Correspondences},
  institution = U_TUM,
  year = 1991,
  number = {TUM-I8902},
  file = {~kahl/doc/pap/schlingloff/Heinle-1991.ps.gz},
  WKloc = {A-0251},
  bibliographies = {RelMiCS},
  abstract = {Usually, modal correspondences are investigated
		  between monomodal and classical predicate logic
		  formulas (van Benthem[76]). We want to propose a more
		  algebraic view, concentrating on correspondences
		  between general multimodal formulas (seen in the
		  context of modal algebras), and relation algebraic
		  formulas (which may be classified as restricted
		  predicate logic formulas). Correspondences will be
		  obtained quite systematically by means of relation
		  algebraic principles of extensionality. For first
		  order expressible correspondences, these will
		  provide for the quantifier elimination, as well as
		  they give the translation of all modal expressible
		  relation algebraic primitives. This leads to a
		  simple method of translation of certain relation
		  algebraic formulas into their multimodal
		  counterparts, which then have to be proved
		  equivalent to the usual monomodal
		  correspondences. Further, we discuss how (by the use
		  of binary operators) the modal operator principle
		  could be extended to capture the entire relation
		  algebra by means of modal correspondences.},
  contents = {1. Basic structures
                  1.1 Relation algebras
                      Subsets andpoints, point axiom
                      Extensionality
                  1.2 Modal algebras
                  2. Interpretation of modal formulas in relation
		       algebra
                  3. Correspondences
                     L\"ob formula, irreflexive terminal orderings
                     Relational complement and infimum, invariance
		       properties
                  4. Other interpretations of modal algebras in
		       relation algebras
                  References
                  Table of correspondences}
}

@Book{Heinle-1995,
  author = {W. Heinle},
  title = {Expressivity and Definability in Extended Modal Languages},
  publisher = SHAKER,
  year = 1995,
  address = {Aachen},
  bibliographies = {RelMiCS}
}

@Unpublished{Heinle-Schlingloff-1994,
  author = {Wolfgang Heinle and Bernd-Holger Schlingloff},
  title = {Modal Rule Correspondences},
  note = {private communication, draft},
  file = {~kahl/doc/pap/schlingloff/Heinle-Schlingloff-1994.ps.gz},
  year = 1994,
  month = JUL,
  authorsAddress = {\{heinle|schlingl\}\@informatik.tu-muenchen.de},
  WKloc = {A-0250}
}

@TechReport{Heinsohn-Kudenko-Nebel-Profitlich-1992,
  year = 1992,
  type = {Research Report},
  title = {An Empirical Analysis of
                  Terminological Representation Systems},
  number = {RR-92-16},
  note = {An abridged version of this paper has been published in
		 {\em Proc.\null{} of AAAI-92}},
  institution = DFKI,
  author = {Jochen Heinsohn and Daniel Kudenko and
                  Bernhard Nebel and Hans-J{\"u}rgen Profitlich},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS},
  abstract = {The family of terminological representation systems has
		  its roots in the representation system KL-ONE.
		  Since the development of this system more than a
		  dozen similar representation systems have been
		  developed by various research groups.  These systems
		  vary along a number of dimensions.  In this paper,
		  we present the results of an empirical analysis of
		  six such systems.  Surprisingly, the systems turned
		  out to be quite diverse leading to problems when
		  transporting knowledge bases from one system to
		  another. Additionally, the runtime performance
		  between different systems and knowledge bases varied
		  more than we expected. Finally, our empirical
		  runtime performance results give an idea of what
		  runtime performance to expect from such
		  representation systems. These findings complement
		  previously reported analytical results about the
		  computational complexity of reasoning in such systems.}
}

@InProceedings{Heitmeyer-2001,
  author = 	 {Constance Heitmeyer},
  title = 	 {Applying `Practical' Formal Methods to the Specification and Analysis of Security Properties},
  booktitle =	 {Proc. Information Assurance in Computer Networks {(MMM-ACNS 2001), St. Petersburg, Russia, May 21--23, 2001}},
  year =	 2001,
  volume =	 2052,
  series =	 LNCS,
  publisher =	 Springer,
  abstract = 	 {The SCR (Software Cost Reduction) toolset contains tools for specifying, debugging, and verifying system and software requirements. The utility of the SCR tools in detecting specification errors, many involving safety properties, has been demonstrated recently in projects involving practical systems, such as the International Space Station, a flight guidance system, and a U.S. weapons system. This paper briefly describes our experience in applying the tools in the development of two secure systems: a communications device and a biometrics standard for user authentication.}
}

@InProceedings{{Heitmeyer-200X,
  author = 	 {Constance Heitmeyer},
  title = 	 {Software Cost Reduction},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {$\geq$ 2001 \unfinished},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1712},
  OPTannote = 	 {}
}

@Article{Heitmeyer-Jeffords-Labaw-1996,
  author = {Constance L. Heitmeyer and Ralph D. Jeffords and Bruce G. Labaw},
  title = {Automated consistency checking of requirements specifications},
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {5},
  number = {3},
  year = {1996},
  issn = {1049-331X},
  pages = {231--261},
  URL = {http://portal.acm.org/citation.cfm?id=234431},
  doi = {http://doi.acm.org/10.1145/234426.234431},
  publisher = {ACM Press},
  address = {New York, NY, USA},
  bibliographies = {Tables},
  annote = {explicit state machine semantics for SCR tables},
  abstract = { This article describes a formal analysis
       technique, called consistency checking, for automatic detection
       of errors, such as type errors, nondeterminism, missing cases,
       and circular definitions, in requirements specifications. The
       technique is designed to analyze requirements specifications
       expressed in the SCR (Software Cost Reduction) tabular
       notation. As background, the SCR approach to specifying
       requirements is reviewed. To provide a formal semantics for the
       SCR notation and a foundation for consistency checking, a
       formal requirements model is introduced; the model represents a
       software system as a finite-state automation which produces
       externally visible outputs in response to changes in monitored
       environmental quantities. Results of two experiments are
       presented which evaluated the utility and scalability of our
       technique for consistency checking in real-world avionics
       application. The role of consistency checking during the
       requirements phase of software development is discussed.}
}

@MastersThesis{Held-1991,
  year = 1991,
  title = {Spezifikation und {Implementierung eines Vergleichs- und Transformationssystems f\"ur eine typisierte DAG-Sprache als HOPS-Teil\-system}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 38/91},
  type = {Diplomarbeit},
  note = {ID 38/91},
  month = DEC,
  author = {Christian Held}
}

@Misc{Held-Zimmermann-1990,
  keywords = {HOPS2},
  year = 1990,
  title = {Ausgabeschnittstelle von {HOPS}},
  month = SEP,
  howpublished = {Trimesterarbeit IT 35/90, Universit\"at der
		  Bundeswehr M\"unchen, Fakult\"at f\"ur Informatik},
  author = {Christian Held and Thomas Zimmermann}
}

@InProceedings{Helke-Neustupny-Santen-1997,
  author = {S. Helke and T. Neustupny and T. Santen},
  title = {Automating Test Case Generation from {Z}
                 Specifications with {Isabelle}},
  crossref = {ZUM1997},
  pages = {52--71},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiS}
}

@InProceedings{Hella-Kolaitis-Luosto-1994,
  title = {How to Define a Linear Order on Finite Models},
  author = {Lauri Hella and Phokion Kolaitis and Kerkko Luosto},
  pages = {40--49},
  crossref = {LICS9},
  abstract = {We embark on a systematic investigation of the definability
      of linear order on classes of finite rigid structures. We obtain
      upper and lower bounds for the expressibility of linear order in
      various logics that have been studied extensively in finite model
      theory, such as fixpoint logic~FP, partial fixpoint logic~PFP,
      infinitary logic~${\cal L}^\omega_{\infty\omega}$ with a finite
      number of variables, as wll as the closures of these logics under
      implicit definitions. Moreover, we show that the upper and lower
      bounds established here can not be improved dramatically, unless
      outstanding conjectures in complexity theory are resolved at the same
      time.}
}

@InProceedings{Helmink-1990,
  author = {Leen Helmink},
  title = {Resolution and Type Theory},
  crossref = {ESOP1990},
  pages = {197--211},
  abstract = {In this paper, an inference mechanism is proposed
		  for proof construction in Constructive Type
		  Theory. An interactive system that implements this
		  method has been developed.},
  keywords = {Type Theory, Calculus of Construction, Typed Lambda
		  Calculus, Natural Deduction}
}

@InProceedings{Hemaspaandra-1994,
  title = {Complexity Transfer for Modal Logic (Extended Abstract)},
  author = {Edith Hemaspaandra},
  pages = {164--173},
  crossref = {LICS9},
  abstract = {In this paper, we prove general theorems about the
      relationship between the complexity of multi-modal logics and the
      complexity of their uni-model fragments. Halper and Moses [Halpern
      and Moses, 1985] show that the complexity of a multi-modal logic
      without any interaction between the modalities may be higher than the
      complexity of the individual fragments. In this paper, we show that
      under reasonable assumptions the complexity can increase only if the
      complexity of all the uni-modal fragments is below PSPACE\@. In
      addition, we completely chracterize what happens if the complexity of
      all fragments is below PSPACE\@.}
}

@InProceedings{Henglein-2008,
 author = {Fritz Henglein},
 title = {Generic discrimination: Sorting and partitioning unshared data in linear time},
 booktitle = {ICFP '08: Proceeding of the 13th ACM SIGPLAN international conference on Functional programming},
 year = {2008},
 isbn = {978-1-59593-919-7},
 pages = {91--102},
 doi = {http://doi.acm.org/10.1145/1411204.1411220},
  bibliographies = {PMC},
 WKloc = {doc/pap/BIB},
 location = {Victoria, BC, Canada},
 publisher = {ACM},
 address = {New York, NY, USA},
  abstract = {We introduce the notion of discrimination as a generalization of both sorting and partitioning and show that worst-case linear-time discrimination functions (discriminators) can be defined generically, by (co-)induction on an expressive language of order denotations. The generic definition yields discriminators that generalize both distributive sorting and multiset discrimination. The generic discriminator can be coded compactly using list comprehensions, with order denotations specified using Generalized Algebraic Data Types (GADTs). A GADT-free combinator formulation of discriminators is also given.

We give some examples of the uses of discriminators, including a new most-significant-digit lexicographic sorting algorithm.

Discriminators generalize binary comparison functions: They operate on n arguments at a time, but do not expose more information than the underlying equivalence, respectively ordering relation on the arguments. We argue that primitive types with equality (such as references in ML) and ordered types (such as the machine integer type), should expose their equality, respectively standard ordering relation, as discriminators: Having only a binary equality test on a type requires $\Theta(n^2)$ time to find all the occurrences of an element in a list of length $n$, for each element in the list, even if the equality test takes only constant time. A discriminator accomplishes this in linear time. Likewise, having only a (constant-time) comparison function requires $\Theta(n log n)$ time to sort a list of n elements. A discriminator can do this in linear time.}
}

@Misc{Henglein-2009d,
  author =    {Fritz Henglein},
  title =     {Optimizing relational algebra operations using
generic partitioning discriminators and lazy products},
  howpublished = {\unfinished},
  year =      {2009},
  WKloc =      {A-1760, doc/pap/BIB},
  abstract = {We show how to implement in-memory execution of the
    core relational algebra operations of projection, selection and cross-product
    efficiently, using discrimination-based joins and lazy products.

    We introduce the notion of (partitioning) discriminator,
    which partitions a list of values according to a specified
    equivalence relation on keys the values are associated with.
    We show how discriminators can be defined generically,
    purely functionally, and efficiently (worst-case linear time)
    on top of the array-based basic multiset discrimination
    algorithm of Cai and Paige (1995). Discriminators provide the basis
    for discrimination-based joins, a new technique for computing joins
    that requires neither hashing nor sorting. Discriminators also provide
    efficient implementations for eliminating duplicates, set union and set
    difference.

    We represent a cross-product lazily as a formal pair of the argument
    sets (relations). This allows the selection operation to recognize on the
    fly whenever it is applied to a cross-product, in which case it can choose
    anefficient discrimination-based equijoin implementation.
    The techniques subsume most of the optimization techniques based
    on relational algebra equalities, without need for a query preprocessing
    phase. They require no indexes and behave purely functionally.
    Full source code in Haskell extended with Generalized Algebraic
    Data Types (GADTS) is included. GADTs are used to represent sets
    (relations), projections, predicates and equivalence denotations in a
    type safe manner. It should be emphasized that the code is only intended
    for and applicable to operations on in-memory data; that is, in
    a random-access memory cost model. Most emphatically,
    for performance reasons it is, as given, inapplicable to
    data stored on disk or on the network.},
  bibliographies = {RATH, RelMiCS}
}

@InProceedings{Henglein-Joergensen-1994,
  author = {Fritz Henglein and Jesper J{\oe}rgensen},
  title = {Formally Optimal Boxing},
  crossref = {POPL1994},
  pages = {213--226},
  keywords = {Representation analysis, polymorphism, type inference}
}

@InProceedings{Henglein-Mossin-1994,
  author = {F. Henglein and C. Mossin},
  title = {Polymorphic Binding-Time Analysis},
  crossref = {ESOP1994},
  pages = {287--301}
}

@Article{Heninger-1980,
  author = {K. L. Heninger},
  title = {Specifying Software Requirements for Complex Systems: New
      Techniques and their Application},
  journal = IEEETSE,
  year = 1980,
  OPTkey = {},
  volume = 6,
  number = 1,
  month = JAN,
  pages = {2--13},
  bibliographies = {RelMiCS}
}

@Article{Henkin-1950,
  author = {Leon Henkin},
  title = {An Algebraic Characterization of Quantifiers},
  journal = FUND,
  volume = 37,
  year = 1950,
  pages = {63--74},
  bibliographies = {RelMiCS}
}

@Book{Henkin-1967,
  author = {Leon Henkin},
  title = {Logic Systems Containing only a Finite Number of Symbols},
  series = SeminaireMontr,
  volume = 21,
  publisher = MontrealP,
  address = {Montr\'eal},
  year = 1967,
  pages = {pp. 48},
  bibliographies = {RelMiCS}
}

@Article{Henkin-1968,
  author = {Leon Henkin},
  title = {Relativization with Respect to Formulas and
		its use in Proofs of Independence},
  journal = COMPOSIT,
  volume = 20,
  year = 1968,
  pages = {86--106},
  bibliographies = {RelMiCS}
}

@Article{Henkin-1970,
  author = {Leon Henkin},
  title = {Extending Boolean Operations},
  journal = PACIF,
  volume = 22,
  year = 1970,
  pages = {723--752},
  bibliographies = {RelMiCS}
}

@InCollection{Henkin-1973,
  author = {Leon Henkin},
  title = {Internal Semantics and Algebraic Logic},
  booktitle = {Truth, Syntax, and Modality},
  series = {Studies in Logic},
  volume = 68,
  editor = {H. Leblanc},
  publisher = NoHo,
  address = {Amsterdam},
  year = 1973,
  pages = {111--127},
  bibliographies = {RelMiCS}
}

@Article{Henkin-1977,
  author = {Leon Henkin},
  title = {Algebraic Aspects of Logic: Past, Present, and Future},
  journal = CNRS,
  volume = 249,
  year = 1977,
  pages = {89--106},
  OPTnote = {MR 80b:03104},
  bibliographies = {RelMiCS}
}

@Article{Henkin-1983,
  author = {Leon Henkin},
  title = {Proofs in first Order Logic with only finitely many
		Variables},
  journal = ABAMS,
  volume = 4,
  year = 1983,
  pages = 8,
  bibliographies = {RelMiCS}
}

@InProceedings{Henkin-Monk-1974,
  author = {Leon Henkin and J. Donald Monk},
  title = {Cylindric Algebras and Related Structures},
  crossref = {TarskiSymp1974},
  pages = {105--121},
  bibliographies = {RelMiCS}
}

@Book{Henkin-Monk-Tarski-1971,
  author = {Leon Henkin and J. Donald Monk and Alfred Tarski},
  title = {Cylindric Algebras, {Part I}},
  publisher = NoHo,
  series = {Studies in Logic and the Foundations of Mathematics},
  volume = 64,
  address = {Amsterdam},
  year = 1971,
  bibliographies = {RelMiCS},
  McMaster = {QA 9 .H415}
}

@Book{Henkin-Monk-Tarski-1985,
  author = {Leon Henkin and J. Donald Monk and Alfred Tarski},
  title = {Cylindric Algebras, {Part II}},
  publisher = NoHo,
  address = {Amsterdam},
  year = 1985,
  bibliographies = {RelMiCS}
}

@Book{Henkin-Monk-Tarski-Andreka-Nemeti-1981,
  author = {Leon Henkin and J. Donald Monk and Alfred Tarski and Hajnal
		Andr\'eka and Istv\'an N{\'e}meti},
  title = {Cylindric Set Algebras},
  series = LNM,
  volume = 883,
  publisher = Springer,
  address = {Berlin},
  year = 1981,
  pages = {v + 323},
  bibliographies = {RelMiCS}
}

@Article{Henkin-Resek-1975,
  author = {Leon Henkin and Diane Resek},
  title = {Relativization of Cylindric Algebras},
  journal = FUND,
  volume = 82,
  year = 1975,
  pages = {363--383},
  bibliographies = {RelMiCS}
}

@InProceedings{Henkin-Tarski-1961,
  author = {Leon Henkin and Alfred Tarski},
  title = {Cylindric Algebras},
  booktitle = {	Lattice Theory},
  series = SymposiaPure,
  volume = 2,
  editor = {R. P. Dilworth},
  publisher = AMS,
  address = {Providence, R.I.},
  year = 1961,
  pages = {83--113},
  bibliographies = {RelMiCS}
}

@Article{Hennessy-1980,
  author = {Matthew C. B. Hennessy},
  title = {A Proof System for the first-order Relational Calculus},
  journal = JCOMSYS,
  volume = 20,
  year = 1980,
  pages = {96--110},
  bibliographies = {RelMiCS}
}

@InProceedings{Hennessy-Ashcroft-1976,
  author = {M. Hennessy and E.A. Ashcroft},
  title = {The Semantics of Nondeterminism},
  booktitle = {Third ICALP, Edinburgh},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  year = 1976,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  pages = {478--493},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Hennessy-Milner-1985,
  author = {Hennessy, M. and Milner, R.},
  journal = JACM,
  pages = {137--162},
  title = {Algebraic laws for indeterminism and concurrency},
  volume = 32,
  year = 1985,
  bibliographies = {RelMiCS}
}

@InProceedings{Hennesy-1994,
  author = {M. Henessy},
  title = {Higher Order Processes and Their Models},
  pages = {286--303},
  crossref = {ICALP1994},
  authorsAddress = {Sussex University}
}

@InProceedings{Hennicker-1990,
  author = {Rolf Hennicker},
  title = {Context Induction: A Proof Principle for Behavioural
		  Abstractions},
  crossref = {DISCO90},
  pages = {101--110},
  annote = {see also \cite{Hennicker-1991}}
}

@Article{Hennicker-1991,
  title = {Context Induction: A Proof Principle for
                       Behavioural Abstractions and Algebraic Implementations},
  author = {Rolf Hennicker},
  journal = {Formal Aspects of Computing},
  volume = 3,
  number = 4,
  pages = {326--345},
  year = 1991
}

@Booklet{Hennicker-1997,
  author = {Rolf Hennicker},
  title = {Structured Specifications with Behavioural Operators: Semantics, Proof Methods and Applications},
  howpublished = {Habilitationsschrift, Institut f\"ur Informatik, Ludwig-Maximilians-Universit\"at M\"unchen},
  month = JUN,
  year = 1997,
  annote = {relational signatures},
  WKloc = {doc/pap/BIB/Hennicker-1997/},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings {Henretty-Stock-Pouchet-Franchetti-Ramanujam-Sadayappan-2011,
   author = {Henretty, Tom and Stock, Kevin and Pouchet, Louis-No{\"e}l and Franchetti, Franz and Ramanujam, J. and Sadayappan, P.},
   affiliation = {The Ohio State University, USA},
   title = {Data Layout Transformation for Stencil Computations on Short-Vector SIMD Architectures},
   booktitle = {Compiler Construction},
   LNCSbooktitle = {{CC 2011}},
   series = LNCS,
   editor = {Knoop, Jens},
   publisher = Springer,
   pages = {225--245},
   volume = {6601},
   DOIurl = {http://dx.doi.org/10.1007/978-3-642-19861-8_13},
   DOI = {10.1007/978-3-642-19861-8_13},
   abstract = {Stencil computations are at the core of applications in many domains such as computational electromagnetics, image processing, and partial differential equation solvers used in a variety of scientific and engineering applications. Short-vector SIMD instruction sets such as SSE and VMX provide a promising and widely available avenue for enhancing performance on modern processors. However a fundamental memory stream alignment issue limits achieved performance with stencil computations on modern short SIMD architectures. In this paper, we propose a novel data layout transformation that avoids the stream alignment conflict, along with a static analysis technique for determining where this transformation is applicable. Significant performance increases are demonstrated for a variety of stencil codes on three modern SIMD-capable processors.},
   year = {2011}
}

@InProceedings{Hense-1991,
  title = {Wrapper Semantics of an Object-Oriented Programming Language
		  with State},
  author = {Andreas V. Hense},
  pages = {548--568},
  crossref = {TACS1991},
  abstract = {The semantics of class inheritance has first been given in
		  operational form (method-lookup-semantics).  While
		  this semantics is well suited for implementing
		  object-oriented programming languages, it may
		  conceal the true nature of inheritance.  The
		  development of denotational semantics for
		  object-oriented languages has culminated in object
		  creation as fixed point operation.  Cook gave a
		  semantics on this basis, using so called {\em
		  wrappers}.  This semantics abstracts from the
		  internal state of objects ({\em instance
		  variables\/}).

                  In this paper we show how wrapper semantics can be
		  extended to an object-oriented programming language
		  {\em with state\/} while keeping the structure of
		  the original definitions.  For this purpose we
		  define a direct denotational semantics of a small
		  example language.  The insertion of state into class
		  definitions can be done before or after the related
	  	  fixed point operation.  The choice of the
		  alternative considerably influences the semantic
		  domains and clauses.}
}

@Misc{Hensel-Spooner-1996,
  author = {Ulrich Hensel and David Spooner},
  title = {A View on Implementing Processes: Categories of Circuits},
  year = 1996,
  WKloc = {A-1275},
  abstract = {We construct a category of circuits: the objects are
      alphabets and the morphisms are deterministic automata. The
      construction differs in several respects from the bicategories of
      circuits appearing previously in the literature: it is parameterized
      by a monad which allows flexibility in the emergent notion of
      process.

      We focus on the circuits which arise from a distributive category and
      the exception monad. These circuits are partial in that they may,
      based on their state, choose to abort on some inputs. Consequently,
      certain circuits determine languages, and safety and liveness
      properties with repect to these languages are captured by circuit
      equationss. Actually, the notions of safety and liveness arise in any
      copy category. Extracting the category of circuitswhich are both safe
      and live corresponds to the extensive completion of a distributive
      copy category.

      Partial circuits coincide with elements of the terminal coalgebra of
      a specific datatype. The co-induction principle provides mechanisms
      for the construction of circuits, the normalization of circuit
      expressions and for the proof of safety and liveness properties.}
}

@InProceedings{Henson-1990,
  title = {Information Loss in the Programming Logic TK},
  pages = {523-559},
  crossref = {IFIP1990},
  author = {Martin C. Henson},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@MastersThesis{Henz-1991,
  author = {Martin Henz},
  title = {Term Rewriting in Associative Commutative
                   Theories with Identities},
  year = 1991,
  school = {State University of New York at Stony Brook},
  month = DEC,
  abstract = {Versions of constraint rewriting for completion of
		  rewrite systems in the presence of associative
		  commutative operators with identities have been
		  proposed, in which constraints are used to limit the
		  applicability of rewrite rules. We extend these
		  approaches such that the initially given equations
		  can contain constraints, and such that a suitable
		  version of unification modulo associativity,
		  commutativity and identity can be interleaved with
		  the process of completion.},
  bibliographies = {RelMiCS}
}

@Book{Herrlich-Strecker-1973,
  author = 	 {H. Herrlich and G. E. Strecker},
  title = 	 {Category Theory},
  publisher = 	 {Allyn and Bacon Inc.},
  year = 	 1973,
  address = 	 {Boston}
}

@Article{Herlitz-2001,
  author = {Jan Christian Herlitz},
  title = {The death of {XML} editors --- and the next-generation client},
  journal = {Markup Languages: Theory \& Practice},
  year = 2001,
  volume = 3,
  number = 1,
  pages = {55--63},
  WKloc = {A-1437},
  annote = {reviewed}
}

@InProceedings{Herman-Marshall-2000,
  author = {Ivan Herman and M. Scott Marshall},
  title = {{GraphXML} --- An {XML}-Based Graph Description Format},
  crossref = {GD2000},
  pages = {52--62},
  URL = {http://www.cwi.nl/InfoVisu/GraphXML/GraphXMLShort.pdf},
  WKloc = {doc/pap/BIB}
}

@Book{Hermes-1967,
  keywords = {lattice theory},
  year = 1967,
  volume = 73,
  title = {Einf\"uhrung in die {Verbandstheorie}},
  series = {Grundlehren der mathematischen Wissenschaften},
  publisher = Springer,
  note = {{$2^{nd}$} edition},
  author = {H. Hermes},
  bibliographies = {RelMiCS}
}

@InProceedings{Hermida-1992,
  author = {Claudio Alberto Hermida},
  title = {On Fibered Adjunctions and Completeness for Fibered
		  Categories},
  crossref = {SADT92},
  pages = {235--252},
  WKloc = {A-0337},
  abstract = {We show how the completeness of the total category
		  of a fibration can be inferred from that of the
		  fibre categories and its base, Our results are
		  somewhat stronger than those in
		  \cite{Burstall-Goguen-Tarlecki-1991} and they are
		  obtained as direct consequences of an important
		  property of general fibred adjunctions. Our aim is
		  to show that fibred category theory can provide
		  insight into constructions of relevance in algebraic
		  specifications, {\sl e.g.\null{}} limits and
		  colimits of many-sorted algebras, by explaining them
		  at a natural level of abstraction.}
}

@TechReport{Hermida-1993,
  author = {Claudio Alberto Hermida},
  title = {Fibrations, Logical Predicates and Indeterminates},
  institution = {Laboratory for Foundations of Computer Science},
  year = 1993,
  number = {ECS-LFCS-93-277},
  address = {University of Edinburgh, Scotland, UK},
  ftpaddress = {ftp.dcs.ed.ac.uk},
  filename = {pub/lfcsreps/93/},
  abstractfilename = {pub/lfcsreps/93/},
  note = {PhD Thesis - Hardcopy price #8.00},
  filetype = {},
  URL = {http://www.dcs.ed.ac.uk/lfcs/},
  crossref = {CST-103-93}
}

@Article{ Hermida-2001,
    author = "C. Hermida",
    title = "From coherent structures to universal properties",
    journal = "Journal of Pure and Applied Algebra",
    volume = "165",
    number = "1",
    pages = "7--61",
    year = "2001",
    CiteSeer = "citeseer.ist.psu.edu/article/hermida99from.html",
  WKloc = {A-1575, doc/pap/BIB}
}

@TechReport{ Hermida-Mateus-2000,
    author = "C. Hermida and P. Mateus",
    title = "Internal paracategories",
    address = "1049-001 Lisboa, Portugal",
    year = "2000",
    CiteSeer = "citeseer.ist.psu.edu/hermida00internal.html",
  WKloc = {A-1574, doc/pap/BIB}
}

@Book{Hertz-Krogh-Palmer-1991,
  author = {J. Hertz and A. Krogh and R. G. Palmer},
  title = {Introduction to the theory of Neural Computation},
  publisher = {Addison Wesley},
  year = 1991,
  address = {Redwood City, CA},
  annote = {general introduction to neural nets}
}

@InCollection{Herzberger-1981,
  author = {Hans G. Herzberger},
  title = {Peirce's Remarkable Theorem},
  booktitle = {Pragmatism and Purpose: Essays Presented to
		Thomas A. Goudge, L. W. Sumner, J. G. Slater, F. Wilson, eds.},
  year = 1981,
  publisher = TorontoP,
  address = {Toronto},
  pages = {41--58},
  bibliographies = {RelMiCS}
}

@Book{HesseK-1996,
  author = {Karen Hesse},
  title = {The Music of Dolphins},
  publisher = {Scholastic Press},
  year = 1996,
  address = {New York},
  ISBN = {0-590-89797-7},
  bibliographies = {Cynthia},
  annote = {read by Cynthia July 2003},
  abstract = {``I thought she was a mermaid at first,'' said U.S. Coast
      Guard LJG Monica Stone. But the rescued girl found off the coast of
      Florida is not a mermaid. She is a human child, a refugee lost at
      sea, raised by dolphins from the age of four. They call her Mila,
      from \emph{miracle} in Spanish, and take her away to a center for
      scientific study. [...]}
}

@Article{Hesselink-1990,
  title = {Axioms and Models of Linear Logic},
  author = {W.H. Hesselink},
  journal = FACOMP,
  year = 1990,
  volume = 2,
  pages = {139--166},
  bibliographies = {RelMiCS}
}

@InProceedings{Hickey-Nogin-Constable-etal-2003,
  author = {Jason Hickey and Aleksey Nogin and Robert L. Constable
            and Brian E. Aydemir and Eli Barzilay and Yegor Bryukhov
            and Richard Eaton and Adam Granicz and Alexei Kopylov
            and Christoph Kreitz and Vladimir N. Krupski
            and Lori Lorigo and Stephan Schmitt and Carl Witty and Xin Yu},
  title = 	 {{MetaPRL} --- A Modular Logical Environment},
  crossref =  {TPHOL2003},
  pages =	 {287--303},
  bibliographies = {HHOL},
  WKloc = 	 {A-1503, doc/pap/BIB},
  abstract = {MetaPRL is the latest system to come out of over
     twenty ve years of research by the Cornell PRL group.W hile
     initially created at Cornell, MetaPRL is currently a collaborative
     project involving several universities in several countries. The
     MetaPRL system combines the properties of an interactive
     LCF-style tactic-based proof assistant, a logical framework,a
     logical programming environment, and a formal methods programming
     toolkit. MetaPRL is distributed under an open-source license and
     can be downloaded from \textsf{http://metaprl.org/}.
     This paper provides an
     overview of the system focusing on the features that did not
     exist in the previous generations of PRL systems.}
}

@Book{Higgins-1974,
  author = {P. J. Higgins},
  title = {Categories and Groupoids},
  publisher = Nostrand,
  year = 1974,
  bibliographies = {RelMiCS}
}

@Article{Higley-1976,
  author = {C. J. Higley},
  title = {Type Checking in a Typeless Language},
  journal = {The Computer Journal},
  volume = 19,
  number = 2,
  pages = {166--169},
  month = may,
  year = 1976,
  coden = {CMPJA6},
  ISSN = {0010-4620},
  bibdate = {Tue Mar 25 13:51:56 MST 1997},
  acknowledgement = ack-nhfb,
  classcodes = {C6110 (Systems analysis and programming); C6140D (High
                 level languages)},
  classification = 723,
  corpsource = {Computing Centre, Univ. of East Anglia, Norwich, UK},
  keywords = {BCPL; checking; computer operating systems --- Program
                 Processors; computer programming languages; correction;
                 error detection; procedure oriented languages; type;
                 typeless language},
  treatment = {P Practical}
}

@Article{Higman-1972,
  author = {D. G. Higman},
  title = {Combinatorial Considerations about Permutation Groups},
  journal = LN,
  publisher = {Mathematical Inst.},
  address = {Oxford},
  year = 1972,
  bibliographies = {RelMiCS}
}

@Article{Higman-1975,
  author = {D. G. Higman},
  title = {Coherent Configurations, I, Ordinary Representation Theory},
  journal = GEOMDED,
  volume = 4,
  year = 1975,
  pages = {1--32},
  bibliographies = {RelMiCS}
}

@InProceedings{Hildebrandt-Panagaden-Winskel-1998,
  author = {Thomas Hildebrandt and Prakash Panagaden and Glynn Winskel},
  title = {A Relational Model of Non-Deterministic Dataflow},
  booktitle = {{CONCUR '98}},
  series = {LNCS},
  publisher = {Springer-Verlag},
  year = 1998,
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0823},
  bibliographies = {RelMiCS}
}

@InProceedings{Hill-1994,
  author = {P. M. Hill},
  title = {A Module System for Meta-Programming},
  booktitle = {LOPSTR '94 / META'94},
  series = {LNCS},
  volume = 883,
  UniBwM = {INF400/Z10076-4},
  pages = {395--409},
  OPTabstract = {},
  WKloc = {A-0456}
}

@Misc{Himsolt-1996,
  author = {Michael Himsolt},
  title = {{GML}: Graph Modelling Language (DRAFT VERSION)},
  year = 1996,
  WKloc = {A-0691}
}

@Misc{Himsolt-199X,
  author = {Michael Himsolt},
  title = {{GML}: A portable Graph File Format},
  year = {199?},
  WKloc = {A-0725}
}

@Book{HinXXX-,
  ALTauthor = 	 {Hin???},
  ALTeditor = 	 {},
  title = 	 {Dot to Dot in the Sky},
  publisher = 	 {},
  year = 	 {},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  HPL = 	 {Dundas J523.80223 HIN},
  bibliographies = {Cynthia}
}

@Book{Hindley-Seldin-1986,
  author =	 {J. Roger Hindley and Jonathan P. Seldin},
  title = 	 {Introduction to combinators and $\lambda$-calculus},
  publisher = 	 {CambridgeUP},
  year = 	 1986,
  volume =	 1,
  series =	 {London Mathematical Society student texts},
  McMaster = 	 {QA 9.5 .H56 1986}
}

@misc{ Hines-1999,
  author = "P. Hines",
  title = "The categorical theory of self-similarity",
  journal = {Theory and Applications of Categories},
  volume = 6,
  pages = {33--46},
  year = 1999,
  CiteSeer = "citeseer.ist.psu.edu/hines99categorical.html",
  WKloc = {doc/pap/BIB}
}

@InProceedings{Hintermeier-Kirchner-Kirchner-1994,
  author = {C. Hintermeier and C. Kirchner and H. Kirchner},
  title = {Dynamically-Typed Computations for Order-Sorted Equational
          Presentations},
  pages = {450--461},
  crossref = {ICALP1994},
  authorsAddress = {INRIA-Lorraine \& CRIN-CNRS},
  abstract = {Equational presentations with ordered sorts encompass
		  partially defined functions and subtyping
		  information in an algebraic framework. In this work
		  we address the problem of computing in order-sorted
		  algebras, with very few restrictions on the allowed
		  presentation. We adopt an algebraic framework where
		  equational, membership and existence formulas can be
		  expressed. A complete deduction calculus is provided
		  to incorporate the interaction between all these
		  formulas. The notion of decorated terms is proposed
		  to memorize local sort information, dyunamically
		  changed by a rewriting process. A completion
		  procedure for equational presentations with ordered
		  sorts computes a set of rewrite rules with which not
		  only equational theorems of the form $(t = t')$, but
		  also typing theorems of the form $(t:A)$, can be proven.}
}

@Misc{Hinze-1998,
  author = {Ralf Hinze},
  title = {A Data-structural Look at Sorting},
  year = 1998,
  WKloc = {A-0496}
}

@InProceedings{Hinze-2000,
  author = {Ralf Hinze},
  title = {Deriving Backtracking Monad Transformers},
  crossref = {ICFP2000},
  URL = {http://www.informatik.uni-bonn.de/~ralf/ICFP00.ps.gz},
  WKloc = {A-1199}
}

@InProceedings{Hinze-2010,
   author = {Hinze, Ralf},
   affiliation = {Computing Laboratory, University of Oxford, Wolfson Building, Parks Road, Oxford, OX1 3QD UK},
   title = {Type Fusion},
   crossref = {AMAST2010},
   pages = {92--110},
   DOIurl = {http://dx.doi.org/10.1007/978-3-642-17796-5_6},
   DOI = {10.1007/978-3-642-17796-5_6},
   abstract = {Fusion is an indispensable tool in the arsenal of techniques for program derivation. Less well-known, but equally valuable is type fusion, which states conditions for fusing an application of a functor with an initial algebra to form another initial algebra. We provide a novel proof of type fusion based on adjoint folds and discuss several applications: type firstification, type specialisation and tabulation.}
}

@InProceedings{Hinze-2012,
   author = {Hinze, Ralf},
   affiliation = {Computing Laboratory, University of Oxford, Wolfson Building, Parks Road, Oxford, OX1 3QD UK},
   title = {{Kan} extensions for program optimisation --- Or: {Art} and {Dan} explain an old trick},
   pages = {324--362},
   DOIURL = {http://dx.doi.org/10.1007/978-3-642-31113-0_16},
   DOI = {10.1007/978-3-642-31113-0_16},
   booktitle = {Mathematics of Program Construction, {MPC 2012}},
   LNCSbooktitle = {MPC 2012},
   year = 2012,
   editor = {Jeremy Gibbons and Pablo Nogueira},
   publisher = Springer,
   series = LNCS,
   volume = 7342,
   abstract = { Many program optimisations involve transforming a program
   in direct style to an equivalent program in continuation-passing
   style. This paper investigates the theoretical underpinnings of this transformation
   in the categorical setting of monads. We argue that so-called
   absolute Kan Extensions underlie this program optimisation. It is known
   that every Kan extension gives rise to a monad, the codensity monad, and
   furthermore that every monad is isomorphic to a codensity monad. The
   end formula for Kan extensions then induces an implementation of the
   monad, which can be seen as the categorical counterpart of continuationpassing
   style. We show that several optimisations are instances of this
   scheme: Church representations and implementation of backtracking using
   success and failure continuations, among others. Furthermore, we
   develop the calculational properties of Kan extensions, powers and ends.
   In particular, we propose a two-dimensional notation based on string
   diagrams that aims to support effective reasoning with Kan extensions.}
}

@Article{Hinze-Jeuring-2003,
  author = {Ralf Hinze and Johan Jeuring},
  title = {Functional Pearl: Weaving a Web},
  journal = JFP,
  year = 2003,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {to appear},
  WKloc = {A-1451},
  bibliographies = {EdComb}
}

@Misc{ Hinze-Jeuring-Loeh-2001,
  author = {R. Hinze and J. Jeuring and A. L{\"o}h},
  title = "Type-indexed datatypes",
  URL = {http://www.cs.uu.nl/~johanj/publications/tidata.ps},
  year = "2001",
  Citeseer = "http://citeseer.ist.psu.edu/article/hinze01typeindexed.html"
}

@InProceedings{Hinze-PeytonJones-2000,
  author = {Ralf Hinze and Peyton Jones, Simon},
  title = {Derivable Type classes},
  booktitle = {Haskell Workshop 2000},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 2000,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  month = SEP,
  OPTorganization = {},
  OPTpublisher = {},
  URL = {http://research.microsoft.com/~simonpj/#derive},
  WKloc = {A-1025, doc/pap/BIB/}
}

@InProceedings{Hirokawa-1991,
  title = {Principal Type-Schemes of {BCI}-Lambda-Terms},
  author = {Sachio Hirokawa},
  pages = {633--650},
  crossref = {TACS1991},
  abstract = {A BCI-$\lambda$-term is a $\lambda$-term in which each
		  variable occurs exactly once.  It represents a proof
		  figure for implicational formula provable in linear
		  logic.  A principal type-scheme is a most general
		  type to the term with respect to substitution.  The
		  notion of ``relevance relation'' is introduced for
		  type-variables in a type.  Intuitively an occurrence
		  of a type-variable $b$ is relevant to other
		  occurrence of some type-variable $c$ in a type
		  $\alpha$, when $b$ is essentially concerned with the
		  deduction of $c$ in $\alpha$. This relation defines
		  a directed graph $G(\alpha)$ for type-variables in
		  the type. We prove that a type $\alpha$ is a
		  principal type-scheme of BCI-$\lambda$-term iff (a),
		  (b) and (c) holds:
     \begin{itemize}
     \item[(a)] Each variable occurring in $\alpha$ occurs exactly twice
		and the occurrences have opposite sign.
     \item[(b)]	$G(\alpha)$ is a tree and the right-most type variable
		in $\alpha$ is its root.
     \item[(c)]	For any subtype $\gamma$ of $\alpha$, each type variable in
		$\gamma$ is relevant to the right-most type variable
		in $\gamma$.
     \end{itemize}
                    A type-schemes of some BCI-$\lambda$-term is
		  minimal iff it is not a non-trivial substitution
		  instance of other type-scheme of BCI-$\lambda$-term.
		  We prove that the set of BCI-minimal types coincides
		  with the set of principal type-schemes of
		  BCI-$\lambda$-terms in $\beta \eta$-normal form.}
}

@Article{Hirsch-Montanari-1999,
  author = {Dan Hirsch and Ugo Montanari},
  title = {Consistent Transformations for Software Architecture Styles of Distributed Systems},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = 1999,
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  WKloc = {A-0857},
  OPTabstract = {},
  OPTcontents = {},
  note = {Proc. FCT'99 Workshop on Distributed Systems, to appear},
  OPTannote = {}
}

@Book{Hirvensalo-2001,
  author = {Mika Hirvensalo},
  title = {Quantum Computing},
  publisher = Springer,
  year = 2001,
  series = {Natural Computing Series},
  ISBN = {3-540-66783-0},
  UniBwM = {ELT840/YG4366}
}

@InProceedings{Hirzel-Nystrom-Bloom-Vitek-2008,
  author = 	 {Martin Hirzel and Nathaniel Nystrom and Bard Bloom and Jan Vitek},
  title = 	 {Matchete: Paths through the Pattern Matching Jungle},
  crossref =  {PADL2008},
  pages = {\unfinished},
  WKloc = {A-1681, doc/pap/BIB},
  bibliographies = {PMC},
  abstract = {Pattern matching is a programming language feature for
                  selecting a handler based on the structure of data
                  while binding names to sub-structures.  By combining
                  selection and binding, pattern matching facilitates
                  many common tasks such as date normalization,
                  red-black tree manipulation, conversion of XML
                  documents, or decoding TCP/IP packets. Matchete is a
                  language extension to Java that unifies different
                  approaches to pattern matching: regular expressions,
                  structured term patterns, XPath, and bit-level
                  patterns. Matchete naturally allows nesting of these
                  different patterns to form composite patterns. We
                  present the Matchete syntax and describe a prototype
                  implementation.}
}

@InProceedings{Hitchcock-Park-1973,
  author = {Hitchcock, P. and Park, David},
  title = {Induction rules and termination proofs.},
  booktitle = {{Proc.\ Automata, Languages and Programming (ICALP '72),
      Rocquencourt, France, July 1972}},
  editor = {Maurice Nivat},
  year = 1973,
  publisher = NoHo,
  pages = {225--251},
  bibliographies = {RelMiCS}
}

@Article{Hitzler-Kroetzsch-ZhangGuoQiang-2006,
  author =       {Pascal Hitzler and Markus Kr{\"o}tzsch and Guo-Qiang Zhang},
  title =        {A Categorical View on Algebraic Lattices in Formal Concept Analysis},
  journal =      FUNDI,
  year =         2006,
  volume =    74,
  number =    {2--3},
  pages =     {301--328},
  WKloc =     {doc/pap/BIB},
  bibliographies =      {RelMiCS},
  abstract =    {Formal concept analysis has grown from a new branch
                  of the mathematical field of lattice theory to a
                  widely recognized tool in Computer Science and
                  elsewhere. In order to fully benefit from this
                  theory, we believe that it can be enriched with
                  notions such as approximation by computation or
                  representability. The latter are commonly studied in
                  denotational semantics and domain theory and
                  captured most prominently by the notion of
                  algebraicity, e.g. of lattices. In this paper, we
                  explore the notion of algebraicity in formal concept
                  analysis from a category-theoretical perspective. To
                  this end, we build on the notion of approximable
                  concept with a suitable category and show that the
                  latter is equivalent to the category of algebraic
                  lattices. At the same time, the paper provides a
                  relatively comprehensive account of the
                  representation theory of algebraic lattices in the
                  framework of Stone duality, relating well-known
                  structures such as Scott information systems with
                  further formalisms from logic, topology, domains and
                  lattice theory.}
}

@Article{Hoare-1969,
  author = {C.A.R. Hoare},
  title = {An axiomatic basis for computer programming},
  journal = CACM,
  volume = 12,
  pages = {578--580},
  year = 1969,
  bibliographies = {RelMiCS}
}

@Article{Hoare-1972,
  author = {C. A. R. Hoare},
  title = {Proof of Correctness of Data Representations},
  journal = {Acta Informatica},
  year = 1972,
  volume = 1,
  pages = {271--281}
}

@Article{Hoare-1978,
  WKloc = {A-0008},
  keywords = {CSP},
  abstract = {This paper suggests that input and output are basic
		  primitives of programming and that parallel
		  composition of communicating sequential processes is
		  a fundamental program structuring method. When
		  combined with a development of Dijkstra's guarded
		  command, these concepts are surprisingly versatile.
		  Their use is illustrated by sample solutions of a
		  variety of familiar programming exercises.},
  year = 1978,
  volume = 21,
  title = {Communicating Sequential Processes},
  pages = {666--677},
  number = 8,
  month = AUG,
  journal = CACM,
  author = {C.A.R. Hoare},
  bibliographies = {RelMiCS}
}

@Book{Hoare-198X,
  author = 	 {C.A.R. Hoare},
  title = 	 {Communicating Sequential Processes},
  publisher = 	 {Prentice Hall},
  year = 	 {},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  WKloc = 	 {owned, \lent{Ridha}}
}

@InProceedings{Hoare-1985,
  author = {C.A.R. Hoare},
  title = {Notes on Communicating Sequential Processes},
  pages = {123--204},
  abstract = {These notes present a coherent and comprehensive introduction
              to the theory and applications of
              Communicating Sequential Processes. Most of the
              illustrative examles have appeared earlier in PRG-22.
              The theory described in PRG-16 has been taken as the
              basis for a number of algebraic laws, which can be used
              for proofs of equivalence and can justify
              correctness-preserving transformations. A complete method
              for specifying processes and proving their correctness
              has been taken over ftom PRG-20 and PRG-23. Many of these
              concepts have been implemented in LISPKIT, as described
              in PRG-32.},
  crossref = {Marktoberdorf-1985}
}

@InCollection{Hoare-1989,
  author = {C.A.R. Hoare},
  title = {Notes on an Approach to Category Theory for Computer Scientists},
  pages = {\unfinished},
  crossref = {Marktoberdorf-1988},
  bibliographies = {RelMiCS}
}

@TechReport{Hoare-1990,
  author = {C.A.R. Hoare},
  title = {Refinement algebra proves correctness of compiling
		  specifications},
  year = 1990,
  number = {PRG-TR-6-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0083},
  bibliographies = {RelMiCS},
  abstract = {A compiler is specified by a description of how each
		  construct of the soure language is translated into a
		  sequence of object code instructions. The meaning of
		  the onject code can be defined by an interpreter
		  written in the source language itself. A proof that
		  the compiler is correct must show that
		  interpretation of the object code is at least as
		  good (for any relevant purpose) as the corresponding
		  source program.

                  The proof is conducted using standard techniques of
		  data refinement. All the calculations are based on
		  algebraic laws governing the source language. The
		  theorems are expressed in a form close to a logic
		  program, which may be used as a compiler prototype,
		  or as a check on the results of a particular
		  compilation. It is suggested that this formal
		  framework provides appropriate interfaces for
		  compiler implementors, and hardware designers, as
		  well as users of the language.}
}

@Misc{Hoare-1994a,
  author = {C.A.R. Hoare},
  title = {Mathematical Models for Computing Science},
  howpublished = {Draft put on WWW as {\tt file://ftp.comlab.ox.ac.uk/pub/Documents/techpapers/Tony.Hoare/mathmodl.ps.Z}},
  year = 1994,
  month = AUG,
  pages = 65,
  WKloc = {B-0036}
}

@Misc{Hoare-1994b,
  author = {C.A.R. Hoare},
  title = {Unified Theories of Programming},
  howpublished = {Draft put on WWW as {\tt file://ftp.comlab.ox.ac.uk/pub/Documents/techpapers/Tony.Hoare/theory94.ps.Z}},
  year = 1994,
  month = JUL,
  pages = 58,
  WKloc = {B-0037}
}

@InProceedings{Hoare-2003,
  author = {Tony Hoare},
  title = {The Verifying Compiler: A Grand Challenge for Computing Research},
  crossref = {CC2003},
  pages = {262--272},
  WKloc = {A-1469, doc/pap/BIB},
  abstract = {I propose a set of criteria which distinguish a grand
      challenge in science or engineering from the many other kinds of
      short-term or long-term research problems that engage the interest of
      scientists and engineers. As an example drawn from Computer Science,
      I revive an old challenge: the construction and application of a
      verifying compiler that guarantees correctness of a program before
      running it.}
}

@article{Hoare-2003a,
 author = {Hoare, Tony},
 title = {The verifying compiler: A grand challenge for computing research},
 journal = JACM,
 issue_date = {January 2003},
 volume = {50},
 number = {1},
 month = jan,
 year = {2003},
 issn = {0004-5411},
 pages = {63--69},
 numpages = {7},
 DOIURL = {http://doi.acm.org/10.1145/602382.602403},
 doi = {10.1145/602382.602403},
 acmid = {602403},
 publisher = {ACM},
 address = {New York, NY, USA},
 abstract = {This contribution proposes a set of criteria that distinguish a grand challenge in science or engineering from the many other kinds of short-term or long-term research problems that engage the interest of scientists and engineers. As an example drawn from Computer Science, it revives an old challenge: the construction and application of a verifying compiler that guarantees correctness of a program before running it.}
}

@InCollection{Hoare-2005,
year={2005},
isbn={978-3-540-24297-0},
booktitle={Verification, Model Checking, and Abstract Interpretation},
volume={3385},
series={Lecture Notes in Computer Science},
editor={Cousot, Radhia},
doi={10.1007/978-3-540-30579-8_5},
title={The Verifying Compiler, a Grand Challenge for Computing Research},
url={http://dx.doi.org/10.1007/978-3-540-30579-8_5},
publisher={Springer Berlin Heidelberg},
author={Hoare, C.A.R.},
pages={78-78}
}

@TechReport{Hoare-Brookes-Roscoe-1981,
  WKloc = {A-0007},
  keywords = {CSP},
  abstract = {A mathematical model for communicating sequential
		  processes is given, and a number of its interesting
		  and useful properties are stated. The possibilities
		  of non-determinism are fully taken into account.},
  year = 1981,
  type = {Technical Monograph},
  title = {A Theory of Communicating Sequential Processes},
  number = {PRG-16},
  institution = {Oxford University Computing Laboratory, Programming
		  Research Group},
  author = {C.A.R. Hoare and S.D. Brookes and A.W. Roscoe},
  bibliographies = {RelMiCS}
}

@Article{Hoare-He-1986,
  year = 1986,
  volume = 4,
  title = {The Weakest Prespecification},
  pages = {51--54, 217--252},
  number = 9,
  journal = FUNDI,
  author = {C. A. R. Hoare and Jifeng He},
  bibliographies = {RelMiCS}
}

@Article{Hoare-He-1986I,
  year = 1986,
  volume = 4,
  title = {The Weakest Prespecification, Part I},
  pages = {51--54},
  number = 9,
  journal = FUNDI,
  author = {C. A. R. Hoare and Jifeng He},
  bibliographies = {RelMiCS}
}

@Article{Hoare-He-1986II,
  year = 1986,
  volume = 4,
  title = {The Weakest Prespecification, Part II},
  pages = {217--252},
  number = 9,
  journal = FUNDI,
  author = {C. A. R. Hoare and Jifeng He},
  bibliographies = {RelMiCS}
}

@Article{Hoare-He-1987,
  year = 1987,
  volume = 24,
  title = {The Weakest Prespecification},
  pages = {127--132},
  journal = IPLET,
  author = {C.A.R. Hoare and Jifeng He},
  bibliographies = {RelMiCS},
  WKloc = {A-1308}
}

@Article{Hoare-He-Sanders-1987,
  author = {Hoare, C. A. R. and He, J. and Sanders, J. W.},
  title = {Prespecification in Data Refinement},
  journal = IPLET,
  volume = 25,
  number = 2,
  year = 1987,
  pages = {71--76},
  bibliographies = {RelMiCS},
  WKloc = {A-1309}
}

@Article{Hoare-Karger-1995,
  author = {C.A.R. Hoare and Burghard von Karger},
  title = {Sequential Calculus},
  journal = IPLET,
  year = 1995,
  volume = 53,
  number = 3,
  pages = {123--130},
  bibliographies = {RelMiCS}
}

@InProceedings{Hoare-Moeller-Struth-Wehrman-2009,
  author =       {C.A.R. Hoare and Bernhard M{\"o}ller and Georg Struth and Ian Wehrman},
  title =        {Concurrent {Kleene} Algebra},
  pages =     {399--414},
  crossref =  {CONCUR2009},
  WKloc = {A-1731},
  lncsbooktitle = {{CONCUR 2009}},
  year =      2009,
  editor =    {M. Bravetti and G. Zavattaro},
  volume =    5710,
  series =    LNCS,
  publisher = Springer
}

@Article{Hoare-etal-1987,
  author = {C. A. R. Hoare and I. J. Hayes and Jifeng He and C. C.
                 Morgan and A. W. Roscoe and J. W. Sanders and I. H.
                 S{\o}rensen and J. M. Spivey and B. A. Sufrin},
  title = {Laws of programming},
  journal = CACM,
  volume = 30,
  number = 8,
  pages = {672--686},
  month = AUG,
  year = 1987,
  CODEN = {CACMA2},
  ISSN = {0001-0782},
  abstract = {A complete set of algebraic laws is given for
              Dijkstra's nondeterministic sequential programming language.
              Iteration and recursion are explained in terms of
              Scott's domain theory as fixed points of
              continuous functionals. A calculus analogous to
              weakest preconditions is suggested as an aid to
              deriving programs from their specifications.},
  note = {Corrigenda in {\bf 30}, 9, p.\null{} 770},
  URL = {http://doi.acm.org/10.1145/27651.27653},
  keywords = {design; languages; theory; verification},
  subject = {{\bf D.1.4}: Software, PROGRAMMING TECHNIQUES,
                 Sequential Programming. {\bf D.3.1}: Software,
                 PROGRAMMING LANGUAGES, Formal Definitions and Theory,
                 Semantics. {\bf D.3.4}: Software, PROGRAMMING
                 LANGUAGES, Processors, Optimization. {\bf F.1.2}:
                 Theory of Computation, COMPUTATION BY ABSTRACT DEVICES,
                 Modes of Computation. {\bf F.3.1}: Theory of
                 Computation, LOGICS AND MEANINGS OF PROGRAMS,
                 Specifying and Verifying and Reasoning about Programs,
                 Pre- and post-conditions. {\bf F.3.1}: Theory of
                 Computation, LOGICS AND MEANINGS OF PROGRAMS,
                 Specifying and Verifying and Reasoning about Programs,
                 Specification techniques. {\bf F.3.2}: Theory of
                 Computation, LOGICS AND MEANINGS OF PROGRAMS, Semantics
                 of Programming Languages, Algebraic approaches to
                 semantics. {\bf I.2.2}: Computing Methodologies,
                 ARTIFICIAL INTELLIGENCE, Automatic Programming, Program
                 transformation.},
  WKloc = {A-1292, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Article{Hodas-Miller-1994,
  author = {Joshua Hodas and Dale Miller},
  title = {Logic Programming in a Fragment of Intuitionistic
		  Linear Logic},
  journal = {Journal of Information and Computation},
  year = 1994,
  WKloc = {A-0302},
  URL = {ftp://ftp.cis.upenn.edu/pub/papers/miller/ic94.ps.Z},
  keywords = {Lolli,linear logic refinement of $\lambda$Prolog,
		  see \cite{Miller-1994a}}
}

@Book{Hodges-1989,
  UniBwM = {MAT009/T290},
  year = 1989,
  title = {Alan Turing, Enigma},
  publisher = Kammerer,
  author = {Andrew Hodges},
  address = {Berlin},
  bibliographies = {RelMiCS}
}

@MastersThesis{Hoefer-1992,
  author = {H{\"o}fer, F.},
  title = {{Inkrementelle Attributauswertung in Graphausdr{\"u}cken}},
  year = 1992,
  address = {RWTH Aachen},
  type = {Diplomarbeit},
  school = {Lehrstuhl f{\"u}r Informatik III}
}

@TechReport{Hoefner-Khedri-Moeller-2006a,
  author = 	 {P. H{\"o}fner and R. Khedri and B. M{\"o}ller},
  title = 	 {Feature Algebras},
  institution =  {Universit{\"a}t Augsburg, Institut f{\"u}r Informatik},
  year = 	 2006,
  number =	 {2006-04},
  WKloc = 	 {A-1658},
  bibliographies = {RelMiCS}
}

@InProceedings{Hoefner-Khedri-Moeller-2006,
  author = 	 {Peter H{\"o}fner and Ridha Khedri and Bernhard M{\"o}ller},
  title = 	 {Feature Algebra},
  crossref =	 {FM2006},
  pages =	 {300--315},
  bibliographies = {RelMiCS},
  WKloc = {A-1661, doc/pap/BIB},
  abstract = {Based on experience from the hardware industry,
      product families have entered the software development process
      as well, since software developers often prefer not to build a
      single product but rather a family of similar products that
      share at least one common functionality while having
      well-identified variabilities. Such shared commonalities, also
      called features, reach from common hardware parts to software
      artefacts such as requirements, architectural properties,
      components, middleware, or code. We use idempotent semirings as
      the basis for a feature algebra that allows a formal treatment
      of the above notions as well as calculations with them. In
      particular models of feature algebra the elements are sets of
      products, i.e. product families. We extend the algebra to cover
      product lines, refinement, product development and product
      classification. Finally we briefly describe a prototype
      implementation of one particular model.}
}

@InProceedings{Hoefting-Wanke-Balmosan-Bergmann-1993,
  author = {Franz H\"ofting and Egon Wanke and Aurel Balmo\^san and Curd
           Bergmann},
  title = {{\em 1st Grade} --- A System for Implementation, Testing and
          Animation of Graph Algorithms},
  pages = {706--707},
  crossref = {STACS1993},
  WKloc = {A-0160},
  abstract = {{\em 1st Grade} is an integrated software system for the
             implementation, test and animation of graph algorithms. The
             system consists of a comfortable graphical editor for
             generating and manipulating graphs and a special programming
             language for the simple formulation of graph algorithms.}
}

@TechReport{Hoehfeld-Smolka-1988,
  author = {Markus H{\"o}hfeld and Gert Smolka},
  title = {Definite Relations over Constraint Languages},
  year = 1988,
  type = {LILOG Report},
  number = 53,
  month = OCT,
  institution = IWBS,
  address = {Postfach 80 08 80, 7000 Stuttgart 80, Germany},
  filename = {LR-53.dvi},
  bibliographies = {RelMiCS},
  abstract = {This paper shows that the nice properties of logic
		  programs extend to definite clause specifications
		  over arbitrary constraint languages.  The notion of
		  a constraint language sees a constraint as a piece
		  of syntax with unknown internal structure that
		  constrains the values variables can take in
		  interpretations. Examples of constraint languages
		  are Predicate Logic and its sublanguages as well as
		  attributive concept description languages developed
		  for knowledge representation.

                  Our framework generalizes the constraint logic
		  programming scheme of Jaffar and Lassez to make it
		  applicable to knowledge representation: the
		  constraint language is not required to be a
		  sublanguage of predicate logic and may come with
		  more than one interpretation, and the
		  interpretations of the constraint language are not
		  required to be solution compact.

                  We present a semantic type discipline for our
		  generalized definite clause specifications and
		  establish a notion of well-typedness that is
		  decidable provided the underlying constraint
		  language is decidable.  Finally, we give a type
		  inference rule for computing most general well-typed
		  weakenings of specifications.}
}

@Misc{Hoell-Schmidt-Stroehlein-Wimmer-1978,
  year = 1978,
  title = {{MASTER --- Ein Programmsystem zur automatischen oder
      rechnerunterst\"utzten Erstellung von Schulstundenpl\"anen}},
  pages = 171,
  howpublished = {{Forschungsauftrag Stundenplan des Bayer.
      Staatsministerium f\"ur Unterricht und Kultus, Institut f\"ur
      Informatik, Technische Universit\"at M\"unchen, Interner Bericht}},
  author = {E. H\"oll and G. Schmidt and T. Str\"ohlein and K. Wimmer}
}

@Book{Hoereth-1999,
  author = {Stefan H{\"o}reth},
  title = {{Effiziente Konstruktion und Manipulation von bin\"aren Entscheidungsgraphen}},
  publisher = {VDI Verlag},
  year = 1999,
  volume = 295,
  series = {{Fortschritt-Berichte VDI, Reihe 20: Rechnerunterst\"utzte Vertfahren}},
  note = {zugl.\null{} Dissertation TU Darmstadt, 1998},
  ISBN = {3-18-329520-2},
  UniBwM = {INF100/YF4797}
}

@InProceedings{Hofbauer-Huber-Kucherov-1994,
  author = {D. Hofbauer and M. Huber and G. Kucherov},
  title = {Some Results on Top-Context-Free Tree Languages},
  crossref = {CAAP94},
  pages = {157--171},
  WKloc = {A-0350},
  abstract = {Top-context-free tree languages (called {\em
		  cor\'egulier} by Arnold and Dauchet [1,2])
		  constitute a natural subclass of context-free tree
		  languages. In this paper, we give further evidence
		  for the importance of this class by exhibiting
		  certain closure properties. We systematically treat
		  closure under the operations {\em replacement} and
		  {\em substitution} as well as under the
		  corresponding {\em iteration} operations. Several
		  other well-known language classes are considered as
		  well. Furthermore, various characterizations of the
		  regular top-context-free languages are given, among
		  others by means of restricted regular expressions.}
}

@Article{Hoffman-ODonnell-1982,
  year = 1982,
  volume = 29,
  title = {Pattern Matching in Trees},
  pages = {68-95},
  number = 1,
  month = JAN,
  journal = {Journal of the ACM},
  author = {C. M. Hoffman and M. J. O'Donnell}
}

@Article{Hoffman-Tucker-1988,
  author = {Hoffman and Tucker},
  title = {Greedy Packing and Series-Parallel Graphs},
  journal = {Journal of Combinatorial Theory, Series A},
  volume = 47,
  year = 1988
}

@Book{Hoffmann-KriegBrueckner-1993,
  author = {Berthold Hoffmann and bernd Krieg-Br\"uckner},
  title = {Program Development by Specification and
		  Transformation: The {PROSPECTRA} Methodology,
		  language family, and system},
  publisher = {Springer},
  year = 1993,
  volume = 680,
  series = {LNCS},
  ISBN = {3-540-56733-X}
}

@InProceedings{Hoffmann-Plump-1988,
  author = {Berthold Hoffmann and Detlef Plump},
  title = {Jungle Evaluation for Efficient Term Rewriting},
  crossref = {ALP1988},
  pages = {191--203},
  bibliographies = {Coconut}
}

@Article{Hoffmann-Plump-1991,
  author = {Berthold Hoffmann and Detlef Plump},
  title = {Implementing Term Rewriting by Jungle Evaluation},
  journal = {Informatique th\'eorique et applications/Theoretical
		  Informatics and Applications},
  year = 1991,
  volume = 25,
  number = 5,
  pages = {445--472},
  WKloc = {A-},
  bibliographies = {Coconut}
}

@InProceedings{Hofmann-2000,
  author = {Martin Hofmann},
  title = {A Type System for Bounded Space and Functional In-Place Update --- Extended Abstract},
  crossref = {ESOP2000},
  pages = {165--179},
  WKloc = {A-1002}
}

@Article{Hofmann-Mislove-1977,
  year = 1977,
  volume = 154,
  title = {The Lattice of Kernel Operators and Topological Algebra},
  pages = {175--188},
  journal = {Math.~Zeitschrift},
  author = {M. W. Mislove and K. H. Hofmann}
}

@InProceedings{Hofmann-Pierce-1994,
  author = {M. Hofmann and B. Pierce},
  title = {A Unifying Type-Theoretic Framework for Objects},
  crossref = {STACS1994},
  pages = {251--262}
}

@InProceedings{Hofmann-Streicher-1994,
  title = {The Groupoid Model Refutes Uniqueness of Identity Proofs},
  author = {Martin Hofmann and Thomas Streicher},
  pages = {208--212},
  crossref = {LICS9},
  abstract = {We give a model of intensional Martin-L\"of type theory
      based on groupoids and fibrations of groupoids in which identity
      types may contain two distinct elements which are not even
      propositionally equal. This shows that the principle of uniqueness of
      identity proofs is not derivable in the syntax.}
}

@InProceedings{Hofstee-1992,
  author = {H. Peter Hofstee},
  title = {Distributing a class of sequential programs},
  pages = {139--162},
  abstract = {A class of sequential programs is distributed through a series
             of program transformations. To construct a concurrent solution,
             a sequential solution is given first. A decision is made about
             the distribution of the variables and the sequential solution
             is transformed so that guards at the outermost level can be
             evaluated using variables that will be allocated to one process
             only. Next we introduce processes and communication. The
             resulting distributed algorithm does not terminate, but it will
             become quiescent, and in this state the original postcondition
             will hold. The distributed algorithm is highly nondeterministic
             and not network specific. A synchronization primitive, the
             nonblocking channel, is introduced, and used to generalize the
             first distributed solution to a larger class of
             networks.

             We give two examples of problems that can be
             solved with this approach. First we show how a more general
             version of the loadbalancing algorithm of [7] can be derived as
             an instance of this class. Next we instantiate our solution to
             arrive at an algorithm for distributed sorting. Finally we
             refine this solution to arrive at a terminating distributed
             sorting algorithm.},
  crossref = {MPC1992},
  WKloc = {A-0238}
}

@TechReport{Hollenberg-Vermeulen-1994,
  author = {Hollenberg, M. and Vermeulen, K.},
  institution = {Dept.\null{} of Philosophy, Utrecht Univ.},
  title = {Counting variables in a dynamic setting},
  year = 1994,
  bibliographies = {RelMiCS}
}

@Book{Holt-1983,
  author = {Richard C. Holt},
  title = {Concurrent {Euclid}, the {UNIX} System, and {Tunis}},
  publisher = {Addison-Wesley},
  year = 1983,
  McMaster = {QA 76.73 .C64 1983},
  bibliographies = {SE3B}
}

@TechReport{Holt-Mancoridis-1994,
  author = 	 {Richard C. Holt and Spiros Mancoridis},
  title = 	 {Using Tube Graphs to Model Architectural Designs of Software Systems},
  institution =  {University of Toronto Computer Science Research Institute},
  year = 	 1994,
  number =	 308,
  month =	 OCT,
  CiteSeer = 	 {http://citeseer.ist.psu.edu/holt94using.html},
  URL = 	 {http://plg.uwaterloo.ca/~holt/cv/papers.html}
}

@InProceedings{Holt-1998,
  author = {Richard C. Holt},
  title = {Structural Manipulations of Software Architecture Using Tarski Relational Algebra},
  booktitle = {5th Working Conference on Reverse Engineering 1998 {(WCRE'98) October 12--14, 1998 in Honolulu, Hawaii, USA}},
  year = 1998,
  publisher = {IEEE},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0681},
  bibliographies = {RelMiCS}
}

@InProceedings{Holt-WinterA-Schuerr-2000,
  author = 	 {Richard C. Holt and Andreas Winter and Andy Sch{\"u}rr},
  title = 	 {{GXL}: Towards a Standard Exchange Format},
  crossref = {WCRE2000},
  WKloc = 	 {A-1517},
  bibliographies = {GXL}
}

@Misc{Holt-Schuerr-WinterA-Sim-2002,
  author = {Ric Holt and Andy Sch{\"u}rr and Susan Elliott Sim and Andreas Winter},
  title =     {{Graph eXchange Language GXL: A Graph-Based Standard Exchange Format for Reengineering}},
  OPThowpublished = {},
  month =     APR,
  year =      {2002},
  note =      {Available at \textsf{http://www.gupro.de/GXL/Introduction/intro.html} (2004-10-10);
apparently updated since its official date.},
  bibliographies = {GXL}
}

@TechReport{Holyer-Daview-Dornan-1995,
  author = {Ian Holyer and Neil Daview and Chris Dornan},
  title = {The {Brisk} Project: Concurrent and Distributed Functional Systems},
  year = 1995,
  month = JUN,
  institution = {University of Bristol, Department of Computer Science},
  number = {CSTR-95-015},
  WKloc = {A-0803}
}

@Article{Holzmann-1997,
  author = {Gerard J. Holzmann},
  title = {The Model Checker {\sc Spin}},
  journal = {IEEE Transactions on Software Engineering},
  year = 1997,
  volume = 23,
  number = 5,
  month = MAY,
  WKloc = {A-1203},
  bibliographies = {SpecTech}
}

@Book{Holzmann-2003,
   author = {Gerard J. Holzmann},
   title = {The {Spin} Model Checker, Primer and Reference Manual},
   publisher = {Addison-Wesley},
   address = {Reading, Massachusetts},
   year = {2003}
}

@InProceedings{Holzmann-Puri-1999,
  author = {Gerard J. Holzmann and Anuj Puri},
  title = {A Minimized Automaton Representation of Reachable States},
  booktitle = {Software Tools for Technology Transfer},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = Springer,
  keywords = {OBDDs, {\sc Spin}},
  WKloc = {A-1202},
  bibliographies = {SpecTech}
}

@InProceedings{Honda-Vasconcelos-Yoshida-2000,
  author = {Kohei Honda and Vasco Vasconcelos and Nobuko Yoshida},
  title = {Secure Information Flow as Typed Process Behaviour},
  crossref = {ESOP2000},
  pages = {180-199},
  URL = {http://www.link.springer.de/link/service/series/0558/bibs/1782/17820180.htm},
  abstract = {We propose a new type discipline for the $\pi$-calculus in
      which secure information flow is guaranteed by static type checking.
      Secrecy levels are assigned to channels and are controlled by
      subtyping. A behavioural notion of types capturing causality of
      actions plays an essential role for ensuring safe information flow in
      diverse interactive behaviours, making the calculus powerful enough
      to embed known calculi for type-based security. The paper introduces
      the core part of the calculus, presents its basic syntactic
      properties, and illustrates its use as a tool for programming
      language analysis by a sound embedding of a secure multi-threaded
      imperative calculus of Volpano and Smith. The embedding leads to a
      practically meaningful extension of their original type discipline.},
  WKloc = {A-1145}
}

@InProceedings{Hong-1994,
  author = {Hoon Hong},
  title = {Problem Solving by Quantifier Elimination},
  crossref = {LPAR94},
  note = {invited tutorial},
  authorsAddress = {RISC, Linz hhong@risc.uni-linz.ac.at},
  abstract = {The tutorial consists of two parts:
                \begin{enumerate}
                \item We describe and illustrate the following view:
                 \quote{``Many non-trivial problem solving processes
		  are essentially quantifier elimination (qe)
		  processes.  In fact, it (qe) is a core of
		  constructive mathematics and computer science.  Thus
		  automating problem solving usually amounts to
		  devising methods for quantifier elimination.''}
                \item We describe a few quantifier elimination
		  algorithms for the first order theory of real
		  numbers (formally called real closed fields).  We
		  give several examples of their use in solving
		  various problems.  If time allows, we give a survey
		  of other theories that also admit quantifier elimination.
                \end{enumerate}}
}

@InProceedings{Honsell-RonchiDellaRocca-1990,
  title = {Reasoning about Interpretations in Qualitative $\lambda$-Models},
  pages = {505--521},
  crossref = {IFIP1990},
  author = {Furio Honsell and Ronchi della Rocca, Simona},
  WKloc = {Q-011},
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Honsell-Sanella-1999,
  author = {Furio Honsell and Donald Sanella},
  title = {Pre-logical Relations},
  year = 1999,
  institution = {University of Edinburgh},
  number = {ES-LFCS-99-405},
  OPTabstract = {},
  WKloc = {A-0900 short version},
  bibliographies = {RelMiCS, LogRel}
}

@Article{Honsell-Sannella-2002,
  author = {Furio Honsell and Donald Sannella},
  title = {Prelogical Relations},
  journal= {Information and Computation},
  year = {2002},
  volume = {178},
  pages = {23--43},
  pdf = {http://homepages.inf.ed.ac.uk/dts/pub/prelogrel-long.pdf},
  bibliographies = {LogRel},
  WKloc = {doc/pap/BIB},
  authorsnote = {People who are interested
      in logical relations might like to have a look at a variant
      called "prelogical relations" which have most of the useful
      properties of logical relations, including the basic lemma,
      but which compose, unlike logical relations.}
}

@InProceedings{Hoogendijk-1992,
  author = {Paul F. Hoogendijk},
  title = {{(Relational) Programming} Laws in the {Boom} Hierarchy
		  of Types},
  crossref = {MPC1992},
  pages = {163--190},
  WKloc = {A-0144},
  note = {Extended version to appear in Science of Computer Programming},
  abstract = {In this paper we demonstrate that the basic rules
		  and calculational techniques used in two extensively
		  documented program derivation methods can be
		  expressed, and, indeed, can be generalised within a
		  relational theory of datatypes. The two methods to
		  which we refer are the so-called ``Bird-Meertens
		  formalism'' (see [22]) and the ``Dijkstra-Feijen
		  calculus'' (see [15]).

                  The current paper forms an abridged, though
		  representative, version of a complete account of the
		  algebraic properties of the Boom hierarchy of types
		  [19,18]. Missing is an account of extensionality and
		  the so-called cross-product.},
  bibliographies = {RelMiCS}
}

@Unpublished{Hoogendijk-1993,
  author = {Paul Hoogendijk},
  title = {Generators, Destructors and Natural Transformations},
  note = {ftp://ftp.win.tue.nl/pub/math.prog.construction},
  year = 1993,
  month = JUL,
  WKloc = {A-0221},
  abstract = {A goal of a theory of datatypes is to identify
		  concepts that are common to (or a large number of)
		  datatypes and that are fundamental to the
		  specification and/or solution of a variety of
		  programming problems. An example is the concept of
		  generators first introduced in the paper ``A Class
		  of Commuting Relators'' with the purpose of
		  specifying the problem of whether two relators
		  commute. We extend the notion of generators given in
		  that paper and introduce a second notion called the
		  destructor of a relator. Basically, a destructor is
		  the generic membership relation. We show the
		  properties of generators and destructors, both in
		  isolation and in combination. A consequence of the
		  existence of generators will also be discussed.}
}

@PhDThesis{Hoogendijk-1997,
  author = {Paul Hoogendijk},
  title = {A Generic Theory of Data Types},
  school = {Eindhoven University of Technology},
  year = 1997,
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html},
  bibliographies = {RelMiCS}
}

@Article{ Hoogendijk-deMoor-2000,
  author = {Paul F. Hoogendijk and de Moor, Oege},
  title = {Container Types Categorically},
  journal = JFP,
  volume = "10",
  number = "2",
  pages = "191--225",
  year = "2000",
  CiteSeer = {http://citeseer.ist.psu.edu/article/hoogendijk00container.html},
  abstract = {A program derivation is said to be \textsl{polytypic} if
      some of its parameters are datatypes. Often these data types are
      container typpes, whose elements store data. Polytypic program
      derivations necessitate a general, non-inductive definition of
      `container (data) type'. Here we propose such a definition: a
      container type is a relator that has a membership. It is shown how
      this definition implies various other properties that are shared by
      all container types. In particular, all container types have a unique
      strength, and all natural transformations between container types are
      strong.},
  WKloc = {A-1154}
}

@InProceedings{Hoogerwoord-1992,
  author = {Rob R. Hoogerwoord},
  title = {A Logarithmic Implementation of Flexible Arrays},
  crossref = {MPC1992},
  pages = {191--207},
  WKloc = {A-0239},
  abstract = {In this paper we derive an implementation of
		  so-called {\em flexible arrays}; a flexible array is
		  an array whose size can be changed by adding or
		  removing elements at either end. By representing
		  flexible arrays by so-called {\em Braun trees}, we
		  are able to implement all array operations with
		  logarithmic---in the size of the array---time
		  complexity.

                  Braun trees can be conveniently defined in a
		  recursive way. Therefore, we use functional
		  programming to derive (recursive) definitions for
		  the functions representing the array operations.
		  Subsequently, we use these definitions to derive
		  (iterative) sequential implementations.}
}

@InProceedings{Hoogerwoord-1992a,
  author = {Rob R. Hoogerwoord},
  title = {A Derivation of Huffman's Algorithm},
  crossref = {MPC1992},
  pages = {375--378},
  WKloc = {A-0244},
  abstract = {We present a semi-formal derivation of Huffman's
		  well-known algorithm for the construction of an {\em
		  optimal encoding tree}.}
}

@article{Hoos-2012,
  author = {Hoos, Holger H.},
  title = {Programming by Optimization},
  journal = CACM,
  issue_date = {February 2012},
  volume = {55},
  number = {2},
  month = feb,
  year = {2012},
  issn = {0001-0782},
  pages = {70--80},
  numpages = {11},
  DOIURL = {http://doi.acm.org/10.1145/2076450.2076469},
  DOI = {10.1145/2076450.2076469},
  acmid = {2076469},
  OPTpublisher = {ACM},
  OPTaddress = {New York, NY, USA},
  abstract = {Avoid premature commitment, seek design alternatives, and automatically generate performance-optimized software.},
  bibliographies = {RATH}
}

@Article{Hoover-Rudniki-1996,
  author =       {H. James Hoover and Piotr Rudnicki},
  title =        {Teaching Freshman Logic with {\sc Mizar-MSE}},
  journal =      {Mathesis Universalis},
  year =         1996,
  volume =    3,
  note =      {ISSN 1426-3513, \textsf{http://www.calculemus.org/MathUniversalis/3/}}
}

@InProceedings{Hopkins-Kozen-1999,
  author = 	 {Mark W. Hopkins and Dexter C. Kozen},
  title = 	 {{Parikh}'s Theorem in Commutative {Kleene} Algebra},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {1999},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 JAN,
  WKloc = {A-1553, doc/pap/BIB},
  OPTpublisher = {},
  abstract = {Parikh's Theorem says that the commutative image of
     every context free language is the commutative image of some
     regular set. Pilling has shown that this theorem is essentially a
     statement about least solutions of polynomial inequalities. We
     prove the following general theorem of commutative Kleene
     algebra, of which Parikh's and Pilling's theorems are special
     cases: Every system of polynomial inequalities $f_i (x_1 ,\ldots,
     x_n ) \leq x_i$, $1 \leq i \leq n$, over a commutative Kleene
     algebra $K$ has a unique least solution in $K^n$; moreover, the
     components of the solution are given by polynomials in the
     coefficients of the $f_i$. We also give a closed form solution in
     terms of the Jacobian matrix.}
}

@Book{Horebeck-Lewi-1989,
  author = {Ivo van Horebeck and Johan Lewi},
  title = {Algebraic Specifications in Software Engineering, An
		  Introduction},
  year = 1989,
  publisher = Springer,
  contents = {1 Introduction
                  2 Abstract Data Types as Initial Algebras
                  3 An Algebraic Specification Language
                  4 Constructive Specifications
                  5 A Case Study: the Ferry Problem
                  6 A Case Study: th mini-PABX
                  7 Error Handling
                  8 Abstract Implementations
                  Conclusions
                  A Syntax
                  B Rapid Prototyping, the Mini-PABX
                  Bibliography
                  Index},
  UniBwM = {INF700/W10427},
  bibliographies = {RelMiCS, SpecTech}
}

@Book{Houser-1985,
  author = {Nathan Houser},
  title = {Peirce's Algebra of Logic and the Law of Distribution},
  note = Doct,
  publisher = Waterloo,
  year = 1985,
  bibliographies = {RelMiCS}
}

@Article{Houser-1987,
  author = {Nathan Houser},
  title = {Peirce's Early Work on the Algebra of Logic:
		Remarks on {Zeman's} Account},
  journal = PEIRCE,
  volume = 23,
  year = 1987,
  pages = {425--440},
  bibliographies = {RelMiCS}
}

@InCollection{Howard-1980,
  author = {W. A. Howard},
  title = {The Formulae-as-Types Notion of Construction},
  crossref = {Seldin-Hindley-1980},
  pages = {479--490},
  note = {privately circulated and cited since 1969}
}

@PhDThesis{Howard-1992,
  author = {Brian T. Howard},
  title = {Fixed points and extensionality in typed functional
		  programming languages},
  school = STANF,
  year = 1992,
  pages = 66,
  note = {also as report STAN-CS-92-1455},
  annote = {GFA},
  bibliographies = {RelMiCS}
}

@Article{Howorka-1977,
  author = {E. Howorka},
  title = {Generators for Algebras of Relations},
  journal = NOTIC,
  volume = 24,
  year = 1977,
  pages = {pp.\null{} A-4, A-5},
  bibliographies = {RelMiCS}
}

@InProceedings{Hrischuk-1999,
  author = {Curtis E. Hrischuk},
  title = {Principles for the Automated Construction of Distributed Application Software Execution Models},
  booktitle = {ICDCS 1999},
  OPTpages = {},
  year = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  WKloc = {A-1363, doc/pap/BIB},
  keywords = {software engineering, distributed system,
    distributed application, software comprehension, domain modeling,
    reverse engineering, design recovery, legacy systems, software modeling},
  abstract = {Models of software execution are used for many purposes,
    such as program understanding, re-engineering, reuse,
    performance analysis, and debugging.
    Often these models are mental pictures that are developed from
    design documents, source code examination, or experience with the system.
    Manually constructing models for a small software system
    is relatively easy, but it is expensive, difficult,
    and uncertain for a distributed software application.
    A Model Making Automation Process (MMAP) has been developed
    for the automated construction of software execution models.
    Automation is used to increase the model's precision
    and reduce the software development cost.
    It starts with a special application level trace (called an ANGIOTRACE)
    which is converted into a standard graph grammar
    (called software proper time) to build a scenario model of the execution.
    Formal, graph-rewriting operations are used for the transformations.
    This paper introduces the principles used for the automation of MMAP,
    how they are applied in MMAP, and examples of using the scenario model
    for analysis or diagnosis of a design.}
}

@Article{Hrischuk-Woodside-2002,
  author = {Curtis E. Hrischuk and C. Murray Woodside},
  title = {Logical Clock Requirements for Reverse Engineering Scenarios from a Distributed System},
  journal = {IEEE Transactions on Software Engineering},
  year = 2002,
  volume = 28,
  number = 4,
  pages = {321--339},
  month = APR,
  WKloc = {A-1361},
  annote = {Reviewed for reviews.com, 2002-10-19}
}

@InProceedings{HuLiyang-Hutton-2009,
  author =       {Liyang HU and Graham Hutton},
  title =        {Compiling COncurrency Correctly: Cutting Out the Middle Man},
  crossref =  {TFP2009},
  pages =     {17--32},
  chapter = {2},
  bibliographies = {ZhaoYuhang},
  abstract =    {The standard approach \cite{} to proving compiler correctness for concurrent languages
    requires the use of multiple translations into an intermediate process calculus.
    We present a simpler approach hat avoids the need for such an intermediate language, using a new method that allows us to directly establish
    a bisimulation between the source and target languages.
    We illustrate the technique on two small languages,
    using the Agda system to present and formally verify our compiler correctness proofs.}
}

@InProceedings{HuZhenjiang-Iwasaki-Takeichi-1996,
  author = {Zhenjiang Hu and Hideya Iwasaki and Masato Takeichi},
  title = {Construction of List Homomorphisms by Tupling and Fusion},
  crossref = {MFCS-1996},
  pages = {407--418},
  OPTabstract = {},
  WKloc = {A-0459}
}

@InProceedings{HuZhenjiang-MuShinCheng-TakeichiMasato-2004,
  author = 	 {Zhenjiang Hu and Shin-Cheng Mu and Masato Takeichi},
  title = 	 {A Programmable Editor for Developing Structured
                  Documents based on Bidirectional Transformations},
  crossref =  {PEPM2004},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  bibliographies = {EdComb},
  WKloc = 	 {A-1754, doc/pap/BIB},
  abstract = {This paper presents a novel editor supporting
     interactive refinement in the development of structured
     documents. The user performs a sequence of editing operations on
     the document view, and the editor automatically derives an
     efficient and reliable document source and a transformation that
     produces the document view. The editor is unique in its
     programmability, in the sense that the transformation can be
     obtained through editing operations. The main tricks behind are
     the utilization of the view-updating technique developed in the
     database community, and a new bidirectional transformation
     language that cannot only describe the relationship between the
     document source and its view, but also data dependency in the
     view.}

}

@Article{HuZhenjiang-MuShinCheng-TakeichiMasato-2008,
  author =       {Zhenjiang Hu and Shin-Cheng Mu and Masato Takeichi},
  title =        {A Programmable Editor for Developing Structured
                  Documents based on Bidirectional Transformations},
  journal =      {Higher-Order and Symbolic Computation},
  year =         2008,
  volume =    21,
  pages =     {89--122},
  WKloc =      {doc/pap/BIB},
  bibliographies = {EdComb},
  abstract =  {This paper presents an application of bidirectional transformation
    to the design and implementation of a novel editor
    supporting interactive refinement in the development of structured documents.
    The user performs a sequence of editing operations on a view of the document,
    and the editor automatically derives an efficient and reliable document source
    and a transformation that produces the document view.
    The editor is unique in its programmability,
    in the sense that transformations can be obtained through editing operations.
    It uses the view-updating technique developed in the database community
    and a new bidirectional transformation language
    that can describe not only the relationship between the document source and its view,
    but also the data dependency in the view.}
}

@TechReport{Huber-Dickmanns-1990,
  year = 1990,
  type = {Arbeitsbericht},
  title = {{Transputer: Softwareumgebungen und Messung
           der \"Ubertragungsleistung}},
  number = {90/01},
  month = {August},
  institution = {UniBwM/INF},
  author = {Huber, Frank and Dickmanns, Dirk},
  annote = {besonders schnell veraltet},
  address = {Neubiberg}
}

@InProceedings{Hudak-1987,
  abstract = {A new framework is presented, based on the notion of
		  a {\em partially ordered multiset} (or {\em
		  pomset}), which is able to provide not only a
		  precise operational semantics of parallel functional
		  program evaluation, but also a handle through which
		  to control such behaviour. As an operational
		  semantics, pomsets are able to distinguish between
		  call-by-value, call-by-name, call-by-need, and
		  call-by-speculation evaluation strategies (even
		  though all but the first of these have the same
		  standard semantics); and as a ``handle'' from which
		  to control operational behaviour, pomsets can
		  express most of the behaviours achieved by
		  previously proposed annotations that control not
		  only evaluation order but also the spatial mapping
		  of program to machine.},
  title = {Pomset Interpretations of Parallel Functional Programs},
  pages = {234--256},
  crossref = {FPCA-1987},
  author = {Paul Hudak}
}

@Article{Hudak-1989a,
  author = {Hudak, Paul},
  title = {Conception, Evolution, and Application of
		  Functional Programming Languages},
  journal = {ACM Computing Surveys},
  year = 1989,
  volume = 21,
  number = 3,
  pages = {359--411},
  WKloc = {A-0041},
  month = sep
}

@Misc{Hudak-1989b,
  author = {Paul Hudak},
  title = {On the Expressiveness of Purely Functional {I/O} Systems},
  year = 1989,
  WKloc = {A-0588}
}

@Article{Hudak-1996,
  author = {Paul Hudak},
  title = {Building domain-specific embedded languages},
  journal = {ACM Computing Surveys},
  volume = 28,
  number = {4es},
  pages = {196--196},
  month = dec,
  year = 1996,
  coden = {CMSVAN},
  ISSN = {0360-0300},
  bibdate = {Thu Jun 26 16:49:57 MDT 1997},
  URL = {http://www.acm.org/pubs/citations/journals/surveys/1996-28-4es/a196-hudak/},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Hudak-1998,
  author = {Paul Hudak},
  title = {Modular Domain Specific Languages and Tools},
  editor = {P. Devanbu and J. Poulin},
  booktitle = {Proceedings: Fifth International Conference on
                 Software Reuse},
  year = 1998,
  publisher = {IEEE Computer Society Press},
  pages = {134--142},
  abstract = {A \emph{domain specific language} (DSL) allows one to
                 develop software for a particular application domain
                 quickly and effectively, yielding programs that are
                 easy to understand, reason about, and maintain. On the
                 other hand, there may be a significant overhead in
                 creating the infrastructure needed to support a DSL. To
                 solve this problem, a methodology is described for
                 building domain specific \emph{embedded} languages
                 (DSELs), in which a DSL is designed within an existing,
                 higher-order and typed, programming language such as
                 Haskell or ML. In addition, techniques are described
                 for building modular interpreters and tools for DSELs.
                 The resulting methodology facilitates reuse of syntax,
                 semantics, implementation code, software tools, as well
                 as look-and-feel.},
  keywords = {software reuse, modularity, abstraction, domain
                 specific languages, functional languages, formal
                 methods},
  WKloc = {A-1242, doc/pap/BIB}
}

@Book{Hudak-2000,
  author = {Paul Hudak},
  title = {The {Haskell} School of Expression,
                  Learning Functional Programming through Multimedia},
  publisher = CUP,
  year = 2000,
  URL = {http://haskell.org/soe/},
  WKloc = {owned},
  note = {ISBN 0-521-64408-9 (paperback), ISBN 0-521-64338-4 (hardback)},
  bibliographies = {FP}
}

@Article{Hudak-Fasel-1992,
  author = {Paul R. Hudak and J.H. Fasel},
  title = {A Gentle Introduction to {Haskell}},
  journal = SIGPLAN,
  year = 1992,
  volume = 27,
  number = 5,
  pages = {1--53},
  keywords = {functional tutorial},
  WKloc = {B-0066}
}

@InProceedings{Hudak-Hughes-PeytonJones-Wadler-2007,
  author =       {Paul Hudak and John Hughes and Peyton Jones, Simon and Philip Wadler},
  title =        {A History of {Haskell}: Being lazy with class},
  pages = {12-1--12-55},
  OPTcrossref =  {},
  OPTkey =       {},
  booktitle = {Third {ACM SIGPLAN History of Programming Languages Conference (HOPL-III)}},
  OPTpages =     {},
  year =      {2007},
 publisher = {ACM},
 isbn = {978-1-59593-766-7},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  abstract = {This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.},
  DOI =      {10.1145/1238844.1238856},
  DOIURL =      {http://dx.doi.org/10.1145/1238844.1238856},
  URL =    {http://research.microsoft.com/en-us/um/people/simonpj/papers/history-of-haskell/}
}

@Misc{Hudak-Jones-1994,
  author = {Paul Hudak and Mark P. Jones},
  title = {Haskell vs.\null{} Ada vs.\null C++ vs.\null{} Awk
		  vs.\null{} $\ldots$, An Experiment in Software
		  Prototyping Productivity},
  year = 1994,
  month = JUL,
  WKloc = {A-0359},
  abstract = {We describe the results of an experiment in which
		  several conventional programming languages, together
		  with the functional language Haskell, were used to
		  prototype a Naval Surface Warfare Center (NSWC)
		  requirement for a {\em Geometric Region Server}. The
		  resulting programs and development metrics were
		  reviewed by a committee chosen by the navy. The
		  results indicate that the Haskell prototype took
		  significantly less time to develop and was
		  considerably more concise and easier to understand
		  than the corresponding prototypes written in several
		  different imperative languages, including Ada and C++.}
}

@Article{Hudak-Young-1991,
  author = {Paul Hudak and Jonathan Young},
  title = {Collecting Interpretations of Expressions},
  year = 1991,
  volume = 13,
  pages = {269--290},
  number = 2,
  month = APR,
  journal = ACM-TOPLAS,
  WKloc = {A-0015},
  ACMCAT = {D32 (Programming Languages): Language Classification
		  --- applicative languages;
                  F32 (Logics and Meanings of Programs): Semantics of
		  Programming languages --- denotational semantics;
                  F43 (Mathematical Logic and Formal Languages):
		  Formal Languages},
  abstract = {A {\em collecting interpretation of expressions} is
		  an interpretation of a program that allows one to
		  answer questions of the sort: ``What are the
		  possible values to which an expression might
		  evaluate during program execution?'' Answering such
		  questions in a denotational framework is akin to
		  traditional data flow analysis and, when used in the
		  context of abstract interpretation, allows one to
		  infer properties that approximate the run-time
		  behaviour of expression evaluation.

                  Exact collection interpretations of expressions are
		  developed for three abstract functional languages: a
		  strict first-order language, a nonstrict first-order
		  language, and a nonstrict higher order language (
		  the full untyped lambda calculus with constants). It
		  is argued that the method is simple (in particular,
		  no powerdomains are needed), natural (it captures
		  the intuitive operational behaviour of a cache), yet
		  more expressive than existing methods (it is the
		  first exact collecting interpretation for either
		  nonstrict or higher order languages). Correctness of
		  the interpretations with respect to the standard
		  semantics is shown via a generalization of the
		  notion of strictness. It is further shown how to
		  form abstractionsof these exact interpretations,
		  using as an example a collecting strictness analysis
		  which yields compile-time information not previously
		  captured by conventional strictness analyses.},
  bibliographies = {RelMiCS}
}

@TechReport{Hudson-1987,
  author = {S.E. Hudson},
  title = {Incremental attribute evaluation: an algorithm for lazy
		  evaluation in graphs},
  number = {87-20},
  year = 1987,
  institution = {University of Arizona}
}

@Article{Huet-1975,
  author = {G. P. Huet},
  title = {A Unification Algorithm for Typed $\lambda$-Calculus},
  journal = {Theoretical Computer Science},
  year = 1975,
  OPTkey = {},
  volume = 1,
  OPTnumber = {},
  OPTmonth = {},
  pages = {27--57},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Huet-1990,
  author = {G. Huet},
  title = {Design Issues for a Computer-Aided Environment for
		  Constructive Mathematics},
  crossref = {DISCO90},
  pages = 55,
  note = {invited lecture}
}

@Article{Huet-1993,
  author = {G\'erard Huet},
  title = {An Analysis of {Bohm}'s Theorem},
  journal = TCS,
  year = 1993,
  volume = 121,
  pages = {145--167},
  abstract = {[When I set to understand fully Bohm's theorem, down to
      programming a running Bohm discriminator, I used DB indices, which
      generalise well to Bohm trees (ie potentially infinite lambda-terms).
      I do not think I would have succeded in programming this stuff
      without this explicit and concrete control on variable references.
      And I could even publish the code itself, which is available as
      commented Caml code in [this] paper]}
}

@Article{Huet-1994,
  author = {G\'erard Huet},
  title = {Residual theory in lambda calculus: a formal development},
  journal = JFP,
  year = 1994,
  volume = 4,
  number = 3,
  pages = {371--394},
  abstract = {[where a fair portion of the
       syntactic theory of beta-reduction is done
       completely formally, using de Bruijn indices again.]}
}

@Article{Huet-1997,
  author = {G\'erard Huet},
  title = {The {Zipper}},
  journal = JFP,
  year = 1997,
  volume = 7,
  number = 5,
  pages = {549--554},
  note = {Functional Pearl},
  WKloc = {A-0981},
  bibliographies = {EdComb}
}

@Article{Huet-Lang-1978,
  author = {G. P. Huet and B. Lang},
  title = {Proving and Applying Program Transformations
		  Expressed with Second-Order Patterns},
  journal = {Acta Informatica},
  year = 1978,
  volume = 11,
  pages = {31--55},
  annote = {cited in \cite{Wolfram-1991}}
}

@TechReport{Huet-Levy-1979,
  author = {G{\'e}rard Huet and Jean-Jacques L{\'e}vy},
  title = {Call By Need Computations in Non-Ambiguous Linear Term Rewriting Systems},
  institution = {INRIA},
  year = 1979,
  number = 359,
  keywords = {lazy pattern matching},
  annote = {Cited by \cite{Kennaway-1990a}.
                  Strongly sequential term rewriting systems
                  also explained in \cite{Klop-1992}.}
}

@InCollection{Huet-Levy-1991,
  author = {G{\'e}rard Huet and Jean-Jacques L{\'e}vy},
  title = {Computations in orthogonal rewriting systems, {I and II}},
  year = 1991,
  booktitle = {Computational Logic: Essays in Honor of Alan Robinson},
  editor = {J.-L. Lassez and G. Plotkin},
  pages = {395--443},
  publisher = {MIT Press},
  address = {Cambridge, MA}
}

@InProceedings{Huet-Saibi-1998,
  author =       {G{\'e}rard Huet and Amokrane Sa{\"{\i}}bi },
  title =        {Constructive Category Theory},
  booktitle = {Proceedings of the Joint CLICS-TYPES Workshop on Categories and Type Theory},
  year =      1998,
  DOI =     {10.1.1.39.4193},
  WKloc = {doc/pap/BIB},
  annote =    {These notes are part of a preliminary attempt
       at developing abstract algebra in constructive type theory.
       The developments that follow are inspired
       from a previous axiomatization by P. Aczel in LEGO in Jan. 1993,
       as an initial step to a program of formal development of Galois Theory 1].
       Our version of type theory is the Calculus of Inductive COnstructions,
       as implemented in Coq V5.10.
       This paper give the full transcript of the Coq axiomatisation.

       In this note we develop one possible axiomatisation
       of the notion of category
       by modeling objects as types
       and Hom-sets as Hom-setoids of arrows
       parameterized by their domain and codomain types.
       Thus we may quotient arrows, but not objects.
       We develop in this setting functors,
       as functions on objects, and extentional maps on arrows.
       We show that CAT is a category,
       and we do not need to distinguish to this effect
       ``small'' and ``big'' categories.
       We rather have implicitly
       categories as relatively small structures indexed by a universe.
       Thus we just need two instances of the same notion of category
       in order to define CAT.

       We then construct the Functor Category,
       with the natural definition of natural transformations.
       We then show the Interchange Law,
       which exhibits the 2-categorical structure of the Functor Category.
       We end this paper by giving a corollary to Yoneda's lemma.

       This incursion in Constructive Category Theory shows
       that Type Theory is adequate to represent faithfully categorical reasoning.
       Three ingredients are essential:
       \Sigma-types, to represents structures,
       dependent types,
       so that arrows are indexed with their domains and codomains,
       and a hierarchy of universes,
       in order to escape the foundational difficulties.
       Some amount of type reconstruction is necessary,
       in order to write equations between arrows
       without having to indicate their type other than at their binder,
       and notational abbreviations,
       allowing e.g. infix notation,
       are necessary to offer the formal mathematician
       a language close to the ordinary informal categorical notation.}
}

@InProceedings{Huet-Saibi-2000,
  author =       {G{\'e}rard Huet and  Amokrane Sa{\"{\i}}bi},
  title =        {Constructive Category Theory},
  crossref = {MilnerFestschrift2000},
  pages = {239--275}
}

@InProceedings{Hughes-1985,
  author = {John Hughes},
  title = {Lazy Memo-functions},
  booktitle = {Conference on Functional Programming Languages and
                 Computer Architecture, Nancy, France},
  year = 1985,
  series = LNCS,
  volume = 201,
  editor = {Jean-Pierre Jouannaud},
  pages = {129--146},
  publisher = {Springer-Verlag},
  WKloc = {A-1718}
}

@Article{Hughes-1989,
  AUTHOR = {John Hughes},
  TITLE = {Why Functional Programming Matters},
  JOURNAL = {Computer Journal},
  VOLUME = {32},
  NUMBER = {2},
  PAGES = {98--107},
  YEAR = {1989},
  WKloc = {doc/pap/BIB},
  bibliographies = {FP}
}

@InProceedings{Hughes-1992,
  author = {John Hughes},
  title = {Pretty-printing: An Exercise in Functional Programming},
  pages = {11--13},
  abstract = {At the heart of functional programming is the treatment of
             functions as data. Just as arithmetic operators compute
             numbers, so higher-order functions can be used to compute new
             functions old ones. In the limit, as Backus advocated, the
             programmer might write no first-order code at all but simply
             compute the desired program using the operators of an algebra
             of programs. More commonly, higher-order functions such as map,
             filter and foldr are used to express commonly occurringidioms,
             and other parts are programmed in a first-order style.},
  crossref = {MPC1992},
  WKloc = {A-0234}
}

@InProceedings{Hughes-1995,
  author = {John Hughes},
  title = {{The Design of a Pretty-printing Library}},
  pages = {53--96},
  crossref = {AFP1995},
  WKloc = {A-0740, B-0116},
  AuthorURL = {http://www.cse.chalmers.se/~rjmh/Papers/pretty.html},
  bibliographies = {FP}
}

@Article{Hughes-1998,
  author = {John Hughes},
  title = {Type Specialization},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 14},
  WKloc = {A-0902, 57--62},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Hughes-1999,
  author = {John Hughes},
  title = {Restricted Datatypes in {Haskell}},
  crossref = {Haskell1999},
  URL = {http://www.cs.chalmers.se/~rjmh/Papers/restricted-datatypes.ps},
  WKloc = {A-1591, doc/pap/BIB},
  pages = {85--100}
}

@InProceedings{Hughes-2000,
  author = {John Hughes},
  title = {The Correctness of Type Specialisation},
  crossref = {ESOP2000},
  pages = {215--229},
  WKloc = {A-1003}
}

@Article{Hughes-2000a,
  author = {John Hughes},
  title =        {Generalising Monads to Arrows},
  journal =      SCICOP,
  year =         2000,
  volume =    37,
  DOI = {10.1016/S0167-6423(99)00023-4},
  DOIURL = {http://dx.doi.org/10.1016/S0167-6423(99)00023-4},
  number = {1--3},
  pages =     {67--111},
  month =     MAY,
  abstract = {Monads have become very popular for structuring functional programs
    since Wadler introduced their use in 1990.
    In particular, libraries of combinators are often based on a monadic type.
    Such libraries share (in part) a common interface, from which numerous benefits flow,
    such as the possibility to write generic code which works together with any library.
    But, several interesting and useful libraries
    are fundamentally incompatible with the monadic interface.
    In this paper I propose a generalisation of monads, which I call arrows,
    with significantly wider applicability.
    The paper shows how many of the techniques of monadic programming
    generalise to the new setting,
    and gives examples to show that the greater generality is useful.
    In particular, three non-monadic libraries for efficient parsing,
    building graphical user interfaces, and programming active web pages
    fit naturally into the new framework.}
}

@InProceedings{Hughes-Launchbury-1992,
  author = {John Hughes and John Launchbury},
  title = {Reversing Abstract Interpretations},
  crossref = {ESOP1992},
  pages = {269--286},
  authorsAddress = {University of Glasgow,
		  $\{$rjmh,jl$\}$\@dcs.glasgow.ac.uk},
  abstract = {Many semantic analyses of functional languages have
		  been developed using the Cousots' {\em abstract
		  interpretation} framework [CC77]. Some, such as
		  Mycroft's poneering strictness analysis [Myc81] and
		  Burn, Hankin and Abramsky's extension of it to
		  higher-order [BHA86], operate on {\em abstract
		  values} representing the past history of the
		  computation, and are therefore called {\em forward
		  analyses}. Others, such as Wadler and Hughes'
		  projection-based strictness analysis
		  \cite{WadlerHughes87}, or Hall's analysis of
		  strictness patterns [Hal87] propagate {\em abstract
		  contexts} representing the future of the
		  computation, and are called {\em backwards
		  analyses}. However, although the type of abstract
		  information may suggest a ``natural'' direction, it
		  is in fact possible to perform any analysis in
		  either direction. The goal of this paper is to show
		  how to reverse any given analysis.}
}

@Misc{Hughes-Sparud-1995,
  author = {John Hughes and Jan Sparud},
  title = {{Haskel++}: An Objecto-Oriented Extension of {Haskell}},
  year = 1995,
  month = APR,
  WKloc = {A-0821}
}

@TechReport{HughesDJD-2006,
  author = 	 {Dominic J. D. Hughes},
  title = 	 {Proofs Without Syntax},
  institution =  {Stanford University},
  year = 	 {2006},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  note = {to appear in Annals of Mathematics},
  URL = 	 {http://arxiv.org/abs/math/0408282},
  OPTnote = 	 {arXiv:math.lO/0408282v1 20 Aug 2004},
  WKloc = 	 {A-1562}
}

@PhdThesis{HughesJesse-2001,
  author = 	 {Jesse Hughes},
  title = 	 {A Study of Categories of Algebras and Coalgebras},
  school = 	 {Carnegie Mellon University},
  year = 	 2001,
  month = 	 MAY,
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {MonCoAlg},
  annote = {source  for ``creates pushouts'': Def.~1.2.2 p.~17},
  abstract = 	 {This thesis is intended to help develop the theory of coalgebras by,
    first, taking classic theorems in the theory of universal algebras and dualizing them
    and, second, developing an internal logic for categories of coalgebras.

    We begin with an introduction to the categorical approach to algebras and the
    dual notion of coalgebras. Following this, we discuss (co)algebras for a (co)monad
    and develop a theory of regular subcoalgebras which will be used in the internal
    logic. We also prove that categories of coalgebras are complete, under reasonably
    weak conditions, and simultaneously prove the well-known dual result for categories
    of algebras. We close the second chapter with a discussion of bisimulations in which
    we introduce a weaker notion of bisimulation than is current in the literature, but
    which is well-behaved and reduces to the standard definition under the assumption
    of choice.

    The third chapter is a detailed look at three theorem's of G. Birkhoff [Bir35, Bir44],
    presenting categorical proofs of the theorems which generalize the classical
    results and which can be easily dualized to apply to categories of coalgebras. The
    theorems of interest are the variety theorem, the equational completeness theorem and
    the subdirect product representation theorem. The duals of each of these theorems
    is discussed in detail, and the dual notion of ``coequation'' is introduced and several
    examples given.

    In the final chapter, we show that first order logic can be interpreted in categories
    of coalgebras and introduce two modal operators to first order logic to allow reasoning
    about ``endomorphism-invariant'' coequations and bisimulations internally. We also
    develop a translation of terms and formulas into the internal language of the base
    category, which preserves and reflects truth. Lastly, we introduce a Kripke-Joyal style
    semantics for $\mathcal{L}(\mathcal{E}_{\mathbbb{G}})$,
    as well as a pointwise semantics which reflects the intuition of coequation forcing
    at a point or subset of a coalgebra.}
}

@Article{Humberstone-1979,
  author = {I. L. Humberstone},
  title = {Interval Semantics for Tense Logic: some Remarks},
  journal = JPHIL,
  volume = 8,
  year = 1979,
  pages = {171--196},
  bibliographies = {RelMiCS}
}

@Article{Humberstone-1983,
  author = {I. L. Humberstone},
  title = {Inaccessible Worlds},
  journal = NOTRE,
  volume = 24,
  year = 1983,
  pages = {346--352},
  bibliographies = {RelMiCS}
}

@InProceedings{Hungar-1991,
  title = {Complexity of Proving Program Correctness},
  author = {Hardi Hungar},
  pages = {459--474},
  crossref = {TACS1991},
  abstract = {The spectrum of a formula is the set of finite data
		  structures in which it is valid. It is known that
		  for some program logics the classes of spectra form
		  complete subclasses of well known complexity
		  classes. This means that for those logics we know
		  how hard it is to {\em decide} the set of finite
		  models. We extend those results by determining
		  complexity classes corresponding to partial
		  correctness assertions about programs from
		  sublanguages of Clarke's language {\bf L4}.

                  We proceed to show that syntax-directed proof
		  systems are adequate tools for {\em proving} partial
		  correctness assertions: It is not more difficult to
		  construct a proof for a valid assertion than to
		  decide its validity.  This holds if the programs are
		  simple while-programs or if they belong to some
		  sublanguage of {\bf ALGOL} like {\bf L4}, for which
		  relatively complete proof systems are rather
		  sophisticated.}
}

@InProceedings{Hungar-1993,
  author = {Hardi Hungar},
  title = {The Complexity of Verifying Functional Programs},
  pages = {428--439},
  crossref = {STACS1993},
  WKloc = {A-0158},
  abstract = {The set of finite interpretations in which a formula is valid
             is called the spectrum of the formula. For some program logics,
             the classes of spectra form complete subclasses of well known
             complexity classes. For various imperative and functional
             programming languages we know the complexity classes
             corresponding to the classes of spectra of partial correctness
             formulae. This means that for these formulae we know how hard
             it is to {\em decide} the sets of finite models.

             For some
             imperative languages it has already been shown that {\em
             constructing formal proofs} for valid formulae is of the same
             order of complexity. In this paper we prove the same result,
             i.e.~that proofs can be constructed efficiently, for functional
             languages where recursive functions of arbitrary finite type
             are allowed.

             Since denotational semantics translates
             imperative programs into functional terms, the proof system for
             functional programs gives one for imperative programs as well.
             Choosing the right denotational semantics, we can show the
             effiency of the resulting verification method for
             Clarke'slanguage {\bf L4}.}
}

@Article{Huntington-1904,
  author = {Edward V. Huntington},
  title = {Sets of Independent Postulates for the Algebra of Logic},
  journal = TRAMS,
  volume = 5,
  year = 1904,
  pages = {288--309},
  bibliographies = {RelMiCS}
}

@Article{Huntington-1933,
  author = {Edward V. Huntington},
  title = {New Sets of Independent Postulates for the Algebra of
                Logic, with Special Reference to {Whitehead} and {Russell's}
                Principia Mathematica},
  journal = TRAMS,
  year = {    1933},
  pages = {    274--304},
  volume = {   35},
  bibliographies = {RelMiCS}
}

@Article{Huntington-1933a,
  author = {Edward V. Huntington},
  title = {Boolean Algebra. {A} Correction},
  journal = TRAMS,
  year = {    1933},
  pages = {    557--558},
  volume = {   35},
  bibliographies = {RelMiCS}
}

@Article{Hurtubise-Kamran-1992,
  author = {Hurtubise, J.C. and Kamran, N.},
  title = {Projective connections, double fibrations, and
		  formal neighbourhoods of lines},
  journal = {Mathematische Annalen},
  year = 1992,
  volume = 292,
  number = 3,
  pages = {383--}
}

@Book{Huth-2001,
  author = {Michael R. A. Huth},
  title = {Secure Communicating Systems: Design, Analysis, and Implementation},
  publisher = CambridgeUP,
  year = 2001,
  ISBN = {052180731X},
  URL = {http://uk.cambridge.org/order/WebBook.asp?ISBN=052180731X},
  bibliographies = {RelMiCS, SpecTech}
}

@InProceedings{Huth-Jung-Keimel-1994,
  title = {Linear Types, Approximation, and Topology},
  author = {Michael Huth and Achim Jung and Klaus Keimel},
  pages = {110--114},
  crossref = {LICS9},
  abstract = {We enrich the $*$-autonomous category of complete lattices
      and maps preserving all suprema with the important concept of {\em
      approximation\/} by specifying a $*$-autonomous full subcategory LFS
      of {\em linear FS-lattices}. This is the greatest $*$-autonomous full
      subcategory of linked bicontinuous lattices. The modalities !() and
      ?() mediate a duality between the upper and lower powerdomains. The
      distributive objects in LFS give rise to the {\em compact closed\/}
      $*$-autonomous full subcategory CD of {\em completely distributive
      lattices}. We characterize algebraic objects in LFS by forbidden
      substructures `\`a la Plotkin.'}
}

@Book{Huth-Ryan-2000,
  author = {Michael R. A. Huth and Mark D. Ryan},
  title = {Logic in Computer Science, Modelling and Reasoning about Systems},
  year = 2000,
  publisher = CambridgeUP,
  ISBN = {0-521-65200-6 hardback, 0-521-65602-9 paperback},
  URL = {http://www.cs.bham.ac.uk/research/lics/},
  UniBwM = {Mag/YF799},
  McMaster = {QA 76.9 .L63 H88 2000},
  bibliographies = {SpecTech, RelMiS}
}

@Book{Huth-Ryan-2004,
  author = {Michael R. A. Huth and Mark D. Ryan},
  title = {Logic in Computer Science, Modelling and Reasoning about Systems},
  year = 2004,
  publisher = CambridgeUP,
  edition = {2nd},
  ISBN = {0-521-54310-X},
  URL = {http://www.cs.bham.ac.uk/research/lics/},
  bibliographies = {SpecTech, RelMiS}
}

@InProceedings{Hutter-2000,
  author = 	 {Dieter Hutter},
  title = 	 {Management of Change in Structured Verification},
  crossref = {ASE2000},
  pages =	 {23--34},
  year =	 {2000},
  bibliographies = {OPG},
  abstract = {The use of formal methods in large complex
     applications implies the need for an evolutionary formal program
     development in which specification and verification phases are
     interleaved. But any change of a specification either by adding
     new parts or by changing erroneous parts affects existing
     verification work in a subtle way. In this paper we present a
     truth maintenance system for structured specification and
     verification. It is based on the simple but powerful notion of a
     development graph as an underlying datastructure to represent an
     actual consistent state of a formal development. Based on this
     notion we try to minimize the consequences of changes of existing
     verification work.}
}

@Article{Hutton-1992,
  author = {Hutton, Graham},
  title = {Higher-Order Functions for Parsing},
  journal = {Journal Functional Programming},
  year = 1992,
  volume = 2,
  number = 3,
  pages = {323--343},
  month = JUL,
  authorsAddress = {Glasgow / {\tt graham@cs.chalmers.se}},
  WKloc = {A-0282},
  annote = {Some definitions entered by Bernd Grobauer as {\tt
		  Parse.hs}},
  abstract = {In {\em combinator parsing}, the text of parsers
		  resembles BNF notation. We present the basic method,
		  and a number of extensions. We address the sepcial
		  problems presented by whitespace, and parsers with
		  separate lexical and syntactical phases. In
		  particular, a combining form for handling the
		  'offside rule' is given. Other extensions to the
		  basic method includ an 'into' combining form with
		  many useful applications, and a simple means by
		  which combinator parsers can produce more
		  informative error messages.},
  review = {This paper is a fine example of the use of
		  higher-order functions in a real application: {\em
		  parsing}. The viewpoint of parsing that is presented
		  is simple: parsers are functions, and big parsers
		  are made from little parsers by combining
		  functions. The framework thus centres on the design
		  of the combinators that serve as te 'glue'. A
		  variety of combinators are defined --- ones for
		  sequencing, alternation, repetition, etc.\null{} ---
		  and it is clear from the methodology hpw to create
		  new combinators to serve one's particular prser
		  application. The simple parser design presented at
		  the outset is gradually refined to handle more and
		  more complex situations, including such things as
		  good error propagation. Overall the paper serves two
		  roles: it describes an interesting, elegant
		  framework for building parsers; and it demonstrates
		  the utility of higher-order functions.}
}

@PhDThesis{Hutton-1992PhD,
  author = {Graham Muir Hutton},
  title = {Between Functions and Relations in Calculating Programs},
  year = 1992,
  month = OCT,
  school = {Department of Computing Science, University of Glasgow},
  WKloc = {A-0853},
  bibliographies = {RelMiCS}
}

@Article{Hutton-Meijer-1993,
  author = {Graham Hutton and Erik Meijer},
  title = {Back to Basics: Deriving Representation Changers
		  Functionally},
  journal = {Journal for Functional Programming},
  year = 1993,
  volume = 1,
  number = 1,
  month = JAN,
  WKloc = {A-0387},
  bibliographies = {RelMiCS},
  abstract = {A representation changer is a function that converts
		  a concrete representation of an abstract value into
		  a different concrete representation of that value.
		  Many useful functions can be recognised as
		  representation changers; examples include compilers,
		  and arithmetic functions such as addition and
		  multiplication. Functions that can be specified as
		  the right inverse of other functions are special
		  cases of representation changers.

                  In recent years, a number of authors have used a
		  relational calculus to derive representation
		  changers from their specifications.  In this paper
		  we show that the generality of relations is not
		  essential, and representation changers can be
		  derived within the more basic setting of functional
		  programming.  We illustrate our point by deriving a
		  carry-save adder and a base-converter, two functions
		  which have previously been derived relationally.}
}

@Article{Hutton-Meijer-1998,
  author = {Graham Hutton and Erik Meijer},
  title = {Monadic Parsing in {Haskell}},
  journal = JFP,
  year = 1998,
  volume = 8,
  number = 4,
  month = JUL,
  pages = {437--444}
}

@TechReport{Huwig-Poigne-1986,
  author = {Huwig, Hagen and Poign{\'e}, Axel},
  title = {A note on inconsistencies caused by fixpoints in a cartesian closed category},
  institution = {Universit\"at Dortmund},
  year = 1986,
  type = {Forschungsbericht},
  number = 216
}

@Misc{Inets,
  author =    {Abubakar Hassan and Eugen Jiresch},
  title =     {Interaction Nets Programming Language},
  howpublished = {\url{https://gna.org/projects/inets/}, \url{https://gna.org/svn/?group=inets}, last accessed 2015-11-26},
  month =     MAR,
  year =      2012,
  note =      {(Source code for the ``Inets'' system.)}
}

@Misc{ISOZ,
  title = {{ISO/IEC~13568:2002} Information Technology --- {Z} Formal Specification Notation --- Syntax, Type System and Semantics},
  organisation = {ISO/IEC},
  key = {{ISO/IEC~13568:2002}},
  note = {International Standard},
  year = 2002,
  annote = {http://web.comlab.ox.ac.uk/oucl/research/groups/zstandards/index.html}
}

@TechReport{Ibrahim-vanEmden-1992,
  author = 	 {H. Ibrahim and M.H. van Emden},
  title = 	 {Towards Applicative Relational Programming},
  institution =  {arXiv},
  year = 	 {1992},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTnumber = 	 {cs.PL/0602099 v1 28 Feb. 2006},
  OPTaddress = 	 {},
  OPTmonth = 	 {16 March},
  WKloc = {A-1636, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Functional programming comes in two flavours: one
     where ``functions are first-class citizens'' (we call this
     applicative) and one which is based on equations (we call this
     declarative). In relational programming clauses play the role of
     equations. Hence Prolog is declarative. The purpose of this paper
     is to provide in relational programming a mathematical basis for
     the relational analog of applicative functional programming. We
     use the cylindric semantics of first-order logic due to Tarski
     and provide a new notation for the required cylinders that we
     call tables. We define the Table/Relation Algebra with operators
     sufficient to translate Horn clauses into algebraic form. We
     establish basic mathematical properties of these operators. We
     show how relations can be first-class citizens, and devise
     mechanisms for modularity, for local scoping of predicates, and
     for exporting/importing relations between programs.}
}

@InProceedings{Iglewski-Madey-1996,
  author = {M. Iglewski and Jan Madey},
  title = {Software Engineering Issues Emerged from Critical
                  Control Applications},
  booktitle = {{{$2^{nd}$} IFAC Workshop on Safety and Reliability in
      Emerging Control Technologies, Daytona Beach, FL, 1-3 November 1995}},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {??},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  year = 1996,
  OPTorganization = {},
  publisher = Elsevier,
  OPTaddress = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Imielinski-Lipski-1984,
  author = {T. Imielinski and  Lipski, {Witold Jr.}},
  title = {The Relational Model of Data and Cylindric Algebras},
  journal = JCOMSYS,
  volume = 28,
  year = 1984,
  pages = {80--102},
  bibliographies = {RelMiCS}
}

@Article{Immerman-Kozen-1989,
  author = {Immerman, Neil and Kozen, Dexter},
  title = {Definability with Bounded Number of Bound Variables},
  journal = {Information and computation},
  year = 1989,
  volume = 83,
  number = 2,
  pages = {121--},
  month = NOV
}

@InProceedings{Impagliazzo-Pitassi-Urquhart-1994,
  title = {Upper and Lower Bounds for Tree-Like Cutting Planes Proofs},
  author = {Russell Impagliazzo and Toniann Pitassi and Alasdair Urquhart},
  pages = {220--228},
  crossref = {LICS9},
  abstract = {In this paper we study the complexity of Cutting Planes (CP)
      refutations, and tree-like CP refutations. Tree-like CP proofs are
      natural and still quite powerful. In particular, the propositional
      pigeonhole principle (PHP) has been shown to have polynomial-sized
      tree-like CP proofs. Our main result shows that a family of
      tautologies, introduced in this paper requires exponential-sized
      tree-like CP proofs. We obtain this result by introducing a new
      method which relates the size of a CP refutation to the communication
      complexity of a related search problem. Because these tautologies
      have polynomial-sized Frege proofs, it follows that tree-like CP
      cannot polynomially simulate Frege systems}
}

@Book{Ince-1993,
  author = {D. C. Ince},
  title = {An Introduction to Discrete Mathematics, Formal System
                  Specification and {Z}},
  publisher = {Oxford University Press},
  edition = {2nd},
  series = {Oxford Applied Mathematics and Computing Science Series},
  year = 1993,
  length = 296,
  ISBN = {0-19-853837-5 or 0-19-853836-7 (pbk)},
  price = {\pounds30.00 (hbk) \pounds15.00 (pbk)},
  other = {1st edition, 1988},
  note = {ISBN 0-19-853836-7 (pbk), 296 pages, URL: \textsf{http://www.oup.co.uk/isbn/0-19-853836-7}},
  URL = {http://www.oup.co.uk/isbn/0-19-853836-7},
  UniBwM = {I-MAT001/R17132, L-MAT001/R17265},
  WKloc = {owned, \lent{Mark Lawford}}
}

@InProceedings{Indyk-1995,
  author = {P. Indyk},
  title = {Optimal Simulation of Automata by Neural Nets},
  crossref = {STACS1995},
  pages = {337--348},
  authorsAddress = {Warszawa},
  abstract = {The problem of simulation of automata by neural
		  networks is investigated. In the case of discrete
		  networks with polynomially bounded weights, the
		  optimal lower and uper bounds for the number of
		  neurons necessary to simulate any finite automata of
		  size $n$ are presented. For the analog case we prove
		  the 15-neuron upper bound for any finite
		  automaton. By extending this construction we show
		  that a 25-neuron network may simulate any Turing
		  machine, and hence its behaviour is undecidable.}
}

@Book{InherHier90,
  UniBwM = {KYB800/V7740},
  year = 1991,
  title = {Inheritance Hierarchies in Knowledge Representation and Programming Languages},
  publisher = {John Wiley \& Sons},
  editor = {Maurizio Lenzerini and Daniele Nardi and Maria Simi}
}

@InProceedings{Inverardi-VenturiniZilli-1994,
  author = {Paola Inverardi and Venturini Zilli, Marisa},
  title = {Rational Rewriting},
  crossref = {MFCS94},
  pages = {433--442},
  authorsAddress = {Pisa/L'Aquila},
  WKloc = {A-0313},
  abstract = {Recently, a great amount of work has been dedicated
		  to the study of non-terminating rewrite relations as
		  they naturally arise when one wants to rewrite
		  infinite terms.},
  annote = {=== TGVnote ===}
}

@InProceedings{Inverardi-VenturiniZilli-1994-x,
  author = {Paola Inverardi and Venturini Zilli, Marisa},
  title = {Rational Rewriting},
  pages = {433--442},
  authorsAddress = {Pisa/L'Aquila},
  WKloc = {A-0313},
  abstract = {Recently, a great amount of work has been dedicated
		  to the study of non-terminating rewrite relations as
		  they naturally arise when one wants to rewrite
		  infinite terms.},
  annote = {=== TGVnote ===},
  booktitle = {Mathematical Foundations of Computer Science 1994,
		  19th International Symposium, {MFCS '94, Ko\v{s}ice,
		  Slovakia, August 1994, Proceedings}},
  year = 1994,
  editor = {Igor Pr\'{\i}vara and Branislav Rovan and Peter Ru\v{z}i\v{c}ka},
  volume = 841,
  series = {LNCS},
  publisher = {Springer-Verlag},
  UniBwM = {INF700/Z5830-19}
}

@MastersThesis{Ireland-1989,
  author = {Evan Peter Ireland},
  title = {Writing Interactive and File-Processing Functional
			Programs: A Method and Implementation},
  school = {Victoria University of Wellington},
  year = 1988,
  month = MAR,
  file = {~kahl/doc/pap/ireland-thesis},
  WKloc = {B-0032},
  abstract = {Functional programming languages promise to
		  revolutionise the programming process, but at
		  present fail to be seriously considered by
		  ``real-world'' programmers. This is in part due to
		  inadequate techniques for writing interactive and
		  file-processing functional programs. The aim of this
		  thesis was to develop and implement an effective
		  technique of writing such programs while retaining
		  all the advantages and essential properties of
		  functional programming languages.}
}

@Book{Ishiguro-19900,
  title = {Leibniz' Philosophy of Logic and Language},
  author = {Hid{\'e} Ishiguro},
  publisher = {Duckworth},
  address = {London},
  year = 1990,
  edition = 2,
  bibliographies = {RelMiCS}
}

@Article{Itkin-1980,
  author = {V.E. Itkin},
  title = {Characterization of Structured Control-Flow Graphs},
  journal = {Doklady Akademii Nauk SSSR},
  year = 1980,
  note = {(In Russian)},
  volume = 250,
  number = 2,
  pages = {1077-1080},
  annote = {Some topological criteria for compositional structure of a
      flow graph are proposed in terms of mixed computation.}
}

@InCollection{Itkin-1983,
  author = {V.E. Itkin},
  title = {Natural Modularity and Symmetry of Structured Programs},
  booktitle = {Compilation and Program Models},
  pages = {13-22},
  publisher = CCN,
  year = 1983,
  note = {(In Russian)},
  annote = {Some compositional criteria for topological structure of a
      parallel flow graph are given.}
}

@InCollection{Itkin-1983a,
  author = {V.E. Itkin},
  title = {Dynamic Program Parallelization Based on Mixed Computation},
  booktitle = {Theoretical Problems in Parallel Programming and
      Multiprocessor Computers},
  pages = {110-126},
  publisher = {Novosibirsk, USSR},
  year = 1983,
  note = {(In Russian)},
  annote = {Mixed computation of programs is defined in abstract terms as
      a set of interactions between a group of constant assignment
      statements and other program statements.}
}

@InCollection{Itkin-1983b,
  author = {V.E. Itkin},
  title = {Incompleteness As an Attribute of Research Programs},
  booktitle = {Methodological Problems of Research Programs},
  pages = {54-64},
  publisher = {Novosibirsk, USSR},
  year = 1983,
  note = {(In Russian)},
  annote = {Incompleteness has been regarded as a motor of human
      constructive activities. Some criteria of effectivity of partial
      activity are investigated. A general definition of program and a
      concept of generator as a reflexive superstructure upon the program
      are given.}
}

@InCollection{Itkin-1983c,
  author = {V.E. Itkin},
  title = {On Partial and Mixed Program Execution},
  booktitle = {Program Optimization and Transformation},
  pages = {17-30},
  publisher = CCN,
  year = 1983,
  note = {(In Russian)},
  annote = {Dashed-line and polyvariant strategies of mixed computation
      are developed. The program graph node mark 'ignoring' and an
      indication of direction for a residual program representation is
      proposed. The thesis is advanced that the concept of a program being
      structured is similar to that of a program being suited to effective
      mixed computation.}
}

@Article{Itkin-1984,
  author = {V.E. Itkin},
  title = {Algebra of Mixed Program Execution},
  journal = {Doklady Akademii Nauk SSSR},
  year = 1984,
  note = {(In Russian)},
  volume = 275,
  number = 6,
  pages = {1332-1336},
  annote = {An algebra of programs, deterministic memory states and
      subsets of the set of memory locations is given. By means of this
      algebra, the correctness of some superoperators of mixed computation
      is proved.}
}

@InCollection{Itkin-1984a,
  author = {V.E. Itkin},
  title = {Program Parallelization Algebra},
  booktitle = {Theoretical and Applied Aspects of Parallel Information
      Processing},
  pages = {3-24},
  publisher = CCN,
  year = 1984,
  note = {(In Russian)},
  annote = {The correctness of some superoperators of the dashed-line and
      polyvariant mixed computation is proved using algebraic and axiomatic
      tools.}
}

@InCollection{Itkin-1984b,
  author = {V.E. Itkin},
  title = {Axiomatics of Complete and Partial Program Execution},
  booktitle = {Program Compilation and Transformation},
  pages = {69-93},
  publisher = CCN,
  year = 1984,
  note = {(In Russian)},
  annote = {An axiomatic system for the concepts of completeness of input
      information, program input, and program output is proposed. This is
      used to prove the correctness of mixed computation.}
}

@InCollection{Itkin-1985,
  author = {V.E. Itkin},
  title = {On Algebra and Axiomatics of Program Parallelization},
  booktitle = {Theory of Programming and Representation of Discrete
      Systems Parallelism},
  pages = {38-53},
  publisher = CCN,
  year = 1985,
  note = {(In Russian)},
  annote = {A method of two-step parallel execution of the superposition
      (concatenation) of two programs is proposed.}
}

@InCollection{Itkin-1986,
  author = {V.E. Itkin},
  title = {On Algebra and Logic of Parallelization of Function
      Superposition},
  booktitle = {Theoretical Aspects of Information Processing},
  pages = {18-33},
  publisher = CCN,
  year = 1986,
  note = {(In Russian)},
  annote = {An algebra of deterministic memory states and subsets of the
      set of memory locations is transformed first into an algebra of
      non-deterministic memory states and then further into a quasi-boolean
      algebra of abstract `information elements'. A method of parallel
      execution of superposition of programs is formulated in terms of this
      algebra.}
}

@InProceedings{Itkin-1988,
  author = {V.E. Itkin},
  title = {An Algebra and Axiomatization System of Mixed Computation},
  booktitle = PEMC,
  editor = BEJ,
  pages = {209-224},
  publisher = NoHo,
  year = 1988,
  annote = {An axiomatic system for the concepts of completeness of input
      information, program input and program output is proposed. It is used
      to prove the correctness of mixed computation and parallel execution
      of superposition of programs.}
}

@InCollection{Itkin-1989,
  author = {V.E. Itkin},
  title = {An Algebra and Axiomatization System of Mixed Computation},
  booktitle = {Methods of Theoretical and Experimental Computer Science},
  publisher = {Computing Center, Siberian Branch of the USSR Academy
		 of Sciences},
  year = 1989,
  editor = {V.E. Kotov},
  pages = {23-41},
  address = {Novosibirsk, USSR},
  OPTnote = {}
}

@Article{Itkin-1991,
  author = {V.E. Itkin},
  title = {An Algebra of Mixed Computation},
  journal = TCS,
  year = 1991,
  volume = 90,
  number = 1,
  pages = {81-93},
  note = {Also in D.\ Bj{\o}rner and V.\ Kotov: Images of
		 Programming, North-Holland, 1991}
}

@InCollection{Jackson-1994,
  author = {M. A. Jackson},
  title = {Software Development Method},
  crossref = {Roscoe-1994},
  pages = {211--230},
  chapter = 13,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{JacksonD-2000,
  author = {D. Jackson},
  title = {Automating first-order relational logic},
  crossref = {FSE2000},
  OPTpages = {},
  keywords = {Alloy: a first-order relational language, Alloy Analyzer (AA)},
  bibliographies = {RelMiCS, SQRL}
}

@Article{JacksonD-2001,
  author = {D. Jackson},
  title = {Alloy: A lightweight object modeling notation},
  journal = {ACM Transactions on Software Engineering and Methodology},
  year = 2001,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  OPTnote = {},
  keywords = {Alloy: a first-order relational language, Alloy Analyzer (AA)},
  bibliographies = {RelMiCS, SQRL}
}

@InProceedings{JacksonD-Schechter-Shlyakhter-2000,
  author = {D. Jackson and I. Schechter and I. Shlyakhter},
  title = {{ALCOA}: The {Alloy} constraint analyzer},
  crossref = {ICSE2000},
  OPTpages = {},
  OPTannote = {},
  keywords = {Alloy: a first-order relational language, Alloy Analyzer (AA)},
  bibliographies = {RelMiCS, SQRL}
}

@PhdThesis{JacksonPB-1995,
  author =       {Paul Bernard Jackson},
  title =        {Enhancing the {Nuprl} Proof Development System and
    Applying it to Computational Abstract Algebra},
  school =       {Cornell University},
  year =         {1995},
  DOI =       {10.1.1.47.5744},
  OPTtype =      {},
  OPTaddress =   {},
  OPTmonth =     JAN,
  OPTnote =      {},
  abstract =    {This thesis describes substantial enhancements
    that were made to the software tools in the Nuprl system
    that are used to interactively guide the production of formal proofs.
    Over 20,000 lines of code were written for these tools.
    Also, a corpus of formal mathematics was created
    that consists of roughly 500 definitions and 1300 theorems.
    Much of this material is of a foundational nature
    and supports all current work in Nuprl.
    This thesis concentrates on describing the half of this corpus
    that is concerned with abstract algebra
    and that covers topics central to the mathematics of the computations
    carried out by computer algebra systems.

    The new proof tools include those that solve linear arithmetic problems,
    those that apply the properties of order relations,
    those that carry out inductive proof to support recursive definitions,
    and those that do sophisticated rewriting.
    The rewrite tools allow rewriting with relations of differing strengths
    and take care of
    selecting and applying appropriate congruence lemmas automatically.
    The rewrite relations can be order relations as well as equivalence relations.
    If they are order relations, appropriate monotonicity lemmas are selected.

    These proof tools were heavily used
    throughout the work on computational algebra.
    Many examples are given that illustrate their operation
    and demonstrate their effectiveness.

    The foundation for algebra
    introduced classes of monoids, groups, rings and modules,
    and included theories of order relations and permutations.
    Work on finite sets and multisets illustrates
    how a quotienting operation hides details of datatypes
    when reasoning about functional programs.
    Theories of summation operators were developed
    that drew indices from integer ranges, lists and multisets,
    and that summed over all the classes mentioned above.
    Elementary factorization theory was developed
    that characterized when cancellation monoids are factorial.
    An abstract data type
    for the operations of multivariate polynomial arithmetic was defined,
    and the correctness of an implementation of these operations was verified.
    The implementation is similar to those found in current computer algebra systems.

    This work was all done in Nuprl's constructive type theory.
    The thesis discusses the appropriateness of this foundation,
    and the extent to which the work relied on it.}
}

@Book{Jacky-1997,
  author = {Jonathan Jacky},
  title = {The way of {Z}},
  year = 1997,
  publisher = {Cambridge University Press},
  note = {ISBN 0-521-55976-6},
  URL = {http://staff.washington.edu/jon/z-book/}
}

@Article{Jacobs-1993,
  author = {Bart Jacobs},
  title = {Comprehension Categories and the Semantics of Type
		  Dependency},
  year = 1993,
  volume = 107,
  pages = {169--207},
  journal = TCS,
  WKloc = {A-0043},
  contents = {1 Introduction
                  2 Fibrations
                  3 Category theory over a basis
                  4 Comprehension categories
                  5 Quantification},
  authorsAddress = {Utrecht bjacobs\@math.ruu.nl},
  abstract = {A {\em comprehension category} is defined as a
		  functor ${\cal P}:{\bf E}\tfun{\bf B}^{\tfun}$
		  satisfying (a) $cod \o {\cal P}$ is a fibration, and
		  (b) $f$ is cartesian in {\bf E} implies that ${\cal
		  P} f$ is a pullback in {\bf B}. This notion captures
		  many structures which are used to describe type
		  dependency (like display-map categories (Taylor
		  (1986), Hyland and Pitts (1989) and Lamarche (1988)),
		  categories with attributes (Cartmell (1978) and
		  Moggi (1991)), D-categories (Ehrhard (1988)) and
		  comprehensive fibrations (Pavlovic (1990))). It also
		  captures comprehension as occurring in topos theory
		  and as described by Lawvere's (1970) hyperdoctrines.
		  This paper is meant as an introduction to these
		  comprehension categories.

                  A comprehension category will be called {\em closed}
		  if it has appropriate dependent products and sums. A
		  few examples of closed comprehension categories will
		  be described here; more of them may be found in
		  Jacobs (1991); applications occur in Jacobs (1991)
		  and Jacobs et~al.~(1991).},
  bibliographies = {RelMiCS}
}

@InProceedings{Jacobs-1993a,
  author = {Bart Jacobs},
  title = {Semantics of lambda-I and of other substructure lambda calculi},
  pages = {194--208},
  abstract = {The ordinary untyped $\lambda$-calculus (the main object of
             study in [3] will be denoted here by $\lambda K$. Church
             originally introduced the $\lambda I$-calculus, which can be
             understood as the $\lambda K$-calculus without {\em weakening}: one
             cannot throw away variables. Similarly there is a {\em affine}
             calculus $\lambda A$ without {\em contraction}: there, one cannot
             duplicate variables. There is also a {\em linear calculus}
             $\lambda L$
             in which one has neither weakening nor contraction. In
             $\lambda L$ variables occur precisely once.

             We give a
             systematic description of the semantics of these four calculi.
             It starts with two sorts of domain theoretic models: graph
             models and filter models (of intersection types) are
             constructed for each of these calculi. Later on, we describe an
             appropriate categorial way to capture such structures in terms
             of monoidal categories (with diagonals or projections).},
  crossref = {TLCA93},
  WKloc = {A-0184}
}

@Book{Jacobs-1999,
  author =	 {Bart Jacobs},
  title = 	 {Categorical Logic and Type Theory},
  publisher = 	 {Elsevier Science},
  year = 	 1999,
  volume =	 141,
  series =	 {Studies in logic and the foundations of mathematics},
  pages = 	 {xviii, 760},
  ISBN = 	 {0444501703; 0444508538 (pbk)},
  McMaster = 	 {QA 9 .J27 1999},
  bibliographies = {RelMiCS}
}

@InCollection{Jabobs-2006,
  author={Jacobs, Bart},
  title={A Bialgebraic Review of Deterministic Automata, Regular Expressions and Languages},
  pages={375--404},
  DOI={10.1007/11780274_20},
  DOIURL={http://dx.doi.org/10.1007/11780274_20},
  bibliographies={RelMiCS},
  abstract = {This papers reviews the classical theory of deterministic automata
    and regular languages from a categorical perspective.
    The basis is formed by Rutten’s description of the
    Brzozowski automaton structure in a coalgebraic framework.
    We enlarge the framework to a so-called bialgebraic one,
    by including algebras together with suitable distributive laws
    connecting the algebraic and coalgebraic structure
    of regular expressions and languages.
    This culminates in a reformulated proof
    via finality of Kozen’s completeness result.
    It yields a complete axiomatisation of observational equivalence
    (bisimilarity) on regular expressions.
    We suggest that this situation is paradigmatic
    for (theoretical) computer science as the study of ``generated behaviour''.},
  year={2006},
  isbn={978-3-540-35462-8},
  booktitle={Algebra, Meaning, and Computation},
  volume={4060},
  series=LNCS,
  editor={Futatsugi, Kokichi and Jouannaud, Jean-Pierre and Meseguer, Jos{\'e}},
  publisher={Springer Berlin Heidelberg},
}

@Article{Jacobs-Gries-1986,
  author = {Dean Jacobs and David Gries},
  title = {General Correctness: A Unification of Partial and Total Correctness},
  journal = ACTIN,
  year = 1986,
  volume = 22,
  pages = {67--83},
  WKloc = {A-1307},
  bibliographies = {RelMiS},
  abstract = {General correctness, which subsumes partial and total
                 correctness, is defined for both weakest preconditions
                 and strongest postconditions. Healthiness properties
                 for general-correctness predicate transformers are more
                 uniform and complete than those for
                 partial-and-total-correctness systems. In fact, the
                 healthiness properties for partial and total
                 correctness are simple restrictions of those for
                 general correctness. General correctness allows simple
                 formulations of the connections between weakest and
                 strongest postconditions and between the notions of
                 weakest precondition under the ``demonic'' and
                 ``angelic'' interpretations of nondeterminism. A
                 problem that plagues \$sp - sp(P, C)\$ is undefined if
                 execution of \$C\$ begun in some state of \$P\$ may not
                 terminate - disappears with the generalization. This
                 paper is a study of some simple theory underlying
                 predicate transformer semantics, and as yet has little
                 bearing on current programming practices. The theory
                 uses a relational model of programs.}
}

@InProceedings{Jacobs-Moggi-Streicher-1991,
  author = {Bart Jacobs and Eugenio Moggi and Thomas Streicher},
  title = {Relating Models of Impredicative Type Theories},
  pages = {197--218},
  crossref = {CTCS1991},
  WKloc = {A-0100},
  bibliographies = {RelMiCS},
  abstract = {The object of study of this paper is the categorical
      semantics of three impredicative type theories, viz.~Higher Order
      $\lambda$-calculus $F\omega$, the Calculus of Constructions and
      Higher Order ML. The latter is particularly interesting because it is
      a two-level type theory with type dependency at both levels. Having
      described appropriate categorical structures for these calculi, we
      establish translations back and forth between all of them. Most of
      the research in the paper concerns the theory of fibrations and
      comprehension categories.}
}

@TechReport{Jacobs-Rideau-1992,
  author = {Ian Jacobs and Laurence Rideau},
  title = {A Centaur Tutorial},
  year = 1992,
  type = {Programme 2: Calcul symbolique, Programmation et
		  G\'enie logiciel},
  number = 141,
  month = AUG,
  institution = {INRIA-Sophia Antipolis},
  WKloc = {B-0009, ~/doc/pap/RT-141.ps},
  abstract = {This paper presents the Centaur system through a
		  tutorial describing the creation of an environment
		  for a small  language of mathematical expressions
		  called Exp. With Centaur,  the user may
		  interactively generate programming language
		  environments, including  structured editors,
		  debuggers, interpreters, and other tools. In this
		  tutorial, all phases of language specification are
		  covered: the design of the abstract and concrete
		  syntax of Exp in Metal and Sdf, the pretty printing
		  rules in Ppml, and the semantics of an Exp
		  interpreter in Typol. The tools generated by Centaur
		  based on these specifications are enhanced by a user
		  interface built with Centaur graphic primitives.},
  bibliographies = {RelMiCS}
}

@Article{Jacobs-Rutton-1997,
  author = {Bart Jacobs and Jan Rutton},
  title = {A Tutorial on (Co)Algebras and (Co)Induction},
  year = 1997,
  WKloc = {A-0436},
  journal = {EATCS Bulletin},
  volume = 62,
  pages = {222--259},
  URL = {http://www.cs.kun.nl/~bart/PAPERS/JR.ps.Z}
}

@Misc{JacobsenE-1991,
  author =    {Erik Jacobsen},
  title =     {Unification and anti-unification},
  howpublished = {\url{http://erikjacobsen.com/pdf/unification.pdf} (checked 2015-04-04)},
  month =     JUN,
  year =      1991,
  annote =    {The presented term category is a partial order category
    with terms as objects and substitutions as morphisms.},
  abstract = {Unification as defined by Robinson [Rob65]
    is one of two dual concepts, the other is called anti-unification.
    These concepts are presented in terms of category theory
    and concepts from algebra.

    The unification algorithm from [Rob65] is presented
    and I argue that it has a worst-case exponential runtime.
    Modifying this algorithm, I remove one source of exponential runtime.
    Paterson & Wegman showed in [PW78] that a linear unification algorithm exists.
    I present this algorithm and compare actual runtimes for all algorithms.

    I then present an algorithm for anti-unification by Plotkin [Plo69] and
    Reynolds [Rey69]. This algorithm has $O(n^2)$ runtime.
    Two new linear algorithms, based on the same principle
    as the linear Paterson \& Wegman unification algorithm, are presented.
    Finally a parallel anti-unification algorithm from [KMPP88] is presented.}
}

@PhDThesis{Jacquet-1999,
  author = {H{\'e}l{\`e}ne Jacquet},
  title = {Une approche cat{\'e}gorique de la r{\'e}{\'e}criture de sommets dans les graphes},
  school = {Universit{\'e} Bordeaux 1},
  year = 1999,
  month = JAN,
  WKloc = {B-0059, doc/pap/BIB},
  URL = {http://dept-info.labri.u-bordeaux.fr/~jacquet/recherche.html,
                 http://dept-info.labri.u-bordeaux.fr/~jacquet/these.ps.gz},
  abstract = {Le principal objet de cette thèse est de montrer que
      l'utilisation de la notion de produit fibré dans la catégorie des
      graphes (orientés, avec arcs multiples et boucles) et des morphismes
      de graphes fournit un modèle catégorique -inexistant jusqu'alors- de
      la réécriture de sommets dans un graphe. La particularité du
      formalisme que nous proposons réside d'une part dans le fait de
      définir l'étiquetage d'un graphe par l'intermédiaire d'un morphisme
      vers un graphe possédant une structure particulière -appelé graphe
      alphabet, et d'autre part de définir une règle de réécriture comme
      étant un morphisme vers ce même graphe alphabet. En résulte que
      l'application d'une règle à un graphe s'obtient "automatiquement" en
      effectuant le produit fibré de ces deux morphismes. Une démarche
      permettant de mettre en exergue les relations qui existent entre
      l'approche de la réécriture par produit fibré et les approches dites
      ensemblistes (NLC, edNCE etc.) est proposée dans le cinquième
      chapitre. En particulier, nous montrons que tout langage sous-jacent
      engendré par une grammaire edNCE peut être engendré par une grammaire
      de produit fibré. Au delà des graphes, nous indiquons de quelle façon
      l'approche de la réécriture par produit fibré peut s'étendre
      uniformément au cadre de la réécriture dans les hypergraphes et, plus
      généralement au cadre des graphes structurés -que nous proposons
      comme généralisation de la notion de graphe.}
}

@InProceedings{Jacquet-KlempienHinrichs-1998,
  author = {H{\'e}l{\`e}ne Jacquet and Renate Klempien-Hinrichs},
  title = {Node Replacement in Hypergraphs:
      Translating {NCE} Rewriting into the Pullback Approach},
  crossref = {TAGT1998},
  pages = {117--130},
  WKloc = {A-1082}
}

@InProceedings{Jagadeesan-Pingali-1992,
  author = {Radha Jagadeesan and Keshav Pingali},
  title = {Abstract Semantics for a Higher-Order Functional Language with
          Logic Variables},
  pages = {355--366},
  abstract = {Although there is considerable experience in using languages
             that combine the functional and logic programming paradigms,
             the problem of providing an adequate semantic foundation for
             such languages remained open. In an earlier paper, we solved
             this problem for first-order languages by reducing the problem
             to that of solving simultaneous fixpoint equations involving
             closure operators over a Scott domain and showing that the
             resulting semantics was fully abstract with respect to the
             operational semantics [4]. These results showed that the
             first-order fragment could be viewed as a language of
             incremental definition of data structures through constraint
             intersection. The problem for higher-order languages remained
             open, in part because higher-order functions can interact with
             logic variables in complicated ways to give rise to behavior
             reminiscent of own variables in Algol-60. We solve this problem
             in this paper. We show that in the presence of logic variables,
             higher-order functions may be modeled extensionally as closure
             operators on function graphs ordered in a way reminiscent of
             the ordering on extensible records in studies of inheritance
             [1]. We then extend the equation solving semantics of the
             first-order subset to the full language, and prove the usual
             soundness and adequacy theorems for this semantics. These
             results show that a functional language with logic variables
             can be viewed as a language of incremental definition of
             functions.},
  crossref = {POPL1992},
  WKloc = {A-0166}
}

@InProceedings{Jagannathan-1994,
  author = {Suresh Jagannathan},
  title = {Dynamic Modules in Higher-Order Languages},
  crossref = {ICCL94},
  pages = {74--87},
  WKloc = {A-0392},
  abstract = {Providing programmers the ability to construct
		  meaningful abstractions to help manage complexity is
		  a serious language design issue. Many languages
		  define a module system that can be used to specify
		  distinct namespaces, and build user-defined data
		  abstractions; however, few languages support {\em
		  dynamic modules}, {\em i.e.}, modules which are true
		  first-class objects. In this paer, we define a
		  module semantics for a dialect of Scheme called
		  Rascal. Modules are defined in terms of {\em reified
		  environments}, and are first-class objects which may
		  be dynamically created, freely assigned, used as
		  arguments to procedures, etc.. If defined naively,
		  however, implementing modules using environments can
		  entail the capture of unwanted bindings, leading to
		  potentially severe violations of lexical abstraction
		  and locality.

                  We address these concerns by giving users great
		  flexibility to manipulate environments, and to
		  constrain the extent and scope of the environment
		  reification process. We argue that the techniques
		  and operators developed define a cohesive and
		  semantically sound basis for building large-scale
		  modular systems in dynamically-typed higher-order
		  languages. Using meta-level environments for
		  implementing modules and complex data abstractions
		  requires few syntactic extensions to the base
		  language, is amenable to aggressive compile-time
		  analysis, and permits the description of a number of
		  modularity issues within a well-defined formal framework.}
}

@InProceedings{Jamison-Pfaltz-2000,
  author = {Robert E. Jamison and John Pfaltz},
  title = {Closure Systems and Their Structure},
  crossref = {RelMiCS2000-PP},
  pages = {121--132},
  OPTabstract = {},
  WKloc = {A-0894 improved version}
}

@TechReport{Janas-Metzner-Wiehle-Wilke-1986,
  authorsAddress = {inf31, inf2},
  year = 1986,
  title = {A Proposal of Service Primitives for Distributed
		  Application Processing with Special Regard to
		  Transaction Processing},
  number = 8611,
  month = DEC,
  institution = {Hochschule der Bundeswehr M\"unchen, Fachbereich Informatik},
  author = {Janas, J.M. and Metzner, S. and Wiehle, H.R. and
		  Wilke, M.}
}

@TechReport{Janas-Schmitz-Wiehle-1985,
  authorsAddress = {inf31, inf2},
  year = 1985,
  title = {{Systemdarstellung mit Hilfe eines programmbeschrifteten Netzmodells}},
  number = 8507,
  month = SEP,
  institution = {Hochschule der Bundeswehr M\"unchen, Fachbereich Informatik},
  author = {Janas, J.M. and Schmitz, L. and Wiehle, H.R.}
}

@Article{Janas-Schmitz-Wiehle-1986,
  authorsAddress = {inf31 inf2},
  year = 1986,
  volume = 1,
  title = {{Systemdarstellung mit Hilfe eines
		  programmbeschrifteten Netzmodells in der DIN 66 265}},
  pages = {181--198},
  journal = {Informatik Forschung und Entwicklung},
  author = {Janas, J.M. and Schmitz, L. and Wiehle, H.R.}
}

@InProceedings{Janas-Wiehle-1986,
  authorsAddress = {inf31, inf2},
  year = 1986,
  title = {{Kooperierende Transaktionssysteme: Vorausschau auf
		  eine Nutzungsform globaler Rechnernetze}},
  publisher = {VDE-Verlag},
  pages = {181-191},
  booktitle = {{ Architektur und Betrieb von
      Rechensystemen; Vortr\"age der NTG/GI-Fachtagung 1986}},
  author = {Janas, J.M. and Wiehle, H.R.},
  address = {Berlin/Offenbach}
}

@InProceedings{Janas-Wiehle-1988,
  authorsAddress = {inf31, inf2},
  year = 1988,
  title = {On the State of Affairs in {OSI} Transaction Processing},
  pages = {53-61},
  booktitle = {Proc. IEEE Computer Networking Symposium},
  author = {Janas, J.M. and Wiehle, H.R.},
  address = {Washington D.C.}
}

@InProceedings{Janicki-1995,
  author = {Ryszard Janicki},
  title = {Towards a Formal Semantics of {Parnas} Tables},
  crossref = {ICSE1995},
  pages = {231--240},
  bibliographies = {RelMiCS}
}

@InProceedings{Janicki-2000,
  author = {Ryszard Janicki},
  title = {Remarks on Mereology of Direct Products and Relations},
  crossref = {RelMiCS2000-M},
  pages = {65--83},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {The concept of being a \emph{part of} for direct products
    and relations is analysed. Some operations based on this concept
    are introduced and analysed.}
}

@Article{Janicki-2008,
  author =       {Ryszard Janicki},
  title =        {Relational structures model of concurrency},
  journal =      ACTIN,
  year =         {2008},
  DOI =       {10.007/s00236-008-0071-6},
  volume =    {45},
  pages =     {279--320}
}

@Article{Janicki-Khedri-2001,
  author = {Ryszard Janicki and Ridha Khedri},
  title = {On a Formal Semantics of Tabular Expressions},
  journal = SCICOP,
  year = 2001,
  volume = 39,
  pages = {189--213},
  WKloc = {A-1303},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings{Janicki-Wassyng-2003,
  author = 	 {Ryszard Janicki and Alan Wassyng},
  title = 	 {On Tabular Expressions},
  crossref =  {CASCON2003},
  pages =	 {92--106},
  bibliographies = {Tables},
  WKloc = 	 {A-1529, doc/pap/BIB},
  URL = {http://portal.acm.org/citation.cfm?id=961322.961338},
  abstract = {Tabular expressions (Parnas et al. [18, 26, 28, 29])
     are a means to represent the complex relations that are used to
     specify or document software systems. A formal model and a
     semantics for tabular expressions are presented. The model covers
     most known types of tables used in Software Engineering, and
     admits precise classification and definition of new types of
     tables. The practical importance of the semantics of tabular
     expressions is discussed briefly.},
  annote = {No clear advance over \cite{Janicki-Khedri-2001}.}
}

@Article{Janicki-Wassyng-2005,
  author =       {Ryszard Janicki and Alan Wassyng},
  title =        {Tabular Expressions and Their Relational Semantics},
  journal =      FUNDI,
  year =         2005,
  volume =    68,
  pages =     {343--370},
  abstract =    {Tabular Expressions (Parnas et al. [20, 28, 32, 33])
    are means to represent the complex relations
    that are used to specify or document software systems.
    A formal model and a semantics for tabular expressions are presented.
    The model covers most known types of tables used in software engineering,
    and admits precise classification and definition of new types of tables.
    The practical importance of the semantics of tabular expressions
    is also discussed.},
  annote = {No clear advance over \cite{Janicki-Khedri-2001}.},
  WKloc = {A-1721, doc/pap/BIB}
}

@InProceedings{Janssens-1991,
  author = {D. Janssens},
  title = {Equivalence of Computations in Actor Grammars},
  crossref = {GraTra91},
  pages = {145--180}
}

@InProceedings{Janssens-1993,
  author = {D. Janssens},
  title = {{ESM} Systems and the Composition of Their Computations},
  crossref = {GTCS93},
  pages = {203--217},
  abstract = {ESM systems are graph rewriting systems where
		  productions are morphisms in a suitable category,
		  ESM. The way graphs are transformaed in ESM systems
		  is essentially the same as in actor grammars, which
		  were introduced in \cite{Janssens-Rozenberg-1989}. It
		  is demonstrated that a rewriting step corresponds to
		  a (single) pushout construction, as in the approach from
		  \cite{Loewe-1991}. Rewriting processes in ESM systems
		  are represented by computation structures, and it is
		  shown that communication of rewriting processes
		  corresponds to a gluing operation on computation
		  structures. In the last section we briefly sketch
		  how one may develop a semantics for ESM systems,
		  based on computation structures, that is
		  compositional w.r.t.\null{} the union of ESM systems.},
  WKloc = {A-0292}
}

@Article{Janssens-Rozenberg-1980,
  author = {D. Janssens and Gregoresz Rozenberg},
  title = {On the Structure of Node-Label Controlled Graph Grammars},
  journal = {Information Science},
  year = 1980,
  volume = 20,
  pages = {191--216},
  keywords = {NLC graph grammars}
}

@Article{Janssens-Rozenberg-1989,
  author = {D. Janssens and Gregoresz Rozenberg},
  title = {Actor Grammars},
  journal = {Math.~Systems Theory},
  year = 1989,
  volume = 22,
  pages = {75--107}
}

@Unpublished{Jaoua-1987,
  author = {Ali Jaoua},
  title = {Recouvrement avant de Programmes sous les Hypoth{\`e}ses de
      Sp{\'e}\-ci\-fi\-ca\-tions D{\'e}terministes et non-d{\'e}terministes},
  year = 1987,
  note = {Diss.\null{} de Doctorat d'Etat d\`es sciences, Univ.\null{} de
      Toulouse},
  bibliographies = {RelMiCS}
}

@Unpublished{Jaoua-Beaudry-1989,
  author = {Ali Jaoua and M. Beaudry},
  title = {Difunctional Relations: A Formal Tool for Program Design},
  year = 1989,
  note = {Rapport de recherche no 55, D{\'e}partement de Math{\'e}matique
      et d'Informatique, Univ.\null{} de Sherbrooke, Qu{\'e}bec, Canada},
  bibliographies = {RelMiCS}
}

@Article{Jaoua-Belkhiter-Desharnais-Khedri-1991,
  author = {Ali Jaoua and Nadir Belkhiter and Jules Desharnais
                  and Ridha Khedri},
  title = {Augmentation automatique de la fiabilit{\'e} d'un logiciel},
  journal = ICO_Quebec,
  year = 1991,
  volume = 3,
  number = 3,
  pages = {332--337},
  bibliographies = {RelMiCS}
}

@Article{Jaoua-Belkhiter-Desharnais-Moukam-1992,
  author = {Ali Jaoua and N. Belkhiter and J. Desharnais and
                  T. Moukam},
  title = {Propri\'et\'es des D\'ependances Difonctionelles
                  dans les Bases de Donn\'ees Relationnelles},
  journal = INFOR,
  year = 1992,
  volume = 30,
  number = 3,
  month = AUG,
  pages = {297--316},
  bibliographies = {RelMiCS}
}

@InCollection{Jaoua-Belkhiter-Ounalli-Moukam-1997,
  author = {Ali Jaoua and Nadir Belkhiter and Habib Ounalli and Th{\'e}eodore Moukam},
  title = {Databases},
  chapter = 13,
  pages = {197--210},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Article{Jaoua-Boudriga-Durieux-Mili-1991,
  author = {Ali Jaoua and N. Boudriga and J. L. Durieux and A. Mili},
  title = {Regularity of Relations: A Measure of Uniformity},
  journal = TCS,
  volume = 79,
  year = 1991,
  pages = {323--339},
  bibliographies = {RelMiCS},
  abstract = {In their most general form, program specifications can be represented as binary relations. The study of binary relations for the purpose of discussing program construction, program fault tolerance and program exception handling have led us to discover an interesting property of relations: regularity. The interest of this property is twofold: first it is very general, i.e. it is verified by several specifications we encounter; second, it is very strong, i.e. it allows us to simplify our formal computations rather dramatically.},
  DOIURL = {http://dx.doi.org/10.1016/0304-3975(91)90335-Y},
  DOI = {10.1016/0304-3975(91)90335-Y},
  DirectURL = {http://www.sciencedirect.com.libaccess.lib.mcmaster.ca/science/article/pii/030439759190335Y},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Jaoua-Elloumi-BenYahia-Alvi-2000,
  author = {Ali Jaoua and Samir Elloumi and Ben Yahia, Sadok and Faisal Alvi},
  title = {A Galois Connection on Fuzzy Binary Relations, Applications for Discovering Association Rules and Decision Making},
  crossref = {RelMiCS2000-M},
  pages = {85--115},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {Galois connections on crisp binary relations have proved
    to be useful for several applications in computer science.
    This paper aims at defining the notion of fuzzy Galois connection
    corresponding to a fuzzy binary relation in two steps:
    Firstly by defining the term fuzzy maximal rectangle and secondly,
    by extending the Galois lattice structure to fuzzy binary relations.
    Applications concerning the discovery of fuzzy association rules
    and decision making are also presented.}
}

@InProceedings{Jaoua-Ounalli-Belkhiter-1994,
  author = {Ali Jaoua and H. Ounalli and N. Belkhiter},
  title = {Automatic Entity Extraction From an {\sl n}-ary Relation:
      Towards a General Law for Information Decomposition},
  booktitle = {Joint Conf.\null{} on Information Sciences (JCIS)},
  year = 1994,
  month = nov,
  address = {Pinehurst, Duke Univ., NC},
  pages = {92--95},
  bibliographies = {RelMiCS}
}

@TechReport{Jarvis-Marlow-PeytonJones-Wilcox-1998,
  author = {Stephen Jarvis and Simon Marlow and Peyton Jones, Simon and Eric Wilcox},
  title = {A Grammar for Self-Describing Documents},
  institution = {Oxford University Computing Laboratory},
  year = 1998,
  keywords = {XML, XML-Data lite},
  number = {PRG-TR-4-98},
  WKloc = {A-0995}
}

@InProceedings{Jarzabek-LiShubiao-2003,
  author = 	 {Stan Jarzabek and Li, Shubiao},
  title = 	 {Eliminating Redundencies with a ``Composition with Adaptation'' Meta-programming Technique},
  booktitle =	 {ESEC-FSE '03},
  pages =	 {237--246},
  year =	 2003,
  month =	 SEP,
  WKloc = 	 {A-1499, doc/pap/BIB}
}

@Article{Jarzabek-WangGuosheng-1998,
  author = 	 {Stan Jarzabek and Guosheng Wang},
  title = 	 {Model-based Design of Reverse Engineering Tools},
  journal = 	 {J.~Softw.\null{} Maint.: Res.\null{} Pract.},
  year = 	 1998,
  volume =	 10,
  pages =	 {353--380},
  WKloc = 	 {A-1498, doc/pap/BIB}
}

@TechReport{Jaspars-Krahmer-1995,
  author = {Jaspars, J. and Krahmer, E.},
  title = {Unified dynamics},
  institution = {CWI, Amsterdam},
  number = {CS-R95},
  year = 1995,
  bibliographies = {RelMiCS}
}

@Article{Jay-1989a,
  author = {C. B. Jay},
  title = {Languages For Monoidal Categories},
  journal = JPAA,
  volume = 59,
  pages = {61--85},
  year = 1989
}

@Article{Jay-1989b,
  author = {C. B. Jay},
  title = {A Note on Natural Numbers Objects in Monoidal
                    Categories},
  journal = {Studia Logica},
  volume = {XLVIII},
  pages = {389--393},
  year = 1989
}

@Misc{Jay-1993,
  author = {C. Barry Jay},
  title = {Partial Functions, Ordered Categories, Limits and Cartesian Closure},
  year = 1993,
  WKloc = {A-0763}
}

@Misc{Jay-1998,
  author = {C. B. Jay},
  title = {The {\bf FISh} language definition},
  year = 1998,
  WKloc = {A-0735}
}

@Misc{Jay-199X,
  author = {C. Barry Jay},
  title = {An Introduction to Categories in Computing},
  year = {199?},
  WKloc = {A-0741}
}

@InProceedings{Jay-2001,
  author = {C. Barry Jay},
  title = {Distinguishing Data Structures and  Functions:
	the Constructor Calculus and Functorial Types},
  crossref = {TLCA2001},
  pages = {217--239},
  URL = {http://link.springer.de/link/service/series/0558/bibs/2044/20440217.htm},
  abstract = {The expressive power of functional programming can be
      improved by identifying and exploiting the characteristics that
      distinguish data types from function types. Data types support
      generic functions for equality, mapping, folding, etc. that do not
      apply to functions. Such generic functions require case analysis, or
      pattern-matching, where the branches may have incompatible types,
      e.g. products or sums. This is handled in the \emph{constructor
      calculus} where \emph{specialisation} of program \emph{extensions} is
      governed by constructors for data types. Typing of generic functions
      employs polymorphism over functors in a \emph{functorial type
      system}. The expressive power is greatly increased by allowing the
      functors to be polymorphic in the number of arguments they take, i.e.
      in their \emph{arities}. The resulting system can define and type the
      fundamental examples above. Some basic properties are established,
      namely subject reduction, the Church-Rosser property, and the
      existence of a practical type inference algorithm.},
  WKloc = {A-1236, doc/pap/BIB}
}

@Article{Jay-Belle-Moggi-1998,
  author = {C. Barry Jay and G. Bell{\`e} and Eugenio Moggi},
  title = {Functorial {ML}},
  journal = {Journal of Functional Programming},
  year = 1998,
  volume = {??},
  OPTnumber = {},
  OPTmonth = {},
  pages = {??},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0633},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Jay-Cockett-1994-x,
  author = {C. Barry Jay and J. Cockett},
  title = {Shapely types and shape polymorphism},
  crossref = {ESOP1994},
  pages = {302--316},
  year = 1994,
  authorsAddress = {Univ. of Technology, Sydney; Univ. Calgary},
  WKloc = {A-0266},
  abstract = {Shapely types separate data, represented by lists,
		  from shape, or structure. This separation supports
		  shape polymorphism, where operations are defined for
		  arbitrary shapes, and shapely operations, for which
		  the shape of the result is determined by that of the
		  input, permitting static type checking. They include
		  both arrays and the usual algebraic types (of trees,
		  graphs, etc.), and are closed under the formation of
		  initial algebras.},
  keywords = {Typing of Catamorphisms}
}

@Article{Jay-Ghani-1995,
  author = {C. Barry Jay and Neil Ghani},
  title = {The Virtues of Eta-Expansion},
  journal = {ournal of Functional Programming},
  year = 1995,
  volume = 5,
  number = 2,
  month = APR,
  pages = {135--154},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0866},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Jayaraman-Moon-1995,
  author = {Bharat Jayaraman and Kyonghee Moon},
  title = {The {SuRE} Programming Framework},
  crossref = {AMAST1995},
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0627}
}

@InProceedings{Jayaraman-Osorio-Moon-1995,
  author = {Bharat Jayaraman and Mauricio Osorio and Kyonghee Moon},
  title = {Partial Order Programming (Revisited)},
  crossref = {AMAST1995},
  pages = {561--575},
  OPTabstract = {},
  WKloc = {A-0625}
}

@InProceedings{Jefferson-Lee-Friedman-1990,
  author = {Stanley Jefferson and Shinn-Der Lee and Daniel P. Friedman},
  title = {A Syntactic Theory of Transparent Parameterization},
  crossref = {ESOP1990},
  pages = {212--226},
  WKloc = {A-0249},
  abstract = {We present a calculus for Lamping's programming
		  system of transparent and orthogonal
		  parameterization. The calculus is shown to be
		  consistent, to have a standardization procedure, and
		  to correspond with an operational semantics obtained
		  from the denotational semanticsby viewing the
		  semantic equations as state transition
		  rules. Lamping's system is remarkable because it is
		  small, having only four constructions, yet it can
		  easily express a wide variety of parameterization
		  mechanisms including lexical variables, dynamic
		  variables, procedure calls, first-class
		  environments, modules, and method lookup and
		  inheritance mechanisms of onject-oriented
		  systems. Due to its orthogonal and transparent
		  parameterization mechanisms, every object, including
		  data and code, in Lamping's programming system can
		  be parameterized, and a parameterized object can be
		  manipulated as if it were a ground object. This
		  blurs the distinction between data and code,
		  allowing one to think of data as code and vice versa.}
}

@Misc{Jeffery-Henderson-Somogyi-1998,
  author = {David Jeffery and Fergus Henderson and Zoltan Somogyi},
  title = {Type classes in {Mercury} (DRAFT)},
  year = 1998,
  WKloc = {A-0729}
}

@InProceedings{Jeffrey-1994,
  title = {A Fully Abstract Semantics for Concurrent Graph Reduction},
  author = {Alan Jeffrey},
  pages = {82--91},
  crossref = {LICS9},
  WKloc = {A-0366},
  abstract = {This paper presents a fully abstract semantics for a
		  variant of the untyped $\lambda$-calculus with
		  recursive declarations.  We first present a summary
		  of existing work on full abstraction for the untyped
		  $\lambda$-calculus, concentrating on Abramsky and
		  Ong's work on the lazy $\lambda$-calculus.  Abramsky
		  and Ong's work is based on leftmost outermost
		  reduction without sharing.  This is notably
		  inefficient, and many implementations model sharing
		  by reducing syntax graphs rather than syntax trees.
		  Here we present a concurrent graph reduction
		  algorithm for the $\lambda$-calculus with recursive
		  declarations, in a style similar to Berry and
		  Boudol's Chemical Abstract Machine.  We adapt
		  Abramsky and Ong's techniques, and present a program
		  logic and denotational semantics for the
		  $\lambda$-calculus with recursive declarations, and
		  show that the three semantics are equivalent.}
}

@Article{Jeffrey-1996,
  author = {Alan Jeffrey},
  title = {A Fully Abstract Semantics for a Higher-Order Functional language with Nondeterministic Computation},
  journal = {???},
  year = 1996,
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  WKloc = {A-0761},
  abstract = {This paper is about the relationship between the theory of
      {\em monadic types} and the practice of {\em concurrent functional
      programming}. We present a typed functional programming language {\sl
      CMML}, with a type system based on Moggi's monadic metalanguage, and
      concurrency based on Reppy's Concurrent ML. We present an operational
      and denotational semantics for the language, and show that the
      denotational semantics is fully abstract for may-testing. We show
      that a fragment of CML can be translated into CMML, and that the
      translation is correct up to weak bisimulation.},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Jeffrey-1997,
  author = {Alan Jeffrey},
  title = {Premonoidal Categories and a Graphical View of Programs},
  year = 1997,
  WKloc = {A-0766}
}

@InProceedings{Jeffrey-Rathke-1999,
  author = {Alan Jeffrey and Julian Rathke},
  title = {Towards a Theory of Bisimulation for Local Names},
  crossref = {LICS-1999},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0762}
}

@Misc{Jeffrey-Wakeman-1997,
  author = {Alan Jeffrey and Ian Wakeman},
  title = {A Survey of Semantic Techniques for Active Networks},
  year = 1997,
  WKloc = {A-0752}
}

@Misc{Jeltsch-2002,
  author = {Wolfgang Jeltsch},
  title = {Convenient website implementation with {Haskell}},
  URL = {http://www.wolfgang.jeltsch.net/},
  month = FEB,
  year = 2002,
  WKloc = {A-1420}
}

@Misc{Jensen-Mackie-199X,
  author = {Thomas P. Jensen  and Ian Mackie},
  title = {Flow Analysis in the Geometry of Interaction},
  year = {199X},
  WKloc = {A-0429}
}

@InProceedings{Jeuring-1990,
  title = {Algorithms from Theorems},
  pages = {247--266},
  crossref = {IFIP1990},
  author = {Johan Jeuring},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@InProceedings{ Jeuring-1995,
    author = "Johan Jeuring",
    title = "Polytypic Pattern Matching",
    booktitle = "Functional Programming Languages and Computer Architecture",
    pages = "238-248",
    year = "1995",
    CiteSeer = "citeseer.ist.psu.edu/jeuring95polytypic.html",
  WKloc = {doc/pap/BIB},
  abstract = {The (exact) pattern matching problem can be
     informally specified as follows: given a pattern and a text, find
     all occurrences of the pattern in the text. The pattern and the
     text may both be lists, or they may both be trees, or they may
     both be multi-dimensional arrays, etc. This paper describes a
     general pattern-matching algorithm for all datatypes definable as
     an initial object in a category of $F$-algebras, where $F$ is a
     regular functor. This class of datatypes includes mutual
     recursive... }
}

@InProceedings{Jeuring-Jansson-1996,
  author = {Johan Jeuring and Patrik Jansson},
  title = {Polytypic Programming},
  crossref = {AFP1996},
  pages = {68--114},
  bibliographies = {FP, RelMiCS},
  WKloc = {B-0126},
  keywords = {PolyP}
}

@InProceedings{Jeuring-Swierstra-1994,
  author = {Johan Jeuring and Doaitse Swierstra},
  title = {Bottom-Up Grammar Analysis: A Functional Formulation},
  crossref = {ESOP1994},
  pages = {317--332},
  authorsAddress = {Utrecht},
  WKloc = {A-0328},
  abstract = {This paper discusses bottom-up grammar analysis
		  problems such as the {\sc Empty} problem and the
		  {\sc First} problem. It defines a general class of
		  bottom-up grammar analysis problems, and from this
		  definition it derives a functional program for
		  performing bottom-up grammar analysis. The
		  derivation is purely calculational, using theorems
		  from lattice theory, the Bird-Meertens calculus, and
		  laws for list comprehensions. Sufficient conditions
		  guaranteeing the existence of a solution emerge as a
		  byproduct of hte calculation. The resulting program
		  is used to construct programs for the {\sc Empty}
		  problem and the {\sc First} problem.}
}

@Misc{Jeuring-Swierstra-199X,
  author = {Johan Jeuring and Doaitse Swierstra},
  title = {Grammars and Parsing},
  howpublished = {Lecture Notes},
  OPTmonth = {},
  OPTyear = {},
  bibliographies = {FP},
  WKloc = {A-1170}
}

@InProceedings{Jim-Meyer-1991,
  title = {Full Abstraction and the Context Lemma (Preliminary Report)},
  author = {Trevor Jim and Albert R. Meyer},
  pages = {131--151},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {A general notion of rewriting system of the kind used for
		  evaluating simply typed $\lambda$-terms in Scott's
		  PCF is defined. Any simply typed $\lambda$-calculus
		  with PCF-like rewriting semantics is shown
		  necessarily to satisfy Milner's Context Lemma.  A
		  simple argument demonstrates that any denotational
		  semantics which is adequate for PCF and in which
		  certain simple Boolean functionals exist, cannot be
		  fully abstract for {\em any} extension of PCF
		  satisfying the Context Lemma.  An immediate
		  corollary is that Berry's stable domains cannot be
		  fully abstract for any extension of PCF definable by
		  PCF-like rules.  Thus, the idea of adding a
		  combinator to PCF analogous to the ``parallel-or''
		  combinator which establishes full abstraction for
		  the familiar cpo model cannot be generalized for
		  models such as stable domains.}
}

@MastersThesis{JingMin-2000,
  author = {Jing, Min},
  title = {A Table Checking Tool},
  school = {Department of Electrical and Computer Engineering, McMaster University},
  year = 2000,
  type = {M.Eng.\null{} thesis},
  WKloc = {A-1461 abandoned in Tunis 2008, doc/pap/BIB},
  abstract = {Software is used increasingly in safety critical and mission
      critical systems. The software products are required to be reliable
      and accurate. Good documentation, especially precise mathematical
      documentation, contributes to increasing reliability and accuracy of
      software products. Tabular notation advocated by Parnas et al. has
      been found to be easier to understand, formulate and inspect than
      traditional mathematical notations. The formulae in the tabular
      expressions must satisfy certain mathematical conditions. When these
      tables are used in practice, we have found that the documents
      submitted for review often fail to satisfy those conditions. Manually
      checking these simple and application-independent properties is
      time-consuming and error-prone.

      This work involves the design and development of a prototype tool to
      automate the table completeness and consistency checking process. The
      checking is performed by formulating theorems from a tabular
      expression and verifying theorems by using PVS (Prototype
      Verification System) without human interaction. It helps the software
      documentation reviewers to do preliminary checking and can also be
      used by other tools when they need to manipulate the domain of the
      table.}
}

@Booklet{Jipsen-1989,
  author = {Peter Jipsen},
  title = {Infinite RA's that have no Finite Nontrivial
		Subalgebras},
  note = {Preprint, April 17, 1989},
  year = 1989,
  bibliographies = {RelMiCS}
}

@InProceedings{Jipsen-1991,
  author = {Peter Jipsen and Erzs\'ebet Luk\'acs},
  title = {Representability of Finite Simple Relation Algebras
		with many identity Atoms},
  crossref = {AL1991},
  bibliographies = {RelMiCS}
}

@PhDThesis{Jipsen-1992,
  author = {Peter Jipsen},
  title = {Computer-aided investigations of relation algebras},
  school = {Vanderbilt University},
  year = 1992,
  month = MAY,
  bibliographies = {RelMiCS},
  URL = {http://proof.mth.uct.ac.za/~pjipsen/dissertation/jipsendis.ps.gz},
  WKloc = {doc/pap/BIB/Jipsen-1992.ps.gz}
}

@TechReport{Jipsen-2000a,
  author = {Peter Jipsen},
  title = {A Gentzen system and decidability for residuated lattices},
  institution = {},
  year = 2000,
  URL = {http://www.chapman.edu/~jipsen/reslat/gentzenrl.htm},
  PDF = {http://www.chapman.edu/~jipsen/reslat/gentzenrl.pdf},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = JAN,
  OPTnote = {},
  OPTannote = {}
}

@Misc{Jipsen-2001PCP,
  author = {Peter Jipsen},
  title = {Implementing quasi-equational logic on the web},
  howpublished = {Talk given at the AMS Sectional Meeting, University of South Carolina},
  month = {March 16-18},
  year = 2001,
  note = {\url{http://www.chapman.edu/~jipsen/PCP/usctalk.html}},
  bibliographies = {RelMiCS}
}

@Misc{Jipsen-2003PCP,
  author = {Peter Jipsen},
  title = {{PCP}: Point and Click Proofs},
  howpublished = {Web-based system at URL: \url{http://www.chapman.edu/~jipsen/PCP/PCPhome.html}},
  year = 2003,
  bibliographies = {RelMiCS}
}

@InProceedings{Jipsen-2012,
  author = 	 {Peter Jipsen},
  title =  {Categories of Algebraic Contexts
            Equivalent to Idempotent Semirings and Domain Semirings},
  crossref =  {RelMiCS2012},
  pages = 	 {195--206},
  bibliographies = {RelMiCS, RelMiCS13},
  DOI = {10.1007/978-3-642-33314-9_13},
  SpringerURL = {http://link.springer.com/chapter/10.1007/978-3-642-33314-9_13},
  abstract = {A categorical equivalence between algebraic contexts
                  with relational morphisms and join-semilattices with
                  homomorphisms is presented and extended to
                  idempotent semirings and domain semirings. These
                  contexts are the Kripke structures for idempotent
                  semirings and allow more efficient computations on
                  finite models because they can be logarithmically
                  smaller than the original semiring. Some examples
                  and constructions such as matrix semirings are also
                  considered.}
}

@InCollection{Jipsen-Brink-Schmidt-1997,
  author = {Peter Jipsen and Chris Brink and Gunther Schmidt},
  title = {Background Material},
  chapter = 1,
  pages = {1--21},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Article{Jipsen-Maddux-1997,
  title = {Nonrepresentable Sequential Algebras},
  author = {Peter Jipsen and Roger Maddux},
  pages = {565--574},
  volume = 5,
  number = 4,
  year = 1997,
  journal = {Logic Journal of the IGPL},
  abstract = {The sequential calculus of von~Karger and Hoare
         \cite{vonKargerHoare1995} is designed for reasoning about
         sequential phenomena, dynamic or temporal logic,
         and concurrent or reactive systems. Unlike the
         classical calculus of relations, it has no operation
         for forming the converse of a relation. Sequential algebras
         \cite{vonKarger1995} are algebras that satisfy certain equations
         in the sequential calculus. One standard example of a
         sequential algebra is the set of relations
         included in a partial ordering. Nonstandard examples arise
         by relativizing relation algebras to elements that are
         antisymmetric, transitive, and reflexive.
         The incompleteness and non-finite-axiomatizability
         of the sequential calculus are examined here from a
         relation-algebraic point of view. New constructions
         of nonrepresentable relation algebras are used to prove
         that there is no finite axiomatization
         of the equational theory of antisymmetric
         %confluent, dense %left linear,
         right linear, locally linear sequential algebras.
         The constructions improve on previous examples in certain
         interesting respects and give yet another proof
         that the classical calculus of relations is not
         finitely axiomatizable.},
  annote = {Full version of an invited paper presented at the
           {\em 3rd Workshop on Logic, Language, Information and Computation}
           ({\em WoLLIC'96\/}), May 8--10, 1996, Salvador (Bahia), Brazil,
           organised by UFPE and UFBA, and sponsored by IGPL, FoLLI, ASL.},
  bibliographies = {RelMiCS},
  URL = {http://www3.oup.co.uk/igpl/Volume_05/Issue_04/#jipsen},
  WKloc = {A-1022, doc/pap/BIB/Jipsen-Maddux-1997}
}

@InProceedings{Jiresch-2014,
  author =       {Eugen Jiresch},
  title =        {Towards a {GPU}-based Implementation of Interaction Nets},
  booktitle = {8th International Workshop on Developments in Computational Models, {DCM 2012}},
  year =      2014,
  editor =    {Benedikt L{\"o}we and Glynn Winskel},
  volume =    143,
  series =    {EPTCS},
  pages =     {41--53},
  DOI = {10.4204/EPTCS.143.4},
  OPTnote =      {\url{http://arxiv.org/abs/1404.0076}},
  abstract =    {We present ingpu, a GPU-based evaluator for interaction nets that heavily utilizes their potential for parallel evaluation. We discuss advantages and challenges of the ongoing implementation of ingpu and compare its performance to existing interaction nets evaluators.}
}

@InProceedings{Joergensen-1992,
  author = {Jesper J{\o}rgensen},
  title = {Generating a Compiler for a Lazy Language by Partial
		  Evaluation},
  crossref = {POPL1992},
  pages = {258--268},
  authorsAddress = {knud\@diku.dk},
  WKloc = {A-0161},
  abstract = {Compiler generation is often emphasized as being the
		  most important application of partial evaluation.
		  But most of the larger practical applications have,
		  to the best of our knowledge, been outside this
		  field. Especially, no one has generated compilers
		  for languages other than small languages. This paper
		  describes a large application of partial evaluation
		  where a realistic compiler was generated for a
		  strongly typed lazy functional language. The
		  language, that was called BAWL, was modelled after
		  the language in \cite{Bird-Wadler-1988} and is a
		  combinator language with pattern matching, guarded
		  alternatives, local definitions and list
		  comprehensions. The paper describes the most
		  important techniques used, especially the binding
		  time improvements needed in order to get small and
		  efficient target programs. Finally, the performance
		  of the compiler is compared with two compilers for
		  similar languages: Miranda and LML.},
  keywords = {Compiler generation, partial evaluation, binding
		  time improvements, lazy functional languages}
}

@Article{Johnson-1892,
  author = {W. E. Johnson},
  title = {The Logical Calculus, {I}, General Principles},
  journal = MIND,
  volume = {1, New Series},
  year = 1892,
  pages = {3--30},
  bibliographies = {RelMiCS}
}

@Article{Johnson-1892a,
  author = {W. E. Johnson},
  title = {The Logical Calculus, {II}},
  journal = MIND,
  volume = {1, New Series},
  year = 1892,
  pages = {235--250},
  bibliographies = {RelMiCS}
}

@Article{Johnson-1892b,
  author = {W. E. Johnson},
  title = {The Logical Calculus, {III}},
  journal = MIND,
  volume = {1, New Series},
  year = 1892,
  pages = {340--357},
  bibliographies = {RelMiCS}
}

@Article{Johnson-1969,
  author = {J. Johnson},
  title = {Nonfinitizability of Classes of Representable Polyadic Algebras},
  journal = JSYLO,
  volume = 34,
  year = 1969,
  pages = {344--352},
  bibliographies = {RelMiCS}
}

@Article{Johnson-1973,
  author = {J. Johnson},
  title = {Axiom Systems for Logic with finitely many Variables},
  journal = JSYLO,
  volume = 38,
  year = 1973,
  pages = {576--578},
  bibliographies = {RelMiCS}
}

@InProceedings{Johnson-Pearson-Pingali-1994,
  author = {Richard Johnson and David Pearson and Keshav Pingali},
  title = {The program structure tree: computing control regions in linear time},
  pages = {171--185},
  abstract = {In this paper, we describe the program structure tree (PST),
   a hierarchical
   representation of program structure based on single entry single exit (SESE)
   regions of the control flow graph. We give a linear-time algorithm for
   finding SESE regions and for building the PST of arbitrary control flow
   graphs (including irreducible ones). Next, we establish a connection between
   SESE regions and control dependence equivalence classes, and show how to
   use the algorithm to find control regions in linear time. Finally, we discuss
   some applications of the PST. Many control flow algorithms, such as
   construction of Static Single Assignment form, can be speeded up by applying
   the algorithms in a divide-and-conquer style to each SESE region on its own.
   The PST is also used to speed up data flow analysis by exploiting "sparsity".
   Experimental results from the Perfect Club and SPEC89 benchmarks confirm
   that the PST approach finds and exploits program structure.},
  URL = {http://www.acm.org/pubs/citations/proceedings/pldi/178243/p171-johnson/},
  WKloc = {A-0952},
  bibliographies = {OPG},
  crossref = {PLDI1994},
  ACMcats = {Theory of Computation -Logics and Meanings of Programs - Studies of
       Program Constructs (F.3.3): Control primitives; Software -Programming
       Languages - Language Constructs and Features (D.3.3): Control structures;
       Software -Programming Languages - Processors (D.3.4): Optimization;
       Theory of Computation -Analysis of Algorithms and Problem Complexity -
       Nonnumerical Algorithms and Problems (F.2.2): Computations on discrete
       structures}
}

@Misc{Johnson-Seifert-1967,
  author = {J. Johnson and R. L. Seifert},
  title = {A survey of multi-unary algebras},
  howpublished = {Mimeo\-graphed seminar notes, U. C. Berkeley},
  year = 1967,
  note = {26 pages}
}

@Article{Johnson-Walters-1987,
  author = {M.S. Johnson and R.F.C. Walters},
  title = {The nerve of an n-category},
  journal = CTGD,
  year = 1987,
  pages = {257--282}
}

@Article{Johnson-Walters-1992,
  author = {M.S. Johnson and R.F.C. Walters},
  title = {Algebra objects and algebra families for finite limit theories},
  journal = JPAA,
  year = 1992,
  pages = {283--293},
  volume = 83
}

@InProceedings{Johnson-Walters-1992a,
  author = {M.S. Johnson and R.F.C. Walters},
  title = {Category theoretic modelling of digital circuits and systems},
  booktitle = {Pan-Commonwealth conference on mathematicam modelling in
      circuit designs},
  year = 1992,
  OPTpages = {},
  OPTvolume = {},
  OPTmonth = {},
  OPTnote = {}
}

@Misc{JohnssonM-1991,
  author = {Michael Johnsson},
  title = {Linear Term rewriting systems are higher dimensional string rewriting systems},
  year = 1991,
  WKloc = {A-0424}
}

@InProceedings{JohnsonR-1992,
  author = 	 {Ralph E. Johnson},
  title = 	 {Documenting Frameworks Using Patterns},
  crossref =  {OOPSLA1992},
  DOIURL = {http://doi.acm.org/10.1145/141936.141943},
  optDOI = {10.1145/141936.141943},
  pages = 	 {63--76},
  annote = 	 {HotDraw}
}

@InProceedings{Johnsson-1987,
  author = {Thomas Johnsson},
  title = {Attribute Grammars as a Functional Programming Paradigm},
  crossref = {FPCA-1987},
  pages = {154--173},
  abstract = {The purpose of this paper is twofold. Firstly we
		  show how attributes in an attribute grammar can be
		  simply and efficiently avaluated using a lazy
		  functional language. The class of attribute grammars
		  we can deal with are the most general ones possible:
		  attributes may debend on each other in an arbitrary
		  way, as long as there are no truly circular data
		  dependencies.

		  Secondly, we describe a methodology based on
		  attribute grammars, where, in a fairly
		  straightforward way, we can develop efficient
		  functional programs where direct, conventional
		  solutions yield less efficient programs. We review
		  two examples from a paper by R. Bird (\cite{Bird84})
		  where he transforms simple but inefficient multipass
		  programs into more efficient single pass ones, but
		  which on their own can be very hard to understand.
		  We show how such efficient but tangled programs can
		  have natural formulations as attribute grammars.

		  We also propose a language construct, called {\bf
		  case rec} (akin to the {\bf case} expression in
		  Standard ML and Lazy ML), that defines an attribute
		  grammar over a data structure in the language. In
		  effect, a {\bf case rec} expression defines a
		  recursion operator that can handle multiple values,
		  both upwards-propagating and downwards-propagating ones.}
}

@incollection{Johnsson-1991,
   author = {Johnsson, Thomas},
   affiliation = {Chalmers University of Technology Department of Computer Science S-412 96 Göteborg Sweden S-412 96 Göteborg Sweden},
   title = {Parallel evaluation of functional programs: The $(\nu,G)$-machine approach},
   booktitle = {{PARLE '91 Parallel Architectures and Languages Europe}},
   series = LNCS,
   editor = {Aarts, Emile and van Leeuwen, Jan and Rem, Martin},
   publisher = {Springer Berlin / Heidelberg},
   isbn = {978-3-540-54151-6},
   keyword = {Computer Science},
   pages = {1--5},
   volume = {505},
   DOIURL = {http://dx.doi.org/10.1007/BFb0035092},
   DOI = {10.1007/BFb0035092},
   year = {1991},
   WKloc = {A-1745, doc/pap/BIB}
}

@Article{Johnsson-2004,
  author = {Johnsson, Thomas},
  title = {Efficient compilation of lazy evaluation},
  journal = {SIGPLAN Not.},
  issue_date = {April 2004},
  volume = {39},
  number = {4},
  month = apr,
  year = {2004},
  issn = {0362-1340},
  pages = {125--138},
  numpages = {14},
  url = {http://doi.acm.org/10.1145/989393.989409},
  doi = {10.1145/989393.989409},
  acmid = {989409},
  publisher = {ACM},
  address = {New York, NY, USA},
  WKloc = {A-1744, doc/pap/BIB}
}

@Book{Johnstone-1977,
  author =       "Peter T. Johnstone",
  title =        "Topos Theory",
  publisher =    "Academic Press, Inc. Ltd.",
  year =         "1977",
  number =       "10",
  series =       "London Mathemathical Society Monographs",
  address =      "London",
  ISBN =         "0-12-387850-0",
  McMaster = {QA 169 .J63}
}

@Book{Johnstone-1982,
  author = {Peter T. Johnstone},
  title = {{Stone} Spaces},
  publisher = CambridgeUP,
  year = 1982,
  address = {Cambridge},
  bibliographies = {RelMiCS}
}

@Book{Johnstone-2002,
  author =	 {Peter T. Johnstone},
  title = 	 {Sketches of an Elephant: A Topos Theory Compendium},
  publisher = 	 {Oxford University Press},
  year = 	 2002,
  ISBN = 	 {0-19-852496-X (2-volume set)},
  URL = 	 {http://www.oup.co.uk/isbn/0-19-852496-X},
  McMaster = {QA 169 .J62 2002 v.1/v.2}
}

@InProceedings{Johnstone-Power-Tsujishita-Watanabe-Worrell-1998,
  author = {Peter Johnstone and John Power and Toru Tsujishita and Hiroshi Watanabe and James Worrell},
  title = {An Axiomatics for Categories of Transition Systems as Coalgebras},
  pages = {},
  OPTabstract = {},
  OPTurl = {http://www.etl.go.jp/~hirowata/publication/src/lics.ps},
  WKloc = {A-0930},
  bibliographies = {ETL},
  year = 1998,
  OPTmonth = {},
  OPTcrossref = {},
  booktitle = {{Proceedings of LICS'98, 21-24 June 1998 in Indianapolis, Indiana}},
  OPTeditor = {},
  OPTpublisher = {},
  OPTseries = {},
  OPTvolume = {}
}

@TechReport{Jones-1990,
  author = {Mark P. Jones},
  title = {Computing with lattices: an application of type classes},
  year = 1990,
  number = {PRG-TR-11-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0085},
  bibliographies = {RelMiCS},
  abstract = {This report presents a simple framework for
		  performing calculations with the elements of
		  (finite) lattices. A particular feature of this work
		  is the use of type classes to enable the use of
		  overloaded function sysmbols within a strongly typed
		  language. Previous applications of type classes have
		  been in areas that are of most interest to language
		  implementors. This report suggests that type classes
		  might also be useful as a general tool in the
		  development of clear and modular programs.}
}

@TechReport{Jones-1991,
  author = {Mark P. Jones},
  title = {Towards a theory of qualified types},
  year = 1991,
  number = {PRG-TR-6-91},
  note = {preliminary version of \cite{Jones-1992}},
  month = APR,
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0088},
  bibliographies = {RelMiCS},
  abstract = {In a language with a polymorphic type system, a term
		  of type $\all t.f(t)$ can be treated (possibly after
		  suitable instantiation) as having any of the types
		  in the set:
		  $$ \{ f(a) | a \mbox{ is a type} \}. $$
		  A natural extension of such systems supports a more
		  restricted form of polymorphism in which, rather
		  than simply taking on all possible values, the type $a$ is
		  constrained to satisfy a specified predicate $\pi(a)$.}
}

@TechReport{Jones-1991a,
  author = {Mark P. Jones},
  title = {Type Inference for Qualified Types},
  year = 1991,
  number = {PRG-TR-10-91},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0090},
  abstract = {Qualified types provide a general framework for type
		  systems with various forms of overloading, by
		  introducing an intermediate stage between
		  monomorphism and polymorphism.

                  This report describes an extension of the ML type
		  system to support qualified types. We show that the
		  resulting system is suitable for use in a language
		  based on type inference, in the sense that the set
		  of all derivable typings for a term can be
		  characterised by a single {\em principal type
		  scheme}, calculated using a type inference
		  algorithm.

                  Detailed proofs of the results presented in this
		  report are included in an appendix.}
}

@InProceedings{Jones-1992,
  author = {Mark P. Jones},
  title = {A Theory of Qualified Types},
  crossref = {ESOP1992},
  pages = {287--306},
  note = {also as TechReport \cite{Jones-1991} (A-0088)},
  authorsAddress = {PRG Oxford},
  bibliographies = {RelMiCS},
  abstract = {In a language with a polymorphic type system, a term
		  of type $\all t.f(t)$ can be treated (possibly after
		  suitable instantiation) as having any of the types
		  in the set:
		  $$ \{ f(t) | t \mbox{is a type} \}. $$
		  It is natural to consider a more restricted form of
		  polymorphism in which the value taken by $t$ may be
		  constrained to a particular subset of types. In this
		  situation we write $\all t.\pi(t)\Rightarrow f(t)$,
		  where $\pi(t)$ is a predicate of types, for the type
		  of an object which can be treated (after suitable
		  instantiation) as having any of the types in the
		  set:
		  $$
		  \{ f(t) | t \mbox{is a type such that $\pi(t)$
		  holds} \}.
		  $$
		  A term with a restricted polymorphic type of this
		  kind is often said to be {\em overloaded}, having
		  different interpretations for different argument
		  types.

		  This paper presents a general theory of overloading
		  based on the use of {\em qualified types}, which are
		  types of the form $\pi\Rightarrow\sigma$ denoting
		  those instances of type $\sigma$ which satisfy the
		  predicat $\pi$. The main benefits of using qualified
		  types are:
		  \begin{itemize}
		  \item A general approach which includes a range of
		  familiar type systems as special cases. Results and
		  tools developed for the general system are
		  immediately applicable to each particular
		  application.
		  \item A precise treatment of the relationship
		  between implicit and explicit overloading. This is
		  particularly useful for describing the
		  implementation of systems supporting qualified
		  types.
		  \item The ability to include local constraints as
		  part of the type of an object. This enables the
		  definition and use of polymorphic overloaded values
		  within a program.},
  annote = {partly motivated by HASKELL}
}

@InProceedings{Jones-1993,
  author = {Mark P. Jones},
  title = {A system of constructor classes: overloading and implicit higher-order polymorphism},
  booktitle = {FPCA '93: Proceedings of the conference on Functional programming languages and computer architecture},
  year = {1993},
  isbn = {0-89791-595-X},
  pages = {52--61},
  location = {Copenhagen, Denmark},
  doi = {http://doi.acm.org/10.1145/165180.165190},
  publisher = {ACM Press},
  address = {New York, NY, USA},
  WKloc = {doc/pap/BIB},
  abstract = {This paper describes a flexible type system which
      combines overloading and higher-order polymorphism in an implicitly
      typed language using a system of constructor classes --- a
      natural generalization of type classes in Haskell.  We present a wide
      range of examples which demonstrate the usefulness of such a
      system. In particular, we show how constructor classes can be used to
      support the use of monads in a functional language.  The underlying
      type system permits higher-order polymorphism but retains many of
      many of the attractive features that have made the use of
      Hindley/Milner type systems so popular.  In particular, there is an
      effective algorithm which can be used to calculate principal types
      without the need for explicit type or kind annotations.  A prototype
      implemen- tation has been developed providing, amongst other things,
      the first concrete implementation of monad comprehensions known to us
      at the time of writing.}
}

@TechReport{Jones-1994,
  author = {Mark Jones},
  title = {The Implementation of the {Gofer} Functional Programming System},
  institution = {Yale University, Department of Computer Science},
  year = 1994,
  number = {YALEU/DCS/RR-1030},
  month = MAY,
  WKloc = {B-0096}
}

@InCollection{Jones-1995,
  author = {Mark P. Jones},
  title = {Functional Programming with Overloading and Higher-Order Polymorphism},
  crossref = {AFP1995},
  pages = {97--136},
  DOI = {10.1007/3-540-59451-5_4},
  DOIURL = {http://dx.doi.org/10.1007/3-540-59451-5_4},
  abstract = {The Hindley/Milner type system has been widely adopted
    as a basis for statically typed functional languages.
    One of the main reasons for this is that it provides
    an elegant compromise between flexibility, allowing a single value
    to be used in different ways, and practicality, freeing the programmer
    from the need to supply explicit type information.

    Focusing on practical applications rather than implementation or
    theoretical details, these notes examine a range of extensions
    that provide more flexible type systems while retaining
    many of the properties that have made the original
    Hindley/Milner system so popular. The topics discussed,
    some old, but most quite recent, include
    higher-order polymorphism and type and constructor class overloading.
    Particular emphasis is placed on the use of these features
    to promote modularity and reusability.},
  WKloc = {A-1086, doc/pap/BIB},
  bibliographies = {FP}
}

@InProceedings{Jones-1995a,
  author = {Mark P. Jones},
  title = {From Hindley-Milner Types to First-Class Structures},
  crossref = {Haskell1995},
  OPTpages = {},
  OPTURL = {http://www.cse.ogi.edu/~mpj/pubs/haskwork95.html},
  abstract = {We describe extensions of the Hindley-Milner type system to
      support higher-order polymorphism and first-class structures with
      polymorphic components. The combination of these features results in
      a `core language' that rivals the expressiveness of the Standard ML
      module system in some respects and exceeds it in others.},
  WKloc = {A-1109}
}


@Article{Jones-1995b,
  author = {Mark P. Jones},
  title = 	 {Dictionary-Free Overloading by Partial Evaluation},
  journal = 	 {Lisp and Symbolic Computation},
  year = 	 1995,
  volume =	 8,
  pages =	 {229--248},
  WKloc = 	 {A-1709}
}

@InProceedings{Jones-1996,
  author = {Mark P. Jones},
  title = {Using Parameterized Signatures to Express Modular Structure},
  year = 1996,
  crossref = {POPL1996},
  pages = {68--78},
  abstract = {Module systems are a powerful, practical tool for managing
      the complexity of large software systems. Previous attempts to
      formulate a type-theoretic foundation for modular programming have
      been based on existential, dependent, or manifest types. These
      approaches can be distinguished by their use of different quantifiers
      to package the operations that a module exports together with
      appropriate implementation types. In each case, the underlying type
      theory is simple and elegant, but significant and sometimes complex
      extensions are needed to account for features that are important in
      practical systems, such as separate compilation and propagation of
      type information between modules.

      This paper presents a simple type-theoretic framework for modular
      programming using parameterized signatures. The use of quantifiers is
      treated as a necessary, but independent concern. Using familiar
      concepts of polymorphism, the resulting module system is easy to
      understand and admits true separate compilation. It is also very
      powerful, supporting high-order, polymorphic, and first-class modules
      without further extension.},
  WKloc = {A-0848}
}

@Manual{Jones-1996a,
  title = {{Hugs 1.3, The Haskell user's Gofer System, User Manual}},
  author = {Mark P. Jones},
  organization = {Department of Computer Science, The University of Nottingham},
  month = AUG,
  year = 1996,
  WKloc = {B-0066}
}

@InProceedings{Jones-1998,
  author = {Mark P. Jones},
  title = {The Functions of {Java} Bytecode},
  booktitle = {OOPSLA '98 workshop on Formal Underpinnings of Java},
  year = 1998,
  month = OCT,
  URL = {http://www.cse.ogi.edu/~mpj/},
  WKloc = {A-1153}
}

@InProceedings{Jones-1999,
  author = {Mark P. Jones},
  title = {Typing {Haskell} in {Haskell}},
  booktitle = {Third International {Haskell} Workshop 1999},
  year = 1999,
  pages = {},
  WKloc = {A-0865, doc/pap/BIB}
}

@InProceedings{Jones-2000,
  author = {Mark P. Jones},
  title = {Type Classes with Functional Dependencies},
  crossref = {ESOP2000},
  pages = {230--244},
  WKloc = {A-1001}
}

@TechReport{Jones-Duponcheel-1993,
  author = 	 {Mark P. Jones and Luc Duponcheel},
  title = 	 {Composing Monads},
  institution =  {Yale University},
  year = 	 1993,
  type =	 {Research Report},
  number =	 {YALEU/DCS/RR-1004},
  address =	 {New Haven, Connecticut, USA},
  month =	 DEC,
  URL = 	 {http://www.cse.ogi.edu/~mpj/pubs/composing.html},
  WKloc = {doc/pap/BIB},
  abstract = {Monads are becoming an increasingly important tool
       for functional programming. Different monads can be used to
       model a wide range of programming language features. However,
       real programs typically require a combination of different
       features, so it is important to have techniques for combining
       several features in a single monad. In practice, it is usually
       possible to construct a monad that supports some specific
       combination of features. However, the techniques used are
       typically ad-hoc and it is very difficult to find general
       techniques for combining arbitrary monads.

       This report gives three general constructions for the
       composition of monads, each of which depends on the existence
       of an auxiliary function linking the monad structures of the
       components. In each case, we establish a set of laws that the
       auxiliary function must satisfy to ensure that the composition
       is itself a monad.

       Using the notation of constructor classes, we describe some
       specific applications of these constructions. These results are
       used in the development of a simple expression evaluator that
       combines exceptions, output and an environment of variable
       bindings using a composition of three corresponding monads.}
}

@TechReport{Jones-Hudak-1993,
  author = {Mark P. Jones and Paul Hudak},
  title = {Implicit and Explicit Parallel Programming in Haskell},
  year = 1993,
  month = AUG,
  institution = {Department of Computer Science, Yale University},
  type = {Research Report},
  number = {YALEU/DCS/RR-982},
  WKloc = {A-0831}
}

@Manual{Jones-Peterson-1997,
  title = {{Hugs 1.4, The Nottingham and Yale Haskell User's System}},
  author = {Mark P. Jones and John C. Peterson},
  month = APR,
  year = 1997,
  WKloc = {B-0110, includes related stuff, and Paul Hudak's Haskore Music Tutorial}
}

@Misc{Jones-PeytonJones-1999,
  author = {Mark P. Jones and Peyton Jones, Simon L.},
  title = {Lightweight Extensible Records for {Haskell}},
  year = 1999,
  month = JUN,
  WKloc = {A-0856}
}

@TechReport{Jones-Sheeran-1990,
  author = {Geraint Jones and Mary Sheeran},
  title = {The study of butterflies},
  year = 1990,
  number = {PRG-TR-14-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0081},
  bibliographies = {RelMiCS},
  keywords = {Ruby, relation algebra, RelAlg},
  abstract = {Butterfly networks arise in many signal processing
		  circuits and in parallel algorithms for many sorts
		  of message-passing computers. This paper attempts to
		  explain why this should be, and what butterfly
		  networks are, using a new and elegant formulation
		  based on a language of relations.

                  Most of the material covered by this paper has
		  appeared in a less tractable form in earlier papers
		  [7,8]. The novelty here is in the simplicity and
		  elegance of presentation, which derives from an
		  appropriate choice of high-level structures. These
		  structures are represented by functions which are
		  used to compose circuits from components, and are
		  chosen to have simple mathematical properties. This
		  presentation makes it easier to explain how the
		  design comes about, showing that butterflies are
		  natural implementations of divide-and-conquer algorithms.}
}

@TechReport{Jones-Sheeran-1990a,
  author = {Geraint Jones and Mary Sheeran},
  title = {Relations and refinement in circuit design},
  year = 1990,
  number = {PRG-TR-13-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0087},
  keywords = {RelAlg},
  bibliographies = {RelMiCS, RelMiS},
  abstract = {A language of relations and combining forms is
		  presented in which to describe both the behaviour of
		  circuits and the specifications which they must
		  meet. We illustrate a design method that starts by
		  selecting representations for the values on which a
		  circuit operates, and derive the circuit from these
		  representations by a process of refinement entirely
		  within the language.}
}

@InProceedings{Jones-Sheeran-1990b,
  author = {Geraint Jones and Mary Sheeran},
  title = {Circuit Design in {Ruby}},
  year = 1990,
  booktitle = {Formal Methods in {VLSI} Design},
  editor = {J{\oslash}rgen Staunstrup},
  publisher = {North-Holland},
  pages = {13--70},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings{Jones-Sheeran-1992,
  author = {Geraint Jones and Mary Sheeran},
  title = {Designing Arithmetic Circuits by Refinement in Ruby},
  crossref = {MPC1992},
  pages = {208--232},
  WKloc = {A-0240},
  note = {see also \url{http://web.comlab.ox.ac.uk/oucl/work/geraint.jones/ruby/}},
  abstract = {This paper presents in some detail the systematic
		  derivation of a static bit-level parallel algorithm
		  to implement multiplication of integers, that is to
		  say one which might be implemented as an electronic
		  circuit. The circuit is well known, but the
		  derivation shows that its design can be seen as the
		  consequence of decisions made (and explained) in
		  terms of the abstract algorithm. The systematic
		  derivation serves both as an explanation of the
		  circuit, and as a demonstration that it is correct
		  `by construction'. We believe that the technique is
		  applicable to a wide range of similar algorithms.}
}

@Book{JonesCB-1980,
  author = {Cliff B. Jones},
  title = {Software Development: A Rigorous Approach},
  publisher = Prentice,
  year = 1980,
  bibliographies = {RelMiS}
}

@PhDThesis{JonesCB-1981,
  author = {C. B. Jones},
  month = {June},
  note = {Printed as: Programming Research Group, Technical Monograph 25},
  school = {Oxford University},
  title = {Development Methods for Computer Programs including a Notion of Interference},
  author-local-url = {file://localhost/Users/ncbj/Documents/CBJ-pubs/Other-TRs/Jon81_thesis_PRG_25.pdf},
  year = {1981}
}

@InProceedings{Jones-1983,
  author    = {Cliff B. Jones},
  title     = {Specification and Design of (Parallel) Programs},
  booktitle = {Proceedings of IFIP'83},
  publisher = {North-Holland},
  year      = {1983},
  pages     = {321--332}
}

@Book{JonesCB-1986,
  author = {Cliff B. Jones},
  title = {Systematic Software Development Using {VDM}},
  publisher = Prentice,
  year = 1986,
  series = PrenticeCS,
  McMaster = {QA 76.76 .D47 J66 1986},
  bibliographies = {RelMiS},
  WKloc = {doc/pap/BIB}
}

@InCollection{JonesCB-1994,
  author = {C. B. Jones},
  title = {Process Algebra Arguments about an Object-based Design Notation},
  crossref = {Roscoe-1994},
  pages = {231--245},
  chapter = 14
}

@Misc{JonesCB-2001,
  author = {C. B. Jones},
  title = {On the Search fo Tractable Ways of Reasoning about Programs},
  month = JUL,
  year = 2001,
  WKloc = {A-1320},
  bibliographies = {RelMiS},
  abstract = {This paper traces the important steps in the history --- up
      to around 1990 --- of research on reasoning about programs. The main
      focus is on sequential imparative programs but some comments are made
      on concurrency. Initially, researchers focussed on ways of verifying
      that a program satisfies its specification (or that two programs were
      equivalent). Over time it became clear that \emph{post facto}
      verification is only practical for small programs and attention
      turned to verification methods which support the development of
      programs; for larger programs it is necessary to exploit a notation
      of compositionality. Coping with concurrent algorithms is much more
      challenging --- this and other extensions are considered briefly. The
      main thesis of this paper is that the idea of reasoning about
      programs has been around since they were first written; the search
      has been to find tractable methods.},
  annote = {Includes full first names and year-of-birth of key people.}
}

@Article{JonesCV-1990,
  author = {C. V. Jones},
  title = {An Introduction to Graph-Based Modeling Systems, Part~{I}:
		  Overview},
  journal = {ORSA Journal on Computing},
  volume = 2,
  number = 2,
  pages = {136--151},
  year = 1990
}

@Article{JonesCV-1991,
  author = {C. V. Jones},
  title = {An Introduction to Graph-Based Modeling Systems,
		  Part~{II}: Graph-Grammars and the Implementation},
  journal = {ORSA Journal on Computing},
  volume = 3,
  number = 3,
  pages = {180--206},
  year = 1991
}

@Article{JonesCV-1992,
  author = {C. V. Jones},
  title = {Attributed Graphs, Graph-Grammars, and Structured
		  Modeling},
  journal = {Annals of OR},
  volume = 38,
  pages = {281--324},
  year = 1992
}

@Article{JonesCV-1993,
  author = {C. V. Jones},
  title = {An Integrated Modeling Environment Based on Attributed
		  Graphs and Graph-Grammars},
  journal = {Decision Support Systems},
  volume = 10,
  pages = {255--275},
  year = 1993,
  added = {1996-06-13-16-37-07}
}

@Article{JonesCV-1995,
  author = {C. V. Jones},
  title = {Developments in Graph-based Modeling for Decision
		  Support},
  journal = {Decision Support Systems},
  volume = 13,
  number = 1,
  pages = {61--74},
  year = 1995,
  added = {1996-06-13-16-37-07}
}

@Article{JonesLG-1990,
  author = {Larry G. Jones},
  title = {Efficient evaluation of circular attribute grammars },
  journal = TOPLAS,
  year = 1990,
  volume = 12,
  number = 3,
  pages = {429--462},
  URL = {http://www.acm.org/pubs/citations/journals/toplas/1990-12-3/p429-jones/},
  WKloc = {A-1053, doc/pap/BIB},
  abstract = {We present efficient algorithms for exhaustive and incremental evaluation of circular
       attributes under any conditions that guarantee finite convergence. The algorithms are
       derived from those for noncircular attribute grammars by partitioning the underlying
       attribute dependency graph into its strongly connected components and by ordering the
       evaluations to follow a topological sort of the resulting directed acyclic graph. The
       algorithms are efficient in the sense that their worst-case running time is proportional to
       the cost of computing the fixed points of only those strongly connected components
       containing affected attributes or attributes directly dependent on affected attributes. When
       the attribute grammar is noncircular or the specific dependency graph under
       consideration is acyclic, both algorithms reduce to the standard optimal algorithms for
       noncircular attribute evaluation.}
}

@InProceedings{JonesND-1993,
  author = {Neil D. Jones},
  title = {The Essence of Program Transformation by Partial
                 Evaluation and Driving},
  pages = {206--224},
  summary = {A new abstract framework is developed for program
                 specialization algorithms that is more general than the
                 projection-based partial evaluation methods formalised
                 in the author's earlier `Re-examination' paper.
                 Turchin's `driving' transformation, known to yield more
                 efficient specialised programs, is in this paper for
                 the first time put on a solid semantic foundation which
                 is not tied to any particular programming language or
                 data structure. The new approach includes both
                 projection-based methods and driving in a natural and
                 simple way, and eases formulating correctness of
                 residual code generation.},
  crossref = {LNCS792}
}

@InProceedings{JonesND-1995,
  author = {Neil D. Jones},
  title = {{MIX} ten years after},
  pages = {24--38},
  crossref = {PEPM1995}
}

@Article{JonesND-1996,
  author = {Neil D. Jones},
  year = 1996,
  title = {An Introduction to Partial Evaluation},
  journal = {ACM Computing Surveys},
  volume = 28,
  number = 3,
  pages = {480--504},
  URL = {http://www.acm.org/pubs/citations/journals/surveys/1996-28-3/p480-jones/}
}

@InProceedings{JonesND-1999,
  author = {Neil D. Jones},
  year = 1999,
  title = {The Essence of Program Transformation by Partial
                 Evaluation and Driving},
  pages = {62--79},
  crossref = {PSI1999},
  keywords = {Program transformation, partial evaluation, driving},
  summary = {An abstract framework is developed to describe program
                 transformation by {\em specializing} a given program to
                 a restricted set of inputs. Particular cases include
                 partial evaluation and Turchin's more powerful
                 ``driving'' transformation. Such automatic program
                 speedups have been seen to give quite significant
                 speedups in practical applications. This paper's aims
                 are similar to those of (Jones, 1987): better to
                 understand the fundamental mathematical phenomena that
                 make such speedups possible. The current paper is more
                 complete than (Jones, 1987), since it precisely
                 formulates correctness of code generation; and more
                 powerful, since it includes program optimizations not
                 achievable by simple partial evaluation. Moreover, for
                 the first time it puts Turchin's driving methodology on
                 a solid semantic foundation which is not tied to any
                 particular programming language or data
                 structure.\medskip This paper is dedicated to Satoru
                 Takasu with thanks for good advice early in my career
                 on how to do research, and for insight into how to see
                 the essential part of a new problem.},
  semno = {D-387}
}

@Book{Jones-Gomard-Sestoft-1993,
  author =	 {Neil D. Jones and Carsten K. Gomard and Peter Sestoft},
  title = 	 {Partial Evaluation and Automatic Program Generation},
  publisher = 	 {Prentice Hall},
  year = 	 1993,
  WKloc = 	 {B-0131, doc/pap/BIB},
  URL = 	 {http://www.dina.kvl.dk/~sestoft/pebook/pebook.html}
}

@InProceedings{JonesND-Mycroft-1986,
  author = {Neil D. Jones and A. Mycroft},
  title = {Data FLow Analysis of Applicative Programs Using
		  Minimal Function Graphs},
  crossref = {POPL1986},
  pages = {296--306}
}

@InCollection{JonesND-Nielson-1995,
  author = {Neil D. Jones and Flemming Nielson},
  title = {Abstract Interpretation: A Semantics-Based Tool for Program Analysis},
  crossref = {HBLCS-IV},
  pages = {527--635},
  WKloc = {A-1360}
}

@InProceedings{JonesND-Rosendahl-1994,
  author = {Neil D. Jones and Mads Rosendahl},
  title = {Higher-Order Minimal Function Graphs},
  crossref = {ALP1994},
  pages = {242--252},
  keywords = {memoize, memoise},
  abstract = {We present a minimal function graph semantics for a
		  higher-order functional language with applicative
		  evaluation order. The semantics captures the
		  intermediate calls performed during the evaluation
		  of a program. This information may be used in
		  abstract interpretation as a basis for proving the
		  soundness of program analyses. An example of this is
		  the ``closure analysis'' of partial evaluation.},
  annote = {see \cite{Jones-Mycroft-1986} for minimal function graphs}
}

@Book{JonesR-Lins-1996,
  author = {Richard Jones and Rafael Lins},
  title = {Garbage collection: algorithms for automatic dynamic
                 memory management},
  publisher = {Wiley},
  address = {New York, NY, USA},
  pages = {xxvi + 377},
  year = 1996,
  ISBN = {0-471-94148-4},
  LCCN = {QA76.9.G37J66 1996},
  note = {Reprinted in 1999 with improved index, and corrected
                 errata.},
  price = {US\$60.00},
  URL = {http://www.ukc.ac.uk/computer_science/Html/Jones/gc.html}
}

@Article{Jonsson-1959,
  author = {Bjarni J{\'o}nsson},
  title = {Representation of Modular Lattices and of Relation Algebras},
  journal = TRAMS,
  volume = 92,
  year = 1959,
  pages = {449--464},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-1962,
  author = {Bjarni J{\'o}nsson},
  title = {Defining Relations for Full Semigroups
		of Finite Transformations},
  journal = MICH,
  volume = 9,
  year = 1962,
  pages = {77--85},
  bibliographies = {RelMiCS}
}

@InCollection{Jonsson-1972,
  author = {Bjarni J{\'o}nsson},
  title = {Extensions of Relational Structures},
  booktitle = {The Theory of Models},
  publisher = NoHo,
  address = {Amsterdam--London},
  year = 1972,
  pages = {146--157},
  bibliographies = {RelMiCS}
}

@Book{Jonsson-1972a,
  author = {Bjarni J{\'o}nsson},
  title = {Topics in Universal Algebra},
  publisher = Springer,
  year = 1972,
  volume = 250,
  series = LNM,
  UniBwM = {MAT200/C14572},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-1982,
  author = {Bjarni J{\'o}nsson},
  title = {Varieties of Relation Algebras},
  journal = ALGU,
  volume = 15,
  year = 1982,
  pages = {273--298},
  bibliographies = {RelMiCS}
}

@InCollection{Jonsson-1984,
  author = {Bjarni J{\'o}nsson},
  title = {Maximal Algebras of Binary Relations},
  booktitle = {Contributions to Group Theory: Papers Published in Honor
                of {Roger Lyndon} on his {$65^{th}$} Birthday},
  note = {Contemporary Mathematics  33,
                edited by Kenneth I. Appel, John G. Ratcliffe, and
                Paul E. Schupp QA171.C683 1984},
  publisher = AMS,
  address = {Providence},
  pages = {299--307},
  year = 1984,
  bibliographies = {RelMiCS}
}

@Booklet{Jonsson-1984a,
  author = {Bjarni J{\'o}nsson},
  title = {The Theory of Binary Relations, a First Draft},
  note = {Preprint, 1984, pp.\null{} 65},
  bibliographies = {RelMiCS}
}

@InProceedings{Jonsson-1986,
  author = {Bjarni J{\'o}nsson},
  title = {On Binary Relations},
  booktitle = {Proc.\null{} of the NIH Conf. on Universal Algebra
		and Lattice Theory},
  editor = {G. Hutchinson},
  publisher = NIH,
  address = {Bethesda, Maryland},
  year = 1986,
  pages = {2--5},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-1988,
  year = 1988,
  volume = 70,
  title = {Relation Algebras and {Schr\"oder} Categories},
  pages = {27--45},
  journal = DISCR,
  author = {Bjarni J{\'o}nsson},
  UniBwM = {MAT Z-1733-70},
  WKloc = {A-0630},
  bibliographies = {RelMiCS}
}

@InProceedings{Jonsson-1991,
  year = 1991,
  title = {The Theory of Binary Relations},
  pages = {245--292},
  author = {Bjarni J{\'o}nsson},
  crossref = {Andreka-Monk-Nemeti-1991},
  bibliographies = {RelMiCS}
}

@InCollection{Jonsson-1993,
  author = {Bjarni J{\'o}nsson},
  title = {A Survey of Boolean Algebras with Operators},
  booktitle = {Algebras and Orders},
  note = {ed.\null{} by Ivo~G.\null{} Rosenberg and Gert Sabidussi
		North American Treaty Organization,
		Advanced Science Institutes Series,
		Series C: Mathematical and Physical Sciences},
  volume = 389,
  publisher = Kluwer,
  address = {Dordrecht},
  year = 1993,
  pages = {239--286},
  bibliographies = {RelMiCS}
}

@InProceedings{Jonsson-Kok-1991,
  title = {Towards a Complete Hierarchy of Compositional Dataflow Models},
  author = {Bengt Jonsson and Joost N. Kok},
  pages = {204--225},
  crossref = {TACS1991},
  abstract = {A dataflow network consists of nodes that communicate by
		  passing data over unbounded FIFO channels.  For
		  dataflow networks containing only deterministic
		  nodes, Kahn has presented a simple and elegant
		  semantic model.  However, the generalization of this
		  model is not compositional for nondeterministic
		  networks.  Past work has shown that compositionality
		  can be attained by models based on traces.  In the
		  paper, we investigate trace models of dataflow
		  networks, with the aim of characterizing
		  compositional and non-compositional models.  We
		  study several compositional trace models, which
		  differ in whether they model liveness, termination
		  or divergence.  We relate the models into a
		  hierarchy, according to their capability to
		  distinguish networks.  A hierarchy is called {\it
		  complete} if any gap between two models in the
		  hierarchy contains no compositional models.  Our
		  main contribution is to prove that most of the gaps
		  in our hierarchy do not contain compositional
		  models.  Several full abstraction results in the
		  literature follow directly from the gaps in our
		  hierarchy.  We also show that by restricting the
		  networks to contain less powerful nondeterministic
		  processes, additional models become
		  compositional. This means that additional models are
		  added to the hierarchy.}
}

@Article{Jonsson-Tarski-1948,
  author = {Bjarni J{\'o}nsson and Alfred Tarski},
  title = {Boolean Algebras with Operators},
  journal = BUAMS,
  note = {Abstract 88},
  volume = 54,
  year = 1948,
  pages = {79--80},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-Tarski-1948a,
  author = {Bjarni J{\'o}nsson and Alfred Tarski},
  title = {Representation Problems for Relation Algebras},
  note = {Abstract 89},
  journal = BUAMS,
  volume = 54,
  year = 1948,
  pages = {80 and 1192},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-Tarski-1951,
  year = 1951,
  volume = 73,
  title = {Boolean Algebras with Operators, {Part I}},
  pages = {891--939},
  journal = AJM,
  author = {Bjarni J{\'o}nsson and Alfred Tarski},
  bibliographies = {RelMiCS}
}

@Article{Jonsson-Tarski-1952,
  year = 1952,
  volume = 74,
  title = {Boolean Algebras with Operators, {Part II}},
  pages = {127--167},
  journal = AJM,
  author = {Bjarni J{\'o}nsson and Alfred Tarski},
  bibliographies = {RelMiCS}
}

@Article{Josephs-1989,
  WKloc = {A-0051},
  year = 1989,
  volume = 68,
  title = {The Semantics of Lazy Functional Languages},
  pages = {105-111},
  journal = {Theoretical Computer Science},
  author = {Mark B. Josephs}
}

@TechReport{Josephs-RedmondPyle-1991,
  author = {Mark B. Josephs and David Redmond-Pyle},
  title = {Entity-relationship models expressed in {Z}: a
		  synthesis of structured and formal methods},
  year = 1991,
  number = {PRG-TR-20-91},
  month = JUL,
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0079},
  keywords = {ZED},
  bibliographies = {RelMiCS},
  abstract = {Structured methods are widely used in systems
		  analysis and design for commercial data processing
		  applications. One of the most important features of
		  these methods is the use of entity-relationship
		  diagrams as a data medelling technique. This paper
		  contributes to the understanding of such methods by
		  taking a typical one, LBMS systems Engineering, and
		  providing a systematic translation of its diagrams
		  into Z. We also demonstrate how the expressiveness
		  and precision of structured methods can be enhanced
		  by specifying in Z further constraints on the data
		  model and the effect of transactions on the system
		  state. In structured methods, the former is usually
		  done by informal comments and the latte by
		  pseudo-code, if at all. This work also has important
		  consequences as far as the widespread adaption of
		  formal methods is concerned. It provides a style of
		  writing Z specifications that could easily be
		  adopted by someone already familiar with
		  entity-relationship modelling, and does so in a way
		  that standardizes the use of schemas as much as possible.}
}

@Article{Joshi-Nelson-ZhouYunhong-2006,
  author = 	 {Rajeev Joshi and Greg Nelson and Yunhong Zhou},
  title = 	 {Denali: A Practical Algorithm for Generationg Optimal Code},
  journal = 	 TOPLAS,
  year = 	 2006,
  DOIURL = 	 {http://doi.acm.org/10.1145/1186632.1186633},
  volume =	 28,
  number =	 6,
  pages =	 {967--989},
  month =	 NOV,
  bibliographies = {Coconut},
  abstract = {This article presents a design for the Denali-2 superoptimizer,
     which will generate minimum-instruction-length machine code
     for realistic machine architectures using automatic theorem-proving technology:
     specifically, using E-graph matching
     (a technique for pattern matching in the presence of equality information)
     and Boolean satisfiability solving.
     This article presents a precise definition
     of the underlying automatic programming problem solved by the Denali-2 superoptimizer.
     It sketches the E-graph matching phase
     and presents a detailed exposition and proof of soundness of the reduction
     of the automatic programming problem to the Boolean satisfiability problem.}
}

@InProceedings{Jouannaud-1991,
  WKloc = {A-0011},
  abstract = {Conventional algebraic specifications are first-order. Using
        higher-order equations in combination with first-order ones raises
        several fundamental model-theoretic and proof-theoretic questions.
        The model theory of higher-order equations is well understood (see
        [20] for a survey of algebraic specifications). The proof theory of
        higher-order equations is equally well understood, it requires
        higher-order matching, and higher-order rewriting therefore providing
	with a simple execution model. Higher-order variables may be
	instantiated by functions described by $\lambda$-expressions,
	bringing in $\lambda$-calculus, whose execution is again  rewriting
	($\beta$-redexes). Hence rewriting is at the heart of all three
	execution models, which makes their combination quite simple on the
	operational side. The main question reviewed in this paper is whether
	the Church-Rosser and termination properties of these three execution
	models are preserved within their combination. We will see that the
	answer is to a large extent positive},
  title = {Executable Higher-Order Algebraic Specifications},
  pages = {16--25},
  crossref = {STACS1991},
  author = {Jean Pierre Jouannaud},
  annote = {second order lambda calculus,
	called ``lambda algebraic theories'',
	type disciplines for decidability of typing,
	type conditions for rules and equations,
	valuable references.}
}

@InProceedings{Jouannaud-1992,
  author = {Jean-Pierre Jouannaud},
  title = {Rewriting Techniques for Software Engineering},
  crossref = {SADT92},
  pages = {30--53},
  note = {invited paper},
  WKloc = {A-0340}
}

@Misc{Jouannaud-1995a,
  OPTkey = {},
  OPTauthor = {Jean-Pierre Jouannaud},
  OPTtitle = {Rewrite Proofs and Computations},
  OPThowpublished = {International Summer School Marktoberdorf,
		  Working Material},
  OPTyear = 1995,
  OPTmonth = JUL,
  OPTnote = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Jouannaud-1995b,
  OPTkey = {},
  OPTauthor = {Jean-Pierre Jouannaud},
  OPTtitle = {A Taste of Rewrite Systems},
  OPThowpublished = {International Summer School Marktoberdorf, Slides},
  OPTyear = 1995,
  OPTmonth = JUL,
  OPTnote = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTcontents = {},
  WKloc = {B-0068}
}

@InProceedings{Jouannaud-Rubio-1996,
  author = {Jean-Pierre Jouannaud and Albert Rubio},
  title = {A Recursive Path Ordering for Higher-Order Terms in
		  $\eta$-Long $\beta$-Normal Form},
  crossref = {RTA96},
  pages = {108--122},
  abstract = {This paper extends the termination proof techniques
		  based on rewrite orderings to a higher-order
		  setting,
		  by defining a recursive path ordering for simply
		  typed higher-order-terms in $\eta$-long
		  $\beta$-normal form.
		  This ordering is powerful enough to show termination
		  of several complex examples.}
}

@InProceedings{Jouannaud-Sadfi-1994,
  author = {Jean-Pierre Jouannaud and W. Sadfi},
  title = {Strong Sequentiality of Left-Linear Overlapping Rewrite Systems},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Paris-Sud}
}

@InProceedings{Jourdan-Maraninchi-1994,
  author = {M. Jourdan and F. Maraninchi},
  title = {Studying Synchronous Communication Mechanism by Abstractions},
  crossref = {PROCOMET94},
  pages = {184--200},
  keywords = {Programming Techniques; Concurrent Programming;
		  Programming Languages, Formal Definitions and
		  Theory; Language Constructs and Features; Process
		  Algebra}
}

@InProceedings{Jourdan-Parigot-1990,
  author = {Matin Jourdan and Didier Parigot},
  title = {Techniques for Improving Grammar Flow Analysis},
  crossref = {ESOP1990},
  pages = {240--255},
  note = {extended abstract},
  authorsAddress = {INRIA Rocquencourt},
  abstract = {Grammar Flow Analysis (GFA) is a computation
		  framework that can be applied to a large number of
		  problems expressed on context-free grammars. In this
		  framework, as was done on programs with Data Flow
		  Analysis, those problems are split into a general
		  resolution procedure and a set of specific
		  propagation functions. This paper presents a number
		  of improvement techniques that act on the resolution
		  procedure, and hence apply to every GFA problem:
		  grammar partitioning, non-terminals static ordering,
		  weak stability and semantic stability. Practical
		  experiments using circularity tests for attribute
		  grammars will show the benefit of these
		  improvements. This paper is a shortened version of [JoP90].}
}

@Article{Joyal-Street-Verity-1996,
  author = {Andr{\'e} Joyal and Ross Street and Dominic Verity},
  title = {Traced Monoidal Categories},
  journal = {Mathematical Proceedings of the Cambridge
                 Philosophical Society},
  volume = 119,
  number = 3,
  month = APR,
  year = 1996,
  pages = {447--468},
  DOI = {10.1017/S0305004100074338},
  DirectURL = {http://journals.cambridge.org/article_S0305004100074338},
  abstract = {Traced monoidal categories are introduced, a structure theorem is proved for them, and an example is provided where the structure theorem has application.}
}

@Book{Jung-1993,
  author = {Udo O. H. Jung},
  title = {An International Bibliography of Computer Assisted
		  Language Learning with Annotations in {German}},
  publisher = {Lang},
  year = 1993,
  volume = 1,
  address = {Frankfurt am Main}
}

@Misc{Jung-Fiore-Moggi-OHearn-Riecke-Rosolini-Stark-1996,
  author = {Achim Jung and Marcelo Fiore and Eugenio Moggi and Peter {O'Hearn} and Jon Riecke and Giuseppe Rosolini and Ian Stark},
  title = {Domains and Denotational Semantics: History, Accomplishments
      and Open Problems},
  year = 1996,
  WKloc = {A-0632}
}

@Book{Jung-Lieber-1993,
  author = {Udo O. H. Jung and Gothild Lieber},
  title = {An International Bibliography of Computer Assisted
		  Language Learning with Annotations in {German}},
  publisher = {Lang},
  year = 1993,
  volume = 2,
  address = {Frankfurt am Main},
  ISBN = {3-631-46376-6}
}

@InProceedings{KIV-1993,
  author = {Rainer Drexler and Wolfgang Reif and Gerhard Schellhorn and Kurt
           Stenzel and Werner Stephan and Andreas Wolpers},
  title = {The {KIV} System, A Tool for Formal Program Development},
  pages = {704--705},
  crossref = {STACS1993},
  WKloc = {A-0159},
  abstract = {In order to keep the tasks of specification, programming and
             verification in manegeable orders of magnitude, a system for
             formal development should support the structuring of the
             development process.

             In the case of the KIV system, the
             paradigm of Tactical Theorem Proving has been enhanced in
             several directions. First and most prominent, proofs in the KIV
             system are realized as {\em data structues}. The main advantage
             of this representation of proofs is that proofs can be stored,
             retrieved, inspected by the user, and inspected by the system
             itself. This last option makes a {\em re-use} of proofs
             possible, for example an old proof can be used as guideline
             when minor changes have been made to a program and its
             specification and a new correctness proof is required.}
}

@Manual{KIV-course,
  title = {A Practical Course on {KIV}},
  author = {M. Balser and W. Reif and G. Schellhorn and K. Stenzel and Andreas Thums},
  year = {2000?},
  WKloc = {B-0064}
}

@TechReport{Kadhim-Waite-1995,
  author = {Basim M. Kadhim and William M. Waite},
  title = {Maptool --- {Mapping} Between Concrete and Abstract Syntaxes},
  year = 1995,
  month = FEB,
  institution = {University of Colorado at Boulder, Department of Computer Science},
  number = {CU-CS-765-95},
  OPTaddress = {},
  WKloc = {A-0563}
}

@Misc{Kaehr-Maler-199X,
  author = {R. Kaehr and Th. Maler},
  title = {Introducing and Modeling Polycontextural Logics},
  year = {199?},
  WKloc = {A-0417}
}

@InProceedings{Kaes-1988,
  author =       {Stefan Kaes},
  title =        {Parametric Overloading in Polymorphic Programming Languages},
  crossref =  {ESOP1988},
  pages =     {131--141},
  bibliographies = {FP},
  annote =    {early reference for Haskell type classes}
}

@Manual{Kahl-1991,
  author = {Wolfram Kahl},
  title = {{HOPS --- Higher Object Programming System},
		  Functional Graphics-Based Interactive Programming,
		  User Manual},
  organization = {Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen},
  year = 1991,
  month = JAN,
  note = {(internal manual)},
  bibliographies = {WK}
}

@InProceedings{Kahl-1992,
  title = {Substitution und {Identifizierung} in {DAG}-{Sprachen}},
  pages = {166-186},
  crossref = {Steinfurt92},
  author = {Wolfram Kahl},
  bibliographies = {WK}
}

@InProceedings{Kahl-1992-x,
  year = 1992,
  title = {{Substitution und Identifizierung in DAG-Sprachen}},
  series = {Bericht 7/92-I},
  publisher = {Institut f\"ur Angewandte Mathematik und Informatik,
		  Universit\"at M\"unster},
  pages = {166--186},
  month = JAN,
  editor = {Wolfram-M. Lippe and Gudrun Stroot},
  booktitle = {{Proc. Workshop ``Programmiersprachen --- Methoden,
		  Semantik, Implementierungen''}},
  author = {Wolfram Kahl},
  address = {Landhaus Rothenberge},
  refsfrom = { <(ifip94:7)> }
}

@InProceedings{Kahl-1994a-x,
  author = {Wolfram Kahl},
  title = {{The {\bf H}igher-{\bf O}bject {\bf P}rogramming
		  {\bf S}ystem ``HOPS''}},
  year = 1994,
  series = {Bericht Nr. 9402},
  publisher = {AG Informatik/FB Mathematik,
		  Justus-Liebig-Universit\"at Gie\3en},
  pages = {1--10},
  month = MAR,
  editor = {Gregor Snelting and Uwe Meyer},
  booktitle = {{Semantikgest\"utzte Analyse,
		  Entwicklung und Generierung von Programmen, Workshop
		  der GI-Fachgruppe 2.1.3, Schlo\3 Rauischholzhausen}},
  filename = {wk/marburg.tex},
  bibliographies = {WK}
}

@InProceedings{Kahl-1994b,
  author = {Wolfram Kahl},
  title = {Can Functional Programming Be Liberated from the
		  Applicative Style?},
  crossref = {IFIP1994A},
  pages = {330--335},
  WKloc = {~/doc/wk/ifip94},
  abstract = {Modern functional languages all support higher-order
		  functions. Nevertheless actual applications are
		  mostly written in an applicative style. We show how
		  working with a language based on slightly shifted
		  concepts and embedded into a powerful environment
		  can bring about change.},
  acmcodes = {D.1.1; D.2.6; D.3.3},
  ACMcats = {Programming Techniques, Applicative/Functional Programming;
		  Software Engineering, Programming Environments;
		  Programming Languages, Language Constructs and Features},
  bibliographies = {WK}
}

@InProceedings{Kahl-1994b-x,
  author = {Wolfram Kahl},
  title = {Can Functional Programming Be Liberated from the
		  Applicative Style?},
  pages = {330--335},
  WKloc = {~/doc/wk/ifip94},
  abstract = {Modern functional languages all support higher-order
		  functions. Nevertheless actual applications are
		  mostly written in an applicative style. We show how
		  working with a language based on slightly shifted
		  concepts and embedded into a powerful environment
		  can bring about change.},
  acmcodes = {D.1.1; D.2.6; D.3.3},
  ACMcats = {Programming Techniques, Applicative/Functional Programming;
		  Software Engineering, Programming Environments;
		  Programming Languages, Language Constructs and Features},
  booktitle = {{Technology and Foundations, Information Processing
		  '94, Proceedings of the IFIP 13th World Computer
		  Congress, Hamburg, Germany, 28 August -- 2
		  September 1994, Volume I}},
  year = 1994,
  editor = {Bj\o{}rn Pehrson and Imre Simon},
  series = {IFIP Transactions},
  volume = {A-51},
  publisher = {North-Holland},
  organization = {IFIP},
  bibliographies = {RelMiCS}
}

@InProceedings{Kahl-1994b-x1,
  author = {Wolfram Kahl},
  title = {Can Functional Programming Be Liberated from the
		  Applicative Style?},
  pages = {330--335},
  WKloc = {~/doc/wk/ifip94},
  booktitle = {{Proc.\null{} IFIP 13th World Computer Congress, Vol.~I}},
  year = 1994,
  editor = {B. Pehrson and I. Simon},
  publisher = {North-Holland}
}

@InProceedings{Kahl-1994c,
  author = {Wolfram Kahl},
  title = {Truly Functional Program Construction by Graphical
		  Interaction},
  crossref = {Honnef94},
  pages = {68--71},
  filename = {wk/honnef-short.shopsfw},
  abstract = {In this talk we presented a case study of graphical
		  interactive program construction employing the
		  Higher Object Programming System ``HOPS''.

                  Programming in HOPS lends itself more naturally to a
		  functional style than programming in conventional
		  functional languages like Haskell
		  \cite{Haskell-Report-1.2} for reasons we explain in
		  section 2. In section 3 we then present the stepwise
		  development of a simple HOPS programm engaging in
		  input and output using the IO-monad.},
  bibliographies = {WK}
}

@InProceedings{Kahl-1994c-x,
  author = {Wolfram Kahl},
  title = {Truly Functional Program Construction by Graphical
		  Interaction},
  year = 1994,
  series = {Bericht Nr. 9412},
  publisher = {Universit\"at Kiel},
  pages = {68--71},
  editor = {F. Simon},
  booktitle = {{Alternative Konzepte f\"ur Sprachen und Rechner,
		  Bad Honnef 1994}},
  conference = {{Semantikgest\"utzte Analyse,
		  Entwicklung und Generierung von Programmen, Workshop
		  der GI-Fachgruppen 2.1.4, 0.1.7. 1.1.1, Bad Honnef,
		  9.--11.~Mai 1994}},
  filename = {wk/honnef-short.shopsfw},
  abstract = {In this talk we presented a case study of graphical
		  interactive program construction employing the
		  Higher Object Programming System ``HOPS''.

                  Programming in HOPS lends itself more naturally to a
		  functional style than programming in conventional
		  functional languages like Haskell
		  \cite{Haskell-Report-1.2} for reasons we explain in
		  section 2. In section 3 we then present the stepwise
		  development of a simple HOPS programm engaging in
		  input and output using the IO-monad.}
}

@InProceedings{Kahl-1994d-x,
  author = {Wolfram Kahl},
  title = {Algebraic Term Graph Rewriting with Bound Variables},
  booktitle = {{Participant's Proceedings of the
		  Fifth International Workshop on Graph Grammars
		  and Applications to Computer Science, Williamsburg,
		  Virginia, 13--18 November, 1994}},
  year = 1995,
  OPTnote = {},
  WKloc = {~doc/wk/gg94},
  bibliographies = {WK}
}

@TechReport{Kahl-1995a,
  author = {Wolfram Kahl},
  title = {{Kategorien von Termgraphen mit gebundenen Variablen}},
  institution = {{Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen}},
  year = 1995,
  month = SEP,
  type = {{Tech\-ni\-scher Be\-richt}},
  number = 9503,
  pages = 191,
  abstract = {With the aim of extending algebraic term graph rewriting to
      the expressiveness of Combinatory Reduction Systems, we first
      introduce a novel definition of term graphs with primitive notions of
      variable binding and variable identity, and with metavariables with
      successors. \hbox{After} discussing identification and sharing in
      these graphs, we introduce intervals and segments to serve as images
      of metavariables, including those with successors. Building on this
      we are able to establish a hierarchy of structure-preserving mappings
      between our term graphs, including at its top a concept of
      homomorphism avoiding ``capture of variables'' and catering for
      multiple instances of metavariables.

      The individual categories this gives rise to have different uses,
      namely in term graph rewriting, and the appendix provides an overview
      over a novel approach to algebraic term graph rewriting (fully
      presented in \cite{Kahl-1996}) together with the two crucial proofs
      concerning the viability of the constructions involved.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-1995b-x,
  author = {Wolfram Kahl},
  title = {Aspects of Typed Term Graphs},
  year = 1995,
  month = DEC,
  series = {Bericht MIP-9519},
  publisher = {Universit\"at Passau, Fakult\"at f\"ur Mathematik
		  und Informatik},
  pages = {104--109},
  editor = {Tiziana Margaria},
  booktitle = {{Kolloquium Programmiersprachen und Grundlagen der
		  Programmierung, Adalbert Stifter Haus, Alt
		  Reichenau, 11.--13. Oktober 1995}},
  filename = {wk/TYPED/typed-abs.lt},
  URL = {http://inf2-www.informatik.unibw-muenchen.de/kahl/papers/typed.ps.gz},
  note = {URL: {\sf http://inf2-www.informatik.unibw-muenchen.de/kahl/papers/typed.ps.gz}},
  abstract = {We apply the principles of second-order term graphs to
		  term graphs that are typed with second-order term
		  graphs again. Using this framework, recent advances
		  of ``{\bf polytypic programming}'' like
		  catamorphisms or generalised element relations can
		  be treated within a strong typing context and are
		  accessible to useful transformations.},
  bibliographies = {WK}
}

@Book{Kahl-1996,
  author = {Wolfram Kahl},
  title = {{Algebraische Termgraphersetzung mit gebundenen
		  Variablen}},
  publisher = {Herbert Utz Verlag Wissenschaft},
  address = {M\"unchen},
  series = {Reihe Informatik},
  year = 1996,
  note = {ISBN 3-931327-60-4;
     also Doctoral Diss.\null{} at Univ.\null{} der Bundeswehr M\"unchen,
     Fakult\"at f\"ur Informatik},
  abstract = {This thesis presents a first algebraic approach to term
      graph rewriting encompassing the treatment of bound variables.

      Building on a novel definition of term graphs with primitive notions
      of variable binding and variable identity, we present a concept of
      homomorphism avoiding ``capture of variables'' and catering for
      multiple instances of metavariables. Rewriting of these term graphs
      within the algebraic approach requires a new extension that is
      interesting in itself, the {\em fibered approach} to rewriting. As
      one result we obtain the first algebraic characterisation of graph
      reduction.

      Summing up, we have extended the algebraic approach to term graph
      rewriting, that so far only covered conventional term rewriting
      systems, to the expressive power of combinatory reduction systems and
      even slightly more general second-order rewriting systems. Thus we
      lay a theoretical foundation for implementations of functional
      programming languages, program transformation systems and other
      symbolic computation systems.},
  bibliographies = {RelMiCS, WK}
}

@Book{Kahl-1996-s,
  author = {Wolfram Kahl},
  title = {{Algebraische Termgraphersetzung mit gebundenen
		  Variablen}},
  publisher = {Herbert Utz Verlag Wissenschaft},
  address = {M\"unchen},
  OPTseries = {Reihe Informatik},
  year = 1996,
  note = {ISBN 3-931327-60-4},
  annote = {also Doctoral Diss.\null{} at Univ.\null{} der Bundeswehr M\"unchen,
     Fakult\"at f\"ur Informatik},
  abstract = {This thesis presents a first algebraic approach to term
      graph rewriting encompassing the treatment of bound variables.

      Building on a novel definition of term graphs with primitive notions
      of variable binding and variable identity, we present a concept of
      homomorphism avoiding ``capture of variables'' and catering for
      multiple instances of metavariables. Rewriting of these term graphs
      within the algebraic approach requires a new extension that is
      interesting in itself, the {\em fibered approach} to rewriting. As
      one result we obtain the first algebraic characterisation of graph
      reduction.

      Summing up, we have extended the algebraic approach to term graph
      rewriting, that so far only covered conventional term rewriting
      systems, to the expressive power of combinatory reduction systems and
      even slightly more general second-order rewriting systems. Thus we
      lay a theoretical foundation for implementations of functional
      programming languages, program transformation systems and other
      symbolic computation systems.},
  bibliographies = {RelMiCS}
}

@InProceedings{Kahl-1996a,
  author = {Wolfram Kahl},
  title = {Algebraic Graph Derivations for Graphical Calculi},
  pages = {224--238},
  crossref = {WG1996},
  URL = {http://ist.unibw-muenchen.de/Publications/Conf/Kahl-1996a.html},
  filename = {wk/CAT/rdproofs-WG96.ps},
  bibliographies = {RelMiCS, GraphCalc, WK},
  abstract = {Relational formalisations can be very concise and precise
      and can allow short, calculational proofs under certain
      circumstances. [\ldots] In situations corresponding to the
      simultaneous use of many variables in predicate logic, however,
      either a style using predicate logic with point variables has to be
      adopted or impractical and clumsy manipulations of tuples have to be
      employed inside relation calculus. In the application of relational
      formalisation to term graphs with bound variables [\ldots] we have
      been forced to employ both methods extensively, and, independently of
      other approaches, have been driven to develop a {\em graphical
      calculus} for making complex relation algebraic proofs more
      accessible.

      It turns out that, although our approach shares many common points
      with those presented in the literature [\ldots], it still is more
      general and more flexible than those approaches since we draw heavily
      on additional background in algebraic graph rewriting}
}

@TechReport{Kahl-1997b,
  author = {Wolfram Kahl},
  title = {A Fibred Approach to Rewriting ---
                  How the Duality between Adding and Deleting
                  Cooperates with the Difference between Matching and Rewriting},
  institution = {Fakult\"at f\"ur Informatik,
                  Universit\"at der Bundeswehr M\"unchen},
  URL = {http://www.cas.mcmaster.ca/~kahl/Publications/TR/Kahl-1997b.html},
  OPTnote = {\url{http://www.cas.mcmaster.ca/~kahl/Publications/TR/Kahl-1997b.html}},
  year = 1997,
  number = 9702,
  month = MAY,
  abstract = {We present a new approach to rewriting obtained by
		  enhancing and unifying existing variants inside the
		  algebraic (or better categorical) approach to
		  (graph) rewriting. Our approach is motivated by
		  second-order term graph rewriting and stresses on
		  one hand the two-step nature of rule application
		  consisting of deleting and adding items and on the
		  other hand the heterogeneous nature of the rewriting
		  setup where rule steps should be clearly
		  distinguished from matching of rule sides into redexes.

		  Complementing the existing opfibration approach with
		  a dual fibration step turns out to yield a natural
		  and flexible approach with useful new
		  applications. The resulting fibred approach takes
		  advantage of the heterogeneous setting and
		  appropriately reflects the duality between deleting
		  and adding in the course of rewriting, in contrast
		  with the double-pushout approach which simplifies
		  this duality into a symmetry. An important
		  contribution is the universal characterisation of
		  the host object, which has to be found as a
		  pushout-complement in the double pushout
		  approach.

		  The fibred approach is presented in
		  abstract and independent from any concrete
		  application categories in the manner of High-Level
		  Replacement Systems. Our original motivation for the
		  development of the fibred approach comes from term
		  graphs with bound variables where all other
		  approaches failed; in this paper we present an
		  unusual view on term rewriting as running example.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-1997c,
  author = {Wolfram Kahl},
  title = {Relational Matching for Graphical Calculi of Relations},
  crossref = {RelMiCS1997-PP},
  pages = {91--100},
  note = {superseded by \cite{Kahl-1999d}},
  bibliographies = {WK}
}

@TechReport{Kahl-1997d,
  author = {Wolfram Kahl},
  title = {A Framework for User-friendly Declarative Typing Systems},
  institution = {Fakult\"at f\"ur Informatik,
       Universit\"at der Bundeswehr M\"unchen},
  year = 1997,
  number = 9704,
  address = {D-85577 Neubiberg},
  month = DEC,
  abstract = {Advanced type systems frequently pose problems to the
		  user, as can already be observed with the still
		  rather simple type systems currently in use
		  e.g.\null{} in Haskell or ML: error messages
		  signaling type errors frequently give complex types
		  and little clue where the real source of the error
		  is.

		  We present a class of typing systems that are
		  designed for online use, preventing input of
		  untypeable constructs and guiding the user through
		  the construction of programs with complex
		  types. Since this implies making explicit the
		  relation between programs and their types, the
		  choice formalism for this purpose is that of term
		  graphs, where much more program structure can
		  usefully be made explicit than in linear notations.

		  After presenting a framework for the definition of
		  typing systems in term graphs, we discuss the
		  advantages of this approach and sketch applications
		  to e.g.\null{} polytypic programming.},
  pages = {15},
  note = {superseded by \cite{Kahl-1998d}},
  bibliographies = {WK}
}

@Article{Kahl-1998a,
  author = {Wolfram Kahl},
  title = {Relational Treatment of Term Graphs With Bound Variables},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 2,
  month = MAR,
  pages = {259--303},
  URL = {http://www.oup.co.uk/igpl/Volume_06/Issue_02/#Kahl},
  bibliographies = {RelMiCS, RelMiCS2, WK},
  abstract = {We show how and why it makes sense
              to use a relational formalisation
              instead of the usual functional one
              in the treatment of term graphs.
              Special attention is paid to term graphs with bound variables,
              that have, to our knowledge,
              never been formalised with such a generality before.

              Besides the novel treatment of term graphs themselves,
              we present an innovative relational homomorphism concept
              that for the first time allows to consider terms,
              resp.\null{} term trees
              as a special case of term graphs
              and still have the full power
              of (second-order) substitution available.}
}

@Manual{Kahl-1998b_hopsmanual,
  author = {Wolfram Kahl},
  title = {The {\bf H}igher {\bf O}bject {\bf P}rogramming {\bf S}ystem
      --- User Manual for \HOPS{}},
  organization = {Fak. f\"ur Informatik, Univ. der
		  Bundeswehr M\"unchen},
  year = 1998,
  note = {electronically available via: \url{http://ist.unibw-muenchen.de/kahl/HOPS/}},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-1998c,
  author = {Wolfram Kahl},
  title = {Demonic Diagrams --- Towards a Graphical Refinement Calculus},
  crossref = {RelMiCS1998-PP},
  pages = {117--121},
  bibliographies = {WK}
}

@InProceedings{Kahl-1998d-s,
  author = {Wolfram Kahl},
  title = {Internally Typed Second-Order Term Graphs},
  year = 1998,
  series = LNCS,
  publisher = Springer,
  volume = 1517,
  pages = {149--163},
  editor = {J. Hromkovi{\v{c}} and O. S{\'y}kora},
  booktitle = {Graph Theoretic Concepts in Computer Science, {WG '98}},
  filename = {wk/HOPS/hopsmanual/ttgv_article.lt},
  OLDURL = {http://ist.unibw-muenchen.de/Publications/Conf/Kahl-1998d.html},
  abstract = {We present a typing concept for second-order term graphs
              that does not consider the types as an external add-on,
              but as an integral part of the term graph structure.

              This allows a homogeneous treatment of term-graph representations
              of many kinds of typing systems, including second-order
              $\lambda$-calculi and systems of dependent types.

              Applications can be found in interactive systems and as
              typed intermediate representation for example in compilers.}
}

@InProceedings{Kahl-1998d-x,
  author = {Wolfram Kahl},
  title = {Internally Typed Second-Order Term Graphs},
  year = 1998,
  series = LNCS,
  publisher = Springer,
  volume = 1517,
  pages = {149--163},
  editor = {Juraj Hromkovi{\v{c}} and Ondrej S{\'y}kora},
  booktitle = {Graph Theoretic Concepts in Computer Science,
		  {24th International Workshop, WG '98, Smolenice Castle,
		  Slovak Republic, June 1998, Proceedings}},
  filename = {wk/HOPS/hopsmanual/ttgv_article.lt},
  URL = {http://ist.unibw-muenchen.de/Publications/Conf/Kahl-1998d.html},
  abstract = {We present a typing concept for second-order term graphs
              that does not consider the types as an external add-on,
              but as an integral part of the term graph structure.
              This allows a homogeneous treatment of term-graph representations
              of many kinds of typing systems, including second-order
              $\lambda$-calculi and systems of dependent types.
              Applications can be found in interactive systems and as
              typed intermediate representation for example in compilers.},
  bibliographies = {RelMiCS, FP, WK}
}

@InProceedings{Kahl-1998e,
  author = {Wolfram Kahl},
  title = {Total-Single-Pushout Derivations for Internal Graph Attribution},
  booktitle = {{TAGT' 98} --- 6th International Workshop
               on Theory and Application of Graph Transformation.
               {Paderborn, November 16--20, 1998}},
  pages = {366--373},
  year = 1998,
  editor = {G. Engels and G. Rozenberg},
  publisher = {Universit\"at-Gesamthochschule Paderborn,
               Fachbereich Mathematik/Informatik},
  note = {Bericht tr-ri-98-201},
  URL = {http://www.cas.mcmaster.ca/~kahl/Publications/Conf/Kahl-1998e.html},
  abstract = {We present a simple, abstract approach that allows
              to integrate graph attribution into the graph structure itself.

              We use a declarative setting where the calculation of the
              attributions relies on single-pushout derivations in a
              category of {\em total} homomorphisms.

              We discuss applications to term graph typing,
              including multi-level typing and type classes,
              and visualisation of compilation.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-1999a,
  author = {Wolfram Kahl},
  title = {Beyond Pretty-Printing:
                  Galley Concepts in Document Formatting Combinators},
  year = 1999,
  series = LNCS,
  publisher = Springer,
  volume = 1551,
  pages = {76--90},
  editor = {Gopal Gupta},
  booktitle = {Practical Aspects of Declarative Languages,
                   First International Workshop, {PADL '99, San Antonio,
                   Texas, USA, January 1999, Proceedings}},
  abstract = {Galleys have been introduced by Jeff Kingston
              as one of the key concepts
              underlying his advanced document formatting system Lout.
              Although Lout is built on a lazy functional programming language,
              galley concepts are implemented as part of that language
              and defined only informally.

              In this paper we present a first formalisation
              of document formatting combinators using galley concepts
              in the purely functional programming language Haskell.},
  bibliographies = {RelMiCS, FP, EdComb, WK}
}

@Article{Kahl-1999b,
  author = {Wolfram Kahl},
  title = {Explicit Graphs and Computer Aided Notation},
  journal = {Semiotica},
  year = 1999,
  volume = 125,
  number = {1/3},
  pages = {143--154},
  abstract = {In many cases, linear notation systems can be seen to encode
              underlying, \emph{implicit} graphs.
              This paper focusses on the way that
              making these graphs \emph{explicit}
              is useful for human understanding,
              and on the use of computers
              to make handling of notations based on explicit graphs
              feasible, efficient and productive.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-1999c,
  author = {Wolfram Kahl},
  title = {The Term Graph Programming System {HOPS}},
  pages = {136--149},
  crossref = {Berghammer-Lakhnech-1999},
  WKURL = {http://www.cas.mcmaster.ca/~kahl/Publications/Conf/Kahl-1999c.html},
  abstract = {[\ldots]
    The \bf{H}igher \bf{O}bject \bf{P}rogramming \bf{S}ystem {HOPS},
    which has been developed by a group led by Gunther Schmidt
    since the mid-eighties [\ldots]
    is a graphically interactive term graph programming system
    designed for transformational program development.

    In {HOPS}, only syntactically correct and well-typed programs
    can be constructed.
    The choice of the language is only constrained by certain restrictions
    of the term graph formalism and of the typing system.

    [\ldots]
    The design of this system relies on recent advances
    in the theories of untyped and typed second-order term graphs.},
  bibliographies = {RelMiCS, FP, WK}
}

@InProceedings{Kahl-1999c-s,
  author = {Wolfram Kahl},
  title = {The Term Graph Programming System {HOPS}},
  pages = {136--149},
  booktitle = {Tool Support for System Specification, Development and Verification},
  year = 1999,
  editor = {R. Berghammer and Y. Lakhnech},
  publisher = {Springer-Verlag},
  address = {Wien},
  note = {ISBN: 3-211-83282-3},
  URL = {http://ist.unibw-muenchen.de/Publications/Conf/Kahl-1999c.html},
  abstract = {[\ldots]
    The \bf{H}igher \bf{O}bject \bf{P}rogramming \bf{S}ystem {HOPS},
    which has been developed by a group led by Gunther Schmidt
    since the mid-eighties [\ldots]
    is a graphically interactive term graph programming system
    designed for transformational program development.

    In {HOPS}, only syntactically correct and well-typed programs
    can be constructed.
    The choice of the language is only constrained by certain restrictions
    of the term graph formalism and of the typing system.

    [\ldots]
    The design of this system relies on recent advances
    in the theories of untyped and typed second-order term graphs.}
}

@Article{Kahl-1999d,
  author = {Wolfram Kahl},
  title = {Relational Matching for Graphical Calculi of Relations},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {253--273},
  URL = {http://www.elsevier.com/gej-ng//10/23/143/47/27/33/abstract.html},
  abstract = {In this paper we extend an earlier approach to
              graphical relation calculi towards relational matching,
              thus allowing proofs with fewer auxiliary steps and
              concentrating more on the essential proof ideas.

              For facilitating the formal argument we introduce
              hierarchical relational diagrams as an
              intermediate structure and employ more of the
              algebraic graph rewriting repertoire for defining
              relational rewriting of these hierarchical diagrams.},
  WKURL = {http://ist.unibw-muenchen.de/Publications/Journals/Kahl-1999d.html},
  keywords = {Graphical calculus; Relation algebra; Relational matching; Hierarchical relational diagrams; Graph transformation},
  bibliographies = {RelMiCS, RelMiCS3, GraphCalc, WK}
}

@Misc{Kahl-1999e,
  author = {Wolfram Kahl},
  title = {Unsharp Demonic Products},
  howpublished = {Talk to be presented at the Fifth International RelMiCS Workshop \unfinished},
  year = 2000
}

@Misc{Kahl-1999f,
  author = {Wolfram Kahl},
  title = {{HOPS --- The Higher Object Programming System}},
  crossref = {Engels-Heckel-1999},
  URL = {http://ist.unibw-muenchen.de/kahl/HOPS/},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-2000a,
  author = {Wolfram Kahl},
  title = {Unsharp Demonic Products and Stratified Term Graphs},
  crossref = {RelMiCS2000-PP},
  pages = {165--174},
  abstract = {Investigating the interplay between demonic operators
                  and direct products in abstract relation algebras
                  against the background of gs-monoidal categories,
                  we discover that the direct product gives rise to a
                  gs-monoidal category with demonic composition, while a
                  new concept of \emph{demonic product} gives rise to a
                  structure that fails to be gs-monoidal mostly through
                  the lack of functoriality of the demonic product.

                  However, this lack is interesting on its own account,
                  since it is an example of what has been studied as
                  \emph{unsharpness} in the context of direct products
                  in relation algebras, and there it can only occur
                  if not all products exist.

                  We show how an intuitive understanding of our demonic
                  products coincides with the intuition behind the
                  unsharpness research, and, generalising the approach
                  of using term graphs as syntax for gs-monoidal categories,
                  discuss the generalisation of \emph{stratified term graphs}
                  to be used for the unsharp variant.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-2000a-z,
  author = {Wolfram Kahl},
  title = {Unsharp Demonic Products},
  booktitle = {RelMiCS 5, Participant's Proceedings},
  editor = {Jules Desharnais},
  publisher = {Universit\'e Laval, D\'epartement d'informatique},
  pages = {165--174},
  year = 2000
}

@InProceedings{Kahl-2000b,
  author = {Wolfram Kahl},
  title = {Stratified Term Graphs},
  pages = {115--122},
  crossref = {GraTra2000},
  abstract = {We propose \emph{stratified term graphs} as enrichment
              of conventional term graph structures with
              \emph{synchronisation borders} that intuitively capture
              constraints such as that values along these borders
              should be available simultaneously in distributed
              implementations.

              This additional structure requires a weakening of the
              algebraic characterisation, and we propose
              \emph{coherent unsharp ps-semigroup categories} as
              generalisation of gs-monoidal categories. These capture
              exactly the essence of stratified term graphs with
              at least one root.},
  bibliographies = {RelMiCS, WK}
}

@Misc{Kahl-2001,
  author = {Wolfram Kahl},
  title = {A Relation-Algebraic Approach to Graph Structure Transformation},
  year = 2001,
  note = {Habil.\null{} Thesis, Fakult\"at f\"ur Informatik,
		   Univ.\null{} der Bundeswehr M\"unchen,
                   Techn.\null{} Report 2002-03,
        \url{http://relmics.mcmaster.ca/~kahl/Publications/RelRew/}},
  pages = {iv+204},
  OPTURL = {http://www.cas.mcmaster.ca/~kahl/Publications/RelRew/},
  bibliographies = {RelMiCS, WK}
}

@Article{Kahl-2001a,
  author = {Wolfram Kahl},
  title = {Parallel Composition and Decomposition of Specifications},
  journal = {Information Sciences},
  year = 2001,
  volume = 139,
  number = {3--4},
  pages = {197--220},
  month = DEC,
  URL = {http://www.elsevier.com/gej-ng/10/23/143/95/33/30/abstract.html},
  bibliographies = {RelMiCS, RelMiCS5, WK},
  abstract = {In the discipline of derivation of programs from specifications
     by refinement the \emph{demonic calculus of relations} is a
     well-established tool, which allows the use of
     \emph{demonic} operators on relations considered as
     specifications of a program's input-output-behaviour together with
     the standard relation algebraic operators to reason about
     specifications ordered by the \emph{refinement} ordering.

     The central contribution of this paper is
     the introduction of a \emph{demonic product} operator
     that embodies a natural concept of parallel composition of specifications.
     Of course, this raises the question of parallel decomposition,
     which in turn motivates the introduction of \emph{demonic implication}
     and \emph{totalisation} operators.
     Finally, we also present first results concerning parallel decomposition.}
}

@TechReport{Kahl-2001b,
  author = {Wolfram Kahl},
  title = {Parallel Composition and Decomposition of Specifications},
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 2001,
  number = {2001-01},
  month = FEB,
  URL = {http://ist.unibw-muenchen.de/Publications/TR/2001-01/},
  bibliographies = {RelMiCS}
}

@InProceedings{Kahl-2001c,
  author = {Wolfram Kahl},
  title = {Software Evolution via Hierarchical Hypergraphs with Flexible Coverage},
  pages = {1--5},
  crossref = {FFSE2001},
  abstract = {We present a simple, abstract approach
              to the use of hierarchical hypergraphs in software evolution.
              Borrowing ideas from graph transformation and attribute grammars,
              we show how these hypergraphs can be used in a flexible way
              to \emph{cover} all or part of a software development process.

              This unifying framework allows to design a \emph{set of tools}
              based on common data structures and representations
              and applicable to diverse tasks and settings.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-2001d,
  author = {Wolfram Kahl},
  title = {A Relation-Algebraic Approach to Graph Structure Transformation},
  crossref = {RelMiCS2001-PP},
  pages = {11--20},
  note = {(Invited Talk --- Extended Abstract)},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-2001e,
  author = {Wolfram Kahl},
  title = {A Relation-Algebraic Approach to Graph Structure Transformation},
  crossref = {RelMiCS2001},
  pages = {1--14},
  note = {(Invited Talk)},
  URL = {http://link.springer.de/link/service/series/0558/bibs/2561/25610001.htm},
  bibliographies = {RelMiCS, WK}
}

@Article{Kahl-2003,
  author = {Wolfram Kahl},
  title = {Refinement and Development of Programs from Relational Specifications},
  journal = ENTCS,
  year = 2003,
  volume = 44,
  number = 3,
  pages = {51--93},
  DOI = {10.1016/S1571-0661(04)80932-0},
  DOIURL = {http://dx.doi.org/10.1016/S1571-0661(04)80932-0},
  abstract = {These tutorial notes present an overview
                  of specification and refinement concepts and methods
                  that are based on relations.},
  bibliographies = {RelMiCS, WK, Coconut}
}

@TechReport{Kahl-2003a,
  author = 	 {Wolfram Kahl},
  title = 	 {Compositional Syntax and Semantics of Tables},
  institution =  {Software Quality Research Laboratory,
                  Department of Computing and Software,
                  McMaster University},
  year = 	 {2003},
  type = 	 {{SQRL Report}},
  number = 	 {15},
  month = 	 OCT,
  note = {available from:\\
          \textsf{http://sqrl.mcmaster.ca/sqrl\_reports.html}},
  bibliographies = {RelMiCS, WK},
  abstract = {Parnas together with a number of colleages
     established the systematic use of certain kinds of tables as a
     useful tool in software documentation and inspection with an
     accessible, multi-dimensional syntax and intuitive semantics.

     Previous approaches to formalisation of table semantics based
     their definitions on the multi-dimensional array structure of
     tables and thus achieved close correspondence with the intuitive
     understanding of tables.

     In this paper, we argue that a different view, supporting a
     compositional semantics, is more advantageous for tool support
     and for reasoning about tables. For this purpose, we also need a
     compositional table syntax, and we perform an analysis of table
     syntax that leads us to a particular compositional view of table
     structure.

     This simple, inductive view of the structure of tables allows us
     to provide highly flexible tools for defining the semantics of
     tabular expressions. The straight-forward compositional
     formalisation of table semantics on the one hand yields very
     general table transformation theorems and enables us to perform
     fully formal proofs for these theorems in a mechanised theorem
     prover, and on the other hand also may serve as basis for the
     implementation of semantics-aware table support tools.}
}

@TechReport{Kahl-2003b,
  author = 	 {Wolfram Kahl},
  title = 	 {Basic Pattern Matching Calculi:
                  Syntax, Reduction, Confluence, and Normalisation},
  institution =  {Software Quality Research Laboratory, McMaster Univ.},
  year = 	 {2003},
  type = 	 {{SQRL Report}},
  number = 	 {16},
  note = {available from
          \url{http://sqrl.mcmaster.ca/sqrl\_reports.html}},
  bibliographies = {RelMiCS, WK},
  month = 	 OCT,
  abstract = {The pattern matching calculus is a refinement of
      $\lambda$-calculus that integrates mechanisms appropriate for
      fine-grained modelling of non-strict pattern matching.

      In comparison with the functional rewriting strategy that is
      usually employed to define the operational semantics of
      pattern-matching in non-strict functional programming languages
      like Haskell or Clean, the pattern matching calculus allows
      simpler and more local definitions to achieve the same effects.

      The main device of the calculus is to further emphasise the
      clear distinction between matching failure and undefinedness
      already discussed in the literature by embedding into
      expressions the separate syntactic category of matchings. This
      separation is also important to properly restrain the possible
      effects of the non-monotonicity that a na\"{\i}ve treatment of
      matching alternatives would exhibit. The language arising from
      that distinction turns out to naturally encompass the pattern
      guards of Peyton Jones and Erwig and conventional Boolean guards
      as special cases of the intermediate stages of matching reduction.

      By allowing a confluent reduction system and a normalising
      strategy, the pattern matching calculus provides a new basis for
      operational semantics of non-strict programming languages and
      also for implementations.}
}

@InProceedings{Kahl-2003c,
  author = 	 {Wolfram Kahl},
  title = 	 {Calculational Relation-Algebraic Proofs in {Isabelle/Isar}},
  crossref =  {RelMiCS2003},
  pages = 	 {178--190},
  bibliographies = {RelMiCS, WK},
  abstract = {We propose a collection of theories in the proof assistant
    Isabelle/Isar that support calculational reasoning in and about
    heterogeneous relational algebras and Kleene algebras.}
}

@InProceedings{Kahl-2003c-p,
  author = {Wolfram Kahl},
  title = {Calculational Relation-Algebraic Proofs in {Isabelle/Isar}},
  crossref = {RelMiCS2003-PP},
  pages = {214--221},
  bibliographies = {WK}
}

@InProceedings{Kahl-2004a,
  author = 	 {Wolfram Kahl},
  title = 	 {Basic Pattern Matching Calculi: A Fresh View on Matching Failure},
  crossref =  {FLOPS2004},
  DOI = {10.1007/978-3-540-24754-8_20},
  SpringerURL = {http://www.springerlink.com/content/3jet4qgw1q2nu0a8/},
  pages = 	 {276--290},
  abstract = {We propose pattern matching calculi as a refinement
      of $\lambda$-calculus that integrates mechanisms appropriate for
      fine-grained modelling of non-strict pattern matching.

      Compared with the functional rewriting strategy
      usually employed to define the operational semantics of
      pattern matching in non-strict functional programming languages
      like Haskell or Clean, our pattern matching calculi
      achieve the same effects using simpler and more local rules.

      The main device is to embed into expressions
      the separate syntactic category of matchings;
      the resulting language naturally encompasses
      pattern guards and Boolean guards as special cases.

      By allowing a confluent reduction system and a normalising strategy,
      these pattern matching calculi provide a new basis for
      operational semantics of non-strict programming languages
      and also for implementations.},
  bibliographies = {WK, RelMiCS, FP}
}

@Article{Kahl-2004b,
  author = 	 {Wolfram Kahl},
  title = 	 {Refactoring Heterogeneous Relation Algebras around Ordered Categories and Converse},
  journal = 	 JoRMiCS,
  OPTjournal = 	 {J.\null{} Rel.\null{} Methods in Comp.\null{} Sci.},
  URL = {http://www.jormics.org/},
  year = 	 2004,
  volume = 	 1,
  pages =	 {277--313},
  bibliographies = {WK, RelMiCS},
  abstract = {We present a reorganisation of popular theories of
      ``reasoning with relational flavour'',
      including allegories, Kleene algebras, and Dedekind categories,
      into a relatively symmetric picture using ordered categories
      as common base and defining converse independently from joins and meets.

      As an example application, we use this
      to regroup results about formalisation of
      algebraic graph rewriting and thus exhibit opportunities for
      applying these approaches in new settings.

      Finally we discuss how this approach influences
      the design of compatible approaches to
      formalisation and mechanisation of relation-algebraic theories.}
}

@Misc{Kahl-2004-HsDep,
  author =	 {Wolfram Kahl},
  title =	 {\textsf{HsDep}: Dependency Graph Generator for {Haskell}},
  howpublished = {Available at \url{http://www.cas.mcmaster.ca/\~kahl/Haskell/}},
  month =	 SEP,
  year =	 2004
}

@InProceedings{Kahl-2006a,
  author = 	 {Wolfram Kahl},
  title = 	 {Semigroupoid Interfaces for Programming with Relations in {Haskell}},
  crossref =  {RelMiCS2006},
  pages = 	 {235--250},
  bibliographies = {RelMiCS, WK},
  DOI = {10.1007/11828563_16},
  DOIURL = {http://dx.doi.org/10.1007/11828563_16},
  abstract = {We present a Haskell interface for
      manipulating finite binary relations as data
      in a point-free relation-algebraic programming style
      that integrates naturally with the current Haskell collection types.
      This approach enables seamless integration
      of relation-algebraic formulations
      to provide elegant solutions of problems that,
      with different data organisation,
      are awkward to tackle.

      Perhaps surprisingly,
      the mathematical foundations
      for dealing with \emph{finite} relations in such a context
      are not well-established,
      so we provide an appropriate generalisation of relational categories
      to semigroupoids
      to serve as specification for our interface.

      After having established an appropriate interface for
      relation-algebraic programming,
      we also need an efficient implementation;
      we find this in BDD-based kernel library KURE
      of recent versions of the Kiel \textsf{RelView} system.
      We show how this combination enables high-level declarative
      and efficient relational programming in Haskell.}
}

@InProceedings{Kahl-2006b,
  author = 	 {Wolfram Kahl},
  title = 	 {Dynamic Symbolic Optimisation for Relation-Algebraic Programming in {Haskell}},
  crossref =  {MACIS2006},
  pages = 	 {92--99},
  bibliographies = {RelMiCS, WK},
  abstract = {When using efficient BDD-based algorithms
      for relation-algebraic calculations,
      the efficiency of evaluating equivalent expressions
      with apparently similar structure may differ widely,
      and in unexpected ways,
      depending on the precise behaviour of the heuristics used in the
      underlying BDD-operations.

      After motivating the relation-algebraic programming style
      and highlighting these efficiency problems,
      we describe the set of strategies we are currently pursuing
      to arrive at a ``self-optimising'' implementation
      that frees the user of the relation-algebraic programming interface
      from having to consider efficiency concerns.}
}

@InProceedings{Kahl-2007_WFLP,
  author = 	 {Wolfram Kahl},
  title = 	 {Call-By-Undetermined-Value in the Pattern Matching Calculus --- Towards Equational Reasoning for Functional-Logic Programming},
  OPTcrossref =  {WFLP2007PP},
  booktitle = {{WFLP 2007, 16th International Workshop on Functional and (Constraint) Logic Programming}},
  editor = {Rachid Echahed},
  URL = {http://wflp2007.imag.fr/documents/wflp07-proceedings.pdf},
  month = JUN,
  year = 2007,
  pages = 	 {79--95},
  bibliographies = {RelMiCS, WK},
  abstract = {The Pattern Matching Calculus was originally shown
      to faithfully embed Haskell pattern matching
      into a confluent rewriting system.

      We show an extension that captures the
      ``lazy call-time choice'' of modern non-strict
      functional-logic programmign languages
      and thus opens up new perspectives on equational reasoning
      about functional-logic programs.}
}

@Article{Kahl-2008a,
  author = 	 {Wolfram Kahl},
  title = 	 {Relational Semigroupoids: Abstract Relation-Algebraic Interfaces for Finite Relations between Infinite Types},
  journal = 	 JLAP,
  year = 	 2008,
  volume = 	 76,
  number = {1},
  pages = 	 {60--89},
  DOI = {10.1016/jlap.2007.10.008},
  DOIURL = {http://dx.doi.org/10.1016/j.jlap.2007.10.008},
  bibliographies = {RelMiCS, WK},
  abstract = {Finite maps or finite relations between infinite sets
    do not even form a category, since the necessary identities are not finite.
     We show relation-algebraic extensions of semigroupoids
     where the operations that would produce infinite results
     have been replaced with variants that preserve finiteness,
     but still satisfy useful algebraic laws.
     The resulting theories allow calculational reasoning
     in the relation-algebraic style with only minor sacrifices;
     our emphasis on generality even provides some concepts in theories
     where they had not been available before.

     The semigroupoid theories presented in this paper
     also can directly guide library interface design
     and thus be used for principled relation-algebraic programming;
     an example implementation in Haskell
     allows manipulating finite binary relations as data
     in a point-free relation-algebraic programming style
     that integrates naturally with the current Haskell collection types.
     This approach enables seamless integration
     of relation-algebraic formulations
     to provide elegant solutions of problems that,
     with different data organisation, are awkward to tackle.}
}

@InProceedings{Kahl-2008b,
  author = 	 {Wolfram Kahl},
  title = {Determinisation of Relational Substitutions in Ordered Categories with Domain},
  crossref =  {RelMiCS2008},
  pages = 	 {243--258},
  bibliographies = {RelMiCS, RelMiCS10, WK},
  DOI = {10.1007/978-3-540-78913-0_19},
  abstract = {We present two different relational generalisations of
                  substitutions, show that they both produce locally
                  ordered categories with domain, and then develop the
                  single-morphism ``determiniser'' concept that relies
                  only on this framework, while still corresponding to
                  conventional two-morphism unification in both
                  examples. Central to this development is the
                  determinacy concept of ``characterisation by
                  domain'' introduced by Desharnais and M\"oller for
                  Kleene algebras with domain; this is here applied in
                  the weakest possible setting.}
}

@InProceedings{Kahl-2009_TFP_PP,
  author =       {Wolfram Kahl},
  title =        {Liberating {Haskell} Type Class Design
                  by Automatic Generation of Instance Modules
                  and Restricted-Type Wrappers},
  crossref =  {TFP2009PP},
  pages =     {312--325},
  WKsource =    {svn/WK/doc/wk/HASKELL/Restricted.lhs},
  abstract =    {Design of Haskell type class hierarchies for complex purposes,
                 including for standard containers, is a non-trivial exercise,
                 and evolution of such designs
                 is additionally hampered by the large overhead
                 of connecting to existing implementations.

                 We systematically discuss this overhead,
                 and propose a tool solution, implemented using the GHC API,
                 to automate its generation.}
}

@InProceedings{Kahl-2009_TFP,
  author =       {Wolfram Kahl},
  title =        {Haskell Module Tools for Liberating Type Class Design},
  crossref =  {TFP2009},
  pages =     {129--144},
  chapter = {9},
  WKsource =    {svn/WK/doc/wk/HASKELL/Restricted.lhs},
  abstract =    {Design of Haskell type class hierarchies for complex purposes,
                 including for standard containers, is a non-trivial exercise,
                 and evolution of such designs
                 is additionally hampered by the large overhead
                 of connecting to existing implementations.

                 We systematically discuss this overhead,
                 and propose a tool solution, implemented using the GHC API,
                 to automate its generation.}
}

@InProceedings{Kahl-2009a,
  author = 	 {Wolfram Kahl},
  title = {Collagories for Relational Adhesive Rewriting},
  crossref =  {RelMiCS2009},
  pages = 	 {211--226},
  bibliographies = {RelMiCS, WK},
  DOIURL = {http://dx.doi.org/10.1007/978-3-642-04639-1_15},
  abstract = {We define collagories essentially as
      ``distributive allegories without zero morphisms'',
      and show that they are sufficient
      for accommodating the relation-algebraic approach to graph transformation.
      Collagories closely correspond to the adhesive categories
      important for the categorical DPO approach to graph transformation.
      but thanks to their relation-algebraic flavour
      provide a more accessible and more flexible setting.}
}

@TechReport{Kahl-2009b,
  author = 	 {Wolfram Kahl},
  title = {Collagories for Relational Adhesive Rewriting},
  institution =  {Software Quality Research Laboratory,
                  Department of Computing and Software,
                  McMaster University},
  year = 	 {2009},
  type = 	 {{SQRL Report}},
  number = 	 {56},
  OPTmonth = 	 JUL,
  OPTpages = {24 pages},
  note = {24 pages. In: \url{http://sqrl.mcmaster.ca/sqrl_reports.html}. (Superseded by \cite{Kahl-2010_CN1})},
  abstract = {We define collagories essentially as
      ``distributive allegories without zero morphisms'',
      and show that they are sufficient for
      accommodating the relation-algebraic approach to graph transformation.
      Collagories closely correspond to the adhesive categories
      important for the categorical DPO approach to graph transformation.
      but thanks to their relation-algebraic flavour
      provide a more accessible and more flexible setting.},
  bibliographies = {RelMiCS, WK}
}

@TechReport{Kahl-2010_CN1,
  author = 	 {Wolfram Kahl},
  title =        {Collagory Notes, Version 1},
  institution =  {Software Quality Research Laboratory,
                  Department of Computing and Software,
                  McMaster University},
  year = 	 {2010},
  type = 	 {{SQRL Report}},
  number = 	 {57},
  OPTmonth = 	 MAR,
  OPTpages =        {53 pages},
  note = {53 pages. In: \url{http://sqrl.mcmaster.ca/sqrl_reports.html}.},
  OPTnote = {(This report supersedes \cite{Kahl-2009b})},
  OPTabstract = {},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-2010a,
  author =       {Wolfam Kahl},
  title =        {Co-tabulations, Bicolimits and {Van-Kampen} Squares in Collagories},
  crossref =      {GTVMT2010},
  DirectURL = {http://journal.ub.tu-berlin.de/index.php/eceasst/article/view/421},
  pages =     {14.1--14.15},
  bibliographies = {RelMiCS, WK},
  abstract = {We previously defined collagories essentially as
    ``distributive allegories without zero morphisms''.
    Collagories are sufficient
    for accommodating the relation-algebraic approach to graph
    transformation, and closely correspond to the adhesive categories
    important for the categorical DPO approach to graph transformation.

    Heindel and Soboci\'nski have recently characterised
    the Van-Kampen colimits used in adhesive categories
    as bicolimits in span categories.

    In this paper,
    we study both bicolimits and lax colimits in collagories.
    We show that the relation-algebraic co-tabulation concept
    is equivalent to lax colimits of difunctional morphisms
    and to bipushouts,
    but much more concise and accessible.
    From this, we also obtain an interesting characterisation
    of Van-Kampen squares in collagories.}
}

@Article{Kahl-2010b,
  author = 	 {Wolfram Kahl},
  title = 	 {Determinisation of Relational Substitutions in Ordered Categories with Domain},
  journal = 	 JLAP,
  year = 	 {2010},
  DOI = 	 {10.1016/j.jlap.2010.07.017},
  DOIURL = {http://dx.doi.org/10.1016/j.jlap.2010.07.017},
  volume = 	 {79},
  number = 	 {8},
  pages = 	 {812--829},
  bibliographies = {WK, RelMiCS},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Kahl-2010c,
  author = 	 {Wolfram Kahl},
  title = {Amalgamating Pushout and Pullback Graph Transformation in Collagories},
  crossref =  {ICGT2010},
  pages = 	 {362--378},
  bibliographies = {RelMiCS, WK},
  SpringerURL = {http://www.springerlink.com/content/0g4j702731686645},
  DOI = {10.1007/978-3-642-15928-2},
  abstract = {The relation-algebraic approach to graph transformation
      replaces the universal category-theoretic characterisations
      of pushout and pullbacks with the local characterisations
      of tabulations and co-tabu\-la\-tions.
      The theory of collagories is a weak axiomatisation
      of relation-algebraic operations
      that closely corresponds to adhesive categories.

      We show how to amalgamate double-pushout and double-pull\-back
      rewriting steps into a fused rewriting concept
      where rules can contain subgraph variables in a natural
      and flexible way, and rewriting can delete or duplicate
      the matched instances of such variables.}
}

@Article{Kahl-2011a,
  author = 	 {Wolfram Kahl},
  title = 	 {Collagories: Relation-Algebraic Reasoning for Gluing Constructions},
  journal = 	 JLAP,
  year = 	 {2011},
  DOI = 	 {10.1016/j.jlap.2011.04.006},
  DOIURL = {http://dx.doi.org/10.1016/j.jlap.2011.04.006},
  DirectURL = "http://www.sciencedirect.com/science/article/pii/S1567832611000178",
  volume = 	 {80},
  number = 	 {6},
  pages = 	 {297--338},
  bibliographies = {WK, RelMiCS},
  OPTnote = 	 {},
  abstract = 	 {The relation-algebraic approach to graph transformation has previously been formalised in the context of complete distributive allegories. Careful analysis reveals that the zero laws postulated for distributive allegories were never used, and that completeness was most importantly used for the difunctional closures necessary for a relation-algebraic characterisation of pushouts.

    We therefore define collagories essentially as ``distributive allegories without zero morphisms'', and also define a variant of Kleene star to produce difunctional closures where necessary.

    Typical collagories relevant for generalised graph structure transformation can be obtained from basic collagories like that of sets and relations via nestable constructions of collagories of semi-unary algebras, which allow natural representations in particular of graph structures, also with fixed label sets, or with type graphs.

    Since collagories are intended as foundation for generalised graph structure transformation in the algebraic tradition, we concentrate particularly on co-tabulations, the core of the relation-algebraic gluing concept. We clarify the precise relationship between co-tabulations and pushouts, and investigate the special case of direct sums, which is particularly affected by the absence of zero laws.

   Finally, we consider Van Kampen squares, the central ingredient of the definition of adhesive categories that has recently become popular as foundation for algebraic graph transformation, and obtain an interesting characterisation of Van Kampen squares in collagories.}
}

@InProceedings{Kahl-2011b,
  author = 	 {Wolfram Kahl},
  title = {Dependently-Typed Formalisation of Relation-Algebraic Abstractions},
  crossref =  {RelMiCS2011},
  pages = 	 {230--247},
  bibliographies = {RelMiCS, RelMiCS12, WK},
  DOI = {10.1007/978-3-642-21070-9_18},
  DOIURL = {http://dx.doi.org/10.1007/978-3-642-21070-9_18},
  SpringerURL = {http://www.springerlink.com/content/5lq21m6660j38412},
  abstract = {We present a formalisation in the dependently-typed programming language Agda2
of basic category and allegory theory,
and of generalised algebras where function symbols
are interpreted in a parameter category.
We use this nestable algebra construction
as the basis for nestable category and allegory constructions,
ultimately aiming at a formalised foundation of
the algebraic approach to graph transformation,
which uses constructions in categories of graph structures considered
as unary algebras.

The features of Agda permit
strongly-typed programming with these nested algebras
and with relational homomorphisms between them
in a natural mathematical style
and with remarkable ease,
far beyond what can be achieved even in Haskell.}
}

@InProceedings{Kahl-2011c,
  author = 	 {Wolfram Kahl},
  title = {The Teaching Tool
    {{\sf {C{\scriptsize{\sf ALC}}C{\scriptsize{\sf HECK}}}}}:
    A Proof-Checker for Gries and Schneider's
    ``Logical Approach to Discrete Math''},
  crossref =  {CPP2011},
  SpringerURL = {http://www.springerlink.com/content/k05455151065817m},
  DOI = {10.1007/978-3-642-25379-9_17},
  pages = 	 {216--230},
  bibliographies = {RelMiCS, WK},
  abstract = {Students following a first-year course
      based on Gries and Schneider's LADM textbook
      had frequently been asking:
      ``How can I know whether my solution is good?''

      We now report on the development of a proof-checker
      designed to answer exactly that question,
      while intentionally not helping to find the solutions in the first place.
      \CalcCheck{} provides detailed feedback to \LaTeX{}-formatted calculational
      proofs,
      and thus helps students to develop confidence in their
      own skills in ``rigorous mathematical writing''.

      Gries and Schneider's book emphasises rigorous development
      of mathematical results,
      while striking one particular compromise between full formality
      and customary, more informal, mathematical practises,
      and thus teaches aspects of both.
      This is one source of several unusual requirements
      for a mechanised proof-checker;
      other interesting aspects arise
      from details of their notational conventions.}
}

@InProceedings{Kahl-2011_AgdaTG,
  author =    {Wolfram Kahl},
  title =     {Dependently-Typed Formalisation of Typed Term Graphs},
  OPTcrossref =  {TermGraph2011},
  booktitle = {Proc. of 6th International Workshop on Computing with Terms and Graphs, {TERMGRAPH 2011}},
  pages =     {38--53},
  year =      2011,
  editor =    {Rachid Echahed},
  volume =    48,
  series =    {EPTCS},
  OPTnote =      {\url{http://eptcs.org/content.cgi?TERMGRAPH2011}}
}

@Misc{Kahl-2011_RATH-Agda-1.0,
  author =       {Wolfram Kahl},
  title =        {{Relation-Algebraic Theories in Agda} --- {RATH-Agda-1.0}},
  institution =  {Software Quality Research Laboratory,
                  Department of Computing and Software,
                  McMaster University},
  year = 	 {2011},
  month = 	 MAY,
  OPTpages = {173 pages},
  howpublished = {Mechanically checked Agda theories available for download,
with 173 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/}}
}

@InProceedings{Kahl-2012_AgdaGT1,
  author = 	 {Wolfram Kahl},
  title = {Towards Certifiable Implementation of Graph Transformation via Relation Categories},
  crossref =  {RelMiCS2012},
  pages = 	 {82--97},
  bibliographies = {RelMiCS, RelMiCS13, WK},
  DOI = {10.1007/978-3-642-33314-9_6},
  SpringerURL = {http://www.springerlink.com/content/d3w42g453606j84g},
  abstract = {The algebraic approach to graph transformation is a general framework
      for the definition of transformation mechanisms for complex structures
      that achieves its generality by using category-theoretic abstractions.

      We present a framework for modular implementations
      of categoric graph transformation mechanisms
      that uses abstractions of relation categories as internal interfaces.
      Doing this in a dependently-typed programming language
      enables us to manage implementations of functionality
      together with their correctness proofs in the same language,
      thus progressing towards fully verified graph transformation system implementations.}
}

@Article{Kahl-2014_Mouldable,
  author =    {Wolfram Kahl},
  title =     {Towards ``Mouldable Code'' via Nested Code Graph Transformation},
  journal =   JLAP,
  year =      {2014},
  volume =    {83},
  number =    {2},
  pages =     {225--234},
  DOI =      {10.1016/j.jlap.2014.02.010},
  DOIURL =   {http://dx.doi.org/10.1016/j.jlap.2014.02.010},
  AuthorURL = {http://authors.elsevier.com/sd/article/S1567832614000113},
  OPTnote =      {Festschrift for Gunther Schmidt},
  OPTannote = {Accepted manuscript (unedited version) available online: 12-FEB-2014
            Final version published online: 3-MAY-2014},
  abstract = {Program transformation is currently de facto restricted
    to abstract syntax tree rewriting.
    However, many program transformation patterns,
    in particular in the realm of high-performance code generation,
    can more naturally be understood and expressed as graph transformations.
    We describe the conceptual organisation of a system
    based on application of algebraic graph transformation rules
    to data-flow and control-flow graphs,
    and outline the work, both theoretical and of implementation nature,
    that still needs to be done to realise this long-term project.}
}

@Misc{Kahl-2014_RATH-Agda-2.0.0,
  author =       {Wolfram Kahl},
  title =        {{Relation-Algebraic Theories in Agda} --- {RATH-Agda-2.0.0}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2014},
  month = 	 JAN,
  OPTpages = {456 pages},
  howpublished = {Mechanically checked Agda theories available for download,
with 456 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  note = {With contributions by Yuhang Zhao.}
}

@Misc{Kahl-2014_RATH-Agda-2.0.1,
  author =       {Wolfram Kahl},
  title =        {{Relation-Algebraic Theories in Agda} --- {RATH-Agda-2.0.1}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2014},
  month = 	 FEB,
  OPTpages = {456 pages},
  OPThowpublished = {Mechanically checked Agda theories available for download,
with 456 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  howpublished = {Mechanically checked Agda theories,
with 456 pages literate document output. \href{http://relmics.mcmaster.ca/RATH-Agda/}{\textsf{http://relmics.mcmaster.ca/RATH-Agda/}}},
  OPTnote = {With contributions by Yuhang Zhao.}
}

@InProceedings{Kahl-2014_AContext,
  author = 	 {Wolfram Kahl},
  title = {A Mechanised Abstract Formalisation of Concept Lattices},
  crossref =  {RelMiCS2014},
  pages = 	 {242--260},
  bibliographies = {RelMiCS, RelMiCS14, WK},
  OPTDOI = {},
  OPTSpringerURL = {},
  OPTabstract = {}
}

@Misc{Kahl-2014_AContext-1.0,
  author =       {Wolfram Kahl},
  title =        {{Abstract Context Lattices Formalised for Concrete Applications} --- {AContext-1.0}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2014},
  month = 	 JAN,
  OPTpages = {456 pages},
  OPThowpublished = {Mechanically checked Agda theories available for download,
with 25 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/#AContext-1.0}.},
  howpublished = {Mechanically checked Agda theories,
25 pp.\null{} literate document output. \href{http://relmics.mcmaster.ca/RATH-Agda/#AContext}{\textsf{http://relmics.mcmaster.ca/RATH-Agda/\#AContext}}}
}

@Misc{Kahl-2014_MonCoAlg-0.1,
  author =       {Wolfram Kahl},
  title =        {{Categories of Coalgebras with Semi-Monadic
Homomorphisms --- MonCoAlg-0.1}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2014},
  month = 	 FEB,
  OPTpages = {105 pages},
  OPThowpublished = {Mechanically checked Agda theories available for download,
with 105 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/#AContext-1.0}.},
  howpublished = {Mechanically checked Agda theories,
105 pp.\null{} literate document output. \href{http://relmics.mcmaster.ca/RATH-Agda/#MonCoAlg-0.1}{\textsf{http://relmics.mcmaster.ca/RATH-Agda/\#MonCoAlg-0.1}}}
}

@InProceedings{Kahl-2014_MonCoAlgMor,
  author = 	 {Wolfram Kahl},
  title = {Categories of Coalgebras with Monadic Homomorphisms},
  crossref =  {CMCS2014},
  OPTnote = {Agda theories available via \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  note = {Agda theories at \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  pages = 	 {151--167},
  bibliographies = {RelMiCS, WK},
  DOI = {10.1007/978-3-662-44124-4_9},
  DOIURL = {http://dx.doi.org/10.1007/978-3-662-44124-4_9},
  SpringerURL = {http://link.springer.com/chapter/10.1007/978-3-662-44124-4_9},
  abstract = {Abstract graph transformation approaches
    traditionally consider graph structures
    as algebras over signatures
    where all function symbols are unary.

    Attributed graphs,
    with attributes taken from (term) algebras over arbitrary signatures
    do not fit directly into this kind of transformation approach,
    since algebras containing function symbols taking two or more arguments
    do not allow component-wise construction of pushouts.

    We show how shifting from the algebraic view
    to a coalgebraic view of graph structures
    opens up additional flexibility,
    and enables treating term algebras
    over arbitrary signatures in essentially the same way as unstructured
    label sets.
    We integrate substitution into our coalgebra homomorphisms
    by identifying a factoring over the term monad,
    and obtain a flexible framework for graphs with symbolic attributes.
    This allows us to prove that pushouts can be constructed for homomorphisms
    with unifiable substitution components.

    We formalised the presented development in Agda,
    which crucially aided
    the exploration of the complex interaction of the different functors,
    and enables us to report all theorems as mechanically verified.}
}

@InProceedings{Kahl-2014_CoAlgRew_PP,
  author = 	 {Wolfram Kahl},
  title = {Graph Transformation with Symbolic Attributes via Monadic Coalgebra Homomorphisms},
  crossref =  {GCM2014-PP},
  OPTnote = {Agda theories available via \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  pages = 	 {3--17},
  bibliographies = {RelMiCS, WK},
  abstract = {We show how a coalgebraic approach
leads to more natural representations of many kinds of graph structures
that in the algebraic approach are frequently dealt with using
ad-hoc constructions.
For the case of symbolically attributed graphs,
we demonstrate how using substituting coalgebra homomorphisms
in double-pushout rewriting steps
yields a powerful and easily understandable transformation mechanism.}
}

@Article{Kahl-2014_CoAlgRew,
  author = 	 {Wolfram Kahl},
  title = {Graph Transformation with Symbolic Attributes via Monadic Coalgebra Homomorphisms},
  crossref =  {GCM2014},
  OPTnote = {Agda theories available via \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  pages = 	 {5.1--5.17},
  DOI = {10.14279/tuj.eceasst.71.999},
  DOIURL = {http://dx.doi.org/10.14279/tuj.eceasst.71.999},
  bibliographies = {RelMiCS, WK},
  abstract = {We show how a coalgebraic approach
leads to more natural representations of many kinds of graph structures
that in the algebraic approach are frequently dealt with using
ad-hoc constructions.
For the case of symbolically attributed graphs,
we demonstrate how using substituting coalgebra homomorphisms
in double-pushout rewriting steps
yields a powerful and easily understandable transformation mechanism.}
}

@InProceedings{Kahl-2015_HINet1,
  author =    {Wolfram Kahl},
  title =     {A Simple Parallel Implementation of Interaction Nets in {Haskell}},
  OPTcrossref =  {DCM2015},
  pages =     {33--47},
  booktitle = {Proc. of 10th International Workshop on Developments in Computational Models, {DCM 2014}},
  editor =    {Ugo Dal Lago and Russ Harmer},
  year =      2015,
  volume =    179,
  series =    {EPTCS},
  OrigURL = {http://eptcs.web.cse.unsw.edu.au/paper.cgi?DCM2014:8},
  DOI = {10.4204/EPTCS.179.3},
  DOIURL = {http://dx.doi.org/10.4204/EPTCS.179.3},
  OPTnote =      {\url{http://eptcs.org/content.cgi?DCM2014}},
  note = {Software available at \url{http://www.cas.mcmaster.ca/~kahl/Haskell/HINet/}},
  abstract = {Due to their ``inherent parallelism'',
     interaction nets have since their introduction been considered
     as an attractive implementation mechanism for functional programming.
     We show that a simple highly-concurrent implementation in Haskell
     can achieve promising speed-ups on multiple cores.}
}

@Misc{Kahl-Alhassy-2015_AContext-2.0,
  author =       {Wolfram Kahl and Musa Al-hassy},
  title =        {{Order Theory and Concept Lattices in Ordered Categories Without Meets} --- {AContext-2.0}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2015},
  month = 	 MAY,
  OPTpages = {69 double pages},
  howpublished = {Mechanically checked Agda theories available for download,
with 69 double pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/#AContext}.}
}

@Misc{Kahl-Alhassy-2015_AContext-2.1,
  author =       {Wolfram Kahl and Musa Al-hassy},
  title =        {{Order Theory and Concept Lattices in Ordered Categories Without Meets, Formalised in {Agda}} --- {AContext-2.1}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2015},
  month = 	 JUL,
  OPTpages = {191 pages},
  howpublished = {Mechanically checked Agda theories,
191 pages literate document output. \href{http://relmics.mcmaster.ca/RATH-Agda/#AContext}{\textsf{http://relmics.mcmaster.ca/RATH-Agda/\#AContext}}}
}

@Misc{Kahl-2017_RATH-Agda-2.2,
  author =       {Wolfram Kahl},
  title =        {{Relation-Algebraic Theories in Agda} --- {RATH-Agda-2.2}},
  institution =  {Department of Computing and Software,
                  McMaster University},
  year = 	 {2017},
  month = 	 JAN,
  OPTpages = {580 pages},
  OPThowpublished = {Mechanically checked Agda theories available for download,
with 580 pages literate document output. \url{http://RelMiCS.McMaster.ca/RATH-Agda/}.},
  howpublished = {Mechanically checked Agda theories,
with 580 pages literate document output. \href{http://relmics.mcmaster.ca/RATH-Agda/}{\textsf{http://relmics.mcmaster.ca/RATH-Agda/}}},
  note = {With contributions by Musa Al-hassy and Yuhang Zhao.}
}

@InProceedings{Kahl-Anand-Carette-2005a,
  author = {Wolfram Kahl and Christopher Kumar Anand and Jacques Carette},
  title = 	 {Choices in Data Flow for Declarative Assembly},
  crossref =  {RelMiCS2005-PP},
  pages = 	 {121--128},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Kahl-Anand-Carette-2005,
  author =   {Wolfram Kahl and Christopher Kumar Anand and Jacques Carette},
  title =    {Control-Flow Semantics for Assembly-Level Data-Flow Graphs},
  crossref = {RelMiCS2005},
  pages = {147--160},
  DOI = {10.1007/11734673_12},
  DOIURL={http://dx.doi.org/10.1007/11734673_12},
  bibliographies = {RelMiCS, WK, Coconut},
  abstract = {As part of a larger project, we have built a
     declarative assembly language that enables us to specify multiple
     code paths to compute particular quantities, giving the
     instruction scheduler more flexibility in balancing execution
     resources for superscalar execution.

     Since the key design points for this language are to only
     describe data flow, have built-in facilities for redundancies,
     and still have code that \emph{looks like} assembler, by virtue
     of consisting mainly of assembly instructions, we are basing the
     theoretical foundations on data-flow graph theory, and have to
     accommodate also relational aspects.

     Using functorial semantics into a Kleene category of
     ``hyper-paths'', we formally capture the
     \emph{data-flow-with-choice} aspects of this language and its
     implementation, providing also the framework for the necessary
     correctness proofs.}
}

@TechReport{Kahl-Braun-Scheffczyk-2000a,
  author = {Wolfram Kahl and Oliver Braun and Jan Scheffczyk},
  title = {Editor Combinators --- A First Account},
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 2000,
  number = {2000-01},
  month = JUN,
  URL = {http://sqrl.mcmaster.ca/~kahl/Publications/TR/2000-01/},
  note = {See also \url{http://ist.unibw-muenchen.de/EdComb/}},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-Carette-Ji-2006a,
  author = 	 {Wolfram Kahl and Jacques Carette and Xiaoheng Ji},
  title = 	 {Bimonadic Semantics for Basic Pattern Matching Calculi},
  crossref =  {MPC2006},
  pages = 	 {253--273},
  DOI = {10.1007/11783596_16},
  SpringerURL = {http://www.springerlink.com/content/2715070606u63648/},
  bibliographies = {WK}
}

@TechReport{Kahl-Carette-Ji-2006b,
  author = 	 {Wolfram Kahl and Jacques Carette and Xiaoheng Ji},
  title = 	 {Bimonadic Semantics for Basic Pattern Matching Calculi},
  institution =  {Software Quality Research Laboratory, McMaster Univ.},
  year = 	 2006,
  type = 	 {{SQRL Report}},
  number = 	 33,
  pages =        32,
  note = {long version of \cite{Kahl-Carette-Ji-2006a}, available from
          \textsf{\url{http://sqrl.mcmaster.ca/sqrl_reports.html}}},
  bibliographies = {RelMiCS, WK},
  month = 	 JUN,
  abstract = { The pattern matching calculi
      introduced by the first author
      are a refinement of the $\lambda$-calculus
      that integrates mechanisms appropriate for
      fine-grained modelling of non-strict pattern matching.

      While related work in the literature
      only uses a single monad, typically \textsf{Maybe},
      for matchings,
      we present an axiomatic approach
      to semantics of these pattern matching calculi using two monads,
      one for expressions and one for matchings.

      Although these two monads only need to be relatively lightly coupled,
      this semantics implies soundness of all core $\PMC$ rules,
      and is a useful tool for
      exploration of the design space for pattern matching calculi.

      Using lifting and \textsf{Maybe} monads,
      we obtain standard Haskell semantics,
      and by adding another level of \textsf{Maybe} to both,
      we obtain a denotational semantics
      of the ``matching failure as exceptions'' approach
      of Erwig and Peyton Jones.
      Using list-like monads opens up
      interesting extensions in the direction of functional-logic programming.

      A short version of this report appears as \cite{Kahl-Carette-Ji-2006a}.}
}

@Article{Kahl-Derichsweiler-2001,
  author = {Wolfram Kahl and Frank Derichsweiler},
  title = {Declarative Term Graph Attribution for Program Generation},
  journal = {J.~UCS},
  year = 2001,
  volume = 7,
  number = 1,
  pages = {54--70},
  URL = {http://ist.unibw-muenchen.de/Publications/Journals/Kahl-Derichsweiler-2001.html},
  bibliographies = {RelMiCS, WK},
  keywords = {Program generation, term graph attribution,
              declarative attribute grammars, graph traversals},
  ACMcats = {D.1.2, --- Automatic Programming,
      	     D.2.6, --- Programming Environments,
      	     F.4.2, --- Grammars and Other Rewriting Systems,
      	     D.2.2, --- Tools and Techniques,
      	     D.1.1 --- Applicative (functional) Programming},
  abstract = {We show how the declarative spirit of attribute grammars
              can be employed to define an attribution mechanism for
              \textit{term graphs}, where the non-uniqueness of
              inherited attributes demands an appropriately
              generalised treatment.

              Since term graphs are a useful data structure for
              symbolic computation systems such as theorem provers
              or program transformation systems, this mechanism
              provides a powerful means to generate concrete programs
              (and other relevant text or data structures)
              from their abstract term graph representations.

              We have implemented this
              declarative term graph attribution mechanism
              in the graphically interactive
              term graph program transformation system \HOPS{}
              and show a few simple examples of its use.}
}

@InProceedings{Kahl-Hattensperger-1998,
  author = {Wolfram Kahl and Claudia Hattensperger},
  title = {Second-Order Syntax in {HOPS} and in {RALF}},
  pages = {140--164},
  crossref = {Buth-Berghammer-Peleska-1998},
  URL = {http://www.cas.mcmaster.ca/~kahl/Publications/Conf/Kahl-Hattensperger-1998.html},
  abstract = {HOPS and RALF are two interactive symbol manipulation systems ---
    one for functional programming and program transformation,
    the other for proving relation algebraic formulae ---
    that both implement interactive application
    of second-order rewriting rules,
    HOPS on term graphs and RALF on conventional terms and formulae.
    Both systems support a larger class of second-order rewriting rules than
    commonly found in other systems.

    In this paper we provide
    a homogeneous underpinning to second-order syntax and rewriting
    for easing the transition from terms to term graphs and vice versa,
    so that aspects that are easier to understand in one view
    also further understanding in the other view,
    altogether making a case for bringing second-order syntax
    more directly to the user interface than usual in most systems today.},
  bibliographies = {RelMiCS, WK}
}

@InProceedings{Kahl-Scheffczyk-2001,
  author = {Wolfram Kahl and Jan Scheffczyk},
  title = {Named Instances for Haskell Type Classes},
  crossref = {Haskell2001},
  pages = {71--99},
  note = {See also: \textsf{http://ist.unibw-muenchen.de/Haskell/NamedInstances/}},
  bibliographies = {RelMiCS, WK},
  abstract = {Although the functional programming language Haskell
              has a powerful type class system,
              users frequently run into situations
              where they would like to be able to define or adapt
              instances of type classes
              only \emph{after} the remainder of a component has been produced.
              However, Haskell's type class system
              essentially only allows late binding
              of type class constraints on free type variables,
              and not on uses of type class members
              at variable-free types.

              In the current paper we propose a language extension that
              enhances the late binding capabilities of Haskell type classes,
              and provides more flexible means for type class instantiation.
              The latter is achieved via \emph{named instances}
              that do not participate in automatic context reduction,
              but can only be used for late binding.
              By combining this capability with the automatic aspects
              of the Haskell type class system,
              we arrive at an essentially conservative extension
              that greatly improves flexibility of programming
              using type classes and opens up new structuring principles
              for Haskell library design.

              We exemplify our extension
              through the sketch of some applications
              and show how our approach could be used
              to explain or subsume other language features
              as for example implicit parameters.
              We present a typed lambda-calculus for our extension
              and provide a working prototype type checker on the basis of
              Mark Jones' ``Typing Haskell in Haskell''.}
}

@TechReport{Kahl-Schmidt-2000,
  author = {Wolfram Kahl and Gunther Schmidt},
  title = {Exploring (Finite) {Relation Algebras}
                 Using {Tools} Written in {Haskell}},
  institution = {Fakult\"at f\"ur Informatik,
                 Universit\"at der Bundeswehr M\"unchen},
  number = {2000-02},
  year = 2000,
  month = OCT,
  OPTnote = {See also the RATH page \url{http://ist.unibw-muenchen.de/relmics/tools/RATH/}},
  URL = {http://www.cas.mcmaster.ca/~kahl/Publications/TR/2000-02/},
  abstract = {During the last few years, relational methods have been
      gaining more and more acceptance and impact in computer science.
      Besides applications of concrete relations, also non-standard models
      of the relation algebraic axioms are important in fields as far apart
      as artificial intelligence and distributed computing. Also weaker
      structures have been considered, such as Dedekind categories in
      connection with fuzzy reasoning, and different kinds of allegories.

      \indent In this report we present a library of Haskell modules that
      allows to explore relation algebras and several weaker structures by
      providing different means to construct and test such algebras.

      \indent The kernel of our library is strictly conformant to the
      Haskell 98 standard, and can therefore be expected to be usable on
      future Haskell systems, too. For ease of use, we additionally provide
      a more elegant interface using non-standard extensions.},
  keywords = {RATH},
  bibliographies = {RelMiCS, WK}
}

@Unpublished{Kahl-Zhao-2012,
  author =       {Wolfram Kahl and Yuhang Zhao},
  title =        {Dependently-Typed Formalisation of Term Graph Representations},
  note =         {submitted 2012},
  OPTkey =       {},
  OPTmonth =     {},
  OPTyear =      {},
  OPTannote =    {MSCS}
}

@Unpublished{Kahl-fiber,
  author = {Wolfram Kahl},
  title = {A Fibered Approach to Rewriting},
  year = 1995,
  note = {submitted to SEGRAGRA '95},
  WKloc = {~doc/wk/CAT/fiber}
}

@Unpublished{Kahl-hogr,
  author = {Wolfram Kahl},
  title = {Higher Order Term Graph Rewriting},
  note = {unpublished draft},
  WKloc = {~doc/wk/hogr}
}

@Unpublished{Kahl-hotlg,
  author = {Wolfram Kahl},
  title = {Higher Order Typed Lambda Graphs, or: How to Peel
		  the Banana},
  year = 1995,
  note = {unpublished draft},
  WKloc = {~doc/wk/TYPED/hotlg}
}

@Unpublished{Kahl-lilk,
  author = {Wolfram Kahl},
  title = {From {$\lambda\!I$} to {$\lambda\!K$} in Term Graphs},
  year = 1995,
  note = {submitted to SEGRAGRA '95},
  WKloc = {~doc/wk/TGV/lilk}
}

@TechReport{Kahl-uchotgr,
  author = {Wolfram Kahl},
  title = {{Universelle Konstruktionen f\"ur Termgraphersetzung
		  zweiter Stufe}},
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der
		  Bundeswehr M\"unchen},
  year = 1995,
  type = {{Technischer Bericht}},
  note = {to appear}
}

@InProceedings{Kahrs-1992,
  author = {Stefan Kahrs},
  title = {Unlimp, Uniqueness as a Leitmotiv for Implementation},
  pages = {115--129},
  crossref = {PLILP1992},
  WKloc = {A-0021},
  authorsAddress = {Edinburgh, smk\@dcs.ed.ac.uk},
  abstract = {When evaluation in functional programming languages
		  is explained using $\lambda$-calculus and/or term
		  rewriting systems, expressions and function
		  definitions are often defined as terms, that is as
		  {\em trees}. Similarly, the collection of all terms
		  is defined as a {\em forest}, that is a directed,
		  acyclic graph where every vertex has at most one
		  incoming edge. Concrete implementations usually drop
		  the last restriction (and sometimes acyclicity as
		  well), i.e.\ many terms can share a common subterm,
		  meaning that different paths of subterm edges reach
		  the same vertex in the graph.

		  Any vertex in such a graph represents a term. A term
		  is represented uniquely in such a graph if there are
		  no two different vertices representing it. Such a
		  representation can be established by using {\em
		  hash-consing} for the creation of heap objects. We
		  investigate the consequences of adopting uniqueness
		  in this sense as a leitmotiv for implementation
		  (called Unlimp), i.e.\ not {\em allowing} any two
		  different vertices in a graph to represent the same term.},
  annote = {--- PLGnotes ---
		  `fully collapsed jungle' corresponds to maxid. DAG.
		  motivation for maxid.
		  preservation of maxid. via hash consing, memoization.
		  unreferenced vertices are `improper garbage'}
}

@InProceedings{Kahrs-1992a,
  author = {Stefan Kahrs},
  title = {Polymorphic Type Checking by Interpretation of Code},
  year = 1992,
  crossref = {PIFL92},
  WKloc = {A-0001, ~kahl/doc/pap/92-19-05.ps},
  FTP = {ftp.informatik.rwth-aachen.de:/pub/reports/92-19-05.ps.Z},
  authorsAddress = {University of Edinburgh},
  abstract = {The type system of most modern functional
		  programming languages is based on Milner's
		  polymorphism. A compiler or interpreter usually
		  checks (or infers) the types of functions and other
		  values by directly inspecting the source code of a
		  program.

                  Here, another approach is taken: The program is
		  first translated into code for a stack machine and
		  then a non-standard interpreter applied to this code
		  checks (or infers) the type of the corresponding
		  values. This can be seen as an abstract
		  interpretation of the code of the program.}
}

@InProceedings{Kahrs-1993,
  title = {Compilation of combinatory reduction systems},
  author = {Stefan Kahrs},
  pages = {169--189},
  crossref = {HOA1993},
  WKloc = {A-0110},
  abstract = {Combinatory Reduction Systems generalise Term
		  Rewriting Systems. They are powerful enough to
		  express $\beta$-reduction of $\lambda$-calculus as a
		  single rewrite rule. The additional expressive power
		  has its price --- CRSs are much harder to implement
		  than ordinary TRSs.

                  We propose an abstract machine suitable for
		  executing CRSs. We define what it means to execute
		  an instruction in this machine, and give a
		  translation (including optimisation) from CRS rules
		  into sequences of instructions. Applying a rewrite
		  rule to a term is realised by initialising the
		  machine with this term, and then successively
		  executing the instructions of the compiled rule.}
}

@InProceedings{Kahrs-1994,
  author = {Stefan Kahrs},
  title = {First-Class Polymorphism for {ML}},
  crossref = {ESOP1994},
  pages = {333--347},
  note = {also as Tech.~Report ECS-LFCS-94-284, Laboratory for
		  Foundations of Computer Science, University of Edinburgh},
  authorsAddress = {Edinburgh},
  URL = {http://www.dcs.ed.ac.uk/lfcs/,
		  ftp://ftp.dcs.ed.ac.uk/pub/lfcsreps/94/ (TR)},
  WKloc = {A-0325},
  OPTkeywords = {},
  OPTabstract = {},
  OPTannote = {}
}

@InProceedings{Kahrs-Sanella-Tarlecki-1993,
  author = {S. Kahrs and Don Sanella and A. Tarlecki},
  title = {The Semantics of {Extended ML}: A Gentle Introduction},
  crossref = {SoSL93},
  pages = {186--215},
  authorsAddress = {Edinburgh},
  WKloc = {A-0321},
  abstract = {\def\EML{{\sf EML}}\def\SML{{\sf SML}}Extended ML
		  (\EML) is a framework for the formal development of
		  modular Sandard ML (\SML) software
		  systems. Development commences with a specification
		  of the behaviour required and proceeds via a
		  sequence of partial solutions until a complete
		  solution, an executable \SML{} program, is
		  obtained. All stages in this development process are
		  expressed in the \EML{} specification language, an
		  extension of \SML{} with axioms for describing
		  properties of module components.

                  This is a report on the current state of the
		  semantics of the \EML{} specification language as it
		  nears completion. \EML{} is unusual in being built
		  around a ``real'' programming language having a
		  formal semantics. Interesting and complex problems
		  arise both from the nature of this relationship and
		  from interactions between the features of the language.},
  annote = {Incentive for {\tt mod1cc}}
}

@Article{Kaiser-2001,
  author =       {Wolfram Kaiser},
  title =        {Become a programming {Picasso} with {JHotDraw}},
  journal =      {JavaWorld},
  year =         2001,
  URL =     {http://www.javaworld.com/javaworld/jw-02-2001/jw-0216-jhotdraw.html},
  month =     {Feb.~16}
}

@InProceedings{Kajler-1990,
  author = {N. Kajler},
  title = {Building Graphic User Interfaces for Computer
		  Algebra Systems},
  crossref = {DISCO90},
  pages = {235--244},
  WKloc = {A-0353}
}

@InProceedings{Kaldewaij-Dielissen-1994,
  author = {A. Kaldewaij and V.J. Dielissen},
  title = {Decomposable Functions and Leaf Trees: A Systematic Approach},
  crossref = {PROCOMET94},
  pages = {1--17},
  keywords = {Data Structures; Algorithms; Mathematics of Program
		  Derivation}
}

@Article{Kaldewaij-Schoenmakers-1990,
  author = {Anne Kaldewaij and Nerry Schoenmakers},
  title = {Searching by Elimination},
  journal = {Science of Computer Programming},
  year = 1990,
  volume = 14,
  OPTnumber = {},
  OPTmonth = {},
  pages = {243--254},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0719},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Kalleberg-Visser-2006,
  author = 	 {Karl Trygve Kalleberg and Eelco Visser},
  title = 	 {Strategic Graph Rewriting Transforming and Traversing Terms with References},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {{WRS 2006 Participants' Proceedings}},
  pages = 	 {96--111},
  year = 	 {2006},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  BOOKURL = {http://web.cecs.pdx.edu/~antoy/homepage/publications/wrs06/pre-workshop.pdf},
  URL = {http://www.ii.uib.no/~karltk/phd/papers/wrs06.pdf},
  CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.109.846},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = 	 {Some transformations and many analyses on programs
                  are either difficult or unnatural to express using
                  terms. In particular, analyses that involve type
                  contexts, call- or control flow graphs are not
                  easily captured in term rewriting systems. In this
                  paper, we describe an extension to the System S term
                  rewriting system that adds references. We show how
                  references are used for graph rewriting, how we can
                  express more transformations with graph-like
                  structures using only local matching, and how
                  references give a representation that is more
                  natural for structures that are inherently
                  graph-like. Furthermore, we discuss trade-offs of
                  this extension, such as changed traversal
                  termination and unexpected impact of reference
                  rebinding.}
}

@Article{Kalicki-Scott-1955,
  author = {J. Kalicki and D. Scott},
  title = {Equational Completeness of Abstract Algebras},
  journal = INDAG,
  volume = 17,
  year = 1955,
  pages = {650--659},
  bibliographies = {RelMiCS}
}

@Article{Kamareddine-1992,
  author = {Fairouz Kamareddine},
  title = {A System at the Cross-Roads of Functional and Logic
		  Programming},
  year = 1992,
  volume = 19,
  pages = {239--279},
  journal = SCICOP,
  WKloc = {A-0044},
  authorsAddress = {Eindhoven},
  bibliographies = {RelMiCS},
  abstract = {The type-free $\lambda$-calculus is powerful enough
		  to contain all the polymorphic and higher-order
		  nature of functional programming and furthermore
		  types could be constructed inside it. However,
		  mixing the type-free $\lambda$-calculus with logic
		  is not very straightforward (see Aczel [1] and Scott
		  [15]). In this paper, a system that combines
		  polymorphism and higher-order functions with logic
		  is presented. The system is suitable for both the
		  functional and logical paradigms of programming as
		  from the functional paradigm's point of view, the
		  system enables one to have all the polymorphism and
		  higher order that exist in functional languages and
		  much more. In fact even the fixed point operator $Y$
		  which is defined as $\lambda f. (\lambda x.f(x
		  x))(\lambda x.f(x x))$ can be type checked to
		  $((\alpha \tfun \alpha) \tfun \alpha)$ where
		  $\alpha$ is a variable type. $(\lambda x. x
		  x)(\lambda x. x x)$ can be type-checked too,
		  something not allowed in functional languages. From
		  the point of view of theorem proving, the system is
		  expressive enough to allow self-referential
		  sentences and those sentences that lead to Russel's
		  and Curry's paradoxes. However, the paradoxes do not
		  hold due to the notion of circular types which
		  contain the type of propositions. In fact both
		  sentences $\lambda x.\lnot x x$ and $\lambda x.x x
		  \tfun \bottom$ are ill-typed according to the
		  system, because their resulting types are circular.
		  Hence the application of either sentence to itself
		  will not result in a proposition. The system is
		  implementes in Milner's ML and can be seen as
		  extending ML in two important ways. First, it
		  extends the part related to the functional paradigm
		  in taht it can type terms that could not be typed in
		  ML, namely the terms that contain self-application
		  such as the $Y$ term above. Second, our system
		  extends ML by adding logic to it in a consistent way.}
}

@Misc{Kambites-2005,
  author =	 {Mark Kambites},
  title =	 {Automatic {Rees} matrix semigroups over categories},
  howpublished = {\url{http://arxiv.org/abs/math.RA/0509313}},
  month =	 SEP,
  year =	 2005,
  MSC = {20M05, 20M50, 18B40},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  note = {27 pages},
  abstract = {We consider the preservation of the properties of
      automaticity and prefix-automaticity in Rees matrix semigroups
      over semigroupoids and small categories. Some of our results are
      new or improve upon existing results in the single-object case
      of Rees matrix semigroups over semigroups.}
}

@Article{Kambites-2006,
  author = 	 {Mark Kambites},
  title = 	 {Automatic semigroups and categories},
  journal = 	 TCS,
  year = 	 2006,
  volume =	 353,
  pages =	 {272--290},
  WKloc = {doc/pap/BIB},
  AuthorURL = 	 {http://www.maths.manchester.ac.uk/~mkambites/},
  abstract = {We consider various automata-theoretic properties of
     semigroupoids and small categories and their relationship to the
     corresponding properties in semigroups and monoids. We introduce
     natural definitions of finite automata and regular languages over
     finite graphs, generalising the usual notions over finite
     alphabets. These allow us to introduce a definition of
     automaticity for semigroupoids and small categories, which
     generalises those introduced for semigroups by Hudson and for
     groupoids by Epstein. We also introduce a definition of
     prefix-automaticity for semigroupoids and small categories,
     generalising that for certain monoids introduced by Silva and
     Steinberg.

     We study the relationship between automaticity properties in a
     semigroupoid and in a certain associated semigroup. This allows
     us to extend to semigroupoids and small categories a number of
     results about automatic and prefix-automatic semigroups and
     monoids. In the course of our study, we also prove some new
     results about automaticity and prefix-automaticity in semigroups
     and monoids. These include the fact that prefix-automaticity is
     preserved under the taking of cofinite subsemigroups.}
}

@Article{Kambites-2006a,
  author = {Mark Kambites},
  title = {Word problems recognisable by deterministic blind monoid automata},
  journal = 	 TCS,
  year = 	 2006,
  volume =	 362,
  pages =	 {232--237},
  AuthorURL = {http://www.maths.manchester.ac.uk/~mkambites/},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  abstract = {We consider blind, deterministic, finite automata
     equipped with a register which stores an element of a given
     monoid, and which is modified by right multiplication by monoid
     elements. We show that, for monoids M drawn from a large class
     including groups, such an automaton accepts the word problem of a
     group H if and only if H has a finite index subgroup which embeds
     in the group of units of M. In the case that M is a group, this
     answers a question of Elston and Ostheimer.}
}

@Article{Kamel-1952,
  author = {H. Kamel},
  title = {Relational Algebra},
  journal = BUAMS,
  volume = 58,
  pages = 391,
  year = 1952,
  bibliographies = {RelMiCS}
}

@Article{Kamel-1954,
  author = {H. Kamel},
  title = {Relational Algebras and Uniform Spaces},
  journal = JLON,
  volume = 29,
  year = 1954,
  pages = {342--344},
  bibliographies = {RelMiCS}
}

@Misc{Kamin-1998,
  author = {Samuel N. Kamin},
  title = {An Implementation-Oriented Semantics of {Wadler}'s Pretty-Printing Combinators},
  year = 1998,
  WKloc = {A-0744}
}

@Book{Kamin-1990,
  author =	 {Samuel N. Kamin},
  title = 	 {Programming languages: an interpreter-based approach},
  publisher = 	 {Addison-Wesley},
  year = 	 {1990},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  bibliographies = {3E},
  pages = 	 {640},
  ISBN = 	 {0201068249},
  McMaster = 	 {QA 76.7 .K35 1990}
}

@InProceedings{Kamin-Hyatt-1997,
  author = {Samuel N. Kamin and David Hyatt},
  title = {A Special-Purpose Language for Picture-Drawing},
  booktitle = {Proc. {USENIX} Conf. on Domain-specific Languages, {Santa Barbara, Oct. 1997}},
  pages = {297--310},
  year = 1997,
  OPTabstract = {},
  WKloc = {A-0743}
}

@InProceedings{Kaminska-Amellal-1993,
  author = {Kaminska, B. and Amellal, S.},
  title = {Scheduling of a control and data flow graph},
  pages = {1666--},
  OPTabstract = {},
  OPTurl = {},
  year = 1993,
  month = {},
  OPTcrossref = {},
  booktitle = {IEEE International Symposium on Circuits and Systems},
  editor = {},
  publisher = {},
  series = {ISSN 0271-4310},
  number = 3,
  UniBwM = {E-ELT880/Z4701-1993,3}
}

@Article{Kaminska-Amellal-1994,
  author = {Kaminska, B. and Amellal, S.},
  title = {Functional Synthesis of Digital Systems with {TASS}},
  pages = {537--},
  year = 1994,
  journal = {IEEE Transactions on Computer Aided Design of Integrated Circuits and Systems CAD},
  volume = 13,
  number = 3,
  ISSN = {0278-0070}
}

@InProceedings{Kaminska-Oudghiri-Rajski-1997,
  author = {Kaminska, B. and Oudghiri, H. and Rajski, J.},
  title = {A Hardware/Software Partitioning Technique With Hierarchical Design Space Exploration},
  pages = {95--},
  OPTabstract = {},
  OPTurl = {},
  year = 1997,
  month = {},
  OPTcrossref = {},
  booktitle = {Proceedings of the IEEE Custom Integrated Circuits Conference},
  editor = {},
  publisher = {},
  series = {ISSN 0886-5930},
  volume = {}
}

@InProceedings{Kammueller-Wenzel-Paulson-1999,
  author = {Florian Kamm{\"u}ller and Markus Wenzel and Lawrence C. Paulson},
  title = {Locales --- A Sectioning Concept for {Isabelle}},
  crossref = {TPHOL1999},
  pages = {149--166},
  WKloc = {doc/pap/BIB},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  OPTannote = {}
}

@Book{Kamp-Reyle-1993,
  author = {Kamp, H. and Reyle, U.},
  address = {Dordrecht},
  publisher = Kluwer,
  title = {From Discourse to Logic},
  year = 1993,
  bibliographies = {RelMiCS}
}

@Misc{Kamperman-199X,
  author = {J. F. Th. Kamperman},
  title = {{GEL}, a Graph Exchange Language},
  year = {199?},
  WKloc = {A-0419}
}

@Misc{Kamperman-Walters-199X,
  author = {J. F. Th. Kamperman and H. R. Walters},
  title = {lazy Rewriting on Eager Machinery},
  year = {199?},
  WKloc = {A-0422}
}

@InProceedings{Kanazawa-1994,
  author = {M. Kanazawa},
  address = {Amsterdam},
  booktitle = {Proc.\null{} of the {{$9^{th}$} Amsterdam Colloq.}},
  editor = {P. Dekker and M. Stokhof},
  howpublished = {XXXX is dit in JoLLI gepubliceerd?XXXX},
  pages = {377--391},
  publisher = ILLC,
  title = {Completeness and decidability of the mixed style of inference
      with composition},
  year = 1994,
  bibliographies = {RelMiCS}
}

@InProceedings{Kanda-1981,
 author = {Kanda, Akira},
 title = {Constructive Category Theory (No.~1)},
 crossref = {MFCS1981},
 pages = {563--577},
 WKloc = {doc/pap/BIB},
 DOI = {10.1007/3-540-10856-4_125},
 abstract = {In this paper,
    a notion of effective categories and effective functions is presented
    as an attempt to handle effective properties of categories
    with effectiveness constraint.
    Using this notion,
    we study a categorical generalization of the Kleene 1st recursion theorem,
    as the effectively initial algebra theorem.
    Various effective categories and effective functors are studied.}
}

@InProceedings{Kanellakis-Hillebrand-Mairson-1994,
  author = {P. Kanellakis and G. Hillebrand and H. Mairson},
  title = {An Analysis of core-{ML}: Expressive Power and Type Inference},
  pages = {83--105},
  crossref = {ICALP1994},
  authorsAddress = {Brown University, HM: Brandeis University}
}

@Misc{Kanellakis-Mairson-Mitchell-1991,
  author = {Paris C. Kanellakis and John C. Mitchell},
  title = {Unification and {ML} Type Reconstruction},
  year = 1991,
  month = FEB,
  WKloc = {A-0526},
  annote = {follow-up to \cite{Kanellakis-Mitchell-1989}}
}

@InProceedings{Kanellakis-Mitchell-1989,
  author = {Paris C. Kanellakis and John C. Mitchell},
  title = {Polymorphic Unification and {ML} Typing},
  crossref = {POPL1989},
  pages = {105--115},
  WKloc = {A-0206},
  abstract = {We study the complexity of type inference for a core
		  fragment of ML with lambda abstraction, function
		  application, and the polymorphic {\tt let}
		  declaration. Our primary tool is the unification
		  problem for a class of ``polymorphic'' type
		  expressions. This form of unification, which we call
		  {\em polymorphic unification}, allow us to separate
		  a combinatorial aspect of type inference from the
		  sntax of ML programs. After observing that ML typing
		  is in DEXPTIME, we show that polymorphic unification
		  is PSPACE hard. From this, we prove that recognizing
		  the typable core ML programs is also PSPACE hard.
		  Our lower bound stands in contrast to the common
		  belief that typing ML programs is ``efficient'', and
		  to practical experience which suggests that the
		  algorithms commonly used for this task do not slow
		  compilation substantially.}
}

@InProceedings{Kanneganti-Cartwright-Felleisen-1992,
  author = {Ramarao Kanneganti and Robert Cartwright and
		  Matthias Felleisen},
  title = {{SPCF}: Its Model, Calculus, and Computational Power
		  (Preliminary Version)},
  crossref = {REX92},
  pages = {318--347},
  WKloc = {A-0256},
  abstract = {SPCF is an idealized sequential programming
		  language, based on Plotkin's language PCF, that
		  permits programmers and programs to observe the
		  evaluation order of procedures. In this paper, we
		  construct a fully abstract model of SPCF using a new
		  mathematical framework suitable for defining fully
		  abstract models of sequential functional
		  languages. Then, we develop an extended typed
		  $\lambda$-calculus to specify the operational
		  semantics of SPCF and show that the calculus is
		  complete for the constant-free
		  sub-language. Finally, we prove that SPCF is {\em
		  computationally complete}: it can express all the
		  computable (recursively enumerable) elements in its
		  fully abstract model.}
}

@InProceedings{Kanovich-1991,
  title = {Efficient Program Synthesis: Semantics, Logic, Complexity},
  author = {Max I. Kanovich},
  pages = {615--632},
  crossref = {TACS1991},
  bibliographies = {RelMiCS},
  abstract = {The problem of program synthesis is considered.
                \begin{itemize}
		\item[1.] A computational semantics is introduced
		  for relational knowledge bases. Our semantics
		  naturally arises from practical experience of
		  databases and knowledge bases.
		\item[2.] It is stated that the corresponding logic
		  coincides exactly with the intui\-tionistic one.
		\item[3.] Our methods of proof of the general theorems
		  turn out to be very useful for designing new
		  efficient algorithms.\\ In particular, one can
		  construct a program synthesizer that runs in linear
		  space.\\ As a corollary, we can explain why there
		  exist programs that solve PSPACE-complete problems
		  ``in a reasonable time'' despite of their
		  theoretical exponential uniform lower bound.
                \end{itemize}}
}

@Misc{Kantorowich-1956,
  author = {Leonid Witaljewich Kantorowich},
  title = {???},
  year = 1956,
  note = {Introduction of term trees as representation of context-free derivations.}
}

@Misc{Karczmarczuk-1999,
  author = {Jerczy Karczmarczuk},
  title = {Scientific Computation and Functional Programming},
  month = JAN,
  year = 1999,
  WKloc = {A-1135}
}

@InProceedings{Karlsen-1998,
  author = {Einar W. Karlsen},
  title = {The {UniForM} WorkBench - a Higher Order Tool Integration Framework},
  crossref = {FMTrends1998},
  WKloc = {A-1321, doc/pap/BIB},
  URL = {http://www.informatik.uni-bremen.de/~ewk/papers/wb98.ps.gz}
}

@InProceedings{Karp-Miller-Rosenberg-1972,
  year = 1972,
  title = {Rapid Identification of Repeated Patterns in Strings, Trees and
           Arrays},
  publisher = {ACM},
  pages = {125-136},
  booktitle = {Proc. of the 4th Annual ACM Symposium on Theory of Computing},
  author = {R. M. Karp and R. E. Miller and A. L. Rosenberg}
}

@Unpublished{Kasangian-Walters-1982,
  author = {S. Kasangian and R.F.C. Walters},
  title = {An Abstract Notion of Glueing},
  note = {Preprint},
  year = 1982
}

@Article{Kasangian-Walters-1990,
  author = {S. Kasangian and R.F.C. Walters},
  title = {The duality of flow charts and circuits},
  journal = BuAuMS,
  year = 1990,
  pages = {71--79},
  volume = 42
}

@Article{Kastenberg-2006,
  author = {Kastenberg, Harmen},
  title = {Towards Attributed Graphs in Groove},
  journal = ENTCS,
  volume = {154},
  issue = {2},
  month = {May},
  year = {2006},
  issn = {1571-0661},
  pages = {47--54},
  numpages = {8},
  DOIURL = {http://dx.doi.org/10.1016/j.entcs.2005.03.030},
  DOI = {10.1016/j.entcs.2005.03.030},
  acmid = {1706783},
  OPTpublisher = {Elsevier Science Publishers B. V.},
  address = {Amsterdam, The Netherlands, The Netherlands},
  keywords = {algebra graph, attributed graphs, graph transformation, signature structure},
  abstract = {Graphs are a very expressive formalism for system modeling,
    especially when attributes are allowed.
    Our research is mainly focused on the use of graphs for system verification.
    Up to now, there are two main different approaches
    of modeling (typed) attributed graphs and specifying their transformation.
    Here we report preliminary results of our investigation on a third approach.
    In our approach we couple a graph
    to a data signature that consists of unary operations only.
    Therefore, we transform arbitrary signatures into a structure
    comparable to what is called a graph structure signature in the literature,
    and arbitrary algebras into the corresponding algebra graph. }
}

@Manual{Kastens-,
  title = {{LIDO} --- A Language for Attribute Grammar Specification},
  author = {Uwe Kastens},
  organization = {University of Paderborn},
  OPTaddress = {},
  edition = {Revision 1.4},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1180}
}

@PhDThesis{Kathail-1990,
  author = {Vinod Kathail},
  title = {Optimal Interpreters for Lambda-Calculus Based
		  Functional Languages},
  school = {MIT},
  year = 1990,
  month = MAY,
  type = {{PhD} thesis}
}

@Article{Katis-Sabadini-Walters-1997,
  author = {Katis, P. and Sabadini, N. and Walters, R.F.C.},
  title = {Bicategories of Processes},
  journal = JPAA,
  volume = 115,
  pages = {141--178},
  year = 1997,
  WKloc = {A-0973}
}

@Misc{Katis-Sabadini-Walters-1998,
  author = {P. Katis and N. Sabadini and R. F. C. Walters},
  title = {The Algebra of Feedback and Systems With Boundary},
  note = {http://cat.maths.usyd.edu.au/~giuliok/},
  year = 1998
}

@Misc{Katis-Sabadini-Walters-1998a,
  author = {Piergiulio Katis and N. Sabadini and R. F. C. Walters},
  title = {On partita doppia},
  year = 1998,
  month = SEP,
  WKloc = {A-1259},
  abstract = {In 1494 Fra Luca Pacioli published in Venice on of the first
      printed mathematical books \cite{Pacioli-1494}. One section, Computis
      e Scripturis, ist the first published description of \emph{partita
      doppia} or double-entry bookkeeping, the foundation of accounting.
      Double-entry bookkeeping had been developed over a period of 200
      years by Italian merchants and bankers. The aim of accounting is the
      measurement of a distributed concurrent system, and it is our
      contention that it is one of the earliest and most successful
      mathematical theories of concurrency. It is interesting that at the
      time negative numbers were not accepted; in the following century
      mathematicians repaired this deficiency though there seems to have
      been no attempt to explicate the distributed algebra underlying
      partita doppia. [...]

      In this paper we give a precise mathematical account of partita
      doppia in terms of an algebraic structure on \textbf{Span(RGraph)}
      --- the bicategory of spans of reflexive graphs. Some interesting new
      mathematical consideratrions arise in the study of this example.}
}

@InProceedings{Katiyar-Luckham-Mitchell-1994,
  author = {Dinesh Katiyar and David Luckham and John Mitchell},
  title = {A Type System for Prototyping Languages},
  crossref = {POPL1994},
  pages = {138--150},
  authorsAddress = {Stanford},
  keywords = {{\sc Rapide}}
}

@InProceedings{Kats-Visser-2010,
  title = {The {Spoofax} Language Workbench. {Rules} for Declarative Specification of Languages and {IDEs}},
  author = {Lennart C. L. Kats and Eelco Visser},
  year = {2010},
  DOI = {10.1145/1869459.1869497},
  DOIURL = {http://dx.doi.org/10.1145/1869459.1869497},
  researchr = {http://researchr.org/publication/KatsVisser2010},
  pages = {444--463},
  OPTbooktitle = {Proceedings of the ACM international conference on Object oriented programming systems languages and applications},
  OPTseries = {OOPSLA '10},
  isbn = {978-1-4503-0203-6},
  location = {Reno/Tahoe, Nevada, USA},
  booktitle = {Proceedings of the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications, OOPSLA 2010, October 17-21, 2010, Reno, NV, USA},
  editor = {Martin Rinard},
  abstract = {Spoofax is a language workbench for efficient, agile
                  development of textual domain-specific languages
                  with state-of-the-art IDE support. Spoofax
                  integrates language processing techniques for parser
                  generation, meta-programming, and IDE development
                  into a single environment. It uses concise,
                  declarative specifications for languages and IDE
                  services. In this paper we describe the architecture
                  of Spoofax and introduce idioms for high-level
                  specifications of language semantics using rewrite
                  rules, showing how analyses can be reused for
                  transformations, code generation, and editor
                  services such as error marking, reference resolving,
                  and content completion. The implementation of these
                  services is supported by language-parametric editor
                  service classes that can be dynamically loaded by
                  the Eclipse IDE, allowing new languages to be
                  developed and used side-by-side in the same Eclipse
                  environment.}
}


@Misc{Kawaguchi-Sakabe-Inagaki-199?,
  author = {Nobuo Kawaguchi and Toshiki Sakabe and Yasoyoshi Inagaki},
  title = {{TERSE}: {TErm Rewriting Support Environment}},
  year = {199?},
  WKloc = {A-0579}
}

@Article{Kawahara-1973,
  author = {Yasuo Kawahara},
  title = {Relations in Categories with Pullbacks},
  journal = MFSKU-A,
  year = 1973,
  pages = {149--173},
  volume = 27,
  number = 1,
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1973a,
  author = {Yasuo Kawahara},
  title = {Matrix Calculus in {I}-Categories and an Axiomatic
      Characterization of Relations in a Regular Category},
  journal = MFSKU-A,
  year = 1973,
  pages = {249--273},
  volume = 27,
  number = 2,
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1973b,
  author = {Yasuo Kawahara},
  title = {Notes on the Universality of Relational Functors},
  journal = MFSKU-A,
  year = 1973,
  volume = 27,
  number = 2,
  pages = {275--289},
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1975,
  author = {Yasuo Kawahara},
  journal = Communications_in_Algebra,
  pages = {851--857},
  number = 9,
  title = {On the Class of Regular Epimorphisms},
  volume = 3,
  year = 1973,
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1978,
  author = {Yasuo Kawahara},
  journal = Bull_Kyushu_Inst_Tech,
  pages = {31--40},
  title = {A Relation Theoretic Proof of a Tripleability Theorem over Exact Categories},
  volume = 25,
  year = 1978,
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1980,
  author = {Yasuo Kawahara},
  journal = Bull_Kyushu_Inst_Tech,
  pages = {17--25},
  title = {Relational Tree Automata and Context-Free Sets},
  volume = 27,
  year = 1980,
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1988,
  author = {Yasuo Kawahara},
  title = {Applications of Relational Calculus to Computer Mathematics},
  journal = BUIC,
  year = 1988,
  pages = {67--78},
  volume = 23,
  number = {1$\sim$2},
  bibliographies = {RelMiCS}
}

@Article{Kawahara-1990,
  author = {Yasuo Kawahara},
  title = {Pushout-Complements and Basic Concepts of Grammars in Toposes},
  journal = TCS,
  DOI = {10.1016/0304-3975(90)90171-D},
  year = 1990,
  volume = 77,
  pages = {267--289},
  WKloc = {A-0284, doc/pap/BIB},
  abstract = {An existence theorem of pushout-complements is
		  given in an elementary topos by using category
		  theory of binary relations, called relational
		  calculus, and it is also shown more explicitly in
		  the category of directed graphs, which is a typical
		  example of toposes, as an application. Moreover an
		  embedding theorem and Church-Rosser theorem on
		  grammars (derivations) in a topos are proved.},
  bibliographies = {RelMiCS}
}

@InProceedings{Kawahara-1995,
  author = {Yasuo Kawahara},
  title = {Relational Set Theory},
  crossref = {CTCS1995},
  pages = {44--58},
  bibliographies = {RelMiCS},
  CiteSeer = "citeseer.ist.psu.edu/article/kawahara95relational.html",
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB}
}

@Unpublished{Kawahara-1998,
  author = {Yasuo Kawahara},
  title = {Lattices in {Dedekind} Categories},
  year = 1998,
  month = SEP,
  WKloc = {A-0474},
  note = {Draft},
  bibliographies = {RelMiCS}
}

@InProceedings{Kawahara-2001,
  author = {Yasuo Kawahara},
  title = {Groups in Allegories},
  crossref = {RelMiCS2001-PP},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1189, doc/pap/BIB}
}

@Unpublished{Kawahara-2001a,
  author = {Yasuo Kawahara},
  title = {Groups in Allegories},
  annote = {submission to Proc.~RelMiCS~6},
  note = {unpublished draft},
  WKloc = {A-1297, doc/pap/BIB}
}

@InCollection{Kawahara-2001_LiD,
  author={Kawahara, Yasuo},
  title={Lattices in {Dedekind} Categories},
  year={2001},
  isbn={978-3-662-00362-6},
  booktitle={Relational Methods for Computer Science Applications},
  volume={65},
  chapter={15},
  series={Studies in Fuzziness and Soft Computing},
  editor={Orłowska, Ewa and Szałas, Andrzej},
  DOI={10.1007/978-3-7908-1828-4_15},
  DOIURL={http://dx.doi.org/10.1007/978-3-7908-1828-4_15},
  publisher={Physica-Verlag HD},
  keywords={lattice; Dedekind category; Allegory; relational calculus},
  pages={247--260},
  language={English},
  abstract = {Lattice structures are fundamental and useful
    in mathematics and theoretical computer science.
    It is well-known that lattice structures with meet and join operations
    satisfying associative, commutative and absorption laws
    are equivalent to lattice structures defined by ordering relations
    having joins and meets.
    This paper defines a notion of lattices in Dedekind categories
    and studies some basic properties of lattice structures.
    Following relational calculus,
    an element-free representation of these properties is discussed.}
}

@Article{Kawahara-Furusawa-2001,
  author = {Yasuo Kawahara and Hitoshi Furusawa},
  title = {Crispness in {Dedekind} Categories},
  journal = BUIC,
  year = 2001,
  volume = 33,
  number = {1--2},
  pages = {1--18},
  bibliographies = {RelMiCS},
  WKloc = {A-1317},
  abstract = {This paper studies notions of scalar relations and
     crispness of relations in terms of Dedekind categories.
     It is well-known that a category of $L$-relations
     in the sense of Goguen is a Dedekind category.
     To compare with an ordinary notion of crispness of $L$-relations,
     we introduce three notions of crispness in Dedekind categories.}
}

@InProceedings{Kawahara-Furusawa-Mori-1996,
  author = {Yasuo Kawahara and Hitoshi Furusawa and Masao Mori},
  title = {Categorical Representation Theorems of Fuzzy Relations},
  OPTcrossref = {},
  pages = {190--197},
  booktitle = {Proceedings of 4th International Workshop on Rough Sets,
               Fuzzy Sets, and Machine Discovery {(RSFD '96)}},
  OPTeditor = {},
  year = 1996,
  OPTpublisher = {},
  abstract = {},
  WKloc = {A-0704},
  bibliographies = {RelMiCS}
}

@Article{Kawahara-Furusawa-Mori-1999,
  author = {Yasuo Kawahara and Hitoshi Furusawa and Masao Mori},
  title = {Categorical Representation Theorems of Fuzzy Relations},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {235--251},
  abstract = {This paper provides a notion of Zadeh categories as a
      categorical structure formed by fuzzy relations with sup-min
      composition, and proves two representation theorems for Dedekind
      categories (relation categories) with a unit object analogous to
      one-point set, and for Zadeh categories without unit objects.},
  keywords = {Relation algebra; Fuzzy relation; Representation theorem; Dedekind category; Zadeh category},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/32/abstract.html},
  bibliographies = {RelMiCS}
}

@Article{Kawahara-Mizoguchi-1992,
  author = {Yasuo Kawahara and Mizoguchi, Yoshihiro},
  title = {Categorical Assertion Semantics in Topoi},
  journal = Advances_in_Software_Science_and_Technology,
  pages = {137--150},
  year = 1992,
  volume = 4,
  bibliographies = {RelMiCS}
}

@InProceedings{Kawahara-Mizoguchi-1993,
  author = {Yasuo Kawahara and Yoshihiro Mizoguchi},
  title = {Relational Structures and Their Partial Morphisms in
		  View of Single Pushout Rewriting},
  crossref = {GTCS93},
  pages = {218--233},
  abstract = {In this paper we present a basic notion of
		  relational structures which includes simple graphs,
		  labelled graphs and hypergraphs, and introduce a
		  notion of partial morphisms between them. An
		  existence theorem of pushouts in the category of
		  relational structures and their partial morphisms is
		  proved under a certain functorial condition, and it
		  enables us to discuss single pushout rewritings of
		  relational structures.},
  WKloc = {A-0293},
  annote = {nice short summary of category theory in the appendix},
  bibliographies = {RelMiCS}
}

@Article{Kawahara-Mizoguchi-1994,
  author = {Mizoguchi, Yoshihiro and Yasuo Kawahara},
  title = {Relational Graph Rewritings},
  journal = TCS,
  year = 1995,
  volume = 141,
  pages = {311--328},
  bibliographies = {RelMiCS}
}

@Book{KeYBook2007,
   editor =        {Bernhard Beckert and Reiner H\"ahnle and Peter H. Schmitt},
   title =         {Verification of Object-Oriented Software: The {KeY} Approach},
   series        = {LNCS 4334},
   publisher     = {Springer-Verlag},
   year          = {2007}
}

@Article{Kearney-Staples-1991,
  author = {Kearney, Peter and Staples, John},
  title = {An extensional fixedpoint semantics for
		  nondeterministic data flow},
  journal = {Theoretical Computer Science},
  year = 1991,
  volume = 91,
  number = 2,
  pages = {129--179},
  abstract = {A fixed point semantics for nondeterministic data
		  flow is introduced which refines and extends work of
		  Park (1983). It can be seen also as an extension to
		  the general case of Kahn's (1974) successful fixed
		  point semantics for deterministic data flow. An
		  associativity result for network construction is
		  proved which shows that anomalies such as those of
		  Brock and Ackerman do not arise in this
		  semantics. The semantics is shown to be extensional,
		  in the natural sense that nondeterministic processes
		  which induce identical --- output relations in all
		  contexts are equal.},
  bibliographies = {RelMiCS}
}

@Article{Keenan-Faltz-1978,
  author = {Edward Keenan and Leonard Faltz},
  title = {Logical Types for Natural Language},
  year = 1978,
  journal = UCLA_Occasional_Papers_in_Linguistics,
  bibliographies = {RelMiCS}
}

@Book{Keenan-Faltz-1985,
  author = {Edward Keenan and Leonard Faltz},
  year = 1985,
  title = {Boolean Semantics for Natural Language},
  address = {Dordrecht},
  publisher = Reidel,
  bibliographies = {RelMiCS}
}

@InProceedings{Keller-Simons-1996,
  author = {Gabriele Keller and Martin Simons},
  title = {A Calculational Approach to Flattening Nested Data Parallelism in Functional Languages},
  pages = {},
  OPTabstract = {},
  booktitle = {The 1996 {Asian Computer Science Conference}},
  editor = {J. Jaffar},
  series = {LNCS},
  publisher = {Springer-Verlag},
  year = 1996,
  WKloc = {A-0699}
}

@Book{Kelly-1982,
  author = {F.A.A. Kelly},
  title = {Basic Concepts of Enriched Category Theory},
  year = 1982,
  publisher = {Cambridge University Press},
  series = {London Mathematical Society Lecture Notes},
  volume = 64
}

@InProceedings{Kelly-1990,
  author = {G. M. Kelly},
  title = {A note on relations relative to a factorization
                 system},
  crossref = {CAT1990},
  pages = {249--261},
  bibliographies = {RelMiCS}
}

@Article{Kelly-Lack-1997,
  author = {G. M. Kelly and Stephen Lack},
  title = {On Property-Like Structures},
  journal = {Theory and Applications of Categories},
  year = 1997,
  OPTkey = {},
  OPTvolume = 3,
  OPTnumber = 9,
  OPTpages = {213--250},
  WKloc = {A-1268},
  abstract = {A category may bear many monoidal structures, but (to within
      a unique isomorphism) only one structure of ``category with finite
      products''. To capture such distinctions, we consider on a 2-category
      those 2-monads for which algebra structure is essentially unique if
      it exists, giving a precise mathematical definition of ``essentially
      unique'' and investigating its consequences. We call such 2-monads
      \emph{property-like}. We further consider the more restricted class
      of \emph{fully property-like} 2-monads, consisting of those
      property-like 2-monads for which all 2-cells between (even lax)
      algebra morphisms are algebra 2-cells. The consideration of lax
      morphisms leads us to a new characterization of these monads, studied
      by Kock and Z\"oberlein, for which ``structure is adjoint to unit'',
      and which we now call \emph{lax-idempotent} 2-monads: both these and
      their \emph{colax-idempotent} duals are fully property-like. We end
      by showing that (at least for finitary 2-monads) the classes of
      property-likes, fully property-likes, and lax-idempotents are each
      coreflective among all 2-monads.},
  OPTannote = {}
}

@Article{Kelly-Lack-Walters-1993,
  author = {G. M. Kelly and S. Lack and R.F.C. Walters},
  title = {Coinverters in categories with structure},
  journal = {Applied Categorical Structures},
  year = 1993,
  pages = {95-102},
  volume = 1
}

@Article{Kelly-Laplaza-1980,
  author = {G. M. Kelly and M. L. Laplaza},
  title = {Coherence for Compact Closed Categories},
  journal = {Journal of Pure and Applied Algebra},
  volume = 19,
  year = 1980,
  pages = {193--213}
}

@InProceedings{Kelly-Street-1974,
  author = {G. M. Kelly and R. H. Street},
  title = {Review of the elements of 2-categories},
  booktitle = {Lecture Notes in Mathematics},
  volume = 420,
  pages = {75--103},
  publisher = {Springer-Verlag},
  year = 1974,
  WKloc = {A-0406}
}

@TechReport{Kelsen-1993,
  author = {P. Kelsen},
  title = {Efficient Computation of Extremal Structures in
		  Graphs and Hypergraphs},
  institution = {University of Illinois Urbana-Champaign, Department
		  of Computer Science},
  year = 1993,
  type = {Report UIUCDCS},
  number = {93-1810},
  annote = {tubibmue}
}

@InProceedings{Kelsey-Hudak-1989,
  author = {Richard Kelsey and Paul Hudak},
  title = {Realistic Compilation by Program Transformation ---
		  Detailed Summary},
  crossref = {POPL1989},
  DOIURL = {http://dx.doi.org/10.1145/75277.75302},
  DOI = {10.1145/75277.75302},
  pages = {281--292},
  WKloc = {A-0209},
  abstract = {Using concepts from denotational semantics, we have
		  produced a very simple compiler that can be used to
		  compile standard programming languages and produces
		  object code as efficient as that of production
		  compilers. The compiler is based entirely on
		  source-to-source transformations performed on
		  programs that have been translated into an
		  intermediate language resembling the lambda
		  calculus. The output of the compiler, while still in
		  the intermediate language, can be trivially
		  translated into machine code for the target machine.
		  The compilation by transformation strategy is
		  simple: the goal is to remove any dependencies on
		  the intermediate language semantics that the target
		  machine cannot implement directly. Front-ends have
		  been written for Pascal, BASIC, and Scheme and the
		  compiler produces code for the MC68020 microprocessor.},
  annote = {--- HOPSnotes ---}
}

@Article{Kempe-1886,
  author = {A. B. Kempe},
  title = {Theory of Mathematical Forms},
  journal = PHILLON,
  year = 1886,
  pages = {1--70},
  bibliographies = {RelMiCS}
}

@InProceedings{Kempf-1992,
  abstract = {The universal properties of parallelism, synchronity
		  and sequentiality of data driven evaluation are
		  established by category theoretic means. They can be
		  used in the design of functional languages for
		  parallel programming},
  title = {Categorical Description of Parallelism, Synchronity
		  and Sequentiality},
  pages = {328--342},
  crossref = {Steinfurt92},
  author = {Peter Kempf}
}

@InProceedings{Kempf-1992x,
  abstract = {The universal properties of parallelism, synchronity
		  and sequentiality of data driven evaluation are
		  established by category theoretic means. They can be
		  used in the design of functional languages for
		  parallel programming},
  year = 1992,
  title = {Categorical Description of Parallelism, Synchronity
		  and Sequentiality},
  series = {Bericht 7/92-I},
  publisher = {Institut f\"ur Angewandte Mathematik und Informatik,
		  Universit\"at M\"unster},
  pages = {328--342},
  month = JAN,
  editor = {Wolfram-M. Lippe and Gudrun Stroot},
  booktitle = {{Proc. Workshop ``Programmiersprachen --- Methoden,
		  Semantik, Implementierungen''}},
  author = {Peter Kempf},
  address = {Landhaus Rothenberge}
}

@Unpublished{Kempf-1994a,
  author = {Peter Kempf},
  title = {{Typtheoretische Kalk\"ule: Eine \"Ubersicht}},
  note = {internal memo of UniBwM INF2},
  year = 1994,
  month = DEC,
  WKloc = {A-0382}
}

@Misc{Kempf-1997,
  OPTkey = {},
  OPTauthor = {Peter Kempf},
  OPTtitle = {Programmiersprachen II},
  OPThowpublished = {Vorlesungsskriptum},
  OPTmonth = SEP,
  OPTyear = 1997,
  OPTnote = {},
  WKloc = {B-0088}
}

@InProceedings{Kempf-Hattensperger-1997,
  author = {Peter Kempf and Claudia Hattensperger},
  title = {Towards a Formal Framework for Heterogeneous
		  Relation Algebra},
  crossref = {RelMiCS1997-PP},
  pages = {341--350}
}

@Article{Kempf-Hattensperger-1999,
  author = {Peter Kempf and Claudia Hattensperger},
  title = {Towards a Formal Framework for Heterogeneous
		  Relation Algebra},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  pages = {193--203},
  abstract = {We define a language for polymorphic typed relations and
      introduce a typing discipline for terms of that language. A modular
      type inference system for the derivation of the most general type of
      a term is presented and correctness and well as weak completeness of
      the type inference system w.r.t.\null{} the typing discipline is
      proven. Finally, we give an interpretation ot our language based on
      the classical model of relation algebra.},
  bibliographies = {RelMiCS, RelMiCS3}
}

@TechReport{Kempf-Schmidt-Winter-1994,
  year = 1994,
  title = {{Konstruktion semantischer Bereiche aus algebraischen
      Spezifikationen}},
  pages = 46,
  number = {94/04},
  institution = U_BWM,
  author = {Peter Kempf and Gunther Schmidt and Michael Winter},
  bibliographies = {RelMiCS}
}

@Article{Kennaway-1987,
  WKloc = {ThCS},
  abstract = {In 1984, Raoult has given a description of graph rewriting.
	His description is operational, despite the similarity which his
	constructions have to the category-theoretic concept of a pushout.
	We describe a modification to Raoult's description of graph rewriting
	which allows the reduction of a redex to be described as a pushout,
	in a category of graphs where morphisms are not required to preserve
	graph structure. Our description can also handle term rewrite rules
	whose right-hand sides consist of a variable. Raoult specifically
	excludes such rules from his treatment. Rules of this form include
	the K combinator, the identity function, selector functions for
	extracting components of data structures, and the conditional. We
	prove the correctness of this implementation of term
		  rewriting.},
  year = 1987,
  volume = 52,
  title = {{On ``On Graph Rewritings''}},
  pages = {37--58},
  journal = {Theoretical Computer Science},
  author = {Richard Kennaway},
  annote = {--- PLGnotes ---}
}

@Article{Kennaway-1989,
  author = {J. R. Kennaway},
  title = {Sequential Evaluation Strategies for Parallel-Or and
		  Related Reduction Systems},
  journal = {Annals of Pure and Applied Logic},
  year = 1989,
  volume = 43,
  pages = {31--56}
}

@InProceedings{Kennaway-1990,
  author = {Richard Kennaway},
  title = {Graph Rewriting in Some Categories of Partial Morphisms},
  pages = {490--504},
  crossref = {GG1990},
  keywords = {graph rewriting, partial morphism, hypergraph, term graph,
	jungle, category, double pushout, single pushout},
  contents = {1 Two Definitions of graph rewriting
	2 Hypergraphs
	3 Partial Morphisms
	4 Comparison of double- and single-pushout rewriting in P(H)
	5 Comparison of double- and single-pushout rewriting of jungles
	6 Further developments
	7 Concluding remarks
	References},
  abstract = {We present a definition of term graph rewriting as the taking
	of a pushout in a category of partial morphisms, adapting the rather
	ad hoc definitions we gave in [Ken87] so as to use a standard
	category-theoretic concept of partial morphism. This single-pushout
	construction is shown to coincide with the well-known double-pushout
	description of graph rewriting whenever the latter is defined. In
	general, the conditions for the single pushout to exist are weaker
	than those required for the double pushout. In some categories of
	graphs, no conditions at all are necessary.},
  annote = {--- PLGnotes ---},
  WKloc = {A-1190}
}

@InProceedings{Kennaway-1990a,
  author = {Richard Kennaway},
  title = {The Specificity Rule for Lazy Pattern-Matching in
		  Ambiguous Term Rewrite Systems},
  crossref = {ESOP1990},
  pages = {256--270},
  abstract = {Many functional languages based on term rewriting
		  (such as Miranda and ML) allow the programmer to
		  write ambiguous rule systems, with the understanding
		  that rules will be matched against a term in the
		  order in which the rules are written, and that the
		  pattern-matching of a rule against a term proceeds
		  from left to right.

                  This gives a precise semantics to such ambiguous
		  systems, but it has disadvantages. It depends on the
		  textual ordering of the program, whereas the
		  standard theory of term rewriting has no such
		  concept. As a result, equational reasoning is not
		  always valid for this semantics, defeating the
		  primary virtue of functional languages. The
		  semantics also fails to be fully lazy, in that
		  sometimes a non-terminating computation will be
		  performed on a term whihc has a normal form.

                  We define a rule, called {\em specifity}, for
		  computation in ambiguous term rewrite systems. This
		  rule (really a meta-rule) stipulates that a term
		  rewrite rule of the system can only be used to
		  reduce a term which matches it, if that term can
		  never match any other rule of the system which is
		  more specific than the given rule. One rule is more
		  specific than another if the left-hand side of the
		  first rule is a substitution instance of the second,
		  and the reverse is not true. Specificity captures
		  the intuition underlying the use of ambiguity in ML
		  and Miranda, while also providing lazy
		  pattern-matching.

                  A natural generalisation of the idea provides a
		  semantics for Miranda's lawful types.},
  WKloc = {doc/pap/BIB (only a draft?)}
}

@Article{Kennaway-1990b,
  author = {Richard Kennaway},
  title = {Implementing Term Rewrite Languages in {DACTL}},
  journal = {Theoretical Computer Science},
  year = 1990,
  volume = 72,
  number = 2,
  pages = {225--249},
  abstract = {Dactl is a low level language of graph rewriting,
               intended for programming highly parallel machines. The
               language includes, but is not restricted to, the limited
               form of graph rewriting which is commonly used to
               implement functional language such as Miranda, ML, Hope
               and Clean. In contrast to these functional languages,
               where the order in which subterms are evaluated (the
               evaluation strategy) is fixed for all programs, in Dactl
               the evaluation strategy is programmed explicitly. We
               define a translation of a functional language into Dactl,
               describe the problems encountered and their solution,
               and prove that the translation is correct.}
}

@InProceedings{Kennaway-1994,
  author = {R. Kennaway},
  title = {A Conflict between Call-By-Need Computation and Parallelism},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {East Anglia University}
}

@TechReport{Kennaway-Klop-Sleep-1993,
  author = {J. R. Kennaway and J. W. Klop and M. R. Sleep},
  title = {Comparing curried and uncurried rewriting},
  institution = {CWI. Department of Computer Science},
  year = 1993,
  number = {R 9350},
  address = {Amsterdam}
}

@TechReport{Kennaway-Klop-Sleep-1993a,
  author = {J. R. Kennaway and J. W. Klop and M. R. Sleep},
  title = {Event structures and orthognal term graph rewriting},
  institution = {CWI. Department of Computer Science},
  year = 1993,
  number = {R 9347},
  address = {Amsterdam}
}

@InProceedings{Kennaway-Klop-Sleep-deVries-1991,
  author = {J.R. Kennaway and J.W. Klop and M.R. Sleep and
		  de~Vries, F.J.},
  title = {Transfinite Reductions in Prthogonal Term Rewriting Systems},
  crossref = {RTA91},
  pages = {??},
  WKloc = {??},
  abstract = {??}
}

@InCollection{Kennaway-Klop-Sleep-deVries-1993a,
  author = {J.R. Kennaway and J.W. Klop and M.R. Sleep and
		  de~Vries, F.J.},
  title = {An Introduction to Term Graph Rewriting},
  pages = {1--14},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  chapter = 1,
  WKloc = {A-0273}
}

@InCollection{Kennaway-Klop-Sleep-deVries-1993b,
  author = {J.R. Kennaway and J.W. Klop and M.R. Sleep and
		  de~Vries, F.J.},
  title = {Event Structures and Orthogonal Term Graph Rweriting},
  pages = {141--156},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  chapter = 11,
  keywords = {Levy},
  contents = {1 Introduction
                  2 Term graph rewriting
                  3 Reduction Graphs
                  4 L\'evy-equivalence
                  5 Event structures
                  6 Event structures for orthogonal term graph
		  rewriting
                  7 Related work and further developments
                  8 conclusion}
}

@InCollection{Kennaway-Klop-Sleep-deVries-1993c,
  author = {J.R. Kennaway and J.W. Klop and M.R. Sleep and
		  de~Vries, F.J.},
  title = {The Adequacy of Term Graph Rewriting for Simulating Term Rewriting},
  pages = {157--170},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  chapter = 12,
  WKloc = {A-0274},
  contents = {1 Introduction
                  2 Term rewriting
                  3 Graph rewriting
                  4 Adequacy: a precise notion of simulation
                  5 Adequacy of acyclic graph rewriting for finite
		  term rewriting
                  6 Adequacy of finite graph rewriting for rational
		  term rewriting}
}

@Article{Kennaway-Klop-Sleep-deVries-1994,
  author = {Kennaway, J. R. and Klop, J. W. and Sleep, M. R. and de Vries, F. J.},
  year = 1994,
  title = {On the Adequacy of Graph Rewriting for Simulating Term Rewriting},
  journal = TOPLAS,
  volume = 16,
  number = 3,
  pages = {493--523},
  month = MAY,
  abstract = {We formalise the close correspondence between finitary cyclic graph
	rewriting and a restricted form of infinitary term rewriting, called
	rational term rewriting. This subsumes the known relation between
	finitary acyclic graph rewriting and finitary term rewriting.
	Surprisingly, the correspondence breaks down for general infinitary
	rewriting. An example is presented showing that infinitary term
	rewriting is strictly more powerful than infinitary graph rewriting. The
	study also clarifies the technical difficulties resulting from the
	combination of collapsing rewrite rules and cyclic graphs.},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Kennaway-Klop-Sleep-deVries-1995,
  author = {J.R. Kennaway and J.W. Klop and M.R. Sleep and
		  de~Vries, F.J.},
  title = {Infinitary Lambda Calculi and B\"ohm Models},
  crossref = {RTA95},
  pages = {257--270},
  WKloc = {A-0574},
  OPTabstract = {??}
}

@Article{Kennaway-Sleep-1987,
  author = {Kennaway, J. R. and Sleep, M. R.},
  title = {Variable Abstraction in $O(n \log n)$ Space},
  journal = ACTIN,
  year = 1987,
  volume = 24,
  pages = {343--349},
  WKloc = {A-1314}
}

@Article{Kennaway-deVries-vanOostrom-1999,
  author = {Kennaway, J. R. and de Vries, F. J. and van Oostrom, V.},
  year = 1999,
  title = {Meaningless Terms in Rewriting},
  journal = {Journal of Functional and Logic Programming},
  annote = {Supercedes \cite{Kennaway-vanOostrom-deVries-1996}},
  abstract = {We present an axiomatic approach to the concept of undefinedness
	in finite and transfinite term rewriting and lambda calculus,
	and show that some very simple and intuitive axioms imply
	standard properties such as genericity, and confluence modulo
	equality. Concepts of undefinedness give rise to corresponding
	concepts of B{\"o}hm tree and model. The axioms are easily verified
	for all the particular notions of undefinedness that we are aware
	of in the literature, and disproved for some notions that are known
	not to be good characterisations of meaninglessness.},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Kennaway-vanOostrom-deVries-1996,
  author = {J.R. Kennaway and van Oostrom, Vincent and de~Vries, F.J.},
  title = {Meaningless Terms in Rewriting},
  crossref = {ALP-1996},
  pages = {254--268},
  WKloc = {A-0451},
  annote = {Superceded by \cite{KennawayJR-deVries-vanOostrom-1999}}
}

@InProceedings{Kennedy-1994,
  author = {Andrew Kennedy},
  title = {Dimension Types},
  crossref = {ESOP1994},
  pages = {348--363},
  abstract = {Scientists and engineers must ensure that physical
		  equations are dimensionally consistent, but existing
		  programming languages treat all numeric values as
		  dimensionless. This paper extends a strongly-typed
		  programming language with a notion of dimension
		  type. Our approach improves on previous proposals in
		  that dimension types may be
		  polymorphic. Furthermore, any expression which is
		  typable in the system has a most general type, and
		  we describe an algorithm which infers this type
		  automatically. The algorithm exploits equational
		  unification over Abelian groups in addition to
		  ordinary term unification. An implementation of the
		  system is described, extending the ML Kit
		  compiler. Finally, we discuss the problem of
		  obtaining a canonical form for principal types and
		  sketch some more powerful systems which use
		  dependent and higher-order polymorphic types.}
}

@Misc{Kennedy-2000,
  OPTkey = {},
  author = {Ken Kennedy},
  title = {Telescoping Languages: A Compiler Strategy for Implementation of High-Level Domain-Specific Programming Systems},
  OPThowpublished = {},
  OPTmonth = {},
  year = 2000,
  URL = {http://ipdps.eece.unm.edu/2000/papers/KennedyKenTelescope.pdf},
  annote = {superseded by \cite{Kennedy-Broom-Keith-Dongarra-Fowler-Gannon-Johnsson-MellorCrummey-Torczon-2001}, but more examples},
  bibliographies = {Anand},
  WKloc = {A-1475, doc/pap/BIB},
  abstract = {As both machines and programs have become more complex,
    the programming process has become correspondingly more
    labor-intensive. This has created a software gap between the need for
    new software and the aggregate capacity of the current workforce to
    produce it. This problem has been compounded by the slow growth of
    programming productivity over the past two decades. One way to bridge
    this gap is to make it possible for end users to develop programs in
    high-level domain-specific programming systems.  The principal
    impediment to the success of these systems in the past has been the
    poor performance of the resulting applications. To address this
    problem, we are develop- ing a new compiler technology that supports
    script-based telescoping languages, which can be built from base
    languages and domain-specific libraries. By exhaustively com- piling
    the libraries in advance, we can ensure that the performance and
    portability of the applications produced by such systems are high,
    while the compile times for scripts are acceptable to the end
    user. These qualities are essential if script-based systems are to be
    practical for development of production applications.}
}

@Article{Kennedy-Broom-Keith-Dongarra-Fowler-Gannon-Johnsson-MellorCrummey-Torczon-2001,
  author = {Ken Kennedy and Bradley Broom and Keith Cooper and Jack Dongarra and Rob Fowler and Dennis Gannon and Lennart Johnsson and John Mellor-Crummey and Linda Torczon},
  title = {Telescoping Languages: A Strategy for Automatic Generation of
      Scientific Problem-Solving Systems from Annotated Libraries},
  journal = {JPDC},
  year = 2001,
  volume = 61,
  pages = {1803--1826},
  month = OCT,
  URL = {http://www.cs.rice.edu/~johnmc/papers/Telescope-JPDC.pdf},
  annote = {Sounds like a research proposal. A few more examples in \cite{Kennedy-2000}.},
  WKloc = {A-1474, doc/pap/BIB (Oct.~10, 2000 draft)},
  bibliographies = {Anand},
  abstract = {As machines and programs have become more complex, the
    process of programming applications that can exploit the power of
    high-performance systems has become more difficult and correspondingly
    more labor-intensive. This has substantially widened the software
    gap-the discrepancy between the need for new software and the
    aggregate capacity of the workforce to produce it. This problem has
    been compounded by the slow growth of programming productivity,
    especially for high-performance programs, over the past two decades.

    One way to bridge this gap is to make it possible for end users to
    develop programs in high-level domain-specific programming systems. In
    the past, a major impediment to the acceptance of such systems has
    been the poor performance of the resulting applications. To address
    this problem, we are developing a new compiler-based infrastructure,
    called MetaScript, that will make it practical to construct efficient
    script-based high-level languages from annotated component
    libraries. These languages are called telescoping languages, because
    they can be nested within one another.

    For programs written in telescoping languages, high performance and
    reasonable compilation times can be achieved by exhaustively analyzing
    the component libraries in advance to produce a language processor
    that recognizes and optimizes library operations as primitives in the
    language. The key to making this strategy practical is to keep compile
    times low by generating a custom compiler with extensive built-in
    knowledge of the underlying libraries. The goal is to achieve compile
    times that are linearly proportional to the size of the program
    presented by the user, rather than to the aggregate size of that
    program plus the base libraries.}
}

@InProceedings{Kent-1997,
  author = {S. Kent},
  title = {Constraint diagrams: visualising invariants in {OO} models},
  crossref = {OOPSLA1997},
  annote = {Constraint diagrams are an extension to UML class diagrams,
            which permit the visual representation of non-trivial constraints on objects.},
  URL = {http://www.cs.york.ac.uk/puml/publications.html},
  WKloc = {A-0994}
}

@InProceedings{Kesner-1994,
  author = {Delia Kesner},
  title = {Reasoning about Layered, Wildcard and Product Patterns},
  crossref = {ALP1994},
  pages = {253--268},
  WKloc = {A-0318},
  abstract = {We study the extensional version of the simply typed
		  $\lambda$-calculus with product types and fixpoints
		  enriched with {\em layered, wildcard} and {\em
		  product patterns}. Extensionality is expressed by
		  the surjective pairing axiom and a generalization of
		  the $\eta$-conversion to patterns. We obtain a {\em
		  confluent} reduction system by turning the
		  extensional axioms as {\em expansion} rules, and
		  then adding some restrictions to these expansions in
		  order to avoid reduction loops. Confluence is proved
		  by composition of modular properties of the
		  extensional and non-extensional subsystems of the
		  reduction calculus.},
  annote = {GFA, cites as other approaches of $\lambda$-calculi
		  with nested patterns:
		  \cite{vanOostrom-1990,PeytonJones-1987,Howard-1992,BreazuTannen-Kesner-Puel-1993}}
}

@Misc{Kessler-1994,
  author = {Marco Kessler},
  title = {Uniqueness and Lazy Graph Copying, Copyright for the Unique},
  year = 1994,
  WKloc = {A-0418}
}

@InProceedings{Kessler-1996,
  author = {Christoph W. Ke{\ss{}}ler},
  title = {Scheduling Expression DAGs for Minimal Register Need},
  crossref = {PLILP1996},
  pages = {228--242},
  OPTabstract = {},
  WKloc = {A-0446}
}

@Article{Kfoury-Tiuryn-1992,
  title = {A. J. Kfoury and J. Tiuryn},
  author = {Type Reconstruction in Finite Rank Fragments of the
		  Second-Order $\lambda$-Calculus},
  journal = {Information and Computation},
  year = 1992,
  volume = 98,
  pages = {228--257},
  number = 2,
  WKloc = {A-0275},
  abstract = {We prove that the problem of type reconstruction in
		  the polymorphic $\lambda$-calculus of rank 2 is
		  polynomial-time equivalent to the problem of type
		  reconstruction in {\bf ML}, and is therefore
		  DEXPTIME-complete. We also prove that for every $k > 2$,
		  the problem of type reconstruction in the
		  polymorphic $\lambda$-calculus of rank $k$, extended
		  with suitably chosen constants with types of rank 1,
		  is undecidable.}
}

@InProceedings{Kfoury-Wells-1999,
  author = {Assaf J. Kfoury and J. B. Wells},
  title = {Principality and Decidable Type Inference for
                  Finite-Rank Intersection Types},
  crossref = {POPL1999},
  pages = {161--174},
  COMMENT = {Do not under any circumstances add a link to a PDF file
      unless the fonts are scalable. By the way, you do not have the moral
      right to redistribute this work in a way contrary to the authors'
      wishes.},
  URL = {http://types.bu.edu/reports/Kfo+Wel:POPL-1999.html},
  POSTSCRIPT = {http://types.bu.edu/reports/electronic/Kfo+Wel:POPL-1999.ps},
  abstract = {Principality of typings is the property that for
                  each typable term, there is a typing from which all
                  other typings are obtained via some set of
                  operations. Type inference is the problem of finding
                  a typing for a given term, if possible. We define an
                  intersection type system which has principal typings
                  and types exactly the strongly normalizable
                  $\lambda$-terms. More interestingly, every
                  finite-rank restriction of this system (using
                  Leivant's first notion of rank) has principal
                  typings and also has decidable type inference. This
                  is in contrast to System~F where the finite rank
                  restriction for every finite rank at 3 and above has
                  neither principal typings nor decidable type
                  inference. This is also in contrast to earlier
                  presentations of intersection types where the status
                  (decidable or undecidable) of these properties is
                  unknown for the finite-rank restrictions at 3 and
                  above. Furthermore, the notion of principal typings
                  for our system involves only one operation,
                  substitution, rather than several operations (not
                  all substitution-based) as in earlier presentations
                  of principality for intersection types (without rank
                  restrictions). In our system the earlier notion of
                  \emph{expansion} is integrated in the form of
                  \emph{expansion variables}, which are subject to
                  substitution as are ordinary variables. A
                  unification-based type inference algorithm is
                  presented using a new form of unification,
                  $\beta$-unification.},
  CHURCHREPORT = {yes},
  WKloc = {doc/pap/BIB}
}

@Article{Khalil-Wagner-Walters-1993,
  author = {Wafaa Khalil and Eric Wagner and R. F. C. Walters},
  title = {Fix-point semantics for programs in distributive categories},
  journal = {Fundamenta Informaticae},
  year = 1993,
  month = MAY,
  URL = {ftp://ftp.maths.usyd.edu.au/sydcat/papers/walters/}
}

@Article{Khalil-Walters-1993,
  author = {W. Khalil and R.F.C. Walters},
  title = {An Imperative Language Based on Distributive Categories {II}},
  journal = RAIRO-I,
  UniBwM = {Z6878},
  year = 1993,
  WKloc = {A-1160}
}

@Article{Khalil-Walters-1993a,
  author = {Wafaa Khalil and R.F.C. Walters},
  title = {Functional processors and operations on them in extensive categories},
  OPTjournal = {},
  year = 1993,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  month = MAY,
  note = {Submitted. {\tt ftp://maths.usyd.edu.au/sydcat/papers/walters}},
  WKloc = {A-0969}
}

@InProceedings{Khasidashvili-1994,
  author = {Zurab Khasidashvili},
  title = {On Higher Order Recursive Program Schemes},
  crossref = {CAAP94},
  pages = {172--186},
  authorsAddress = {Norwich},
  WKloc = {A-0349},
  keywords = {CRS, HORS, $\lambda\sigma$, explicit substitutions},
  abstract = {We define {\em Higher Order Recursive Program
		  Schemes} (HRPSs) by allowing metasubstitutions (as
		  in the $\lambda$-calculus) in right-hand sides of
		  function and quantifier definitions. A study of
		  several kinds of {\em similarity} of redexes makes
		  it possible to lift properties of (first order)
		  Recursive Program Schemes to the higher order
		  case. The main result is the decidability of weak
		  normalization in HRPSs, which immediately implies
		  that HRPSs do not have full computational power. We
		  analyze the structural properties of HRPSs and
		  introduce several kinds of {\em persistent}
		  expression reduction systems (PERSs) that enjoy
		  similar properties. Finally, we design an optimal
		  evaluation procedure for PERSs.},
  annote = {--- TGVnotes ---}
}

@PhDThesis{Khedri-1998,
  author = {Ridha Khedri},
  title = {Concurrence, bisimulations et \'equation
                  d'interface: une approche relationelle},
  year = 1998,
  month = APR,
  school = {D\'epartment d'Informatique, Facult\'e des Sciences
                  et de Genie, Universit\'e Laval, Qu\'ebec},
  WKloc = {B-0044},
  bibliographies = {RelMiCS}
}

@TechReport{Khedri-Desharnais-1999,
  author = {Ridha Khedri and Jules Desharnais},
  title = {Concurrency: A Relational Approach},
  year = 1999,
  OPTmonth = {},
  institution = {SERG},
  OPTtype = {},
  number = 382,
  URL = {http://www.crl.mcmaster.ca/SERG/serg.publications.html},
  abstract = {We model processes by means of a mathematical entity
                  that we call a relational process. This model
                  describes a process as an open system from which the
                  description of the process as a closed system can be
                  easily obtained. Also, it represents not only the
                  actions of the process but also the resources needed
                  to accomplish its behaviour. Using this model, we
                  first define two operators. Each of these represents
                  an extreme perception of concurrency. One, the
                  interleaved parallel composition operator, reduces
                  concurrency to interleaving and the other, the
                  maximal totally synchronous parallel composition
                  operator, reduces concurrency to a totally
                  synchronous behaviour. Second, by combining these
                  operators, we define the maximal true-concurrency
                  composition operator, which is an operator
                  expressing true concurrency. When many processes
                  interfere on the same resource in order to modify
                  it, each in its way, the two maximal operators
                  express this situation by letting the final value of
                  the variable modelling this resource be
                  indeterminate. So, they allow the detection of
                  interferences between processes. We present some of
                  the properties of these operators.},
  WKloc = {A-0873},
  bibliographies = {RelMiCS}
}

@InProceedings{Khedri-WuRong-Sanga-2003,
  author = 	 {Ridha Khedri and Rong Wu and Bahati Sanga},
  title = 	 {{SCENATOR}:  A Prototype Tool for Requirements
Inconsistency Detection},
  booktitle = 	 {Proceedings of the 1st International Workshop on
Automated Technology for Verification and Analysis},
  pages = 	 {75--86},
  year = 	 {2003},
  editor = 	 {Farn Wang and Insup Lee},
  address = 	 {Taiwan, Republic of China},
  month = 	 {December 10--13},
  organization = {National Taiwan University},
  bibliographies = {RelMiCS}
}

@InProceedings{Khoshnevisan-Afshar-1994,
  author = {Hessam Khoshnevisan and Mohamad Afshar},
  title = {Mechanical Elimination of Commutative Redundancy},
  crossref = {SAS94},
  authorsAddress = {Imperial College, University of Cambridge}
}

@InProceedings{Kiczales-Lamping-MendhekarMaeda-Lopes-Loingtier-Irwin-1997,
  author = {Gregor Kiczales and John Lamping and Anurag Mendhekar and Chris Maeda and Christina Lopes and Jean-Marc Loingtier and John Irwin},
  title = {Aspect-Oriented Programming},
  pages = {220--242},
  DOI = {10.1007/BFb0053381},
  DOIURL = {http://dx.doi.org/10.1007/BFb0053381},
  WKloc = {A-0721},
  booktitle={{ECOOP'97} --- Object-Oriented Programming},
  OPTbooktitle = {Proc. {ECOOP '97, Jyvaskyla, Finland, June 1997}},
  editor = {M. Aksit and S. Matsuoka},
  publisher = Springer,
  series = LNCS,
  volume = 1241,
  year = 1997
}

@Article{Kieboom-Pop-1992,
  author = {Kieboom, R.W. and Pop, I.},
  title = {On approximate weak fibrations},
  journal = {Bulletin de la Societe mathematique de Belgique},
  year = 1992,
  volume = 44,
  number = 2,
  pages = {215--}
}

@Misc{Kieburtz-2002,
  author = {Richard B. Kieburtz},
  title = {P-logic: property verification for {Haskell} programs},
  howpublished = {URL: ftp://cse.ogi.edu/pub/pacsoft/papers/Abstracts.html#patterns},
  month = FEB,
  year = 2002,
  WKloc = {A-1714, doc/pap/BIB},
  abstract = {Proof-supported logical verification of program properties
      has been a topic of research interest for more than 30 years. The
      feasibility of proof construction as a verification technique has
      been demonstrated through many examples of its application, yet it
      remains a technique rarely used in practice for a variety of reasons,
      both technical and sociological. The lack of verification logics for
      modern programming languages remains a strong deterrent to the use of
      proof-supported verification.

      This paper introduces P-logic, a verification logic for Haskell.
      P-logic is a modal mu-calculus that supports direct expression of
      recursively-defined properties of complex data structures. The term
      language of P-logic is Haskell. Logical assertions expressed in
      P-logic can be interleaved among definitions in a Haskell program
      text and can incorporate term variables bound in the program context.
      Properties of finite and infinite data structures can be expressed.

      The paper describes syntax, proof rules, and semantics of P-logic and
      provides a few examples of its use.}
}

@Misc{Kieburtz-2003a,
  author = {Richard B. Kieburtz},
  title =	 {A Logic for {Haskell}},
  howpublished = {slides},
  month =	 {April},
  year =	 2003,
  note =	 {updated from September 25, 2001 version.},
  WKloc = 	 {A-1715}
}

@Misc{Kienle-1997,
  author = {Holger M. Kienle},
  title = {The {SUIF 2.0} Compiler System},
  year = 1997,
  month = DEC,
  WKloc = {A-0470}
}

@Article{Kiesel-Schuerr-Westfechtel-1995,
  author = {Norbert Kiesel and Andy Schuerr and Bernhard Westfechtel},
  title = {{GRAS}, A Graph-Oriented (Software) Engineering Database System},
  journal = {information Systems},
  year = 1995,
  volume = 20,
  number = 1,
  annote = {B-0078}
}

@PhDThesis{Kilpelaeinen-1992,
  year = 1992,
  title = {Tree Matching Problems with Applications to Structured Text
           Databases},
  school = {University of Helsinki, Dept. of Comp. Science},
  month = {November},
  author = {P. Kilpel\"ainen}
}

@InProceedings{Kilpelaeinen-Mannila-1991,
  title = {The Tree Inclusion Problem},
  crossref = {CAAP91},
  pages = {202-214},
  author = {P. Kilpel\"ainen and H. Mannila}
}

@InProceedings{Kilpelaeinen-Mannila-1991-x,
  title = {The Tree Inclusion Problem},
  pages = {202--214},
  crossref = {TAPSOFT1991},
  author = {P. Kilpel\"ainen and H. Mannila}
}

@TechReport{Kilpelaeinen-Mannila-1991b,
  year = 1991,
  type = {Report},
  title = {Ordered and Unordered Tree Inclusion},
  number = {A-1991-4},
  note = {To appear in {\em SIAM Journal on Computing}.},
  month = {August},
  institution = {University of Helsinki, Dept. of Comp. Science},
  author = {P. Kilpel\"ainen and H. Mannila}
}

@InProceedings{Kilpelaeinen-Mannila-1992,
  author = {P. Kilpel\"ainen and H. Mannila},
  title = {Grammatical Tree Matching},
  pages = {162--174},
  crossref = {CPM92},
  WKloc = {A-0153},
  abstract = {In structured text databases documents are
		  represented as parse trees, and different tree
		  matching notions can be used as primitives for query
		  languages. Two useful notions of tree matching, {\em
		  tree inclusion} and {\em tree pattern matching} both
		  seem to require superlinear time. In this paper we
		  give a general sufficient condition for a tree
		  matching problem to be solvable in linear time, and
		  apply it to tree pattern matching and tree
		  inclusion. The application is based on the notion of
		  a {\em nonperiodic} parse tree. We argue that most
		  text documents can be modeled in a ntural way using
		  grammars yielding nonperiodic parse trees. We show
		  how the knowledge that the target tree is
		  nonperiodic can be used to obtain linear time
		  algorithms for the tree matching problems. We also
		  discuss the preprocessing of patterns for
		  grammatical tree matching.}
}

@Article{ KimTaeho-ChaSungdeok-2001,
  author = {Taeho Kim and Sungdeok Cha},
  title = {Automated structural analysis of {SCR}-style software requirements specifications using {PVS}},
  journal = {Software Testing, Verification and Reliability},
  volume = {11},
  number = {3},
  pages = {143--163},
  year = {2001},
  CiteSeer = {citeseer.ist.psu.edu/610508.html},
  bibliographies = {Tables},
  LibAccess = {http://resolver.scholarsportal.info.libaccess.lib.mcmaster.ca/resolve/09600833/v11i0003/143_asaossrsup&form=pdf&file=file.pdf},
  WKloc = {doc/pap/BIB},
  DOI = {http://dx.doi.org/10.1002/stvr.218},
  abstract = {The importance of effective requirements analysis
                  techniques cannot be overemphasized when developing
                  software requiring high levels of
                  assurance. Requirements analysis can be largely
                  classified as either structural or functional. The
                  former investigates whether definitions and uses of
                  variables and functions are consistent, while the
                  latter addresses whether requirements accurately
                  reflect users' needs. Verification of structural
                  properties for large and complex software
                  requirements is often repetitive, especially if
                  requirements are subject to frequent changes. While
                  inspection has been successfully applied to many
                  industrial applications, the authors found
                  inspection to be ineffective when reviewing
                  requirements to find errors violating structural
                  properties. Moreover, current tools used in
                  requirements engineering provide only limited
                  support in automatically enforcing structural
                  correctness of the requirements. Such experience has
                  motivated research to automate straightforward but
                  tedious activities.

                  This paper demonstrates that a
                  theorem prover, PVS (Prototype Verification System),
                  is useful in automatically verifying structural
                  correctness of software requirements specifications
                  written in SCR (Software Cost
                  Reduction)-style. Requirements are automatically
                  translated into a semantically equivalent PVS
                  specification. Users need not be experts in formal
                  methods or power users of PVS. Structural properties
                  to be proved are expressed in PVS theorems, and the
                  PVS proof commands are used to carry out the proof
                  automatically. Since these properties are
                  application independent, the same verification
                  procedure can be applied to requirements of various
                  software systems.}
}

@incollection{Kindler-1997,
  author={Kindler, Ekkart},
  title={A compositional partial order semantics for {Petri} net components},
  year={1997},
  isbn={978-3-540-63139-2},
  booktitle={Application and Theory of Petri Nets 1997},
  volume={1248},
  series=LNCS,
  editor={Az{\'e}ma, Pierre and Balbo, Gianfranco},
  title={A compositional partial order semantics for Petri net components},
  DOIURL={http://dx.doi.org/10.1007/3-540-63139-9_39},
  DOI={10.1007/3-540-63139-9_39},
  publisher={Springer Berlin Heidelberg},
  keywords={Petri net component; compositional semantics; rely-guarantee specification; partial order semantics; fully abstract},
  pages={235-252},
  abstract = {In this paper we introduce the concept of a Petri net
                  component and show how systems can be composed from
                  components. A component communicates with its
                  environment via distinguished input and output
                  places, which formalizes communication by message
                  passing. Then, we present a compositional semantics
                  for components. The semantics is an extension of
                  processes for place/transition systems (partial
                  order semantics). We show that the semantics is
                  fully abstract with respect to the behaviour of
                  closed components (essentially, processes of
                  place/transition systems). A main feature of the
                  compositional semantics is that composition of
                  components corresponds to conjunction. This feature
                  makes the compositional semantics applicable in
                  combination with a temporal logic, which then allows
                  to reason about systems in a compositional way. This
                  is demonstrated by help of a simple temporal logic.}
}

@TechReport{King-1990,
  author = {Steve King},
  title = {A refinement calculus case study},
  year = 1990,
  number = {PRG-TR-7-90},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0084},
  keywords = {ZED},
  abstract = {The reader is introduced to Morgan's Refinement
		  Calculus notation, by means of a simple case study.
		  This example is taken all the way from an abstract
		  specification down to a program in Dijkstra's
		  language of guarded commands, using the laws of the
		  refinement calculus to justify each step. This
		  program is then transliterated into Pascal},
  bibliographies = {RelMiCS}
}

@Misc{King-1990a,
  author = {Paul King},
  title = {Printing Z and Object-Z \LaTeX{} documents},
  year = 1990,
  WKloc = {A-0670}
}

@InProceedings{King-1990b,
  author = {Steve King},
  title = {Z and the Refinement Calculus},
  crossref = {VDM1990},
  pages = {164--188},
  WKloc = {A-1335},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings{King-1994,
  author = {Andy King},
  title = {A Synergisitic Analysis for Sharing and Groundness
		  which Traces Linearity},
  crossref = {ESOP1994},
  pages = {363--378},
  abstract = {Accurate variable sharing information is crucial
		  both in the automatic parallelisation and in the
		  optimisation of sequential logic programs. $\ldots$}
}

@PhDThesis{King-1996,
  author = {David Jonathan King},
  title = {Functional Programming and Graph Algorithms},
  school = {University of Glasgow, Department of Computer Science},
  year = 1996,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  month = MAR,
  OPTnote = {},
  OPTannote = {},
  WKloc = {A-0787}
}

@Unpublished{King-Launchbury-1994,
  author = {David J. King and John Launchbury},
  title = {Lazy Depth-First Search and Linear Graph Algorithms in {Haskell}},
  abstract = {Depth-first search is the key to a wide variety of graph
             algorithms. In this paper we explore the implementation of
             depth first search in a lazy functional language. For the first
             time in such languages we obtain a linear-time implementation.
             But we go further. Unlike traditional imperative presentations,
             algorithms are constructed from individual components, which
             may be reused to create new algorithms. Furthermore, the style
             of program is quite amenable to formal proof, which we
             exemplify through a calculational-style proof of a
             strongly-connected components algorithm.},
  note = {submitted to WG '94 without success},
  annote = {Remarks to authors:
           p.2, col. 2, line 20: list if -> list of
           p.6, sect. 5.1 line -2: prune -> \{\tt prune\}

         Summary:
           For the first time a linear-time
           implementation of depth-first search in graphs is given in a lazy
           functional languages. (The constant factor incurred by the
           functional overhead is determined empirically and should be
           acceptable in general.) Building on recent advances in functional
           programming, the authors construct the depth-first forest as a
           value in linear time, thus achieving a shift from dynamic
           reasoning about the depth-first search process to static
           reasoning about this forest. As an example of the advantages of
           this approach, a very compact, calculational proof of a well
           known connected-components algorithm is given.

           The paper
           exposes its ideas very clearly, presents complete formal
           arguments without overburdening the main text and provides
           intuitive illustrations and illuminating examples. It opens the
           formal clarity and modular design capabilities of pure functional
           programming to implementors of graph algorithms by showing ways
           to achieve the same complexity as in imperative implementations.},
  year = 1994,
  WKloc = {A-0229}
}

@InProceedings{King-Launchbury-1995,
  author = {David J. King and John Launchbury},
  title = {Structuring Depth-First Search Algorithms in {Haskell}},
  crossref = {POPL1995},
  pages = {344--354},
  DOIURL = {http://doi.acm.org/10.1145/199448.199530},
  OPTdoi = {10.1145/199448.199530},
  abstract = { Depth-first search is the key to a wide variety of
     graph algorithms. In this paper we express depth-first search in
     a lazy functional language, obtaining a linear-time
     implementation. Unlike traditional imperative presentations, we
     use the structuring methods of functional languages to construct
     algorithms from individual reusable components. This style of
     algorithm construction turns out to be quite amenable to formal
     proof, which we exemplify through a calculational-style proof of
     a far from obvious strongly-connected components algorithm.},
  WKloc = {A-0718}
}

@InProceedings{King-Wadler-1993,
  author = {David J. King and Philip Wadler},
  title = {Combining Monads},
  booktitle = {{Functional Programming, Glasgow 1992, Ayr, Scotland}},
  editor = {J. Launchbury and P. M. Sansom},
  series = {Workshops in Computing},
  year = 1993,
  publisher = {Springer Verlag},
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0587}
}

@Article{Kingston-1993_lout.design,
  author = {Jeffrey H. Kingston},
  title = {The Design and Implementation of the {Lout} Document Formatting
      Language},
  journal = {Software---Practice and Experience},
  year = 1993,
  volume = 23,
  pages = {1001--1041},
  WKloc = {A-1181}
}

@Book{Kingston-1995_lout.user,
  author = {Jeffrey Howard Kingston},
  title = {A User's Guide to the Lout Document Formatting System (Version 3)},
  publisher = {Basser Department of Computer Science, University of Sydney},
  year = 1995,
  note = {System available from {\tt ftp.cs.su.oz.au:/jeff/lout}}
}

@Misc{Kingston-2002,
  author = {Jeffrey Howard Kingston},
  title = {Prospectus for {Nonpareil}},
  howpublished = {draft},
  URL = {http://www.cs.usyd.edu.au/~jeff/nonpareil/},
  year = 2002,
  note = {http://www.cs.usyd.edu.au/~jeff/nonpareil/},
  WKloc = {A-1396}
}

@Article{Kinoshita-1999,
  author = {Yoshiki Kinoshita},
  title = {{Yoneda} Replaces {Knuth-Bendix} in the Case of Monoid },
  journal = {Computer Software},
  year = 1999,
  volume = 16,
  number = 2,
  pages = {72--75},
  UniBwM = {INF/Z},
  OPTwkloc = {A-09},
  abstract = {It is well-known that there exists a normalizing function
                  which converts a term of the equational theory of monoids
                  to its normal form, and that the completion method due to
                  Knuth and Bendix is effective. Byhn and Dybjer gave another
                  normalizing algorithm without using the idea of completion.
                  At a first glance, their method looks ad hoc.
                  On the contrary, we show that the
                  bicategorical Yoneda embedding and its strictification
                  give a systematic explanation of their algorithm.}
}

@InProceedings{Kinoshita-OHearn-Power-Takeyama-Tennent-1996,
  author = {Y. Kinoshita and O'Hearn, P.W. and A.J. Power and M. Takeyama and R.D. Tennent},
  title = {Axiomatics of logical relations and data refinement},
  crossref = {TACS1997},
  pages = {191--212},
  WKloc = {A-0872},
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Kinoshita-Power-1996,
  author = {Y. Kinoshita and J. Power},
  title = {Data refinement and algebraic structure},
  year = 1996,
  institution = {ETL},
  number = {TR96-2},
  OPTaddress = {},
  OPTwkloc = {A-08}
}

@Misc{Kinoshita-Power-199X,
  author = {Y. Kinoshita and J. Power},
  title = {Data refinement for Call-By-Value Programming Languages},
  year = {199?},
  OPTabstract = {},
  WKloc = {A-0899}
}

@Article{Kinoshita-Power-2000,
  author={Kinoshita, Yoshiki and Power, John},
  title={Data refinement and algebraic structure},
  pages={693--719},
  issn={0001-5903},
  journal={Acta Informatica},
  year={2000},
  volume={36},
  number={9--10},
  DOI={10.1007/s002360050171},
  DOIURL={http://dx.doi.org/10.1007/s002360050171},
  publisher = Springer,
  WKloc = {doc/pap/BIB},
  abstract = {We recall Hoare's formulation of data refinement
    in terms of upward, downward and total simulations
    between locally ordered functors
    from the structured locally ordered category
    generated by a programming language with an abstract data type
    to a semantic locally ordered category:
    we use a simple imperative language with a data type for stacks
    as leading example.
    We give a unified category theoretic account
    of the sort of structures on a category
    that allow upward simulation to extend from ground types and ground programs
    to all types and programs of the language.
    This answers a question of Hoare
    about the category theory underlying his constructions.
    It involves a careful study of algebraic structure
    on the category of small locally ordered categories,
    and a new definition of sketch of such structure.
    This is accompanied by a range of detailed examples.
    We extend that analysis to total simulations
    for modelling constructors of mixed variance such as higher order types.}
}

@InProceedings{Kinoshita-Takahashi-1996,
  author = {Yoshiki Kinoshita and Koichi Takahashi},
  title = {Proving Through Commutative Diagrams},
  pages = {107-116},
  OPTabstract = {},
  OPTurl = {},
  WKloc = {A-0928},
  bibliographies = {RelMiCS, GraphCalc},
  year = 1996,
  month = JUL,
  booktitle = {Proceedings of the Second Conference on
     Information-Theoretic Approach to Logic, Language, and Computation},
  OPTeditor = {},
  OPTpublisher = {},
  OPTseries = {},
  OPTvolume = {},
  note = {Also available as ETL Technical Report, TR96-3}
}

@Article{Kinoshita-Takahashi-200X,
  author = 	 {Yoshiki Kinoshita and Koichi Takahashi},
  title = 	 {Cumulatives for Safety},
  journal = 	 ENTCS,
  year = 	 {200?},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  WKloc = 	 {A-1552, doc/pap/BIB},
  note = 	 {to appear},
  abstract = {We introduce a proof method for safety properties in
     the setting of refinement of imperative programs \`a la
     Hoare-He-Sanders and Kinoshita-Power. In order to show that the
     safety property of the abstract programs implies the
     corresponding safety property of the concrete program, we
     introduce a notion of `semantic cumulative'. This leads to a
     hybrid method of verification which consists of model checking
     and theorem proving. To show the safety of the concrete program,
     our main result says that one has only to show the safety of the
     abstract program, and that can be done by the model checking
     technique, thanks to the extraordinary reduction of the number of
     states. Our main theorem, in turn, can be shown by a theorem
     prover, if one wants a machine checked verification. As an
     example, we report our validation of on-the-fly garbage
     collection using the proposed method.}
}

@Unpublished{Kinoshita-Watanabe-2000a,
  author = {Yoshiki Kinoshita and Hiroshi Watanabe},
  title = {Functorial Approach to Refinement},
  note = {to appear in a volume of Electronic Notes in Computer
	    Science, Elsevier},
  year = 2001,
  WKloc = {A-1111}
}

@InProceedings{Kiselyov-,
  author = 	 {Oleg Kiselyov},
  title = 	 {Number-Parameterized Types},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1646, doc/pap/BIB}
}

@Misc{Kiselyov-2006_restricted,
  author =    {Oleg Kiselyov},
  title =     {Restricted Data Types Now},
  howpublished = {Haskell mailing list message},
  OPTmessageId = {$<$\textsf{20060208080623.E3E47A9D0@Adric.metnet.navy.mil}$>$},
  month =     FEB,
  year =      2006,
  note =      {Available at \url{http://okmij.org/ftp/Haskell/types.html\#restricted-datatypes}}
}

@Misc{ Kiselyov-Laemmel-2005,
	author = {Oleg Kiselyov and Ralf L{\"a}mmel},
	title = {{Haskell}'s overlooked object system},
	year = 2005,
	howpublished = {Draft, submitted for journal publication.
                  Online since 30 Sept.\null{} 2004; full version released 10
                  Sept.\null{} 2005,
                  \url{http://homepages.cwi.nl/\~{}ralf/OOHaskell/}},
  URL = {http://homepages.cwi.nl/~ralf/OOHaskell/},
  OPTnote = {(last accessed 19 Dec.~2008)}
}

@InProceedings{Kiselyov-Laemmel-Schupke-2004,
  author = 	 {Oleg Kiselyov and Ralf L{\"a}mmel and Keean Schupke},
  title = 	 {Strongly Typed Heterogeneous Collections},
  crossref =     {Haskell2004},
  OPTpages = 	 {},
  WKloc = 	 {A-1725 (longer version 2004-08-26), doc/pap/BIB},
  keywords = 	 {HList},
  bibliographies = {FP, Coconut}
}

@InProceedings{Kiselyov-ShanChungchie-2004,
  author = {Oleg Kiselyov and Chung-chieh Shan},
  title = {Functional pearl: implicit configurations --- or, type classes reflect the values of types},
  pages = {33--44},
  crossref = {Haskell2004},
  annote = 	 {Cites \cite{Kahl-Scheffczyk-2001}},
  WKloc = 	 {A-1545},
  abstract = { The configurations problem is to propagate run-time
     preferences throughout a program, allowing multiple concurrent
     configuration sets to coexist safely under statically guaranteed
     separation. This problem is common in all software systems, but
     particularly acute in Haskell, where currently the most popular
     solution relies on unsafe operations and compiler pragmas.We
     solve the configurations problem in Haskell using only stable and
     widely implemented language features like the type-class
     system. In our approach, a term expression can refer to run-time
     configuration parameters as if they were compile-time constants
     in global scope. Besides supporting such intuitive term notation
     and statically guaranteeing separation, our solution also helps
     improve the program's performance by transparently dispatching to
     specialized code at run-time. We can propagate any type of
     configuration data-numbers, strings, IO actions, polymorphic
     functions, closures, and abstract data types. No previous
     approach to propagating configurations implicitly in any language
     provides the same static separation guarantees.The enabling
     technique behind our solution is to propagate values via types,
     with the help of polymorphic recursion and higher-rank
     polymorphism. The technique essentially emulates local type-class
     instance declarations while preserving coherence. Configuration
     parameters are propagated throughout the code implicitly as part
     of type inference rather than explicitly by the programmer. Our
     technique can be regarded as a portable, coherent, and intuitive
     alternative to implicit parameters. It motivates adding local
     instances to Haskell, with a restriction that salvages principal
     types.}
}

@InProceedings{Kiselyov-ShanChungchie-2007,
  author = {Oleg Kiselyov and Chung-Chieh Shan},
  title = 	 {Lightweight static resources, for safe embedded and systems programming},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {Trends in Functional Programming 2007},
  OPTpages = 	 {},
  year = 	 {2007},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  URL = 	 {http://okmij.org/ftp/Haskell/types.html#ls-resources},
  WKloc = 	 {doc/pap/BIB}
}

@InProceedings{Kiselyov-Sabry-ShanChungchieh-Friedman-2005,
  author = 	 {Oleg Kiselyov and Shan, Chung-chie and Daniel P. Friedman and Amr Sabry},
  title = 	 {Backtracking, Interleaving, and Terminating Monad Transformers},
  crossref =  {ICFP2005},
  pages = 	 {192--203},
  WKloc = {A-1614, doc/pap/BIB},
  annote = 	 {LogicT}
}

@InProceedings{Kiselyov-Swadi-Taha-2004,
  author = 	 {Oleg Kiselyov and Kedar N. Swadi and Walid Taha},
  title = 	 {A Methodology for Generating Verified Combinatorial Circuits},
  booktitle = {{EMSOFT'04, September 27­29, 2004, Pisa, Italy}},
  year = 	 2004,
  WKloc = {A-1598, doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = {High-level programming languages offer significant
      expressivity but provide little or no guarantees about resource
      use. Resource-bounded languages -- such as hardware-description
      languages -- provide strong guarantees about the runtime
      behavior of computations but often lack mechanisms that allow
      programmers to write more structured, modular, and reusable
      programs. To overcome this basic tension in language design,
      recent work advocated the use of Resource-aware Programming
      (RAP) languages, which take into account the natural distinction
      between the development platform and the deployment platform for
      resource-constrained software.

      This paper investigates the use of RAP languages for the
      generation of combinatorial circuits. The key challenge that we
      encounter is that the RAP approach does not safely admit a
      mechanism to express a posteriori (post-generation)
      optimizations. The paper proposes and studies the use of
      abstract interpretation to overcome this problem. The approach
      is illustrated using an in-depth analysis of the Fast Fourier
      Transform (FFT). The generated computations are comparable to
      those generated by FFTW.}
}

@Misc{Kiss-Valeriote-1993,
  author = {Emil W. Kiss and Matthew A. Valeriote},
  title = {Abelian Algebras and the {Hamiltonian} Property},
  OPThowpublished = {},
  OPTmonth = {},
  year = 1993,
  OPTnote = {},
  WKloc = {A-1220, doc/pap/BIB}
}

@Misc{Kiss-Valeriote-199X,
  author = {Emil W. Kiss and Matthew A. Valeriote},
  title = {Strongly Abelian Varieties and the {Hamiltonian} Property},
  OPThowpublished = {},
  OPTmonth = {},
  year = {199?},
  OPTnote = {},
  WKloc = {A-1273, doc/pap/BIB}
}

@InProceedings{Klarlund-Schwartzbach-1993,
  author = {Nils Klarlund and Michael I. Schwartzbach},
  title = {Graph Types},
  pages = {196--205},
  abstract = {Recursive data structures are abstractions of simple records
             and pointers. They impose a shape invariant, which is verified
             at compile-time and exploited to automatically generate code
             for building, copying, comparing, and traversing values without
             loss of efficiency. However, such values are always tree
             shaped, which is a major obstacle to practical use. We propose
             a notion of graph types, which allow common shapes, such as
             doubly-linked lists or threaded trees, to be expressed
             concisely and efficiently. We define regular languages of
             routing expressions to specify relative addresses of extra
             pointers in a canonical spanning tree. An efficient algorithm
             for computing such addresses is developed. We employ a second
             order monadic logic to decide well-formedness of graph type
             specifications. This logic can also be used for automated
             reasoning about pointer structures.},
  crossref = {POPL1993},
  WKloc = {A-0196}
}

@InProceedings{Klarlund-Schwartzbach-1994,
  author = {Nils Klarlund and Michael I. Schwartzbach},
  title = {Graphs and Decidable Transductions based on Edge Constraints},
  crossref = {CAAP94},
  pages = {187--201},
  authorsAddress = {Aarhus},
  WKloc = {A-0348},
  abstract = {We give examples to show that not even {\bf
		  c-edNCE}, the most general known notion of
		  context-free graph grammar, is suited for the
		  specification of some common data structures.

                  To overcome this problem, we use monadic
		  second-order logic and introduce {\em edge
		  constraints} as a new means of specifying a large
		  class of graph families. Our notion stems from a
		  natural dichotomy found in programming practice
		  between ordinary pointers forming spanning trees and
		  auxiliary ponters cutting across.

                  Our main result is that for certain transformations
		  of graphs definable in monadic second-order logic,
		  the question whether a graph family geven by a
		  specification ${\cal A}$ is mapped to a family given
		  by a specification ${\cal B}$ is decidable. Thus a
		  decidable Hoare logic arises.}
}

@Misc{Klarlund-Schwartzbach-199X,
  author = {Nils Klarlund and Michael I. Schwartzbach},
  title = {A Domain-Specific Language for Regular Sets of Strings and Trees},
  year = {199X},
  WKloc = {A-0510}
}

@InCollection{Kleene-1956,
  author = 	 {S. C. Kleene},
  title = 	 {Representation of Events in Nerve Nets and Finite Automata},
  crossref =	 {Shannon-McCarthy-1956},
  pages =	 {3--41},
  WKloc = {A-1558}
}

@PhDThesis{Kleyn-1995,
  author = {Michiel Florian Eugene Kleyn},
  title = {A High Level Language For Specifying Graph-Based Languages And Their Programming Environments},
  school = {University of Texas at Austin},
  year = 1995,
  month = AUG,
  WKloc = {A-1051, doc/pap/BIB},
  abstract = {This dissertation addresses the problem of creating
      interactive graphical programming environments for visual programming
      languages that are based on directed graph models of computation.
      Such programming environments are essential to using these languages
      but their complexity makes them difficult and time consuming to
      construct. The dissertation describes a high level specification
      language, Glide, for defining integrated graphical/textual
      programming environments for such languages. It also describes the
      design of a translation system, Glider, which generates an executable
      representation from specifications in the Glide language. Glider is a
      programming environment generator; it automates the task of creating
      the programming environments used for developing programs in
      graph-based visual ...}
}

@InProceedings{Kleyn-Brown-1994,
  author = {M.F. Kleyn and J.C. Brown},
  title = {A high level language for specifying graph based languages and their programming environments},
  booktitle = {Proceedings 15th International Conference on Software Engineering, {ICSE '93, Baltimore, Maryland, 1993}},
  pages = {324--335},
  year = 1994,
  WKloc = {A-1052, doc/pap/BIB (Draft)},
  abstract = {This paper describes a high level language for specifying
       programming environments for programming languages that are based
       on directed attributed graphs. The high level language allows the
       specifier to describe views of portions of a program written in such
       a graph-based language, the editing operations used to create the
       program, animations of the execution of the program, and sufficient
       detail of the execution semantics to support the animations.
       We demonstrate the use of the specification language with two
       simple examples of graph-based languages: Petri Nets, and an
       extension of Petri Nets which includes the ability to
       nest nets hierarchically. We further describe how to generate
       the programming environment for graph-based languages from
       descriptions made in the specification... }
}

@Article{Klimov-1998,
  author = {Andrei Klimov},
  title = {Program Specialization vs. Program Composition},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 3},
  WKloc = {A-0902, 13--16},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Book{Kloesel-1986,
  title = {Writings of Charles S.\null{} Peirce: A Chronological Edition,
      volume 4, 1879-1884.},
  publisher = {Indiana Univ.\null{} Press},
  year = 1986,
  editor = {Kloesel, Christian},
  address = {Bloomington, IN,},
  bibliographies = {RelMiCS}
}

@TechReport{Klop-1980,
  author = {Jan Willem Klop},
  title = {Combinatory Reduction Systems},
  year = 1980,
  type = {Mathematical Centre Tracts},
  number = 127,
  note = {PhD thesis},
  institution = {Centre for Mathematics and Computer Science},
  annote = {according to \cite{Laneve-Montanari-1992}: $\lambda$
		  calculus is not a term rewriting system but a
		  Combinatory Reduction System.
                  (Computing Reviews 22 (1981) 10, 38,497, R. Statman: A-0030)},
  address = {Amsterdam},
  WKloc = {Q-001},
  bibliographies = {FP}
}

@InCollection{Klop-1992,
  author = {Jan Willem Klop},
  title = {Term Rewriting Systems},
  pages = {1--116},
  crossref = {HBLCS-II},
  WKloc = {A-1357},
  bibliographies = {FP}
}

@Article{Klop-vanOostrom-vanRaamsdonk-1993,
  author = {Klop, Jan Willem and van Oostrom, Vincent and van Raamsdonk, Femke},
  title = {Combinatory reduction systems: introduction and survey},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Theoretical Computer Science},
  year = 1993,
  volume = 121,
  number = {1-2},
  pages = {279--308},
  month = DEC,
  WKloc = {A-0414},
  OPTabstract = {},
  keywords = {CRS, CRSs, HRS, HORS},
  OPTannote = {},
  bibliographies = {FP}
}

@TechReport{Klop-vanOostrom-vanRaamsdonk-1993a,
  author = {J. W. Klop and van Oostrom, V. and van Raamsdonk, F.},
  title = {Combinatory reduction systems: introduction and survey},
  institution = {VU Amsterdam. Subfaculteit Wiskunde en Informatica},
  year = 1993,
  number = {TR 327},
  address = {Amsterdam}
}

@TechReport{Klop-vanOostrom-vanRaamsdonk-1993b,
  author = {J. W. Klop and van Oostrom, V. and van Raamsdonk, F.},
  title = {Combinatory reduction systems: introduction and survey},
  institution = {CWI. Department of Computer Science},
  year = 1993,
  number = {R 9362},
  address = {Amsterdam}
}

@TechReport{Kluge-1994,
  author = {Werner E. Kluge},
  title = {A User's Guide for the Reduction System {$\pi$-RED}},
  institution = {Institut f\"ur Informatik unf Praktische
		  Mathematik,Christian-Albrechts-Universit\"at Kiel},
  year = 1994,
  type = {Bericht},
  number = {Nr. 9419}
}

@Article{Knight-1989,
  author = {Kevin Knight},
  title = {Unification: A Multidisciplinary Survey},
  journal = {ACM Computing Surveys},
  year = 1989,
  volume = 21,
  number = 1,
  month = MAR,
  pages = {93--124},
  WKloc = {A-0010},
  contents = {Introduction
   1. The Unification Problem
   2. Unification and Computational Complexity
   3. Unification: Data Structures and Algorithms
   4. Unification and Theorem Proving
   4.1. The Resolution Rule
   4.2. Research
   5. Unification and Logic Programming
   5.1. Example of Unification in Prolog
   5.2. Research
   6. Unification and Higher Order Logic
   6.1. Example of Second Order Unification
   6.2. Research
   7. Unification and Feature Structures
   8. Unification and Natural Language Processing
   8.1. Parsing with a Unification-Based Grammar
   8.2. Research
   9. Unification and Equational Theories
   9.1. Unification as Equation Solving
   9.2. Research
   10. Parallel Algorithms for Unification
   11. Unification, Generalization, and Lattices
   12. Other Applications of Unification
   12.1. Type Inference
   12.2. Programming Languages
   12.3. Machine Learning
   13. Conclusion
   13.1. Some Properties of Unification
   13.2. Trends in Unification Research
   Acknowledgements
   References},
  abstract = {The unification problem and several variants are presented.
	Various algorithms and data structures are discussed. Research on
	unification arising in several areas of computer science is surveyed;
	these areas include theorem proving, logic programming, and natural
	language processing. Sections of the paper include examples that
	highlight particular uses of unification and the special problems
	encoutered. Other topics covered are resolution, higher order logic,
	the occur check, infinite terms, feature structures, equational
	theories, inheritance, parallel algorithms, generalization, lattices,
	and other applications of unification. The paper is intended for
	readers with a general computer science background --- no specific
	knowledge of any of the above topics is assumed.}
}

@InCollection{Knuth-1968,
  key = {Knuth},
  author = {Donald E. Knuth},
  title = {Semantics of Context-Free Languages},
  booktitle = {Mathematical Systems Theory},
  publisher = {Springer-Verlag},
  address = {New York},
  year = 1968,
  month = jun,
  volume = 2,
  pages = {127--145},
  abstract = {Meaning may be assigned to a string in a context-free
                 language by defining attributes of the symbols in a
                 derivation tree for that string. The attributes can be
                 defined by functions associated with each production in
                 the grammar. This paper examines the implications of
                 this process when some of the attributes are
                 synthesized, i.e., defined solely in terms of
                 attributes of the descendants of the corresponding
                 nonterminal symbol, while other attributes are
                 inherited, i.e., defined in terms of attributes of the
                 ancestors of the nonterminal symbol. An algorithm is
                 given which detects when such semantic rules could
                 possibly lead to circular definition of some
                 attributes. An example is given of a simple programming
                 language defined with both inherited and synthesized
                 attributes, and the method of definition is compared to
                 other techniques for formal specification of semantics
                 which have appeared in the literature.},
  bibdate = {Thu Sep 4 13:19:40 1986}
}

@Article{Knuth-1971,
  author = {Donald E. Knuth},
  title = {Semantics of Context-Free Languages: Correction},
  journal = {Mathematical Systems Theory},
  year = 1971,
  volume = 5,
  number = 1,
  pages = {95--96},
  sjb = {Corrects an error in the cycle detection algorithm
                 given in~\cite{Knuth:mst:1968}.}
}

@Article{Knuth-1984,
  author = {Donald E. Knuth},
  title = {Literate Programming},
  journal = {The Computer Journal},
  year = 1984,
  volume = 27,
  number = 2,
  pages = {97--111},
  DOIURL = {http://dx.doi.org/10.1093/comjnl/27.2.97},
  abstract = {The author and his associates have been experimenting
                 for the past several years with a programming language
                 and documentation system called WEB. This paper
                 presents WEB by example and discusses why the new
                 system appears to be an improvement over previous ones.},
  classification = 723,
  keywords = {computer programming; computer programming languages;
                 design; human factors; languages},
  review = {ACM CR 8501-0018},
  subject = {D.2.7 Software, SOFTWARE ENGINEERING, Distribution and
                 Maintenance, Documentation \\ I.7.1 Computing
                 Methodologies, TEXT PROCESSING, Text Editing, Languages
                 \\ H.1.2 Information Systems, MODELS AND PRINCIPLES,
                 User/Machine Systems, Human factors \\ D.3 Software,
                 PROGRAMMING LANGUAGES, Language Constructs \\ I.7.1
                 Computing Methodologies, TEXT PROCESSING, Text Editing,
                 WEB},
  bibliographies = {FP, RelMiCS}
}

@InCollection{Knuth-1990,
  author = {Donald E. Knuth},
  editor = {Pierre Deransart and Martin Jourdan},
  title = {The Genesis of Attribute Grammars},
  booktitle = {Attribute Grammars and their Applications (WAGA)},
  series = LNCS,
  volume = 461,
  pages = {1--12},
  publisher = Springer,
  year = 1990,
  keywords = {divers.divers},
  annote = {Historical recollection of how AGs were born. (mj)},
  conferenceaddress = {Paris}
}

@Book{Knuth-1992,
  author = {Donald E. Knuth},
  title = {Literate Programming},
  publisher = {Center for the Study of Language and Information},
  year = 1992,
  volume = 27,
  series = {CSLI Lecture Notes},
  McMaster = {QA 76.6 .K644 1992},
  bibliographies = {RelMiCS}
}

@Book{Knuth-1993,
  author = {Donald Ervin Knuth},
  title = {The {Stanford} Graphbase: A Platform for
		  Combinatorial Computing},
  publisher = {ACM Press},
  year = 1993,
  UniBwM = {INF400/Y4330},
  bibliographies = {RelMiCS}
}

@Misc{Knuth-1993_interview,
  author = {Donald Knuth},
  title = {Computer Literacy Bookshops Interview},
  howpublished = {WWW page},
  year = 1993,
  month = DEC,
  WKloc = {A-0278},
  keywords = {WEB, CWEB, literate programming}
}

@InProceedings{Knuth-1993a,
  authorsAddress = {Stanford University},
  title = {The Stanford GraphBase: A Platform for Combinatorial
		  Algorithms},
  note = {invited lecture},
  crossref = {SODA1993},
  author = {Donald E. Knuth}
}

@InCollection{Knuth-1994,
  author = {Donald E. Knuth},
  title = {Bracket Notation for the `Coefficient of' Operator},
  crossref = {Roscoe-1994},
  pages = {247--258},
  chapter = 15,
  OPTnote = {},
  OPTannote = {}
}

@Article{Knuth-2003,
  author = 	 {Donald E. Knuth},
  title = 	 {{Robert W> Floyd}, In Memoriam},
  journal = 	 {ACM SIGACT News},
  year = 	 2003,
  volume =	 34,
  number =	 4,
  pages =	 {3--13},
  month =	 DEC,
  WKloc = 	 {A-1494, doc/pap/BIB}
}

@InCollection{Knuth-Bendix-1983,
  year={1983},
  isbn={978-3-642-81957-5},
  booktitle={Automation of Reasoning},
  series={Symbolic Computation},
  editor={Siekmann, J{\"o}rg H. and Wrightson, Graham},
  doi={10.1007/978-3-642-81955-1_23},
  title={Simple Word Problems in Universal Algebras},
  url={http://dx.doi.org/10.1007/978-3-642-81955-1_23},
  publisher={Springer Berlin Heidelberg},
  author={Knuth, D.E. and Bendix, P.B.},
  pages={342-376},
  language={English}
}

@Misc{Koch-1998,
  editor = {Werner Koch},
  title = {RFC2440 and {GnuPG}},
  howpublished = {Conversion of RFC2440 ``OpenPGP Message Format'' to HTML and addition of annotations regarding GnuPG},
  month = SEP,
  year = 1998,
  WKloc = {A-1178}
}

@PhDThesis{Koenig-1999,
  author = {Barbara K{\"o}nig},
  title = {Description and Verification of Mobile Processes with Graph Rewriting Techniques},
  school = {Fakult\"at f\"ur Informatik, Technische Universit\"at M\"unchen},
  year = 1999,
  URL = {http://www7.in.tum.de/gruppen/theorie/publications/Koe99a.shtml},
  WKloc = {doc/pap/BIB},
  abstract = {The aim of this thesis is to describe the semantics of a
      process calculus by means of hypergraph rewriting, creating a
      specification mechanism combining modularity of process calculi and
      locality of graph transformation. Verification of processes is
      addressed by presenting two methods: barbed congruence for relating
      processes displaying the same behaviour and generic type systems. The
      generic type system is a framework which can be instantiated in order
      to check a property (e.g. absence of deadlocks, confluence, privacy).
      The type system satisfies the subject reduction property, has
      principal types and allows automated type inference.}
}

@InProceedings{Koenig-2000,
  author = {Barbara K{\"o}nig},
  title = {A General Framework for Types in Graph Rewriting},
  crossref = {FSTTCS2000},
  pages = {373--384},
  abstract = {A general framework for typing graph rewriting systems
     is presented: the idea is to statically derive a type graph from a
     given graph. In contrast to the original graph, the type graph is
     invariant under reduction, but still contains meaningful
     behaviour information. We present conditions, a type system for
     graph rewriting should satisfy, and a methodology for proving
     these conditions. In two case studies it is shown how to
     incorporate existing type systems (for the polyadic $\pi$-calculus
     and for a concurrent object-oriented calculus) into the
     general framework.},
  WKloc = {A-1244, doc/pap/BIB}
}

@TechReport{Koenig-2000h,
  author = {Barbara K{\"o}nig},
  title = {A general framework for types in graph rewriting},
  institution = {Fakult\"at f\"ur Informatik, Technische Universit\"at M\"unchen},
  year = 2000,
  number = {TUM-I0014},
  WKloc = {doc/pap/BIB},
  abstract = {A general framework for typing graph rewriting systems
     is presented: the idea is to statically derive a type graph from
     a given graph. In. contrast to the original graph, the type graph
     is invariant under reduction, but still contains meaningful
     behaviour information. We present conditions, a type system for
     graph rewriting should satisfy, and a methodology for proving
     these conditions. In three case studies it is shown how to
     incorporate existing type systems (for the polyadic pi-calculus
     and for a concurrent object-oriented calculus) and a new type system
     into the general framework.}
}

@InProceedings{Koenig-Kozioura-2008,
  author =       {Barbara K{\"o}nig and Vitali Kozioura},
  title =        {Towards the Verification of Attributed Graph Transformation Systems},
  crossref =  {ICGT2008},
  pages =     {305--320},
  DOI =      {10.1007/978-3-540-87405-8_21},
  WKloc =    {doc/pap/BIB},
  abstract = {We describe an approach for the verification
    of attributed graph transformation systems (AGTS).
    AGTSs are graph transformation systems
    where graphs are labelled over an algebra.
    We base our verification procedure on so-called approximated unfoldings
    combined with counterexample-guided abstraction refinement.
    Both techniques were originally developed for non-attributed systems.
    With respect to refinement we focus especially on detecting
    whether the spurious counterexample is caused
    by structural over-approximation
    or by an abstraction of the attributes which is too coarse.
    The technique is implemented in the verification tool Augur 2
    and a leader election protocol has been successfully verified.}
}

@Misc{Koh-Ong-,
  author = {T.W. Koh and C.-H. L. Ong},
  title = {Internal Languages for Autononous and $*$-Autonomous Categories},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1263}
}

@InProceedings{Kohlhase-2000,
  author = 	 {Michael Kohlhase},
  title = 	 {{OMDoc}: Towards an Internet Standard for the Administration, Distribution and Teaching of Mathematical Knowledge},
  booktitle = 	 {Artificial Intelligence and Symbolic Computation},
  OPTcrossref =  {},
  OPTpages = 	 {},
  year = 	 {2000},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  series = 	 LNAI,
  OPTmonth = 	 {},
  publisher = Springer,
  bibliographies = {OPG}
}

@InProceedings{Kohlhase-Anghelache-2003,
  author = 	 {Michael Kohlhase and Romeo Anghelache},
  title = 	 {Towards Collaborative Content Management
And Version Control For Structured
Mathematical Knowledge},
  crossref =  {MKM2003},
  pages =	 {147--161},
  bibliographies = {OPG},
  OPTannote = 	 {}
}

@Article{Kolano-2002,
  author = {Paul Z. Kolano},
  title = {Proof Assistance for Real-Time Systems Using an Interactive Theorem Prover},
  journal = TCS,
  year = 2002,
  volume = 282,
  number = 1,
  pages = {53--99},
  URL = {http://www.mathematicsweb.org/mathematicsweb/show/Abstract.htt?Pu=gej%2Dng%2F10%2F41%2F16%2F253%2F27%2F30},
  annote = {A preliminary version of this paper appeared in Proc. 5th AMAST Workshop on Real-Time and Probabilistic Systems, Springer, Berlin, 1999, pp. 315-333.},
  WKloc = {doc/pap/BIB},
  abstract = {This paper discusses the adaptation of the PVS theorem
      prover for performing analysis of real-time systems written in the
      ASTRAL formal specification language. Several issues arose during the
      encoding of ASTRAL that are relevant to the encoding of many
      real-time specification languages such as encoding formulas as types,
      handling partial functions, dealing with noninterleaved concurrency,
      and defining irregular operators. These issues and possible solutions
      are presented as well as how they were handled in the ASTRAL
      encoding. A translator was written that translates any ASTRAL
      specification into its corresponding PVS encoding. After performing
      the proofs of several systems using their translations, PVS
      strategies were developed to automate the proofs of certain types of
      properties. In particular, strategies are presented for fully
      automating the proofs of certain classes of untimed properties. In
      addition, strategies were developed for partially automating the
      derivation of timed executions using transition steps. The encoding
      was used as the basis for a fully automated transition sequence
      generator tool, which has a wide variety of applications.},
  keywords = {Theorem proving; Real-time systems; Formal methods; ASTRAL; Proof assistance},
  bibliographies = {SQRL}
}

@InProceedings{Kolb-1997,
  author = {David Kolb},
  title = {Scholarly Hypertext: Self-Represented Complexity},
  booktitle = {Hypertext 97, Southampton, UK},
  pages = {},
  publisher = {ACM},
  year = 1997,
  OPTabstract = {},
  WKloc = {A-0723}
}

@Article{Koletsos-1991,
  WKloc = {A-0055},
  abstract = {We give a proof of the Church-Rosser property for
	polymorphic lambda calculus using the notion of
	``candidat de monovalence''. The proof is inspired from
	Girard's proof of the normalizability for the same calculus.},
  year = 1991,
  volume = 79,
  title = {Polymorphic Lambda Calculus: The {Church-Rosser} property},
  pages = {365-371},
  journal = {Theoretical Computer Science},
  author = {George Koletsos}
}

@InProceedings{Kolyang-Santen-Wolff-1996,
  author = {Kolyang and T. Santen and B. Wolff},
  title = {A Structure Preserving Encoding of {Z} in {Isabelle/HOL}},
  crossref = {TPHOL1996},
  pages = {283--298},
  WKloc = {A-1319},
  bibliographies = {RelMiS}
}

@@InProceedings{Komendantskaya-McCusker-Power-2010,
   author = {Komendantskaya, Ekaterina and McCusker, Guy and Power, John},
   affiliation = {Department of Computing, University of Dundee, UK},
   title = {Coalgebraic Semantics for Parallel Derivation Strategies in Logic Programming},
   crossref = {AMAST2010},
   pages = {111-127},
   DOIURL = {http://dx.doi.org/10.1007/978-3-642-17796-5_7},
   DOI = {10.1007/978-3-642-17796-5_7},
   abstract = {Logic programming, a class of programming languages based on first-order logic, provides simple and efficient tools for goal-oriented proof-search. Logic programming supports recursive computations, and some logic programs resemble the inductive or coinductive definitions written in functional programming languages. In this paper, we give a coalgebraic semantics to logic programming. We show that ground logic programs can be modelled by either P  f  P  f -coalgebras or P  f  List-coalgebras on Set. We analyse different kinds of derivation strategies and derivation trees (proof-trees, SLD-trees, and-or parallel trees) used in logic programming, and show how they can be modelled coalgebraically.}
}

@Article{Konikowska-1987,
  author = {Konikowska, B.},
  title = {A formal language for reasoning about indiscernibility},
  journal = BUPOL,
  year = 1987,
  volume = 35,
  pages = {239--249},
  bibliographies = {RelMiCS}
}

@InCollection{Konikowska-1994,
  author = {Konikowska, B.},
  title = {A logic for reasoning about similarity},
  booktitle = {Reasoning with incomplete information},
  year = 1994,
  OPTpublisher = {},
  editor = {Ewa Orlowska},
  note = {In preparation for publication},
  bibliographies = {RelMiCS}
}

@Article{Konikowska-Morgan-Orlowska-1998,
  author = {Beata Konikowska and Charles G. Morgan and Ewa Orlowska},
  title = {A Relational Formalisation of Arbitrary Finite Valued Logics},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 5,
  pages = {755--774},
  WKloc = {A-0564},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InCollection{Koopman-Lee-1991,
  author = {Koopman, Philip and Lee, Peter},
  title = {Architectural Considerations for Combinator Graph Reduction},
  crossref = {Lee-1991},
  pages = {369--395},
  abstract = {Lazy functional programming languages, such as SASL (33)
               and Haskell(15), posses a number of theoretical
               properties that make them intriguing candidates for
               study. This idea is based on the well-known observation
               from combinatory logic that all of the variables in a
               functional program can be abstracted by transforming it
               into an applicative expression involiving only
               combinators.}
}

@Article{Koppelman-1971,
  author = {E. Koppelman},
  year = 1971,
  title = {The Calculus of Operations and the Rise of Abstract Algebra},
  journal = ARHIST,
  volume = 8,
  pages = {155--242},
  bibliographies = {RelMiCS}
}

@InProceedings{Korczynski-deJesusCruzGuzman-Oziewicz-2003,
  author = {Waldemar Korczy{\'n}ski and de Jes{\'u}s Cruz Guzm{\'a}n, Jaos{\'e} and Zbigniew Oziewicz},
  title = 	 {Multilevel System as Multigraph},
  crossref =  {ICCS2003},
  pages =	 {832--840},
  WKloc = 	 {A-1570, doc/pap/BIB},
  annote = {$n$-graphs are graph structures with $n$ copies of the
     graph signature glued together in sequence, so that 2-categories
     are 2-graph (satisfying additional properties). Technically,
     linguistically, and motivationally poor.},
  abstract = {Graph based models of hierarchical systems are
     usually seen as ``graphs equipped with some refinements'',
     understood as the homomorphisms or (bi)simulations. In such a
     model it is not possible to consider phenomena happened on
     different levels of the system. We propose a new formalism of
     multi-graphs allowing to see a hierarchical system similar as a
     formula of second order logic, i.e. to consider all levels ``at
     the same time''. The concurrency in hierarchical system is
     modelled in terms of multi-graphs.}
}

@TechReport{Korff-1992,
  author = {Martin Korff},
  title = {Algebraic Transformations of Equationally Defined
		  Graph Structures},
  institution = {Technical University of Berlin},
  year = 1992,
  number = {92/32},
  OPTmonth = {},
  note = {ca.~120 pages},
  OPTabstract = {},
  OPTwkloc = {},
  OPTannote = {}
}

@InProceedings{Korff-1993,
  author = {Martin Korff},
  title = {Single Pushout Transformations of Equationally
		  Defined Graph Structures with Applications to Actor Systems},
  crossref = {GTCS93},
  pages = {234--247},
  abstract = {This work has practically been motivated by an
		  approach of modelling actor systems using algebraic
		  graph grammars. It turned out that essential
		  requirements on graph structures modelling
		  computational states could nicely be expressed as
		  conditional equations.

                  These and other examples lead then to a general
		  investigation of single pushout transformations
		  within categories of equationally defined graph
		  structures i.e., certain algebras satisfying a set
		  of given equations, and partial
		  morphisms. Fundamentally we characterize pushouts in
		  these equationally defined categories as the
		  corresponding pushouts in the supercategory of graph
		  structures without equations if and only if the
		  pushout object already satisfies the given
		  equations. For labeled graph structures this
		  characterization can be inherited from the unlabeled
		  case, but only for a restricted class of
		  equations. For a special kind of so-called local
		  equations in particular, interesting graph
		  transformation results carry over to the new
		  setting. The use and the effects of such equations
		  are illustrated and discussed for corresponding
		  graph grammar modellings of a client/server problem
		  considered as an actor system.},
  keywords = {Algebraic Graph Transformations, Equationally
		  Defined Graph Structures, Actor Systems},
  WKloc = {A-0294}
}

@InProceedings{Korff-Ribeiro-1994,
  author = {Martin Korff and Leila Ribeiro},
  title = {Formal Relationship between Graph Grammars and {Petri} Nets},
  crossref = {GG94},
  pages = {288--303}
}

@MastersThesis{Korittky-1998,
  author = {Joachim Korittky},
  title = {\emph{functional} \textsf{METAPOST}. {Eine Beschreibungssprache f\"ur Grafiken}},
  school = {Universit\"at Bonn, Institut f\"ur Informatik},
  year = 1998,
  OPTkey = {},
  OPTtype = {Diplomarbeit},
  OPTaddress = {},
  OPTmonth = DEC,
  keywords = {Haskell, Metafont, picture},
  OPTannote = {Prof. Dr. A. B. Cremers},
  WKloc = {A-1206 p. 1--20; doc/pap/BIB}
}

@Misc{Korn-199X,
  author = {Jeffrey Korn},
  title = {Tksh: A {Tcl} Library for {KornShell}},
  year = {199?},
  abstract = {This paper describes Tksh, an implementation of the Tcl C library written on top of the library for the new KornShell (ksh93). [$\ldots$]},
  WKloc = {A-1141}
}

@InProceedings{Kornyak-2005,
  author = 	 {Vladimir V. Kornyak},
  title = 	 {On Compatibility of Discrete Relations},
  crossref =  {CASC2005},
  pages =	 {272--284},
  bibliographies = {RelMiCS},
  abstract = {An approach to compatibility analysis of systems of
     discrete relations is proposed. Unlike the Gr\"obner basis
     technique, the proposed scheme is not based on the polynomial
     ring structure. It uses more primitive set-theoretic and
     topological concepts and constructions. We illustrate the
     approach by application to some two-state cellular automata. In
     the two-state case the Gr\"obner basis method is also applicable,
     and we compare both approaches.}
}

@InCollection{Korpiun-1981,
  author = {Christian A. Korpiun},
  title = {{Arbeitsorganisation im Industriebetrieb und Konflikte im Aufbau des Sprachbewu{\ss{}}tseins}},
  booktitle = {{Institutionen --- Konflikte --- Sprache}},
  pages = {126--152},
  year = 1981,
  editor = {Joseph Klein and Gunther Presch},
  address = {T\"ubingen},
  WKloc = {A-1248},
  bibliographies = {Engineering}
}

@InProceedings{Kosaraju-1989,
  year = 1989,
  title = {Efficient Tree Pattern Matching},
  pages = {178-183},
  booktitle = {Proc. of the Symposium on
               Foundations of Computer Science (FOCS'89)},
  author = {S. R. Kosaraju}
}

@TechReport{Kosiuczenko-1997,
  author = {Piotr Kosiuczenko},
  title = {Term Rewriting from an Algebraic Point of View},
  institution = {Ludwig-Maximilians-Universität München, Institut für Informatik},
  year = 1997,
  number = 9702,
  month = APR,
  URL = {http://www.pst.informatik.uni-muenchen.de/~kosiucze/dynalg.ps},
  WKloc = {A-1192, doc/pap/BIB}
}

@Misc{Koslowski-1997,
  author = {J. Koslowski},
  title = {Monads and Interpolads in Bicategories},
  booktitle = {Theory and Applications of Categories 3},
  pages = {182--212},
  year = 1997
}

@Misc{Kott-1978,
  author = {L. Kott},
  title = {About {R. Burstall} and {J. Darlington}'s Transformation System: A Theoretical Study},
  howpublished = {Universit\'e Paris VII},
  year = 1978
}

@TechManual{Koutsofios-North-1993,
  author = {Eleftherios Koutsofios and Stephen C. North},
  title = {Drawing Graphs with {\em dot}},
  institution = {AT\&T Bell Laboratories},
  year = {1993},
  address = {Murray Hill, NJ},
  WKloc = {A-0748, B-0128}
}

@InProceedings{Kozen-1981,
  author = {Kozen, Dexter},
  address = {Berlin},
  booktitle = {Logic of Programs 1981},
  pages = {1--11},
  publisher = Springer,
  series = LNCS,
  volume = 651,
  title = {On the duality of dynamic algebras and {K}ripke models},
  year = 1981,
  bibliographies = {RelMiCS}
}

@Book{Kozen-1991,
  author =	 {Kozen, Dexter},
  title = 	 {The Design and Analysis of Algorithms},
  publisher = 	 Springer,
  year = 	 1991,
  address =	 {New York}
}

@Article{Kozen-1994,
  author = {Kozen, Dexter},
  title = {A Completeness Theorem for {Kleene} Algebras and the Algebra of Regular Events},
  journal = IandC,
  year = 1994,
  volume = 110,
  number = 2,
  pages = {366--390},
  bibliographies = {RelMiCS},
  WKloc = {A-1580, doc/pap/BIB}
}

@InProceedings{Kozen-1994a,
  author = {Kozen, Dexter},
  title = {On Action Algebras},
  booktitle = {Logic and Information Flow},
  pages = {78--88},
  year = 1994,
  editor = {van Eijck, J. and A. Visser},
  publisher = {MIT Press},
  bibliographies = {RelMiCS}
}

@Article{Kozen-1997,
  author = 	 {Dexter Kozen},
  title = 	 {{Kleene} Algebra with Tests},
  journal = 	 TOPLAS,
  year = 	 1997,
  pages =	 {427--443},
  month =	 MAY,
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {We introduce Kleene algebra with tests, an equational
     system for manipulating programs. We give a purely equational
     proof, using Kleene algebra with tests and commutativity
     conditions, of the following classical result: every while
     program can be simulated by a while program with at most one
     while loop. The proof illustrates the use of Kleene algebra with
     tests and commutativity conditions in program equivalence
     proofs. We also show that the universal Horn theory of
     *-continuous Kleene algebras is not finitely axiomatizable.}
}

@TechReport{Kozen-1998,
  author = 	 {Dexter Kozen},
  title = 	 {Typed {Kleene} Algebra},
  institution =  {Computer Science Department, Cornell University},
  year = 	 1998,
  month = MAR,
  number =	 {98-1669},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {In previous work we have found it necessary to argue
     that certain theorems of Kleene algebra hold even when the
     symbols are interpreted as nonsquare matrices. In this note we
     define and investigate typed Kleene algebra, a typed version of
     Kleene algebra in which objects have types $s \rightarrow
     t$. Although nonsquare matrices are the principal motivation,
     there are many other useful interpretations: traces, binary
     relations, Kleene algebra with tests. We give a set of typing
     rules and show that every expression has a unique most general
     typing (mgt). Then we prove the following metatheorem that
     incorporates the abovementioned results for nonsquare matrices as
     special cases. Call an expression 1-free if it contains only the
     Kleene algebra operators (binary) +, (unary) +, 0, and $\cdot$,
     but no occurrence of 1 or *. Then every universal 1-free formula
     that is a theorem of Kleene algebra is also a theorem of typed
     Kleene algebra under its most general typing. The metatheorem is
     false without the restriction to 1-free formulas.}
}

@Article{Kozen-2002,
  author = 	 {Dexter Kozen},
  title = 	 {On the Complexity of Reasoning in {Kleene} Algebra},
  journal = 	 IandC,
  year = 	 2002,
  volume =	 179,
  pages =	 {152--162},
  WKloc = 	 {A-1579, doc/pap/BIB}
}

@TechReport{Kozen-2004c,
  number="TR2004-1942",
  month=JUL,
  title="Natural Transformations as Rewrite Rules and Monad Composition",
  author="Dexter Kozen",
  year="2004",
  institution="Computer Science Department, Cornell University",
  URL = {http://www.cs.cornell.edu/~kozen/papers/papers_collapsed.htm},
  PDF = {http://www.cs.cornell.edu/~kozen/papers/Monad.pdf},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Kozen-Kreitz-Richter-2006,
  author = 	 {Dexter Kozen and Christoph Kreitz and Eva Richter},
  title = 	 {Automating Proofs in Category Theory },
  crossref =  {IJCAR2006},
  URL = 	 {http://www.springerlink.com/content/ft731k472w20p824/},
  pages =	 {32--47},
  bibliographies = {RelMiCS},
  WKloc = {A-1724, doc/pap/BIB},
  abstract = {We introduce a semi-automated proof system
     for basic category-theoretic reasoning.
     It is based on a first-order sequent calculus
     that captures the basic properties of categories,
     functors and natural transformations
     as well as a small set of proof tactics
     that automate proof search in this calculus.
     We demonstrate our approach
     by automating the proof that the functor categories
     $Fun [C \times D, E]$ and $Fun [C, Fun [D, E] ]$ are naturally isomorphic. }
}

@InProceedings{Kozen-Palsberg-1993,
  author = {Dexter Kozen and Jens Palsberg},
  title = {Efficient Recursive Subtyping},
  pages = {419--450},
  bibliographies = {RelMiCS},
  abstract = {Subtyping in the presence recursive types for the
             $\lambda$-calculus was studied by Amadio and Cardelli in 1991
             [1]. In that paper they showed that the problem of deciding
             whether one recursive type is a subtype of another is decidable
             in exponential time.

             In this paper we give an O($n^{2}$)
             algorithm. Our algorithm is based on a simplification of the
             definition of the subtype relation, which allows us to reduce
             the problem to the emptiness problem for a certain finite
             automaton with quadratically many states.

             It is known that
             equality recursive types and the covariant B\"ohm order can be
             decided efficiently by means of finite automata. Our results
             extend the automata-theoretic approach to handle ordering based
             on contravariance.},
  crossref = {POPL1993},
  WKloc = {A-0201}
}

@InProceedings{Kozen-Palsberg-Schwartzbach-1992,
  WKloc = {?},
  keywords = {?},
  authorsAddress = {DK: Cornell; JP,MIS: Aarhus},
  abstract = {?},
  title = {Efficient Inference of Partial Types},
  pages = {?},
  crossref = {FOCS92},
  author = {Dexter Kozen and Jens Palsberg and Michael I. Schwartzbach},
  annote = {?}
}

@Article{Kramer-Gupta-Soffa-1994,
  author = {Robert Kramer and Rajiv Gupta and Mary Lou Soffa},
  title = {Combining {DAG}: a technique for parallel data flow
                 analysis},
  journal = IEEETPDS,
  volume = 5,
  number = 8,
  pages = {805--813},
  month = aug,
  year = 1994,
  CODEN = {ITDSEO},
  ISSN = {1045-9219},
  bibdate = {Fri Apr 11 15:20:39 MDT 1997},
  bibsource = {Compendex database},
  acknowledgement = ack-nhfb,
  affiliation = {Univ of Pittsburgh},
  affiliationaddress = {Pittsburgh, PA, USA},
  classification = {722.4; 723.1; 723.2; 921.4; 921.6; C1160
                 (Combinatorial mathematics); C4240P (Parallel
                 programming and algorithm theory); C5440
                 (Multiprocessor systems and techniques); C6110P
                 (Parallel programming); C6150C (Compilers, interpreters
                 and other processors)},
  conferenceyear = 1994,
  corpsource = {Dept. of Comput. Sci., Pittsburgh Univ., PA, USA},
  journalabr = {IEEE Trans Parallel Distrib Syst},
  keywords = {algorithm; Algorithms; combining directed acyclic
                 graph; compilers; Computer aided software engineering;
                 Control flow graph; control flow graph; cyclic
                 structures; DAG; Data flow analysis; data flow
                 problems; Data reduction; Directed acyclic graph;
                 directed graphs; Graph theory; Iterative methods;
                 multiprocessors; Node listings; parallel; parallel
                 algorithm; parallel algorithms; parallel data flow
                 algorithms; parallel data flow analysis; parallel
                 processing; Parallel processing systems; parallel
                 programming; parallelized sequential algorithm;
                 Performance; performance; Program compilers; program
                 compilers; rapid; sequential; Sequential algorithm;
                 sequential data flow algorithm; software tools;
                 Structured analysis},
  treatment = {P Practical; T Theoretical or Mathematical},
  UniBwM = {???/Z10648}
}

@Article{Kramer-Maddux-1982,
  author = {Richard L. Kramer and Roger Duncan Maddux},
  title = {Equations not Preserved by Complete Extensions},
  journal = ALGU,
  volume = 15,
  year = 1982,
  pages = {86--89},
  OPTnote = {MR 83i:03098 (I. Voilculescu), Zbl 522.03054 (H.-P. Gumm).},
  bibliographies = {RelMiCS}
}

@PhDThesis{Kreowski-1977,
  author = {Hans-J\"org Kreowski},
  title = {Manipulation von Graph Transformationen},
  school = {Technische Universit\"at Berlin},
  year = 1977,
  annote = {Cited in \cite{Corradini-Ehrig-Loewe-Montanari-Rossi-1993a}:
                  Every derivation has, up to isomorphism, a unique
		  shift-equivalent canonical derivation.}
}

@InProceedings{Kreowski-1978,
  author = 	 {Hans-J\"org Kreowski},
  title = 	 {A Pumbing Lemma for Context-Free Graph Languages},
  crossref =	 {GG1078},
  pages =	 {270--283},
  annote =	 {Defines Context-Sensitive Graph Grammars?}
}

@InProceedings{Kreowski-1986,
  author = {Hans-J{\"o}rg Kreowski},
  title = {{Is Parallelism Already Concurrency? Part 1: Derivations
		  in Graph Grammars}},
  crossref = {GG1986},
  pages = {343--360}
}

@InCollection{Kreowski-Kuske-1999,
  author =       {Hans-J\"org Kreowski and Sabine Kuske},
  title =        {Graph Transformation Units and Modules},
  crossref =  {HBGraTraII},
  pages =     {607--638},
  chapter =   15
}

@InCollection{Kreowski-2002,
  author = 	 {Hans-J{\"o}rg Kreowski},
  title = 	 {A Sight-seeing Tour of the Computational Landscape of Graph Transformation},
  crossref =	 {RozenbergFestschrift2002},
  pages =	 {119--137},
  WKloc = {doc/pap/BIB},
  abstract = {In this paper, the basic features of graph
     transformation are introduced as a kind of sight-seeing tour of
     the computational landscape which is based on the application of
     rules to graphs.}
}

@Article{Kreowski-Kuske-Schuerr-1997,
  author = {Hans-J\"org Kreowski and Sabine Kuske and Andy Sch{\"u}rr},
  title = {Nested Graph Transformation Units},
  journal = {International Journal of Software Engineering and Knowledge Engineering},
  year = 1997,
  volume = 7,
  number = 4,
  OPTmonth = {},
  pages = {479--502},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {D-???},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Kreowski-Plump-2000,
  author = {Hans-J\"org Kreowski and Detlef Plump},
  title = {{\sc Appligraph}, Applications of Graph Transformation, {ESPRIT Working Group No.~22565}, Third Annual Progress Report},
  month = MAY,
  year = 2000,
  WKloc = {A-1132}
}

@InProceedings{Kreowski-Wilharm-1986,
  author = {Hans-J{\"o}rg Kreowski and Anne Wilharm},
  title = {{Is Parallelism Already Concurrency? Part 2: Non
		  Sequential Processes in Graph Grammars}},
  crossref = {GG1986},
  pages = {361--377}
}

@Article{Kribs-PowerSC-2004,
  author = 	 {David W. Kribs and Stephen C. Power},
  title = 	 {Free Semigroupoid Algebras},
  journal = 	 {J.\null{} Ramanujan Math.\null{} Soc.\null{}},
  year = 	 {2004},
  PreURL = 	 {http://arxiv.org/abs/math.OA/0309394},
  OPTvolume = 	 {19},
  OPTnumber = 	 {2},
  OPTpages = 	 {117--159 \unfinished 75--117},
  OPTmonth = 	 {},
  MSC = {47L55, 47L75},
  OPTnote = 	 {},
  abstract = {Every countable directed graph generates a Fock
      space Hilbert space and a family of partial isometries. These
      operators also arise from the left regular representations of
      free semigroupoids derived from directed graphs. We develop a
      structure theory for the weak operator topology closed algebras
      generated by these representations, which we call free
      semigroupoid algebras. We characterize semisimplicity in terms
      of the graph and show explicitly in the case of finite graphs
      how the Jacobson radical is determined. We provide a diverse
      collection of examples including; algebras with free behaviour,
      and examples which can be represented as matrix function
      algebras. We show how these algebras can be presented and
      decomposed in terms of amalgamated free products. We determine
      the commutant, consider invariant subspaces, obtain a Beurling
      theorem for them, conduct an eigenvalue analysis, give an
      elementary proof of reflexivity, and discuss
      hyper-reflexivity. Our main theorem shows the graph to be a
      complete unitary invariant for the algebra. This classification
      theorem makes use of an analysis of unitarily implemented
      automorphisms. We give a graph-theoretic description of when
      these algebras are partly free, in the sense that they contain a
      copy of a free semigroup algebra.}
}

@InProceedings{KriegBrueckner-1998,
  author = {Bernd Krieg-Br{\"u}ckner},
  title = {{UniForM} Perspectives for Formal Methods},
  crossref = {FMTrends1998},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  OPTannote = {}
}

@Article{Kripke-1963,
  author = {Kripke, S.},
  title = {Semantical analysis of modal logic I.},
  journal = ZMALOG,
  year = 1963,
  volume = 9,
  pages = {67--96},
  bibliographies = {RelMiCS}
}

@InProceedings{Kripke-1965,
  author = {Kripke, S.},
  title = {Semantical analysis of intuitionistic logic},
  booktitle = {Formal Systems and Recursive Functions},
  editor = {Crossley, J.N. and Dummett, M.A.},
  year = 1965,
  publisher = NoHo,
  address = {Amsterdam},
  bibliographies = {RelMiCS}
}

@TechReport{Kroeger-Lachenmayer-Mors-Schmidt-Schmitz-1991,
  year = 1991,
  title = {{Labor Formale Sprachen}},
  number = 9103,
  month = FEB,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {J. Kr\"oger and P. Lachenmayer and D. Mors and M. Schmidt and
      Lothar Schmitz}
}

@TechReport{Kroeger-Mors-Schmidt-Schmitz-1990,
  year = 1990,
  title = {{SIC - Ein Smalltalk-basierter, Interaktiver Compiler-Compiler
      f\"ur den Unterricht}},
  number = 9009,
  month = NOV,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {J. Kr\"oger and D. Mors and M. Schmidt and Lothar Schmitz}
}

@TechReport{Krone-Ogden-Sitaraman-Weide-2008,
  author =       {J. Krone and W. F. Ogden and M. Sitaraman and B. W. Weide},
  title =        {Refocusing the Verifying Compiler Grand Challenge},
  institution =  {School of Computing, Clemson University, Clemson, SC},
  year =         2008,
  type =      {Tech.~Report},
  number =    {RSRG-08-01},
  PDFURL = {http://www.cs.clemson.edu/resolve/research/reports/RSRG-08-01.pdf},
  ParentURL = {http://www.cs.clemson.edu/resolve/research/research.html},
  pages =   10,
  abstract = {The ideal goal of this grand challenge should be a future in which no
production software is considered properly engineered unless it has been fully
specified and fully verified as satisfying its specifications. The verifying
compiler then becomes the essential central artifact necessary to achieve this
outcome, and its characteristics are determined by the overall goal. From this
perspective, the nature of programming languages that a verifying compiler
could process becomes an immediate issue, and we present several critical
features that such a programming language must possess. Specifically, it must
include specifications as an integral constituent, and it must have clean
semantics, which preclude unexpected side-effecting, aliasing, etc. It must
include mechanisms for writing reusable components that are amenable to
verification, and consequently, it must include an open-ended mechanism for
adding arbitrarily sophisticated mathematical theories in order to specify large
software components concisely. Because the current programming languages
lack these essential characteristics, the verifying compiler grand challenge will
not be met unless it redirects its focus to include the development of a suitable
programming language within which full verification is possible.}
}

@InProceedings{Kruskal-Rudolf-Cytron-1985,
  author = {Clyde P. Kruskal and Larry Rudolf and Ron Cytron},
  title = {The Architecture of Parallel Computers},
  pages = {279--344},
  crossref = {Marktoberdorf-1985}
}

@TechReport{Kuchen-Loogen-MorenoNavarro-1989,
  WKloc = {B-0008},
  abstract = {We investigate the development of a graph reduction machine
	for a higher-order functional logic language by extension of an
	appropriate architecture for purely functional languages. To execute
	logic programs the machine must be capable of performing unification
	and backtracking. We show the integration of these mechanisms in a
	programmed (functional) graph reduction machine. The new machine has
	been implemented on a transputer system.},
  year = 1989,
  type = {Aachener Informatik-Berichte},
  title = {Graph-based Implementation of a Functional Logic Language},
  number = {89-20},
  institution = {RWTH Aaachen},
  author = {Herbert Kuchen and Rita Loogen and Juan Jos\'e Moreno-Navarro}
}

@MastersThesis{Kuehn-1992,
  keywords = {programming language graph traversal algorithms, PLG},
  year = 1992,
  title = {{Durchlaufalgorithmen f\"ur programmiersprachliche
		  Graphen in C und C++}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 09/92},
  type = {Diplomarbeit},
  note = {ID 09/92},
  month = NOV,
  author = {Christoph K{\"u}hn}
}

@Article{Kuhn-1987,
  author = {S. T. Kuhn},
  title = {Review of {van Benthem}, ``The Logic of Time''},
  journal = JSYLO,
  volume = 53,
  number = 3,
  year = 1987,
  month = SEP,
  pages = {874--876},
  bibliographies = {RelMiCS}
}

@Misc{Kuiper-199X,
  author = {Matthijs Kuiper},
  title = {Incremental Language Based Tools},
  year = {199?},
  WKloc = {A-0679}
}

@article{Kuncak-Mayer-Piskac-Suter-2012,
  author = {Kuncak, Viktor and Mayer, Mika\"{e}l and Piskac, Ruzica and Suter, Philippe},
  title = {Software Synthesis Procedures},
  journal = CACM,
  issue_date = {February 2012},
  volume = {55},
  number = {2},
  month = feb,
  year = {2012},
  issn = {0001-0782},
  pages = {103--111},
  numpages = {9},
  DOIURL = {http://doi.acm.org/10.1145/2076450.2076472},
  DOI = {10.1145/2076450.2076472},
  acmid = {2076472},
  OPTpublisher = {ACM},
  OPTaddress = {New York, NY, USA},
  bibliographies = {RATH},
  abstract = {Automated synthesis of program fragments from
                  specifications can make programs easier to write and
                  easier to reason about. To integrate synthesis into
                  programming languages, software synthesis algorithms
                  should behave in a predictable way: they should
                  succeed for a well-defined class of
                  specifications. We propose to systematically
                  generalize decision procedures into synthesis
                  procedures, and use them to compile implicitly
                  specified computations embedded inside functional
                  and imperative programs. Synthesis procedures are
                  predictable, because they are guaranteed to find
                  code that satisfies the specification whenever such
                  code exists. To illustrate our method, we derive
                  synthesis procedures by extending quantifier
                  elimination algorithms for integer arithmetic and
                  set data structures. We then show that an
                  implementation of such synthesis procedures can
                  extend a compiler to support implicit value
                  definitions and advanced pattern matching.}
}

@Article{Kurihara-Ohuchi-1992,
  WKloc = {A-0267},
  abstract = {A term rewriting system is {\em simply terminating}
		  if there exists a simplification odering showing its
		  termination. Let $R_0$ and $R_1$ be term rewriting
		  systems which share no defined symbol (but may share
		  constructors). Constructors are function symbols
		  which do not occur at the leftmost position in
		  left-hand sides of rewrite rules; the rest of the
		  function symbols are defined symbols. In this paper,
		  we prove that $R_0 \union R_1$ is simply terminating
		  if and only if both $R_0$ and $R_1$ are so.},
  year = 1992,
  volume = 103,
  title = {Modularity of simple termination of term rewriting
		  systems with shared constructors},
  pages = {273--282},
  journal = {Theoretical Computer Science},
  author = {Masahito Kurihara and Azuma Ohuchi}
}

@InProceedings{Kurka-1994,
  author = {Peter K{\o{u}}rka},
  title = {A Comparison of Finite and Cellular Automata},
  crossref = {MFCS94},
  pages = {484--493},
  abstract = {We conceive finite and cellular automata as
		  dynamical systems on zero-dimensional spaces and
		  show that they are incomparable in the sense of
		  factorization. Next we study the complexity of
		  languages generated by zero-dimensional systems on
		  clopen partitions of the state space. While finite
		  automata generate only regular languages, cellular
		  automata generate non-deterministic polynomial
		  languages which may be non-regular.}
}

@Book{Kuropatwa-1970,
  author = {Otto Kuropatwa},
  title = {Besondere Eigenschaften von Relationen (Relationen 2)},
  publisher = {Ernst Klett Verlag},
  year = 1970,
  series = {MM-Programm, Programm zur modernen Mathematik},
  address = {Stuttgart},
  TUM = {04001032022, L 49243/ K 04, 2. 95A1442},
  note = {This is a German schoolbook},
  bibliographies = {RelMiCS}
}

@Article{Kurz-Hennicker-2002,
  author =       {Alexander Kurz and Rolf Hennicker},
  title =        {On Institutions for Modular Coalgebraic Specifications},
  journal =      TCS,
  year =         2002,
  volume =    280,
  number =    {1--2},
  pages =     {69--103},
  DOI =     {10.1016/S0304-3975(01)00021-4},
  DOIURL = {http://dx.doi.org/10.1016/S0304-3975(01)00021-4},
  WKloc =      {doc/pap/BIB},
  abstract =    {We present an algebraic extension of standard coalgebraic specification techniques for state-based systems which allows us to integrate constants and $n$-ary operations in a smooth way and which leads to institutions enabling the use of modular specification techniques. A sound and complete proof system for first-order observational properties of modular specifications is given.
   The framework of $(\Omega,\Xi)$-structures that we present can be considered as the result of a transformation of concepts of observational logic as in Hennicker and Bidoit (in: A. Haeberer (Ed.), Algebraic Methodology and Software Technology (AMAST’98), Lecture Notes in Computer Science, vol. 1548, Springer, Berlin, 1999) into the coalgebraic world. Moreover, it is shown that the features of $(\Omega,\Xi)$-structures that make them suitable models for an observational approach to specifications can be categorically expressed by the fact that the operation mapping an $(\Omega,\Xi)$-structure to its behaviour is a fibred idempotent monad.}
}

@Article{Kuske-1995,
  author = {Sabine Kuske},
  title = {Implementing $\beta$-reduction by Hypergraph Rewriting},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = 1995,
  volume = 1,
  OPTnumber = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {},
  WKloc = {A-0573},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@PhDThesis{Kuske-1999,
  author = {Sabine Kuske},
  title = {Transformation Units --- A Structuring Principle for Graph transformation Systems},
  school = {Fachbereich 3 (Mathematik \& Informatik) der Universit\"at Bremen},
  year = 1999,
  month = NOV,
  WKloc = {A-1174}
}

@inproceedings{Kutsia-2013,
  author    = {Temur Kutsia},
  title     = {Anti-Unification: Algorithms and Applications},
  booktitle = {UNIF 2013},
  editor    = {Konstantin Korovin and Barbara Morawska},
  series    = {EPiC Series},
  volume    = {19},
  pages     = {2-2},
  year      = {2013},
  publisher = {EasyChair},
  bibsource = {EasyChair, http://www.easychair.org},
  issn      = {2040-557X},
}

@PhDThesis{Kwatinetz-1981,
  author = {Michael Kwatinetz},
  title = {Problems of Expressibility in Finite Languages},
  school = {Univ.\null{} of California},
  type = Doct,
  publisher = BERKELEY_P,
  address = {Berkeley},
  year = 1981,
  pages = {xi + 106},
  bibliographies = {RelMiCS}
}

@Book{LILOG,
  year = 1991,
  volume = 546,
  title = {Text Understanding in {LILOG}, Integrating Computational Linguistics and Artificial Intelligence, Final Report on the IBM Germany LILOG-Project},
  series = LNAI,
  publisher = Springer,
  editor = {Otthein Herzog and Claus-Rainer Rollinger},
  contents = {0 Introduction
      1 The Linguistic Point of View
      1.1 STUF and Parsing
      1.2 Lexicon
      1.3 Syntax
      1.4 Semantics
      2 Knowledge Representation and Processing
      2.1 Representing Semantic Knowldge
      2.2 Knowledge Engineering and Database Support
      3 Spatial Knowledge
      4 Generating Natural Language
      5 Leu/2}
}

@Article{LabedJilani-Desharnais-Mili-2001,
  author = {Labed Jilani, Lamia and Jules Desharnais and Ali Mili},
  title = {Defining and Applying Measures of Distance Between Specifications},
  journal = IEEETSE,
  year = 2001,
  volume = 27,
  number = 8,
  pages = {673--703},
  WKloc = {A-1207, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Echoing Louis Pasteur's quote, we submit the premise
              that it is advantageous to define measures of distance
              between requirements specifications,
              because such measures open up a wide range of possibilities
              both in theory and in practice.
              In this paper, we present a mathematical basis
              for measuring distances between specifications,
              and show how our measures of distance can be used
              to address concrete problems that arise
              in the practice of software engineering.}
}

@TechReport{Labrou-Finin-1997,
  title = {{A Proposal for a new KQML Specification}},
  author = {Yannis Labrou and Tim Finin},
  month = FEB,
  year = 1997,
  institution = {Computer Science and Electrical Engineering
                 Department, University of Maryland Baltimore County},
  address = {Baltimore, MD, USA},
  number = {TR CS-97-03},
  URL = {http://www.cs.umbc.edu/~jklabrou/publications/tr9703.ps},
  abstract = {We propose a new specification for the Knowledge Query
                 and Manipulation Language (KQML). KQML is a language
                 for the communication between software agents. KQML
                 offers a variety of message types (performatives) that
                 express an attitude regarding the content of the
                 exchange. Performatives can also assist agents in
                 finding other agents that can process their requests.
                 Our starting point for the specification of KQML is
                 [1]. Although the differences regarding the syntax of
                 KQML messages and the reserved performative parameters
                 are minimal, there are significant changes regarding
                 the set of reserved performatives, their meaning and
                 intended use.},
  WKloc = {B-0101}
}

@TechReport{Lack-Sobocinski-2003,
  author =       {Stephen Lack and Pawe{\l} Soboci{\'n}ski},
  title =        {Adhesive Categories},
  institution =  {BRICS},
  year =         2003,
  number =    {RS-03-31},
  month =     OCT,
  WKloc =      {doc/pap/BIB},
  bibliographies = {RelMiCS},
  note = {(Long version of \cite{Lack-Sobocinski-2004}.)},
  abstract =    {We introduce adhesive categories,
    which are categories with structure ensuring that
    pushouts along monomorphisms are well-behaved.
    Many types of graphical structures used in computer science are shown
    to be examples of adhesive categories. Double-pushout graph rewriting
    generalises well to rewriting on arbitrary adhesive categories.}
}

@InProceedings{Lack-Sobocinski-2004,
  author =       {Stephen Lack and Pawe{\l} Soboci{\'n}ski},
  title =        {Adhesive Categories},
  crossref =  {FOSSACS2004},
  pages =     {273--288},
  bibliographies = {RelMiCS},
  WKloc =      {doc/pap/BIB},
  DOI = {10.1007/b95995},
  abstract =    {We introduce adhesive categories,
     which are categories with structure ensuring that
     pushouts along monomorphisms are well-behaved.
     Many types of graphical structures used in computer science
     are shown to be examples of adhesive categories. Double-pushout graph
     rewriting generalises well to rewriting on arbitrary adhesive categories.}
}

@Article{Lack-Sobocinski-2005,
  author =       {Stephen Lack and Pawe{\l} Soboci{\'n}ski},
  title =       {Adhesive and quasiadhesive categories},
  journal =      RAIRO-I,
  year =         2005,
  volume =    39,
  number =    3,
  pages =     {511--545},
  bibliographies = {RelMiCS},
  DOIURL = {http://dx.doi.org/10.1051/ita:2005028},
  DOI = {10.1051/ita:2005028},
  keywords =     {adhesive categories, quasiadhesive categories,
     extensive categories, category theory, graph rewriting},
  ZBL =      {Analyses Zbl 1078.18010
    http://www.zentralblatt-math.org/zmath/en/search/?q=an:1078.18010},
  MSC2000 = {18A30, 18A35, 18D99, 68Q42, 68Q65},
  abstract =    {We introduce adhesive categories,
    which are categories with structure ensuring that
    pushouts along monomorphisms are well-behaved,
    as well as quasiadhesive categories
    which restrict attention to regular monomorphisms.
    Many examples of graphical structures used in computer science
    are shown to be examples of adhesive and quasiadhesive categories.
    Double-pushout graph rewriting generalizes well
    to rewriting on arbitrary adhesive and quasiadhesive categories.}
}

@InProceedings{Ladd-Ramming-1994,
  author = {D.A. Ladd and J.C. Ramming},
  title = {{${\rm A}^*$}: A language for Implementing Language
		  Processors},
  crossref = {ICCL94},
  pages = {1--10},
  keywords = {awk}
}

@Article{Ladd-etal-2002,
  author = {T. D. Ladd and J. R. Goldman and F. Yamaguchi and Y. Yamamoto
            and E. Abe and K. M. Itoh},
  title = {All-Silicon Quantum Computer},
  journal = {Physical Review Letters},
  year = 2002,
  volume = 89,
  number = 017901,
  month = JUL,
  URL = {http://link.aps.org/abstract/PRL/v89/e017901},
  bibliographies = {Quantum},
  WKloc = {doc/pap/BIB},
  abstract = {A solid-state implementation of a quantum computer composed
      entirely of silicon is proposed. Qubits are 29Si nuclear spins
      arranged as chains in a 28Si (spin-0) matrix with Larmor frequencies
      separated by a large magnetic field gradient. No impurity dopants or
      electrical contacts are needed. Initialization is accomplished by
      optical pumping, algorithmic cooling, and pseudo-pure state
      techniques. Magnetic resonance force microscopy is used for ensemble
      measurement.}
}

@TechReport{Ladkin-1986,
  author = {Peter B. Ladkin},
  title = {Two Papers on Time Representation},
  institution = {Kestrel Inst.},
  type = {Research Report},
  number = {KES.U.86.5},
  year = 1986,
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1986a,
  author = {Peter B. Ladkin},
  title = {Primitives and Units for Time Specification},
  pages = {354--359},
  crossref = {AAAI1986},
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1986b,
  author = {Peter B. Ladkin},
  title = {Time representation: A Taxonomy of Enterval Relations},
  crossref = {AAAI1986},
  pages = {360--366},
  bibliographies = {RelMiCS}
}

@PhDThesis{Ladkin-1987,
  author = {Peter B. Ladkin},
  title = {The Logic of Time Representation},
  type = Doct,
  school = BERKELEY,
  note = {Kestrel Inst.\null{} Technical Report KES.U.87.13},
  year = 1987,
  month = NOV,
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1987a,
  author = {Peter B. Ladkin},
  title = {Specification of Time Dependencies and
		Synthesis of Concurrent Processes},
  booktitle = {Proc.\null{} of the {$9^{th}$} Internat.\null{}  Conf.\null{}
		on Software Engineering, Monterey, CA},
  publisher = IEEE,
  year = 1987,
  note = {Kestrel Institute Technical Report KES.U.87.1.},
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1987b,
  author = {Peter B. Ladkin},
  title = {Models of Axioms for Time Intervals},
  crossref = {AAAI1987},
  pages = {234--239},
  note = {{A}lso available in a longer version as
		Kestrel Inst.\null{} Technical Report KES.U.87.4.},
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1987c,
  author = {Peter B. Ladkin},
  title = {The Completeness of a Natural System for Reasoning
		with Time Intervals},
  booktitle = {Proc.\null{} of the {$10^{th}$} Internat.\null{}  Joint Conf.\null{}
		on Artificial Intelligence, Milano, Italy},
  publisher = Kauf,
  year = 1987,
  pages = {462--467},
  note = {Kestrel Institute Technical Report KES.U.87.5},
  bibliographies = {RelMiCS}
}

@TechReport{Ladkin-1987d,
  author = {Peter B. Ladkin},
  title = {Deciding First-order Statements about Time Intervals:
		Preliminary Report},
  institution = {Kestrel Inst.},
  type = {Technical Report},
  number = {KES.U.87.7},
  year = 1987,
  bibliographies = {RelMiCS}
}

@TechReport{Ladkin-1987e,
  author = {Peter B. Ladkin},
  title = {Constaint Satisfaction in Time Intervals Structures I:
		Convex Intervals},
  publisher = Kestrel,
  institution = {Kestrel Inst.},
  type = {Technical Report},
  number = {KES.U.87.11},
  year = 1987,
  month = NOV,
  bibliographies = {RelMiCS}
}

@InProceedings{Ladkin-1988,
  author = {Peter B. Ladkin},
  title = {Satisfying First-order Constraints about Time Intervals},
  booktitle = {Proc.\null{} of {AAAI-88}, the {$7^{th}$} National Conf.
		on Artificial Intelligence},
  year = 1988,
  bibliographies = {RelMiCS}
}

@TechReport{Ladkin-Maddux-1987,
  author = {Peter B. Ladkin and Roger Duncan Maddux},
  title = {The Algebra of Convex Time Intervals},
  institution = {Kestrel Inst.},
  type = {Technical Report},
  number = {KES.U.87.2},
  year = 1987,
  month = MAR,
  bibliographies = {RelMiCS}
}

@Booklet{Ladkin-Maddux-1987a,
  author = {Peter B. Ladkin and Roger Duncan Maddux},
  title = {The Algebra of Constraint Satisfaction Problems
		and Temporal Reasoning},
  note = {Preprint, March 31, 1989, pp.\null{} 61},
  bibliographies = {RelMiCS}
}

@Booklet{Ladkin-Maddux-1988,
  author = {Peter B. Ladkin and Roger Duncan Maddux},
  title = {Representation and Reasoning with Convex Time Intervals},
  note = {Kestrel Inst.\null{} Technical Report KES.U.88.2,
		April 28, 1988, pp.\null{} 38},
  bibliographies = {RelMiCS}
}

@Article{Ladkin-Maddux-1992,
  author = {Peter B. Ladkin and Roger Duncan Maddux},
  title = {On Binary Constraint Problems},
  note = {submitted},
  journal = JACM,
  year = 1992,
  month = APR,
  bibliographies = {RelMiCS}
}

@Article{Ladkin-Maddux-1994,
  author = {Peter B. Ladkin and Roger D. Maddux},
  title = {On Binary Constraint Problems},
  journal = JACM,
  volume = 41,
  year = 1994,
  month = MAY,
  pages = {435--469},
  bibliographies = {RelMiCS}
}

@Unpublished{Laemmel-2004,
  author = "Ralf L{\"a}mmel",
  title = {Modular Generic Function Customisation},
  year = 2004,
  annote = "Draft; To be submitted; Online since 08 Dec 2004",
  note = {See the ``Scrap your boilerplate approach'' website,
    \url{http://www.cs.vu.nl/boilerplate}},
  URL = {http://homepages.cwi.nl/~ralf/syb3/},
  WKloc = {A-1603, doc/pap/BIB},
  abstract = {We enhance the "Scrap your boilerplate" approach to
     generic programming in Haskell by introducing a complementary
     technique for generic function customisation. The programmer can
     hand in type-specific cases in terms of Haskell's type classes,
     which implies that these cases can be scattered over different
     modules. Such modularity was out of reach for the preexisting
     customisation technique, which is based on type-safe cast and
     often combined with parameterisation. The new technique also
     improves on the preexisting technique in terms of efficiency: no
     runtime type checks have to be performed. Still the two
     techniques complement each other in terms of convenience and
     expressiveness. While the addition of modular customisation is a
     major improvement for the generic programmer, its implementation
     does not require any new language extension.}
}

@InProceedings{Laemmel-Ostermann-2006,
  author    = {Ralf L{\"a}mmel and Klaus Ostermann},
  title     = {Software Extension and Integration with Type Classes},
  crossref = {GPCE2006},
  publisher = {ACM Press},
  year      = 2006,
  month     = OCT,
  WKloc = {A-1655, doc/pap/BIB},
  URL = {http://homepages.cwi.nl/~ralf/gpce06/},
  abstract = {The abilities to extend a software module and to
        integrate a software module into an existing software system
        without changing existing source code are fundamental
        challenges in software engineering and programming-language
        design. We reconsider these challenges at the level of
        language expressiveness, by using the language concept of
        \emph{type classes}, as it is available in the functional
        programming language Haskell. A detailed comparison with
        related work shows that type classes provide a powerful
        framework in which solutions to known software extension and
        integration problems can be provided. We also pinpoint several
        limitations of type classes in this context.},
  note      = {To appear}
}

@InProceedings{Laemmel-PeytonJones-2003,
  author = 	 {Ralf L{\"a}mmel and Peyton Jones, Simon},
  title = 	 {Scrap Your Boilerplate: A Practical Design Pattern for Generic Programming},
  crossref =  {TLDI2003},
  pages = 	 {26--37},
  WKloc = 	 {A-1527, doc/pap/BIB},
  note = {See the ``Scrap your boilerplate approach'' website,
    \url{http://www.cs.vu.nl/boilerplate}},
  URL = {http://homepages.cwi.nl/~ralf/syb/},
  abstract = {We describe a design pattern for writing programs
     that traverse data structures built from rich mutually-recursive
     data types. Such programs often have a great deal of boilerplate
     code that simply walks the structure, hiding a small amount of
     real code that constitutes the reason for the traversal. Our
     technique allows most of this boilerplate to be written once and
     for all, or even generated mechanically, leaving the programmer
     free to concentrate on the important part of the algorithm. These
     generic programs are much more adaptive when faced with data
     structure evolution because they contain many fewer lines of
     typespecific code. Our approach is simple to understand,
     reasonably efficient, and it handles all the data types found in
     conventional functional programming languages. It makes essential
     use of rank-2 polymorphism, an extension found in some
     implementations of Haskell. Further it relies on a simple
     type-safe cast operator.}

}

@InProceedings{Laemmel-PeytonJones-2004,
  author    = {Ralf L{\"a}mmel and Peyton Jones, Simon},
  title     = {Scrap more boilerplate:
               reflection, zips, and generalised casts},
  crossref = {ICFP2004},
  pages     = {244--255},
  note = {See the ``Scrap your boilerplate approach'' website,
    \url{http://www.cs.vu.nl/boilerplate}},
  URL = {http://homepages.cwi.nl/~ralf/syb2/},
  WKloc = {A-1602, doc/pap/BIB},
  abstract = {Writing boilerplate code is a royal pain. Generic
     programming promises to alleviate this pain by allowing the
     programmer to write a generic ``recipe'' for boilerplate code,
     and use that recipe in many places. In earlier work we introduced
     the ``Scrap your boilerplate'' approach to generic programming,
     which exploits Haskell's existing type-class mechanism to support
     generic transformations and queries. This paper completes the
     picture. We add a few extra ``introspective'' or ``reflective''
     facilities, that together support a rich variety of serialisation
     and de-serialisation. We also show how to perform generic
     ``zips'', which at first appear to be somewhat tricky in our
     framework. Lastly, we generalise the ability to over-ride a
     generic function with a type-specific one. All of this can be
     supported in Haskell with independently-useful extensions:
     higher-rank types and type-safe cast. The GHC implementation of
     Haskell readily derives the required type classes for
     user-defined data types.}
}

@InProceedings{Laemmel-PeytonJones-2005,
  author   = {Ralf L{\"a}mmel and Peyton Jones, Simon},
  title    = {Scrap Your Boilerplate With Class: Extensible Generic Functions},
  crossref = {ICFP2005},
  pages    = {204--215},
  note = {See also the ``Scrap your boilerplate approach'' website,
    \url{http://www.cs.vu.nl/boilerplate}},
  URL = {http://homepages.cwi.nl/~ralf/syb3/},
  WKloc = {A-1625, doc/pap/BIB},
  abstract = {The "Scrap your boilerplate" approach to generic programming allows the programmer to write generic functions that can traverse arbitrary data structures, and yet have type-specific cases. However, the original approach required all the type-specific cases to be supplied at once, when the recursive knot of generic function definition is tied. Hence, generic functions were closed. In contrast, Haskell's type classes support open, or extensible, functions that can be extended with new type-specific cases as new data types are defined. In this paper, we extend the "Scrap your boilerplate" approach to support this open style. On the way, we demonstrate the desirability of abstraction over type classes, and the usefulness of recursive dictionaries.}
}

@Unpublished{Laemmel-Visser-2001,
  author = {R. L{\"a}mmel and J. Visser},
  title = {{Typed Combinators for Generic Traversal}},
  year = 2001,
  month = AUG,
  note = {To appear as CWI Technical Report},
  URL = {http://www.cs.vu.nl/Strafunski/},
  WKloc = {A-1188, doc/pap/BIB}
}

@InProceedings{Laemmel-Visser-2002,
  author = {R. L{\"a}mmel and J. Visser},
  title = {Design Patterns for Functional Strategic Programming},
  year = 2002,
  month = OCT,
  booktitle = {{RULE '02}},
  URL = {http://www.cs.vu.nl/Strafunski/},
  WKloc = {A-1516, doc/pap/BIB}
}

@inproceedings{Laemmel-Visser-2002PADL,
  author = {Ralf L{\"a}mmel and Joost Visser},
 title       = {Typed Combinators for Generic Traversal},
 booktitle   = "{Proc.\ Practical Aspects of Declarative Programming PADL 2002}",
 publisher   = "Springer-Verlag",
 series      = "LNCS",
 volume      = "2257",
 year        = 2002,
 month       = jan,
 pages       = "137--154"
}

@Unpublished{Laemmel-Visser-2002a,
  author = {Ralf L{\"a}mmel and Joost Visser},
  title = {Strafunski against Autism and Hypersensitivity --- Application Letter},
  year = 2002,
  month = AUG,
  URL = {http://www.cwi.nl/~ralf/tiptoe/},
  WKloc = {A-1353, doc/pap/BIB},
  note = {Draft paper},
  bibliographies = {MathScheme},
  abstract = {Tool development for software re- and reverse engineering,
     quality assessment, maintenance, and renovation poses many challenges.
      The tools must be robust and scalable to deal with real-world
      (big and diverse) software portfolios. Their development must be
      cheap and fast to meet cost and time-to-market expectations.
      Further, they must be flexible and pluggable to keep up
      with evolving requirements. In a selection of case studies,
      we demonstrate that typed functional programming,
      augmented with support for tool integration and generic traversal,
      is very appropriate for the development of practical language processors.
      In particular, we discuss using Haskell for Cobol reverse engineering,
      Java code metrics, and Haskell re-engineering. We emphasise two concerns.
      Firstly, it is crucial to enable integration of external components
      such as parsers, pretty printers,
      and graph visualisation tools based on suitable interchange formats.
      Secondly, generic traversal expressiveness is indispensable
      to deal with complex syntaxes and formats.
      Both concerns are faced by the Strafunski bundle.}
}

@TechReport{Laemmel-Visser-2002justTwo,
 author = "Ralf L{\"a}mmel and Joost Visser",
 title  = {Strategic polymorphism requires just two combinators!},
 institution = "arXiv",
 number = "cs.PL/0212048",
 year   = 2002,
 month  = dec,
 pages  = "15~pages",
 WKloc = {A-1518, doc/pap/BIB}
}

@inproceedings{Laemmel-Visser-2003,
  author = {Ralf L{\"a}mmel and Joost Visser},
  title       = {A Strafunski Application Letter},
 booktitle   = "{Proc.\ of Practical Aspects of Declarative Programming
                 (PADL'03)}",
 editor      = "V.~Dahl and P.~Wadler",
 publisher   = "Springer-Verlag",
 series      = LNCS,
 volume      = 2562,
 year        = 2003,
 month       = JAN,
 pages       = "357--375",
 WKloc = {doc/pap/BIB}
}

@Article{Laeufer-Odersky-1994,
  author = {Konstantin L{\"a}ufer and Martin Odersky},
  title = 	 {Polymorphic type inference and abstract data types},
  journal = 	 TOPLAS,
  year = 	 1994,
  volume = 	 16,
  number = 	 5,
  pages = 	 {1411--1430},
  DOI = 	 {http://doi.acm.org/10.1145/186025.186031},
  annote = 	 {existentially quantified datatypes},
  abstract = 	 {Many statically typed programming languages provide
                  an abstract data type construct, such as the module
                  in Modula-2. However, in most of these languages,
                  implementations of abstract data types are not
                  first-class values. Thus, they cannot be assigned to
                  variables, passed as function parameters, or
                  returned as function results. Several higher-order
                  functional languages feature strong and static type
                  systems, parametric polymorphism, algebraic data
                  types, and explicit type variables. Most of them
                  rely on Hindley-Milner type inference instead of
                  requiring explicit type declarations for
                  identifiers. Although some of these languages
                  support abstract data types, it appears that none of
                  them directly provides light-weight abstract data
                  types whose implementations are first-class
                  values. We show how to add significant expressive
                  power to statically typed functional languages with
                  explicit type variables by incorporating first-class
                  abstract types as an extension of algebraic data
                  types. Furthermore, we extend record types to allow
                  abstract components. The components of such abstract
                  records are selected using the dot
                  notation. Following Mitchell and Plotkin, we
                  formalize abstract types in terms of existentially
                  quantified types. We give a syntactically sound and
                  complete type inference algorithm and prove that our
                  type system is semantically sound with respect to
                  standard denotational semantics.}
}

@Article{Lafave-Gallagher-1998,
  author = {L. Lafave and J. P. Gallagher},
  title = {Extending the Power of Automatic Constraint-Based Partial Evaluators},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 15},
  WKloc = {A-0902, 63--68},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Lafont-1988,
  author = {Yves Lafont},
  title = {The Linear Abstract Machine},
  year = 1988,
  volume = 59,
  pages = {157--180},
  journal = TCS,
  WKloc = {A-0049},
  abstract = {Linear Logic [6] provides a refinement of functional
	programming and suggests a new implementation technique, with
	the following features:
	\begin{itemize}
	\item a synthesis of strict and lazy evaluation,
	\item a clean semantics of side effects,
	\item no garbage collector.
	\end{itemize}}
}

@InProceedings{Lafont-1990,
  author = {Yves Lafont},
  title = {Interaction Nets},
  abstract = {We propose a new kind of programming language, with the
             following features:
             \begin{itemize}
               \item a complete symmetry between constructors and destructors,
               \item a simple graph rewriting semantics,
               \item a type discipline for deterministic and
		  deadlockfree (microscopic) parallelism.
             \end{itemize}
             Interaction nets
             generalise Girard's proof nets of linear logic and illustrate
             the advantage of an integrated logic approach, as opposed to
             the external one. In other words, we did not try to design a
             logic describing the behaviour of some given computational
             system, but a programming language for which the type
             discipline is already (almost) a logic.

             In fact, we shall
             scarcely refer to logic, because we adopt a naive and pragmatic
             style. A typical application we have in mind for this language
             is the design of interactive softwares such as editors or
             window managers.},
  pages = {95--108},
  numpages = {14},
  DOIURL = {http://doi.acm.org/10.1145/96709.96718},
  DOI = {10.1145/96709.96718},
  crossref = {POPL1990},
  WKloc = {A-0168}
}

@Misc{Lafont-1997,
  author = {Yves Lafont},
  title = {Interaction Combinators},
  year = 1997,
  month = MAR,
  WKloc = {A-0829}
}

@Misc{Lai-2001,
  OPTauthor = {Albert Y. C. Lai},
  OPTtitle = {A Tool for A Formal Refinement Method},
  OPThowpublished = {},
  OPTmonth = MAY,
  OPTyear = 2001,
  OPTnote = {slides},
  keywords = {RTFM},
  WKloc = {doc/pap/BIB},
  contents = {\begin{verbatim}
1. Introduction: What is formal refinement? Why need a tool?

Solution: computer aid.
* Theorem prover for basic calculations and logic.
* Syntax-directed editor for subexpression-oriented editing.
Each has been separately available:
* Theorem provers: Coq, PVS, HOL, RefCalc (on top of HOL)
* Syntax-directed editors: MathSPad, maybe Emacs
This work is a start in integrating the two ideas into one.

2. Background: Hehner's theory of refinement.
3. Tool demonstration.
4. Tool critique.
5. Conclusion: Summary and future work.

We have a tool that:
* uses a theorem prover to automate simple calculations
* edits expressions by expression syntax

Future Work
* Provide useful semi-automatic inference commands.
* Record and allow the use of lemmata in inferences.
* Point programmer to unrefined specifications and unproved claims.
* Translate programmer's work into real source code.
* Support keyboard input.
* Provide undo, save, load, print, ...
\end{verbatim}}
}

@Book{Lakatos-1976,
  author =	 {Imre Lakatos},
  title = 	 {Proofs and refutations: the logic of mathematical discovery},
  publisher = 	 CambridgeUP,
  year = 	 1976,
  McMaster = 	 {QA 8.4 .L34},
  note =	 {edited by John Worrall and Elie Zahar}
}

@Article{LamM-1988,
	Address = {New York, NY, USA},
	Author = {M. Lam},
	Date-Added = {2006-08-08 00:43:37 -0400},
	Date-Modified = {2006-08-08 00:43:48 -0400},
	Doi = {http://doi.acm.org/10.1145/960116.54022},
	Issn = {0362-1340},
	Journal = {SIGPLAN Not.},
	Number = {7},
	Pages = {318--328},
	Publisher = {ACM Press},
	Title = {Software pipelining: an effective scheduling technique for {VLIW} machines},
	Volume = {23},
	Year = {1988},
  bibliographies = {Coconut}
}

@Article{Lambek-1958,
  author = {Lambek, J.},
  title = {The mathematics of sentence structure.},
  journal = American_Math_Monthly,
  year = 1958,
  volume = 65,
  number = 3,
  pages = {154--170},
  bibliographies = {RelMiCS}
}

@InCollection{Lambek-1980,
  author = {J. Lambek},
  title = {From $\lambda$-Calculus to Cartesian Closed Categories},
  crossref = {Seldin-Hindley-1980},
  pages = {375--402}
}

@InCollection{Lambek-1993,
  author = {J. Lambek},
  title = {From Categorial Grammar to Bilinear Logic},
  booktitle = {Substructural Logics},
  publisher = OXFORD_P,
  year = 1993,
  editor = {Kosta Do{\v{s}}en and Peter Schroeder-Heister},
  volume = {},
  series = {},
  type = {},
  chapter = {},
  pages = {207--238},
  address = {},
  edition = {},
  month = {},
  note = {},
  bibliographies = {RelMiCS}
}

@Book{Lambek-Scott-1986,
  author = {J. Lambek and P. J. Scott},
  title = {Introduction to Higher Order Categorical Logic},
  publisher = CambridgeUP,
  series = {Cambridge studies in advanced mathematics},
  volume = 7,
  year = 1986,
  WKloc = {Q-004},
  McMaster = {QA 169 .L28 1986},
  bibliographies = {RelMiCS}
}

@InProceedings{Lamping-1988,
  author = {J. Lamping},
  title = {A Unified System of Parametrization for Programming
		  Languages},
  pages = {316-326},
  booktitle = {Proceedings of the 1988 ACM Conference on LISP and
		  Functional Programming, Snowbird, UT},
  year = 1988,
  publisher = {ACM},
  address = {New York, NY},
  WKloc = {A-?},
  keywords = {abstraction}
}

@InProceedings{Lamping-1990,
  WKloc = {A-0555},
  keywords = {Levy},
  authorsAddress = {Xerox PARC},
  abstract = {We present an algorithm for lambda expression
		  reduction that avoids any copying that could later
		  cause duplication of work. It is optimal in the
		  sense defined by L\'evy. The basis of the algorithm
		  is a graphical representation of the kinds of
		  commonality that can arise from substitutions; the
		  idea can be adapted to represent other kinds of
		  expressions besides lambda expressions. The
		  algorithm is also well suited to parallel
		  implementations, consisting of a fixed set of local
		  graph rewrite rules.},
  title = {An Algorithm for Optimal Lambda Calculus Reduction},
  pages = {16--30},
  crossref = {POPL1990},
  author = {John Lamping},
  bibliographies = {RelMiCS},
  annote = {--- PLGnotes ---
		  capsulation modulo marked exits, therewith enabling
		  sharing of intervals}
}

@InProceedings{Lamping-1990-x,
  WKloc = {A-0555},
  keywords = {Levy},
  authorsAddress = {Xerox PARC},
  abstract = {We present an algorithm for lambda expression
		  reduction that avoids any copying that could later
		  cause duplication of work. It is optimal in the
		  sense defined by L\'evy. The basis of the algorithm
		  is a graphical representation of the kinds of
		  commonality that can arise from substitutions; the
		  idea can be adapted to represent other kinds of
		  expressions besides lambda expressions. The
		  algorithm is also well suited to parallel
		  implementations, consisting of a fixed set of local
		  graph rewrite rules.},
  title = {An Algorithm for Optimal Lambda Calculus Reduction},
  pages = {16--30},
  author = {John Lamping},
  annote = {--- PLGnotes ---
		  capsulation modulo marked exits, therewith enabling
		  sharing of intervals},
  UniBwM = {INF400/Z3061-17},
  year = 1990,
  booktitle = {17th Annual ACM Symposium on Principles of
		  Programming Languages},
  publisher = {acm press},
  month = JAN,
  address = {San Francisco, California}
}

@Article{Lamport-1977,
  author = 	 {Leslie Lamport},
  title = 	 {\unfinished},
  journal = 	 {\unfinished},
  year = 	 {1977},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  annote = 	 {Source of the concepts of ``safety: nothing bad ever happens'' and ``liveness: something good eventually happens''.}
}

@Manual{Lamport-2006a,
  title = 	 {The {${}^{+}\mathsc{cal}$} Algorithm Language},
  author =	 {Leslie Lamport},
  organization = {Microsogt Research},
  month =	 JUL,
  year =	 2006,
  WKloc = 	 {A-1673},
  keywords = {pluscal, TLA}
}

@InCollection{Lampson-1994,
  author = {Butler W. Lampson},
  title = {Implementing Coherent Memory},
  crossref = {Roscoe-1994},
  pages = {259--274},
  chapter = 16,
  OPTnote = {},
  OPTannote = {}
}

@Article{Landin-1966,
  author = {P. J. Landin},
  title = {The Next 700 Programming Languages},
  journal = {Communications of the ACM},
  volume = 9,
  number = 3,
  pages = {157--164},
  month = mar,
  year = 1966,
  keywords = {ISWIM},
  WKloc = {A-1733},
  DOIURL = {http://doi.acm.org/10.1145/365230.365257},
  note = {Originally presented at the Proceedings of the ACM
                 Programming Language and Pragmatics Conference, August
                 8--12, 1965.},
  abstract = {A family of unimplemented computing languages is
                 described that is intended to span differences of
                 application area by a unified framework. This framework
                 dictates the rules about the uses of user-coined names,
                 and the conventions about characterizing functional
                 relationships. Within this framework the design of a
                 specific language splits into two independent parts.
                 One is the choice of written appearances of programs
                 (or more generally, their physical representation). The
                 other is the choice of the abstract entites (such as
                 numbers, character-strings, lists of them, functional
                 relations among them) that can be referred to in the
                 language. The system is biased towards ``expressions''
                 rather than ``statements.'' It includes a nonprocedural
                 (purely functional) subsystem that aims to expand the
                 class of users' needs that can be met by a single
                 print-instruction, without sacrificing the important
                 properties that make conventional right-hand-side
                 expressions easy to construct and understand.},
  sjb = {Block structure in ISWIM is indicated by indentation,
                 though it isn't the number of spaces that count, it is
                 vertical alignment -- this allows for proportional
                 fonts. Much of the discussion after the paper is on
                 this issue rather than on the semantic issues raised in
                 the paper! Describes the abstract structure of
                 expressions in ISWIM and gives equivalence rules that
                 can be used to transform ISWIM expressions to a small
                 kernel. Emphasises the elimination of explicit
                 sequencing.},
  bibliographies = {FP}
}

@InCollection{Landin-1966a,
  author = {P. J. Landin},
  editor = {L. Fox},
  title = {A Lambda-Calculus Approach},
  booktitle = {Advances in Programming and Nonnumerical Computation},
  pages = {97--141},
  publisher = {Pergamon Press},
  address = {Oxford, UK},
  year = 1966,
  keywords = {functional}
}

@TechReport{Laneve-1996,
  author = {Cosimo Laneve},
  title = {May and Must Testing in the Join Calculus},
  year = 1996,
  WKloc = {A-0431}
}

@InProceedings{Laneve-Montanari-1992,
  authorsAddress = {Pisa},
  title = {Axiomatizing Permutation Equivalence in the
		  $\lambda$-Calculus},
  pages = {350--363},
  crossref = {ALP1992},
  author = {Cosimo Laneve and Ugo Montanari},
  annote = {Comparison between Meseguer's semantics of his
		  rewriting logic and permutation equivalence},
  WKloc = {A-0838}
}

@Article{Laneve-Montanari-1996,
  author = {Cosimo Laneve and Ugo Montanari},
  title = {Axiomatizing Permutation Equivalence},
  journal = {Math.~Struct.~in Com.~Science},
  year = 1996,
  volume = 6,
  OPTnumber = {},
  OPTmonth = {},
  pages = {219--249},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0839`},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Book{Lano-1996,
  author = {K. Lano},
  title = {The {B} Language and Method: A Guide to Practical Formal Development},
  publisher = Springer,
  year = 1996,
  McMaster = {QA 76.73 .B155 L36 1996}
}

@Book{Lano-2005,
  author =	 {Kevin Lano},
  title = 	 {Advanced Systems Design with {Java}, {UML} and {MDA}},
  publisher = 	 {Butterworth-Heinemann},
  year = 	 2005,
  ISBN = 0750664967,
  contents = 	 {
    * Preface
    * The Challenges of Software Design
    * The Unified Modelling Language
    * The Object Constraint Language
    * UML Dynamic Modelling Notations
    * Platform-Independent Design
    * From Platform-Specific Models to Executable Code
    * Internet System Design
    * Web Services
    * Implementing the Model-Driven Architecture
    * Case Studies of Web System Development
    * Catalogue of Model Transformations
    * Bibliography
    * Appendices
    * Index},
  abstract = {
    * Explains how to design and construct large software systems,
      using a series of examples through the design process

    * Examines issues raised by the Model-Driven Architecture
      approach, particularly construction of PIMs and the
      implementation of PIM to PIM and PIM to PSM transformations

    * Focuses on internet applications and technologies

    * Uses accessible case studies to illustrate complex concepts

    This excellent text shows how to design and construct large
    software systems using interesting and relevant examples a
    Scrabble Player, a juke box using web streaming, a security system
    and others. The benefits of using UML are described, and the
    issues of model construction and transformation raised by the
    Model-Driven Architecture approach to development are explored. A
    style of abstract declarative specification, independent of
    particular implementation choices, is promoted. The text places
    particular emphasis on Internet applications and technologies as
    well as covering web services. Flash. XML XSLT. SOAP, Servlets,
    Javascript and JSP.

    Advanced System Design with Java. UML and MDA is aimed at
    intermediate and advanced students on undergraduate and post
    graduate courses in Computer Science, Information Systems and
    IT. To benefit most from this book, the reader will need an
    introductory knowledge of software engineering, programming in
    Java and a basic knowledge of HTML.}
}

@InProceedings{Laplaza-1972,
  title = {Coherence for Distributivity},
  author = {M. Laplaza},
  pages = {29--66},
  booktitle = {???},
  year = 1972,
  series = {LNM},
  volume = 281,
  publisher = {Springe-Verlag},
  keywords = {distributive categories, bi-monoidal categories, two tensors}
}

@InProceedings{Lassez-1991,
  title = {From {LP} to {LP}: Programming with Constraints},
  author = {Jean-Louis Lassez},
  pages = {420--446},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {Constraint methods for problem solving have a long
		  history. Recently the problem of introducing
		  constraints as primitive constructs in programming
		  languages has been addressed.  A main task that the
		  designers and implementers of such languages face is
		  to use and adapt the concepts and algorithms from
		  the extensive studies on constraints done in areas
		  such as Mathematical Programming, Symbolic
		  Computation, Artificial Intelligence, Program
		  Verification and Computational Geometry.  Borrowing
		  from these areas and synthesizing the various
		  notions leads to an emerging conception of
		  programming with constraints that we will describe
		  here informally.}
}

@InProceedings{Lattner-Adve-2004,
  author    = {Chris Lattner and Vikram Adve},
  title     = {{LLVM}: A Compilation Framework for Lifelong Program Analysis \& Transformation},
  booktitle = {{Proceedings of the 2004 International Symposium on Code Generation and Optimization (CGO'04)}},
  address   = {Palo Alto, California},
  month     = MAR,
  year      = {2004},
  URL = {http://llvm.cs.uiuc.edu/},
  WKloc = {doc/Lang/LLVM}
}

@Book{Lauer-1993,
  ISBN = {3-540-56883-2},
  year = 1993,
  volume = 693,
  title = {Functional Programming, Concurrency, Simulation and
		  Automated Reasoning: International Lecture Series
		  1991--1992, {McMaster Univ.}, {Hamilton, Ontario, Canada}},
  series = LNCS,
  publisher = Springer,
  pages = 398,
  editor = {Peter E. Lauer},
  address = {Berlin},
  bibliographies = {RelMiCS}
}

@InProceedings{Launchbury-1993,
  author = {John Launchbury},
  title = {A Natural Semantics for Lazy Evalution},
  pages = {144--154},
  abstract = {We define an operational semantics for lazy evalution which
             provides an accurate model for sharing. The only computational
             structure we introduce is a set of bindings which corresponds
             closely to a heap. The semantics is set at a considerably
             higher level of abstraction than operational semantics for
             particular abstract machines, so is more suitable for a variety
             of proofs. Furthermore, because a heap is explicitly modelled,
             the semantics provides a suitable framework for studies about
             space behaviour of terms under lazy evalution.},
  crossref = {POPL1993},
  WKloc = {A-0192}
}

@Misc{Launchbury-199X,
  author = {John Launchbury},
  title = {Lazy Imperative Programming},
  year = {199?},
  WKloc = {A-0586}
}

@InProceedings{Launchbury-Paterson-1996,
  author = {John Launchbury and Ross Paterson},
  title = {Parametricity and Unboxing with Unpointed Types},
  crossref = {ESOP1996},
  pages = {204--218},
  abstract = {In lazy functional languages, bottom is typically an element
      of every type. While this provides great flexibility, it also comes
      at a cost. In this paper we explore the consequences of allowing
      unpointed types in a lazy functional language like Haskell. We use
      the type (and class) system to keep track of pointedness, and show
      the consequences for parametricity and for controlling evaluation
      order and unboxing.},
  URL = {http://www.soi.city.ac.uk/~ross/papers/pointed.html},
  WKloc = {A-0461}
}

@InProceedings{Laurent-2001,
  author = {Olivir Laurent},
  title = {A Token Machine for Full Geometry of Interaction (Extended Abstract)},
  crossref = {TLCA2001},
  pages = {283--297}
}

@MastersThesis{Laurikari-2001,
  author = 	 {Ville Laurikari},
  title = 	 {Efficient submatch addressing for regular expressions},
  school = 	 {Helsinki University of Technology, Department of Computer Science and Engineering},
  year = 	 2001,
  month =	 NOV,
  WKloc = 	 {A-1657, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {String pattern matching in its different forms is an
      important topic in theoretical computer science. This thesis
      concentrates on the problem of regular expression matching with
      submatch addressing, where the position and extent of the
      substrings matched by given subexpressions must be provided.

      The algorithms in widespread use at the time either take
      exponential worst-case time to find a match, can handle only a
      subset of all regular expressions, or use space proportional to
      the length of the input string where constant space would
      suffice. In this thesis I propose a new method for solving the
      submatch addressing problem using nondeterministic finite
      automata with transitions augmented by copy-on-write update
      operations.

      The resulting algorithm makes a single pass over the input
      string, always using time linearly proportional to the
      input. Space consumption depends only on the used regular
      expression, and not on the input string. To the author's
      knowledge, this is a new result. A prototype of a POSIX.2
      compatible regular expression matcher using the algorithm was
      done. Benchmarking results indicate that the prototype compares
      favorably against some popular implementations. Furthermore,
      absence of exponential or polynomial time worst cases makes it
      possible to use any regular expression without performance
      problems, which is not the case with previous implementations or
      algorithms.}
}

@Article{Lavalette-1992,
  author = {Renardel de Lavalette, Gerard R.},
  title = {Strictness Analysis via Abstract Interpretation for
                   Recursively Defined Types},
  journal = {Information and computation},
  year = 1992,
  volume = 99,
  number = 2,
  pages = 154,
  month = AUG
}

@Article{Laville-1991,
  author = {A. Laville},
  title = {Comparison of Priority Rules in Pattern Matching and Term Rewriting},
  journal = {Journal of Symbolic Computation},
  year = 1991,
  volume = 11,
  OPTnumber = {},
  OPTmonth = {},
  pages = {321--347},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  OPTwkloc = {},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Unpublished{Lawford-Froebel-Moum-2000,
  author = {M. Lawford and P. Froebel and G. Moum},
  title = {Application of Tabular Methods to the Specification
                  and Verification of a Nuclear Reactor Shutdown
                  System},
  journal = {Formal Methods in System Design},
  note = {Submitted to Formal Methods in System Design},
  URL = {http://www.cas.mcmaster.ca/~lawford/papers/FMSD.html},
  WKloc = {A-1287 abandoned in Tunis 2008, doc/pap/BIB},
  bibliographies = {RelMiS},
  abstract = {This paper describes the use of tabular methods at
       Ontario Power Generation Inc.\ (OPGI)
      \footnote{Ontario Power Generation Inc.\null{} is the
                electricity generation company created from Ontario Hydro
                on April 1, 1999.}
       on the Darlington Nuclear Generating Station Shutdown System (SDS)
       Trip Computer Software Redesign Project.
       We first motivate the selection of tabular methods and provide an
       overview of the Systematic Design Verification (SDV) procedure.
       After reviewing some preliminary concepts, the paper describes how
       the Software Engineering Standards and Methods (SESM) Tool suite
       is used with SRI's automated proof assistant, PVS, to provide
       tool support for the use of
       tabular methods in the software engineering process.
       Examples based upon the Systematic Design Verification
       of an actual SDS subsystem are used to illustrate the benefits
       and limitations of the current implementation of the formal method.
       Finally, the paper discusses related work,
       draws conclusions regarding the effectiveness of the method
       and examines how its limitations can be addressed
       by further theoretical and applied work.}
}

@InProceedings{Lawford-McDougall-Froebel-Moum-2000,
  author =       {M. Lawford and J. McDougall and P. Froebel and
                  G. Moum},
  title =        {Practical application of functional and relational
                  methods for the specification and verification of
                  safety critical software},
  booktitle =    "Proceedings Algebraic Methodology and Software
                  Technology, 8th International Conference, AMAST
                  2000, Iowa City, Iowa, USA, May 2000",
  year =         2000,
  editor =       "T. Rus",
  volume =       1816,
  series =       "LNCS",
  pages =        "73--88",
  publisher =    "Springer",
  key =          "LMFM00"
}

@Article{Lawford-Pantelic-Zhang-2006,
  author = 	 {M. Lawford and V. Pantelic and H. Zhang},
  title = 	 {Towards Integrated Verification of Timed Transition Models},
  journal = 	 FUNDI,
  year = 	 {2006},
  volume =	 {70},
  number =	 {1--2},
  pages =	 {75--110},
  month =	 JAN,
  abstract = {This paper describes an attempt to combine
    theorem proving and model-checking
    to formally verify real-time systems in a discrete time setting.
    The Timed Automata Modeling Environment (TAME)
    has been modified to provide a formal model
    for Time Transition Models (TTMs) in the PVS proof checker.
    Strong and weak state-event observation equivalences
    are formalized in PVS for state-event labeled transition systems (SELTS),
    the underlying semantic model of TTMs.
    The state-event equivalences form the basis
    of truth value preserving abstractions for a real-time temporal logic.
    When appropriate restrictions are placed upon the TTMs,
    their PVS models can be easily translated
    into input for the SAL model-checker.
    A simple real-time control system
    is specified and verified using these theories.
    While these preliminary results indicate
    that the combination of PVS and SAL
    could provide a useful environment to perform equivalence verification,
    model-checking and compositional model reduction of real-time systems,
    the current implementation in the general purpose SAL model-checker
    lags well behind state of the art real-time model-checkers.}
}

@Article{Lawrence-2001,
  author = {Steve Lawrence},
  title = {Online or Invisible?},
  journal = {Nature},
  year = 2001,
  authorsAffiliation = {NEC Research Institute},
  volume = 411,
  number = 6837,
  pages = 521,
  URL = {http://www.neci.nec.com/~lawrence/papers/online-nature01/},
  WKloc = {doc/pap/BIB},
  abstract = {Articles freely available online are more highly cited. For
      greater impact and faster scientific progress, authors and publishers
      should aim to make research easy to access.

      The volume of scientific literature typically far exceeds the ability
      of scientists to identify and utilize all relevant information in
      their research. Improvements to the accessibility of scientific
      literature, allowing scientists to locate more relevant research
      within a given time, have the potential to dramatically improve
      communication and progress in science. With the web, scientists now
      have very convenient access to an increasing amount of literature
      that previously required trips to the library, inter-library loan
      delays, or substantial effort in locating the source. Evidence shows
      that usage increases when access is more convenient [2], and
      maximizing the usage of the scientific record benefits all of
      society.

      Although availability varies greatly by discipline, over a million
      research articles are freely available on the web. Some journals and
      conferences provide free access online, others allow authors to post
      articles on the web, and others allow authors to purchase the right
      to post their articles on the web.

      In this article we investigate the impact of free online availability
      by analyzing citation rates. We do not discuss methods of creating
      free online availability, such as time-delayed release or
      publication/membership/conference charges. Online availability of an
      article may not be expected to greatly improve access and impact by
      itself. For example, efficient means of locating articles via web
      search engines or specialized search services is required, and a
      substantial percentage of the literature needs to be indexed by these
      search services before it is worthwhile for many scientists to use
      them. Computer science is a forerunner in web availability -- a
      substantial percentage of the literature is online and available
      through search engines such as Google (google.com), or specialized
      services such as ResearchIndex [1] (researchindex.org). Even so, the
      greatest impact of the online availability of computer science
      literature is likely yet to come, because comprehensive search
      services and more powerful search methods have only become available
      recently.

      We analyzed 119,924 conference articles in computer science and
      related disciplines, obtained from DBLP (dblp.uni-trier.de). In
      computer science, conference articles are typically formal
      publications and are often more prestigious than journal articles,
      with acceptance rates at some conferences below 10%. Citation counts
      and online availability were estimated using ResearchIndex. The
      analysis excludes self-citations, where a citation is considered to
      be a self-citation if one or more of the citing and cited authors
      match.}
}

@Article{Lawvere-1963,
  author = {F. William Lawvere},
  title = {Functorial Semantics of Algebraic Theories},
  journal = {Proc.\null{} Nat.\null{} Acad.\null{} Sci.\null{} USA},
  volume = 50,
  year = 1963,
  pages = {869--872}
}

@PhDThesis{Lawvere-1963a,
  author = {F. William Lawvere},
  title = {Functorial Semantics of Algebraic Theories},
  school = {Columbia University},
  year = 1963
}

@Article{Lawvere-1964,
  author = {F. William Lawvere},
  title = {An Elementary Theory of the Category of Sets},
  journal = {Proc.\null{} Nat.\null{} Acad.\null{} Sci.\null{} USA},
  volume = 52,
  year = 1964,
  pages = {1506--1511}
}

@InProceedings{Lawvere-1973,
  author = {F. William Lawvere},
  title = {Metric Spaces, Generalised Logic, and Closed Categories},
  year = 1973,
  booktitle = {Rend. del Sem. Mat. e Fis. di Milano},
  volume = 43
}

@Book{Lawvere-Schanuel-1993,
  author = {F. William Lawvere and Stephen H. Schanuel},
  title = {Conceptual Mathematics, A First Introduction to Categories},
  publisher = {Buffalo Workshop Press},
  year = 1991,
  address = {PO Box 171, Buffalo, NY 14226 USA},
  edition = {Revised 1993},
  ISBN = {0-9631805-1-7},
  UniBwM = {MAT200/X10810}
}

@Book{Lawvere-Schanuel-1997,
  author = {F. William Lawvere and Stephen H. Schanuel},
  title = {Conceptual Mathematics --- A First Introduction to Categories},
  publisher = CambridgeUP,
  year = 1997,
  ISBN = {0-521-47249-0 hardcover, 0-521-47817-0 pb},
  McMaster = {QA 169 .L355 2000}
}

@Manual{LeFessant-1999gwml,
  title = {{GWML}: The Generic Windowm-Manager in {ML}},
  author = {Le Fessant, Fabrice},
  organization = {INRIA Rocquencourt},
  month = MAR,
  year = 1999,
  WKloc = {A-0789}
}

@InProceedings{LeFessant-Maranget-2001,
  author = {Le Fessant, Fabrice and Luc Maranget},
  title = {Optimizing Pattern Matching},
  crossref = {ICFP2001},
  pages = {26--37},
  bibliographies = {PMC},
  abstract = {We present improvements to the backtracking technique of
      pattern matching compilation. Several optimizations are introduced,
      such as commutation of patterns, use of exhaustiveness information,
      and control flow optimization through the use of labeled static
      exceptions and context information. These optimizations have been
      integrated in the Objective-Caml compiler. They have shown good
      results in increasing the speed of pattern-matching intensive
      programs, without increasing final code size.}
}

@Book{Lea-1997,
  author = {Doug Lea},
  title = {Concurrent Programming in {Java}, Design Principles and Patterns},
  publisher = {Addison Wesley},
  year = 1997,
  series = {The Java Series},
  URL = {http://java.sun.com/Series/},
  McMaster = {QA 76.73 .J38L4 1997},
  bibliographies = {SE3B},
  contents = {\begin{tabular}{l@{\hskip1em}r}
    Introduction & 1 \\
    Safety & 35 \\
    Liveness & 57 \\
    State-Dependent Action & 79 \\
    Concurrency Control & 121 \\
    Services in Threads & 165 \\
    Flow & 213 \\
    Coordinated Interaction & 251 \\
    Index & 313
  \end{tabular}}
}

@MastersThesis{Ledig,
  year = 1991,
  title = {Spezifikation und {Implementierung} eines
		  zweischichtigen {Typisierungssystems} im {DAG-Kontext}
		  als {HOPS-Teilsystem}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = {ID 48/91},
  month = DEC,
  author = {Michael Ledig}
}

@Book{Lee-1991,
  editor = {Lee, Peter},
  title = {Topics in Advanced Language Implementation},
  publisher = {The MIT Press},
  year = 1991,
  ISBN = {0-262-12151-4},
  address = {Cambridge, Massachusetts},
  abstract = {Topics in Advanced Language Implementation brings
               together fifteen contributions at the leading edge of
               research in programming language implementation
               techniques These take p many of the important issues
               faced by researchers and practitioners in getting
               advanced languages such as LISP, Scheme, and ML to run
               efficiently on tofay's computers and operating system.
               The topics cover advanced implementation techniques,
               practice and experience with advanced implementations,
               parallel and distributed languages, and new
               unconventional languages and techniques. Contributions
               in Part I focus on the implementation of tags and run-
               time type checking, advanced register allocation, flow
               analysis and type recovery for scheme, garbage
               collection, and cocnurrent garbage collection for C++.
               Those in Part II discuss design considerations for CMU
               Common LISP, compilation issues in the Scheme
               implementation for the 88000, and the implementation of
               Oaklisp. Three chapters in Part II look at futures, an
               experimental implementation of Connection Machine Lisp,
               and ineritance of synchronization and recovery
               properties in Avalon/C++. Part IV concludes with
               descriptions of a semifunctional implementation of
               higher-order logic programming, the architecture of the
               PRL mathematics environment, a simple implementation of
               object storage for Common Lisp, and architecture
               considerations for combinator graph reduction. Contents:
               Advanced Implementation Techniques Practice and
               Experience with Advanced Implementations Languages for
               Parallel and Distributed Systems New and Unconventional
               Languages and Techniques.}
}

@Article{Lee-Kedem-2002,
  author = {Peizong Lee and Zvi Meir Kedem},
  title = {Automatic data and computation decomposition on distributed memory parallel computers},
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = 24,
  number = 1,
  year = 2002,
  ISSN = {0164-0925},
  pages = {1--50},
  doi = {http://doi.acm.org/10.1145/509705.509706},
  publisher = {ACM Press},
  bibliographies = {Anand}
}

@InProceedings{Lee-Ryder-Fiuczynski-1994,
  author = {Yong-Fong Lee and Barbara G. Ryder and Marc E. Fiuczynski},
  title = {Region Analysis: A Parallel Elimination Method for
		  Data Flow Analysis},
  crossref = {ICCL94},
  pages = {31--42}
}

@Book{LeeLieQuan-Lumsdaine-Siek-2001,
  author = 	 {Lie-Quan Lee and Andrew Lumsdaine and Jeremy G. Siek},
  title = 	 {The {Boost} Graph Library: User Guide and Reference Manual},
  publisher = 	 {Addison Wesley Professional},
  year = 	 {2001},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  pages = 	 {352},
  OPTseries = 	 {{C++} In-Depth Series},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  ISBN = 	 {0201729148},
  URL = 	 {http://www.awprofessional.com/title/0201729148},
  note = 	 {See also the Boost library page
                  \url{http://www.boost.org/libs/graph/doc/}}
}

@Article{Leeming-Walters-1992,
  author = {Mark Leeming and R.F.C. Walters},
  title = {Computing left {Kan} extensions using the {Todd-Coxeter} Procedure},
  OPTjournal = {},
  year = 1992,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {Submitted. {\tt ftp://maths.usyd.edu.au/sydcat/papers/walters}}
}

@Misc{Leijen-2000,
  author = {Daan Leijen},
  year = 2000,
  title = {Parsec, a fast combinator parser},
  note = {URL:\hfil\strut\penalty-2000 \textsf{http://www.cs.uu.nl/\~{}daan/parsec.html}},
  WKloc = {A-1008, doc/pap/BIB}
}

@Misc{Leijen-2001a,
  author = {Daan Leijen},
  year = 2001,
  month = {4 Oct.},
  title = {Parsec, a fast combinator parser},
  note = {URL:\hfil\strut\penalty-2000 \textsf{http://www.cs.uu.nl/\~{}daan/parsec.html}},
  ORIGURL = {http://www.cs.uu.nl/~daan/parsec.html},
  URL = {http://research.microsoft.com/en-us/um/people/daan/download/parsec/parsec.pdf},
  WKloc = {A-1229, doc/pap/BIB}
}

@InProceedings{Leijen-2005,
  author    = {Leijen, Daan},
  title     = {Extensible records with scoped labels},
  booktitle = {Proceedings of the 2005 Symposium on Trends in Functional Programming (TFP'05)},
  location  = {Tallin, Estonia},
  month     = sep,
  year      = 2005,
  URL = {http://www.cs.uu.nl/~daan/pubs.html},
  WKloc = {doc/pap/BIB},
  abstract = {Records provide a safe and flexible way to construct
     data structures. We describe a natural approach to typing
     polymorphic and extensible records that is simple, easy to use in
     practice, and straightforward to implement. A novel aspect of
     this work is that records can contain duplicate labels,
     effectively introducing a form of scoping over the
     labels. Furthermore, it is a fully orthogonal extension to
     existing type systems and programming languages. In particular,
     we show how it can be used conveniently with standard
     Hindley-Milner, qualified types, and ML-F. The records are
     implemented in the experimental Morrow interpreter. There is
     separate technical report that gives the constructive proofs of
     soundness and completeness.}
}

@InProceedings{Leijen-Meijer-1999,
  author = {Daan Leijen and Erik Meijer},
  title = {Domain Specific Embedded Compilers},
  booktitle = {{2nd USENIX Conference on Domain-Specific Languages (DSL), Austin, USA, October 1999}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  WKloc = {A-1212},
  URL = {http://www.cs.uu.nl/~daan/papers/dsec.html},
  annote = {uses ``phantom types''},
  WKloc = {doc/pap/BIB},
  abstract = {Domain-specific embedded languages (DSELs) expressed in
      higher-order, typed (HOT) languages provide a composable framework
      for domain-specific abstractions. Such a framework is of greater
      utility than a collection of stand-alone domain-specific languages.
      Usually, embedded domain specific languages are build on top of a set
      of domain specific primitive functions that are ultimately
      implemented using some form of foreign function call. We sketch a
      general design pattern for embedding client-server style services
      into Haskell using a domain specific embedded compiler for the
      server's source language. In particular we apply this idea to
      implement HaskellDB, a domain specific embedded compiler that
      dynamically generates of SQL queries from monad comprehensions, which
      are then executed on an arbitrary ODBC database server.}
}

@TechReport{Leijen-Meijer-2001,
  author      = {Daan Leijen and Erik Meijer},
  title       = {Parsec: Direct Style Monadic Parser Combinators for the Real World},
  institution = {Department of Computer Science, Universiteit Utrecht},
  number      = {UU-CS-2001-27},
  year        = 2001,
  note = {See also: \url{http://www.cs.uu.nl/~daan/parsec.html}}
}

@InProceedings{Leino-vandeSnepscheut-1994,
  author = {K.R.M. Leino and van de Snepscheut, J.L.A.},
  title = {Semantics of Exceptions},
  crossref = {PROCOMET94},
  pages = {440--459},
  keywords = {Specifying and Verifying and Reasoning about
		  Programs; Semantics of Programming Languages;
		  Studies of Program Constructs}
}

@Book{Leinster-2004,
  editor = 	 {Tom Leinster},
  title = 	 {Higher Operads, Higher Categories},
  publisher = 	 CUP,
  series = London,
  volume = 298,
  year = 	 {2004},
  McMaster = 	 {QA 1 .L66 v. 298},
  bibliographies = {RelMiCS},
  annote = {source for monoidal categories}
}

@TechReport{Leischner-Gritzner-1992,
  author = {Leischner, M. and Thomas F. Gritzner},
  title = {Relating Relational Products to Categorical Products},
  institution = {Univ.\null{}  M\"unchen},
  number = {No.\ 9201},
  year = 1992,
  bibliographies = {RelMiCS}
}

@InProceedings{Leiss-1993,
  author = {Hans Lei\3},
  title = {Combining Recursive and Dynamic Types},
  pages = {258--273},
  abstract = {A denotational semantics of simple typed lambda calculus with
             a basic type dynamic, modeling values whose type is to be
             inspected at runtime, has been given by Abadi e.a.[1]. We
             extend this interpretation to cover (formally contractive)
             recursive types as well. Soundness of typing reles and freeness
             of run-time type errors for well-typed programs hold.

             The
             interpretation works also for implicitly polymorphic languages
             like ML with dynamic and recursive types, and for explicitly
             polymorphic languages under the types-as ideals interpretation.},
  crossref = {TLCA93},
  WKloc = {A-0185}
}

@Article{Leiss-2004,
  author = 	 {Hans Lei\3},
  title = 	 {{Kleene} Modules and Linear Languages},
  journal = 	 {},
  year = 	 {2004},
  OPTkey = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  WKloc = 	 {A-1554, doc/pap/BIB},
  abstract = {A Kleene algebra $(K, +, \cdot, {}^*, 0, 1)$ is an
     idempotent semiring with an iteration ${}^*$ as axiomatized by
     Kozen. We consider left semiring modules $(A, +, 0)$ over Kleene
     algebras. We call such a left semiring module a Kleene module if
     each linear equation $x = a+r : x$ has a least solution, where
     $:$ is the product from $K \times A$ to $A$. The linear
     context-free languages can be viewed as a Kleene module $A$ over
     a Kleene algebra $R$ of binary regular word relations. Thus, the
     simultaneous linear fixed-point operator $\mu$ on languages can
     be reduced to iteration ${}^*$ on $R$ and the scalar product
     $:$.}
}

@Article{Leivant-1986,
  author = {Daniel Leivant},
  title = {Typing and Computational Properties of Lambda Expressions},
  WKloc = {A-0047},
  year = 1986,
  volume = 44,
  pages = {51-68},
  journal = TCS,
  bibliographies = {RelMiCS}
}

@Book{Lenzerini-Nardi-Simi-1991,
  UniBwM = {KYB800/V7740},
  year = 1991,
  title = {Inheritance Hierarchies in Knowledge Representation and
      Programming Languages},
  publisher = Wiley,
  editor = {Maurizio Lenzerini and Daniele Nardi and Maria Simi},
  bibliographies = {RelMiCS}
}

@Article{Leone-Lee-1998,
  author = {Mark Leone and Peter Lee},
  title = {Dynamic Specialization in the Fabius System},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 23},
  WKloc = {A-0902, 85--90},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@PhdThesis{Leoniuk-2001,
  author = 	 {Barbara Leoniuk},
  title = 	 {{ROBDD}-basierte Implementierung von Relationen und relationalen Operationen mit Anwendungen},
  school = 	 {Institut f\"ur Informatik und Praktische Mathematik,
                   Christian-Albrechts-Universit\"at Kiel},
  year = 	 {2001},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Lerner-Grove-Chambers-2002,
  author = {Sorin Lerner and David Grove and Craig Chambers},
  title = {Composing dataflow analyses and transformations},
  crossref = {POPL2002},
  pages = {270--282},
  WKloc = {doc/pap/BIB},
  bibliographies = {Coconut},
  doi = {http://doi.acm.org/10.1145/503272.503298},
}

@InProceedings{Leroy-1992,
  author = {Xavier Leroy},
  title = {Unboxed Objects and Polymorphic Typing},
  crossref = {POPL1992},
  pages = {177--188},
  abstract = {This paper presents a program transformation that
		  allows languages wit polymorphic typing (e.g.~ML) to
		  be implemented with unboxed, multi-word data
		  representations, more effizient than the
		  conventional boxed representations. The
		  transformation introduces coercions between various
		  representations, based on a typing derivation. A
		  prototype ML compiler utilizing this transformation
		  demonstrates important speedups.}
}

@InProceedings{Leroy-1993,
  author = {Xavier Leroy},
  title = {Polymorphism by name for references and continuations},
  pages = {220--231},
  abstract = {This article investigates an ML-like language with by name
             semantics for polymorphism: polymorphic objects are not
             evaluated once for all at generalization time, but re-evaluated
             at each specialization. Unlike the standard ML semantics, the
             by-name semantics works well with polymorphic references and
             polymorphic continuations: the naive typing rules for
             references and for continuations are sound with respect to this
             semantics. Polymorphism by name leads to a better integration
             of these imperative features into the ML type discipline.
             Practical experience shows that it retains most of the
             efficiency and predictability of polymorphism by value.},
  crossref = {POPL1993},
  WKloc = {A-0197}
}

@InProceedings{Leroy-1994,
  author = {Xavier Leroy},
  title = {Manifest Types, Modules, and Separate Compilation},
  crossref = {POPL1994},
  pages = {109--122},
  authorsAddress = {Stanford},
  WKloc = {A-0395},
  keywords = {manifest types, weak sums},
  abstract = {This paper presents a variant of the SML module
		  system that introduces a strict distinction between
		  abstract typesd and manifest types (types whose
		  definitions are part of the module specification),
		  while retaining most of the expressive power of the
		  SML module system. The resulting module system
		  provides much better support for separate compilation.}
}

@InProceedings{Leroy-1995,
  author = {Xavier Leroy},
  title = {Applicative Functors and Fully Transparent Higher-Order Modules},
  crossref = {POPL1995},
  pages = {142--153},
  OPTabstract = {},
  WKloc = {A-0616, doc/pap/BIB}
}

@TechReport{Leroy-1996,
  author = {Xavier Leroy},
  title = {A Modular Module System},
  year = 1996,
  WKloc = {A-0493},
  institution = {INRIA},
  number = 2866
}

@Article{Leroy-1996a,
  author = {Xavier Leroy},
  title = {A Syntactic Theory of Type Generativity and Sharing},
  journal = {Journal of Functional Programming},
  year = 1996,
  volume = 6,
  number = 5,
  month = SEP,
  pages = {1--32},
  DOI = {10.1017/S0956796800001933},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0767},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Leroy-2003a,
  author = {Xavier Leroy},
  title = {A proposal for recursive modules in {Objective Caml}},
  year = 2003,
  WKloc = {A-1484, doc/pap/BIB},
  keywords = {OCaml},
  bibliographies = {modclass}
}

@article{Leroy-2009_Compcert-CACM,
  author = {Xavier Leroy},
  title = {Formal Verification of a Realistic Compiler},
  journal = {Communications of the ACM},
  year = 2009,
  volume = 52,
  number = 7,
  pages = {107--115},
  OPTURL = {http://gallium.inria.fr/~xleroy/publi/compcert-CACM.pdf},
  urlpublisher = {http://doi.acm.org/10.1145/1538788.1538814},
  hal = {http://hal.archives-ouvertes.fr/inria-00415861/},
  pubkind = {journal-int-mono},
  abstract = {This paper reports on the development and formal verification (proof
of semantic preservation) of CompCert, a compiler from Clight (a
large subset of the C programming language) to PowerPC assembly code,
using the Coq proof assistant both for programming the compiler and
for proving its correctness.  Such a verified compiler is useful in
the context of critical software and its formal verification: the
verification of the compiler guarantees that the safety properties
proved on the source code hold for the executable compiled code as
well.}
}

@InProceedings{Lescanne-1990,
  author = {Pierre Lescanne},
  title = {Implementation of Completion by Transition Rules +
		  Control: {\em ORME}},
  crossref = {ALP1990},
  pages = {262--269}
}

@InProceedings{Lescanne-1994,
  author = {Pierre Lescanne},
  title = {From $\lambda\sigma$ to $\lambda\upsilon$, a Journey
		  Through Calculi of Explicit Substitutions},
  crossref = {POPL1994},
  pages = {60--69},
  WKloc = {A-0396},
  abstract = {This paper gives a systematic description of several
		  calculi of explicit substitutions. These systems are
		  orthogonal and have easy proofs of termination of
		  their substitution calculus. The las system, called
		  $\lambda\upsilon$, entails a very simple environment
		  machine for strong normalization of $\lambda$-terms.},
  annote = {cites \cite{Asperti-1992} as categorical analog.}
}

@Article{Lesniewski-1929,
  author = {S. Le{\'s}niewski},
  title = {{Grundz\"uge eines neuen Systems der Grundlagen der Mathematik}},
  journal = FundMath,
  year = 1929,
  volume = 14,
  pages = {1--81},
  bibliographies = {RelMiCS}
}

@Article{Lester-Gowland-2002,
  author = {David Lester and Paul Gowland},
  title = {Using PVS to validate the algorithms of an exact arithmetic},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  bibliographies = {SQRL},
  note = {to appear},
  abstract = {The whole point of exact arithmetic is to generate answers
      to numeric problems, within some user-specified error. An
      implementation of exact arithmetic is therefore of questionable
      value, if it cannot be shown that it is generating correct answers.
      In this paper, we show that the algorithms used in an exact real
      arithmetic are correct. A program using the functions defined in this
      paper has been implemented in `C' (a Haskell version of which we
      provide as an appendix), and we are now convinced of its correctness.
      The table presented at the end of the paper shows that performing
      these proofs found three logical errors which had not been discovered
      by testing. One of these errors was only detected when the theorems
      were validated with PVS.},
  keywords = {Cauchy sequences; Computable reals; Correctness; Exact arithmetic; PVS},
  OPTannote = {}
}

@InProceedings{Leszcylowski-Wirsing-1991,
  authorsAddress = {L: Polish Academy of Sciences, Warszawa},
  abstract = {Starting from an algebraic specification approach
		  this paper presents a uniform framework for
		  specifying data structures and configurations of
		  systems. The framework covers (simple) algebraic
		  specifications with higher-order functions and
		  shallow polymorphism. The concepts of dependent
		  types are used to define parameterised
		  specifications and classes of specification
		  schemata, so-called polymorphic specifications. Due
		  to the introduction of a second level, configuration
		  of modules can be abstractly specified;
		  relationships between different classes of objects
		  such as specifications, signatures or parameterised
		  specifications can be described.},
  title = {Polymorphism, Parameterization and Typing: An
		  Algebraic Specification Perspective},
  pages = {1--24},
  crossref = {STACS1991},
  author = {Jacek Leszcylowski and Martin Wirsing}
}

@PhdThesis{Letouzey-2004,
  author = 	 {Pierre Letouzey},
  title = 	 {Certified Functional Programming --- Program Extraction within {Coq} Proof Assistant},
  school = 	 {Universit\'e Paris 7 Denis Diderot},
  year = 	 2004,
  address =	 {Jussieu},
  month =	 JUL,
  URL = 	 {http://www.pps.jussieu.fr/~letouzey/download/these_letouzey_English.ps.gz},
  WKloc = 	 {doc/pap/BIB}
}

@InProceedings{Leviathan-Pnueli-2002,
 author = {Raya Leviathan and Amir Pnueli},
 title = {Validating software pipelining optimizations},
 booktitle = {CASES '02: Proceedings of the 2002 international conference on Compilers, architecture, and synthesis for embedded systems},
 year = {2002},
 OPTISBN = {1-58113-575-0},
 pages = {280--287},
 location = {Grenoble, France},
 DOIURL = {http://doi.acm.org/10.1145/581630.581676},
 DOI = {10.1145/581630.581676},
 publisher = {ACM},
 address = {New York, NY, USA},
 WKloc = {doc/pap/BIB},
 bibliographies = {Coconut},
 abstract = { The paper presents a method for translation validation
                  of a specific optimization, software pipelining
                  optimization, used to increase the instruction level
                  parallelism in EPIC type of architectures. Using a
                  methodology as in [15] to establish simulation
                  relation between source and target based on
                  computational induction, we describe an algorithm
                  that automatically produces a set of decidable proof
                  obligations. The paper also describes SPV, a
                  prototype translation validator that automatically
                  produces verification conditions for software
                  pipelining optimizations of the SGI Pro-64
                  compiler. These verification conditions are further
                  checked automatically by the CVC [12] checker.}
}

@InCollection{Levy-1980,
  author = {Jean-Jacques L{\'e}vy},
  title = {Optimal Reductions in the Lambda-Calculus},
  crossref = {Seldin-Hindley-1980},
  pages = {159--191}
}

@InProceedings{Levy-1987,
  author = {Jean-Jaques L{\'e}vy},
  title = {Sharing in the Evaluation of lambda Expressions},
  pages = {183-189},
  crossref = {ProgFutGenCompII},
  WKloc = {A-0023},
  abstract = {This short note is to refresh an old problem that several
	researchers have tried to tackle unsuccessfully. This problem
	originated by Wadsworth's PhD dissertation can be fortunately easy
	stated: ``how to evaluate efficiently lambda expressions?''. By
	efficient, we mean optimal which is maybe not the same. And optimal
	means without duplication of contraction of redexes.},
  bibliographies = {RelMiCS}
}

@InProceedings{Levy-1996,
  author = {Jordi Levy},
  title = {Linear Second-Order Unification},
  crossref = {RTA96},
  pages = {332--346},
  keywords = {unification},
  WKloc = {A-0464}
}

@InProceedings{Levy-Agusti-1992,
  author = {Jordi Levy and Jaume Agust{\'{\i}}},
  title = {Implementing Inequality and Nondeterministic
		  Specifications with Bi-Rewriting Systems},
  crossref = {SADT92},
  pages = {252--267},
  WKloc = {A-0336},
  abstract = {Rewriting with non-symmetric relations can be
		  considered as a computational model of many
		  specification languages based on non-symmetric
		  relations. $\ldots$},
  bibliographies = {RelMiCS}
}

@InProceedings{Levy-Agusti-1993,
  author = {J. Levy and J. Agust\'{\i}},
  title = {Bi-rewriting, a Term Rewriting Technique for Monotonic Order Relations},
  crossref = {RTA93},
  bibliographies = {RelMiCS}
}

@Book{Lewis-1918,
  author = {Clarence Irving Lewis},
  title = {A Survey of Symbolic Logic},
  year = 1918,
  pages = {vi+406},
  publisher = BERKELEY_P,
  address = {Berkeley},
  note = {Reprint of Chapters I--IV by Dover Publications, 1960,
		New York},
  bibliographies = {RelMiCS}
}

@Book{Lewis-Langford-1959,
  author = {Clarence Irving Lewis and Cooper Harold Langford},
  title = {Symbolic Logic},
  year = 1959,
  pages = {v+518},
  publisher = Dover,
  address = {New York},
  bibliographies = {RelMiCS}
}

@InProceedings{Lewis-Shields-Meijer-Launchbury-2000,
  author = {Jeffrey R. Lewis and Mark B. Shields and Erik Meijer and John Launchbury},
  title = {Implicit Parameters: Dynamic Scoping with Static Types},
  crossref = {POPL2000},
  WKloc = {A-1107}
}

@TechReport{Li-1988,
  year = 1988,
  type = {Master's Thesis},
  title = {Pattern Matching in Trees},
  number = {CS-88-23},
  month = MAY,
  institution = Waterloo,
  author = {P. Li},
  bibliographies = {RelMiCS},
  abstract = {This thesis examines a tree automata approach to tree
		  matching problems. This approach is motivated by the
		  finite automata approach which has been very
		  succesful in designing string matching algorithms.
		  In particular, we show how the KMP algorithm can be
		  generalized to give tree matching algoritms which
		  preprocess the pattern tree. We also define
		  structures for trees which are analogous to suffix
		  tries and DAWGs for strings and show how they can be
		  used for tree pattern matching. Additionally, we
		  explore some other approaches to tree matching problems.}
}

@InProceedings{LiPeng-Zdancewic-2007,
  author = 	 {Peng Li and Steve Zdancewic},
  title = 	 {Combining Events And Threads For Scalable Network Services},
  crossref =  {PLDI2007},
  pages = 	 {189--199},
  year = 	 2007,
  WKloc = {A-1689 (draft), doc/pap/BIB},
  bibliographies = {FLP},
  URL = {http://www.cis.upenn.edu/~stevez/papers/abstracts.html#LZ07},
  abstract = 	 {This paper proposes to combine two seemingly opposed
                  programming models for building massively concurrent
                  network services: the event-driven model and the
                  multithreaded model. The result is a hybrid design
                  that offers the best of both worlds-the ease of use
                  and expressiveness of threads and the flexibility
                  and performance of events.

                  This paper shows how the
                  hybrid model can be implemented entirely at the
                  application level using concurrency monads in
                  Haskell, which provides type-safe abstractions for
                  both events and threads. This approach simplifies
                  the development of massively concurrent software in
                  a way that scales to real-world network
                  services. The Haskell implementation supports
                  exceptions, symmetrical multiprocessing, software
                  transactional memory, asynchronous I/O mechanisms
                  and application-level network protocol
                  stacks. Experimental results demonstrate that this
                  monad-based approach has good performance: the
                  threads are extremely lightweight (scaling to ten
                  million threads), and the I/O performance compares
                  favorably to that of Linux NPTL.}
}

@InProceedings{Liang-Hudak-1996,
  author = {Sheng Liang and Paul Hudak},
  title = {Modular Denotational Semantics for Compiler Construction},
  crossref = {ESOP1996},
  pages = {219--234},
  abstract = {We show the benefits of applying modular monadic semantics
     to compiler construction. Modular monadic semantics allows us to define
     a language with a rich set of features from reusable building blocks,
     and use program transformation and equational reasoning to improve code.
     Compared to denotational semantics, reasoning in monadic style offers
     the added benefits of highly modularized proofs and
     more widely applicable results. To demonstrate, we present
     an axiomatization of environments, and use it to prove
     the correctness of a well-known compilation technique.
     The monadic approach also facilitates generating code
     in various target languages with different sets of built-in features.},
  WKloc = {A-0460, doc/pap/BIB}
}

@InProceedings{Liang-Hudak-Jones-1995,
  author = {Sheng Liang and Paul Hudak and Mark Jones},
  title = {Monad Transformers and Modular Interpreters},
  crossref = {POPL1995},
  pages = {333--343},
  DOI = {10.1145/199448.199528},
  DOIURL = {http://doi.acm.org/10.1145/199448.199528},
  WKloc = {A-0617, doc/pap/BIB},
  bibliographies = {FP, Coconut},
  abstract = { We show how a set of building blocks can be used to
                  construct programming language interpreters, and
                  present implementations of such building blocks
                  capable of supporting many commonly known features,
                  including simple expressions, three different
                  function call mechanisms (call-by-name,
                  call-by-value and lazy evaluation), references and
                  assignment, nondeterminism, first-class
                  continuations, and program tracing.The underlying
                  mechanism of our system is monad transformers, a
                  simple form of abstraction for introducing a wide
                  range of computational behaviors, such as state,
                  I/O, continuations, and exceptions.Our work is
                  significant in the following respects. First, we
                  have succeeded in designing a fully modular
                  interpreter based on monad transformers that incudes
                  features missing from Steele's, Espinosa's, and
                  Wadler's earlier efforts. Second, we have found new
                  ways to lift monad operations through monad
                  transformers, in particular difficult cases not
                  achieved in Moggi's original work. Third, we have
                  demonstrated that interactions between features are
                  reflected in liftings and that semantics can be
                  changed by reordering monad transformers. Finally,
                  we have implemented our interpreter in Gofer, whose
                  constructor classes provide just the added power
                  over Haskell's type classes to allow precise and
                  convenient expression of our ideas. This
                  implementation includes a method for constructing
                  extensible unions and a form of subtyping that is
                  interesting in its own right.}
}

@MastersThesis{Libera-1987,
  author = {Klaus Libera},
  title = {{DAG-Transformationen f\"ur benutzerdefinierte Typen
		  im graphisch gest\"utzten Dialog}},
  school = {TU M\"unchen},
  year = 1987,
  type = {Diplomarbeit},
  WKloc = {C-0016}
}

@Misc{Libera-1988,
  author = {Klaus Libera},
  title = {Portierung des Hops-Systems},
  howpublished = {handwritten notices},
  month = FEB,
  year = 1988,
  WKloc = {C-0017}
}

@Article{Ligozat-1986,
  author = {G. Ligozat},
  title = {Points et Intervalles Combinatoirs},
  journal = TAINF,
  volume = 27,
  year = 1986,
  pages = {3--15},
  bibliographies = {RelMiCS}
}

@Article{Ligozat-Bestougeff,
  author = {G. Ligozat and H. Bestougeff},
  title = {On Relations Between Intervals},
  journal = IPLET,
  note = {submitted},
  bibliographies = {RelMiCS}
}

@InProceedings{Limet-Rety-Seidl-2001,
  author = {Limet, S. and R{\'e}ty, P. and Seidl, H.},
  title = {Weakly Regular Relations and Applications},
  crossref = {RTA2001},
  abstract = {A new class of tree-tuple languages is introduced:
    the weakly regular relations.
    It is an extension of the regular case (regular relations) and
    a restriction of tree-tuple synchronized languages, that has
    all usual nice properties, except closure by complement.
    Two applications are presented: to unification modulo a rewrite system,
    and to one-step rewriting.},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  URL = {http://www.univ-orleans.fr/SCIENCES/LIFO/Members/rety/publications.html}
}

@InProceedings{Limongelli-Mele-Regio-Temperini-1990,
  author = {C. Limongelli and M.B. Mele and M. Regio and M. Temperini},
  title = {Abstract Specification of Mathematical Structures
		  and Methods},
  crossref = {DISCO90},
  pages = {61--70},
  keywords = {OO, LogLan}
}

@Misc{LinLogList,
  KEY = {LinLogList},
  note = {Electronic forum on linear logic. To ask for registration, send
      e-mail to {\tt linear-request@cs.stanford.edu}.},
  bibliographies = {RelMiCS}
}

@Article{Lincoln-1992,
  author = {P. Lincoln},
  title = {Linear Logic},
  journal = ACM_SIGACT_News,
  volume = 23,
  number = 2,
  year = 1992,
  pages = {29--37},
  bibliographies = {RelMiCS}
}

@InProceedings{Lincoln-Mitchell-1992,
  author = {Patrick Lincoln and John C. Mitchell},
  title = {Algorithmic Aspects of Type Inference With Subtypes},
  pages = {293--304},
  abstract = {We study the complexity of type inference for programming
             languages with subtypes. There are three language variations
             that effect the problem: (i) basic functions may have
             polymorphic typeor more limited types, (ii) the subtype
             hierarchy may be fixed or vary as a result of subtype
             declarations within a program, and (iii) the subtype hierarchy
             may be an arbitrary partial order or may have a more restricted
             form, such as a tree or lattice. The naive algorithm for
             inferring a most general polymorphic type, under variable
             subtype hypotheses, requires deterministic exponential time. If
             we fix the subtype ordering, this upper bound grows to
             nondeterministic exponential time. We show that it is NP-hard
             to decide whether a lambda term has a type with respect to a
             fixed subtype hierarchy (involving only atomic type names).
             This lower bound applies to monomorphic or polymorphic
             languages. We give PSPACE upper bounds for deciding polymorphic
             typability if the subtype hierarchy has a lattice structure or
             the subtype hierarchy varies arbitrarily. We also give a
             polynomial time algorithm for the limited case where there are
             no function constants and the type hierarchy is either variable
             or any fixed lattice.},
  crossref = {POPL1992},
  authorsAddress = {[lincoln,mitchell]\@cs.stanford.edu},
  WKloc = {A-0163}
}

@TechReport{Lincoln-Mitchell-Scedrov-Shankar-1991,
  author = {P. Lincoln and J. Mitchell and Scedrov, Andre and N. Shankar},
  title = {Decision Problems for Propositional Linear Logic},
  institution = CSLI,
  year = 1991,
  type = {Report},
  number = {CSLI-91-147},
  address = {CA},
  month = mar,
  bibliographies = {RelMiCS}
}

@Article{Lincoln-Mitchell-Scedrov-Shankar-1992,
  author = {P. Lincoln and J. Mitchell and Scedrov, Andre and N. Shankar},
  title = {Decision Problems for Propositional Linear Logic},
  journal = ANPURE,
  year = 1992,
  volume = 56,
  pages = {239--311},
  month = apr,
  bibliographies = {RelMiCS}
}

@InProceedings{Lincoln-Shankar-1994,
  title = {Proof Search in First-Order Linear Logic and Other Cut-Free
      Sequent Calculi},
  author = {P. D. Lincoln and N. Shankar},
  pages = {282--291},
  crossref = {LICS9},
  abstract = {We present a general framework for proof search in
      first-order cut-free sequent calculi and apply it to the specific
      case of linear logic. In this framework, Herbrand functions are used
      to encode universal quantification, and unification is used to
      instantiate existential quantifiers so that the eigenvariable
      conditions are respected. We present an optimization of this
      procedure that exploits the permutabilities of the subject logic. We
      prove the soundness and completeness of several related proof search
      procedures. This proof search framework is used to show that
      provability for first-order MALL is in {\sc nexptime}, and
      first-order MLL is in {\sc np}. Performance comparisons based on
      Prolog implementations of the procedures are also given. The
      optimization of the quantifier steps in proof search can be combined
      effectively with a number of other optimizations that are also based
      on permutability.}
}

@Manual{LindNielsen-2002buddy,
  title = 	 {{BuDDy}: Binary Decision Diagram package Release 2.2},
  author =	 {J{\o}rn Lind-Nielsen},
  month =	 NOV,
  year =	 2002,
  WKloc = 	 {A-1645}
}

@Article{Lindenmayer-CulikII-1979,
  author = {Aristid Lindenmayer and Karel {Culik II}},
  title = {Growing Cellular Systems: Generation of Graphs by Parallel
		  Rewriting},
  journal = {Int. J. General Systems},
  volume = 5,
  pages = {45--55},
  year = 1979
}

@Book{Lindholm-Yellin-1999,
  author = {Tim Lindholm and Frank Yellin},
  title = {The Java Virtual Machine Specification},
  publisher = {Addison-Wesley},
  year = 1999,
  series = {The Java Series},
  edition = {Second Edition},
  month = MAY,
  WKloc = {doc/pap/BIB},
  ISBN = {0-201-43294-3},
  abstract = {The nucleus of the Java(TM) 2 platform, the Java(TM) virtual
      machine is the technology that enables the Java 2 platform to host
      applications on any computer or operating system without rewriting or
      recompiling. The Java virtual machine is also responsible for the
      compactness of applications targeting the Java 2 platform, and is the
      basis for its security capabilities.

      This book was written by those directly responsible for the design
      and implementation of the Java virtual machine, and is the complete
      and definitive specification for the technology. It is an essential
      reference for writers of compilers for the Java programming language
      and implementors of the Java virtual machine. This second edition
      specifies the newest version of the Java virtual machine and provides
      a fascinating view into the inner workings of the Java 2 platform.

      In this book you will find comprehensive coverage of the class file
      format, the hardware, operating system, and
      implementation-independent binary format for compiled code. The
      authors fully describe the instruction set of the Java virtual
      machine. You will find an entire chapter of examples that demonstrate
      how to compile code written in the Java programming language into the
      instruction set of the Java virtual machine. In addition, the book
      presents a discussion on the vital topic of Java virtual machine
      threads and their interaction with memory.

      All in all, this comprehensive, detailed, and methodical presentation
      provides the information necessary to implement your own fully
      compatible Java virtual machine.}
}

@TechReport{Lindig-1993,
  author = {Christian Lindig},
  email = {lindig@ips.cs.tu-bs.de},
  title = {Style -- {A} practical Type Checker for Scheme},
  institution = {Technical University of Braunschweig, Germany},
  number = {93-10},
  month = oct,
  year = 1993,
  URL = {ftp://ftp.ips.cs.tu-bs.de/pub/local/softech/papers/style.ps.gz},
  pages = 14,
  checked = 19940312,
  sjb = {Much simpler type system than Wright's
                 approach~\cite{Wright:Cartwright:rice:218:1993}, but
                 had the advantage of producing comprehensible types.
                 Implemented in Scheme using Aubry Jaffer's SCM. Doesn't
                 have a separate list type, treats them as pairs and has
                 a hack to deal with the terminator. Can't type some
                 procedures, such as call/cc. Indicates that the
                 software is available for ftp, but doesn't include the
                 ftp address.},
  abstract = {This paper describes an new tool for finding errors in
                 $R^{4}RS$-compliant Scheme programs. A polymorphic type
                 system in the style of Damas \&
                 Milner~\cite{Damas:Milner:popl:1982} with an additional
                 maximum type is used to type Scheme code. Although
                 Scheme is dynamically typed, most parts of programs are
                 statically typeable; type inconsistencies are regarded
                 as hints to possible programming errors. The paper
                 first introduces a type system which is a careful
                 balance between rigorous type safety and pragmatic type
                 softness. An efficient and portable implementation
                 based on order sorted unification in Scheme is then
                 described. We obtained very satisfactory results on
                 realistic programs, including the programs in Abelson,
                 Sussman \&
                 Sussman~\cite{Abelson:Sussman:Sussman:1985}.}
}

@Misc{Lindig-Snelting-199X,
  author = {Christian Lindig and Gregor Snelting},
  title = {Assessing Modular Structur of legacy Code Based on Mathematical Concept Analysis},
  year = {199?},
  WKloc = {A-0656}
}

@TechReport{Lindsay-1995,
  author = {Peter Lindsay},
  title = {A syntax for system specification that integrates {VDM-SL} and {Z}},
  year = 1995,
  month = DEC,
  institution = {Software Verification Research Centre, Department of Computer Science, The University of Queensland},
  number = {95-11},
  WKloc = {A-0648}
}

@Misc{Lindsey-1994,
  author = {C. H. Lindsey},
  title = {Undefined: Is it ``{I} don't care'' or ``{I} won't say''?},
  howpublished = {Draft, forty-sixth meeting of the IFIP Working Group
      2.1, Renkum, The Netherlands},
  month = jan,
  year = 1994,
  bibliographies = {RelMiCS}
}

@TechReport{Lindstroem-1993,
  author = {Erik Lindstr\"om},
  title = {Parallel Unification Algorithms: From Theory to Practice},
  institution = {University of Ume\o{a}, Institute of Information
		  Processing, Department of Computing Science},
  year = 1993,
  number = {UNINF-93.21},
  OPTaddress = {S-901 87 Ume\o{a}, Sweden},
  ISSN = {0348-0542},
  WKloc = {C-0007},
  abstract = {This paper is divided into four parts which deal
		  with background and theoretical foundations (1),
		  parallel $E_0$-unification (2), parallel
		  $AC$-unification (3), and implementation details
		  (4). The four parts together serve as a practical
		  introduction to parallel unification algorithms from
		  basics in ordinary unification to applications of
		  $AC$-unification. Our discussions of algorithms
		  focus on potential parallelism and possibilities to
		  implement the algorithms on parallel computers with
		  shared memory (SMM{}s). We give strong
		  circumstantial evidences that unification is
		  practically unsuitable to parallelise on a shared
		  memory multi-processor.

                  We present ideas of how data structures such as
		  lists, should be implemented in a computer system
		  with a hierarchical memory mechanism. The
		  implementation and functionality of a system for
		  programming and reasoning with algebraic structures,
		  equations and rewrite rules, Noether, are shortly
		  described.

                  We also present an analysis of a parallel algorithm
		  for solving systems of linear homogeneous and
		  inhomogeneous Diophantine equations. An outline for
		  a complexity analysis of the algorithm is given
		  together with reasoning about parallel
		  correctness. A collection of Diophantine equation
		  systems used for benchmarking is also
		  presented. Equations generated during completion of
		  equational theories are analysed and the uniform
		  result is the main motivation for our approach to
		  obtain efficient implementations of Diophantine
		  solvers as parts of an equational reasoning software.},
  keywords = {Unification, AC, parallel algorithms, Diophantine
		  equations, garbage collection, memory management, Noether}
}

@InProceedings{Lingas-Syslo-1988,
  author = {A. Lingas and Maciej M. Syslo},
  title = {A Polynomial-Time Algorithm for Subgraph Isomorphism of
		  Two-Connected Series-Parallel Graphs},
  pages = {394--409},
  crossref = {ICALP1988}
}

@InCollection{Lippi-2002,
  year={2002},
  isbn={978-3-540-43916-5},
  crossref={RTA2002},
  DOI = {10.1007/3-540-45610-4_29},
  title={in$^2$: A Graphical Interpreter for Interaction Nets},
  DOIURL={http://dx.doi.org/10.1007/3-540-45610-4_29},
  publisher= Springer,
  address = {Berlin Heidelberg},
  author={Lippi, Sylvain},
  pages={380--385},
  abstract = {in$^2$ can be considered as an attractive and didactic tool
     to approach the interaction net paradigm.
     But it is also an implementation in C
     of the core of a real programming language
     featuring a user-friendly graphical syntax
     and an efficient garbage collector free execution.}
}

@InProceedings{Lipton-Chapman-1998,
  author = {Jim Lipton and Emily Chapman},
  title = {Some Notes on Logic Programming with a Relational Machine},
  pages = {1--34},
  abstract = {We study the use of relation calculi for compilation and execution
                   of Horn Clause programs with an extended notion of input and output.
                   We consider various other extensions to the Prolog core.},
  editor = {Ali Jaoua and Peter Kempf and Gunther Schmidt},
  booktitle = {Using Relational Methods in Computer Science},
  year = 1998,
  month = JUL,
  series = {Technical Report Nr.\null{} 1998-03},
  publisher = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  bibliographies = {RelMiCS}
}

@InProceedings{Lipton-ODonnell-1994,
  author = {James Lipton and O'Donnell, Michael J.},
  title = {Intuitive Counterexamples for Constructive Fallacies},
  crossref = {MFCS94},
  pages = {87--111},
  abstract = {Formal countermodels may be used to justify the
		  unprovability of formulae in the Heyting calculus
		  (the best accepted formal system for constructive
		  reasoning), on the grounds that unprovable formaulae
		  are not constructively valid. We argue that the {\em
		  intuitive} impact of such countermodels becomes more
		  transparent and convincing as we move from
		  Kripke/Beth models based on possible worlds, to
		  L\"auchli realizability models. We introduce a new
		  semantics for constructive reasoning, called {\em
		  relational realizability}, which strengthens
		  further the intuitive impact of L\"auchli
		  realizability. But, none of htese model theories
		  provides countermodels with the compelling impact of
		  classical truth-table countermodels for classically
		  unprovable formulae.

                  We outline a proof that the Heyting calculus is
		  sound for relational realizability, and conjecture
		  that there is a constructive choice-free proof of
		  completeness. In this respect, relational
		  realizability improves the metamathematical
		  constructivity of L\"auchli realizability (which
		  uses choice in two crucial ways to prove
		  completeness) in the same sort of way Berth
		  semantics improves Kripke semantics.},
  annote = {cites \cite{Statman-1985} for logical relations},
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Litak-Hidders-Mikulas-2009,
  author          = {Tadeusz Litak and Jan Hidders and Szabolcs Mikulas},
  title           = {Relational Lattices: An Introduction},
  institution     = {Delft University of Technology},
  year            = {2009},
  url             = {http://www.st.ewi.tudelft.nl/~hidders/docs/tacl2009_submission_92.pdf},
  WKloc           = {doc/pap/BIB}
}

@Article{Litovsky-Metivier-1993,
  author = {Litovsky, I. and Metivier, Y.},
  title = {Computing with graph rewriting systems with priorities},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Theoretical computer science},
  year = 1993,
  volume = 115,
  number = 2,
  pages = {191--},
  month = JUL,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@TechReport{Little-MacCaull-Spencer-2001,
  author = {R. Little and Wendy MacCaull and B. Spencer},
  title = {{ReVaT} --- Relational Validation by Analytic Tableaux},
  institution = {University of New Brunswick},
  year = 2001,
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Liu-1992,
  author = {Junbo Liu},
  title = {A Semantic Basis of Logic-Independent Transformation},
  crossref = {SADT92},
  pages = {268--279},
  authorsAddress = {Bremen},
  WKloc = {A-0335},
  abstract = {$\ldots$}
}

@InProceedings{LiuYanhong-2000,
  author = {Yanhong A. Liu},
  title = {Efficiency by Incremetalization: An Introduction},
  booktitle = {???},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = {Kluwer},
  OPTnote = {},
  WKloc = {A-1146}
}

@TechReport{Livesey-Siekmann-1976,
  author = 	 {M. Livesey and J. Siekmann},
  title = 	 {Unification of sets and multisets},
  institution =  {Institut f\"ur Informatik I, Universit\"at Karlsruhe},
  year = 	 {1976},
  OPTannote = 	 {Treats set unification as ACI unification.}
}

@Misc{Livshits-,
  author = {Leo Livshits},
  title = {On Decomposability of Periodic Semigroups of Non-Negative Matrices},
  OPThowpublished = {},
  OPTmonth = {},
  year = {???},
  WKloc = {A-1376},
  bibliographies = {PosMat}
}

@InProceedings{Llabres-Rossello-2000,
  author =       {Merc{\`e}. Llabr{\'e}s and Francesc Rossell{\'o}},
  title =        {Pushout Complements for Arbitrary Partial Algebras},
  crossref =  {TAGT1998},
  pages =     {131--144},
  DOI = {10.1007/b75045},
  annote =      {Cites \cite{Kawahara-1990}.},
  abstract =    {To develop a double-pushout approach to transformation
     in a specific category, two basic preliminary questions must be answered:
     a) when a given rule can be applied through a given occurrence?, and
     b) when the result of such an application is unique?
     We solve these problems in the usual category
     of partial algebras over an arbitrary signature. }
}

@Article{Llabres-Rossello-2004,
  author =       {M. Llabr{\'e}s and F. Rossell{\'o}},
  title =        {The uniqueness condition for the double pushout transformation of algebras },
  journal =      {Information Sciences},
  year =         {2004},
  DOI =          {10.1016/j.ins.2004.03.019},
  volume =    {171},
  number =    {1--3},
  pages =     {93--124},
  month =     MAR,
  annote =      {Cites \cite{Kawahara-1990}.},
  abstract = {The double pushout approach
     to the algebraic graph transformation of hypergraphs
     was invented 30 years ago
     and it has been generalized since then to more general objects,
     like for instance relational systems or total and partial unary algebras.
     We have recently introduced the double pushout transformation
     of partial and partly total algebras over an arbitrary signature.
     In this paper we study the uniqueness condition
     for these rewriting formalisms,
     which turns out to be given by
     a suitable generalization to partial algebras
     of the well known congruence extension property for total algebras.}
}

@InProceedings{Loader-1994,
  title = {Linear Logic, Totality and Full Completeness},
  author = {R. Loader},
  pages = {292--298},
  crossref = {LICS9},
  abstract = {I give a ``totality space'' model for linear logic (Girard
      1986), derived by taking an abstract view of computations on a
      datatype. The model has similarities with both the coherence space
      model and game-theoretic models (Abramsky and Jagadeesan; Hyland and
      Ong 1993), but is based upon a notion of total object. Using this
      model, I prove a full completeness result, along the lines of the
      results for game theoretic models in (Abramsky and Jagadeesan) and
      (Hyland and Ong 1993). In other words, I show that the mapping of
      proofs to their interpretations (here collections of total objects
      uniform for a given functor) in the model is a surjection.}
}

@Misc{Lodaya-1999,
  author = {Kamal Lodaya},
  title = {Logic and automata on words and terms},
  note = {prepared for the Summer School on Computability, Logic and Applications, {Calcutta, 24-30 June 1999}},
  year = 1999,
  URL = {http://www.imsc.ernet.in/~kamal/la.ps.gz},
  WKloc = {doc/pap/BIB/Lodaya-1999.ps.gz}
}

@InProceedings{Lodaya-Ramanujam-2000,
  author = {Kamal Lodaya and R. Ramanujam},
  title = {An automaton model of user-controlled navigation on the web},
  booktitle = {{Proc. CIAA (London, Canada)}},
  series = LNCS,
  publisher = Springer,
  year = 2000,
  URL = {http://www.imsc.ernet.in/~kamal/usernav.ps.gz}
}

@InProceedings{Lodaya-Weil-1998,
  author = {Kamal Lodaya and Pascal Weil},
  title = {Series-Parallel Posets: Algebra, Automata and Languages},
  pages = {555--565},
  crossref = {STACS1998},
  WKloc = {A-1175}
}

@Article{Lodaya-Weil-2000,
  author = {Kamal Lodaya and Pascal Weil},
  title = {Series-parallel languages and the bounded width property},
  journal = TCS,
  volume = 237,
  number = {1-2},
  year = 2000,
  pages = {347-380},
  URL = {http://www.imsc.ernet.in/~kamal/splbwp.ps.gz},
  WKloc = {A-1012, doc/pap/BIB/Lodaya-Weil-2000.ps.gz}
}

@Book{Loeckx-Sieber-1987,
  author = {Jacques Loeckx and Kurt Sieber},
  title = {The Foundations of Program Verification},
  publisher = {B. G. Teubner / John Wiley \& Sons},
  year = 1987,
  series = {Wiley-Teubner Series in Computer Science},
  edition = {2nd},
  note = {First edition: 1984},
  UniBwM = {I-LB449 Loeckx},
  bibliographies = {SpecTech}
}

@TechReport{Loewe-1990,
  author = {Michael L{\"o}we},
  title = {Algebraic Approach to Graph Transformation Based on
		  Single Pushout Derivations},
  institution = {TU Berlin},
  year = 1990,
  number = {90/05},
  WKloc = {B-0028},
  abstract = {The single pushout approach to graph transformation
		  presented in this report interprets a double pushout
		  transformation rule of the classical algebraic
		  approach which consists of two {\em total} graph
		  morphisms as a single {\em partial} morphism from
		  the left- to the right-hand side. The notion of a
		  double pushout diagram for the transformation
		  process can then be substituted by a single pushout
		  diagram in an appropriate category of partial
		  morphisms.

                  It can be shown that this kind of transformation
		  generalizes the double pushout framework. Hence, the
		  classical approach can be seen as a special (and
		  very important) case of the new concept. It can be
		  reobtained from the single pushout approach by
		  imposing an application condition on the redices
		  which formulates the gluing conditions in the new
		  setting. On the other hand, single pushout
		  transformations are always possible even if the
		  gluing conditions for the redex are violated.

                  The simpler structure of a direct transformation
		  (one pushout diagram instead of two) simplifies many
		  proofs. Hence, the whole theory for double pushout
		  transformations including derived rules, parallel
		  and sequential composition, and amalgamation and
		  distributed transformations can be reformulated and
		  generalized in the new framework. Some constructions
		  provide new effects and properties which are
		  discussed in detail.}
}

@InProceedings{Loewe-1991,
  author = {Martin L{\"o}we},
  title = {Algebraic Approach to Single-Pushout Graph Transformation},
  crossref = {GraTra91},
  pages = {181--224},
  WKloc = {A-0151},
  abstract = {The single-pushout approach to graph transformation
		  interprets a double-pushout transformation rule of
		  the classical algebraic approach which consists of
		  two {\em total} morphisms as a single {\em partial}
		  morphism from the left- to teh right-hand side. The
		  notion of a double-pushout diagram for the
		  transformation process can then be substituted by a
		  single-pushout diagram in an appropriate category of
		  partial morphisms.

                  It can be shown that this kind of transformation
		  generalizes the double-pushout framework. Hence, the
		  classical approach can be seen as a special (and
		  very important) case of the new concept. It can be
		  reobtaines from the single-pushout approach by
		  imposing an application condition on the redices
		  which formulates the gluing conditions in the new
		  setting. On the other hand, single-pushout
		  transformations are always possible even if the
		  gluing conditions for the redex are violated.

                  The much simpler structure of a direct
		  transformation (one pushout diagram instead of two)
		  simplifies many proofs. Hnece, the whole theory for
		  double-pushout transformations including sequential
		  composition, parallel composition, and amalgamation
		  can be reformulated and generalized in the new
		  framework.

                  Some constructions provide new effects and
		  properties which are discussed in detail.}
}

@Article{Loewe-1993,
  author =       {Martin L{\"o}we},
  title =        {Algebraic Approach to Single-Pushout Graph Transformation},
  journal =      TCS,
  year =         1993,
  DOI =       {10.1016/0304-3975(93)90068-5},
  volume =    109,
  number =    {1--2},
  pages =     {181--224},
  month =     MAR,
  abstract =    {The single-pushout approach to graph transformation
    interprets a double-pushout transformation rule
    of the classical algebraic approach
    which consists of two total graph morphisms
    as a single partial morphism from the left- to the right-hand side.
    The notion of a double-pushout diagram for the transformation process
    can then be substituted by a single-pushout diagram
    in an appropriate category of partial morphisms.

    It can be shown that this kind of transformation
    generalizes the double-pushout framework.
    Hence, the classical approach can be seen as a special
    (and very important) case of the new concept.
    It can be reobtained from the single-pushout approach
    by imposing an application condition on the redices
    which formulates the gluing conditions in the new setting.
    On the other hand, single-pushout transformations are always possible
    even if the gluing conditions for the redex are violated.

    The simpler structure of a direct transformation
    (one pushout diagram instead of two) simplifies many proofs.
    Hence, the whole theory for double-pushout transformations
    including sequential composition, parallel composition,
    and amalgamation can be reformulated and generalized in the new framework.

    Some constructions provide new effects and properties
    which are discussed in detail.}
}

@InCollection{Loewe-2012,
  crossref={ICGT2012},
  doi={10.1007/978-3-642-33654-6_8},
  title={Refined Graph Rewriting in Span-Categories},
  url={http://dx.doi.org/10.1007/978-3-642-33654-6_8},
  author={L{\"o}we, Michael},
  pages={111--125},
  abstract = {There are three major algebraic approaches to graph transformation,
    namely the double-pushout (DPO), single-pushout (SPO),
    and sesqui-pushout approach (SqPO).
    In this paper, we present a framework that generalises all three approaches.
    The central issue is a gluing construction,
    which is a generalisation of the construction introduced in [14].
    It has pushout-like properties wrt. composition and decomposition,
    which allow to reestablish major parts of the theory
    for the algebraic approaches on a general level.
    We investigate parallel independence here.}
}

@InProceedings{Loewe-Beyer-1993,
  author = {Martin L\"owe and M. Beyer},
  title = {{AGG} --- An Implementation of Algebraic Graph Rewriting},
  crossref = {RTA93},
  pages = {451--456},
  WKloc = {A-0134},
  OPTabstract = {The AGG-system (Algebraic Graph Grammar System) is a
		  prototype implementation of the algebraic approach
		  to graph transformation \cite{Ehrig-1979}. It has been
		  programmed in EIFFEL and runs on SUN workstations
		  under X Window 11.5. It consists of a flexible
		  graphical editor and a derivation component. The
		  editor allows the graphical manipulation of rules,
		  redices and derivation results. The derivation
		  component performs direct transformation steps for
		  user-selected rules and redices}
}

@InProceedings{Loewe-Dingel-1993,
  author = {Michael L{\"o}we and J\"urgen Dingel},
  title = {Parallelism in Single-Pushout Graph Rewriting},
  crossref = {GTCS93},
  pages = {248--264},
  abstract = {The single-pushout approach to graph transformation
		  comes equipped with an interesting parallel
		  composition of rules and derivations: Some parallel
		  steps cannot be decomposed into sequences with the
		  component rules. Thus parallelism in single-pushout
		  rewriting is not equivalent to ``independence of
		  order'', which makes up a major difference with
		  respect to the classical algebraic approach to graph
		  transformation based on double-pushout constructions
		  for direct derivations.

                  But as we show in this paper, the equivalence that
		  arises from different sequentializations contains a
		  distinguished member which is uniquely determined up
		  to a suitable notion of isomorphism between
		  derivations and which realizes maximal parallelism
		  also in single-pushout rewriting. The amount of
		  parallelism, however, which we find in these
		  so-called canonical sequences considerably exceeds
		  the parallelism obtained in the double-pushout framwork.},
  WKloc = {A-0295}
}

@InProceedings{Loewe-Ehrig-1990,
  author = {Michael L\"owe and Hartmut Ehrig},
  title = {Algebraic Approach to Graph Transformation Based on
		  Single Pushout Derivations},
  crossref = {WG90},
  pages = {338--353},
  DOI = {10.1007/3-540-53832-1_52},
  abstract = {The Berlin approach to graph transformation,
     which uses double pushout derivations
     in the category of graphs and total graph morphisms,
     is modified using single pushout derivations
     in the category of graphs and partial graph morphisms.
     It is shown that the single pushout approach generalizes
     the classical approach in the sense that
     all double pushout derivations correspond
     to single pushout transformations but not vice versa.

     The chances which lie in the extended expressive power
     are exhibited in the following.
     We show that some complex proofs
     within the framework of double pushout derivations
     become much simpler in the new context.
     Moreover, the simple derivation structure allows
     to consider asynchronous derivations
     which might provide an adequate model for distributed computations.

     Finally, the approach is generalized
     in order to be applicable to more general algebraic structures.
     We characterize these so-called graph structures
     as categories of algebras w.r.t.
     signatures containing unary operator symbols only.
     Many representations of graphs and hypergraphs known from the literature
     turn out to be special graph structures
     such that the theoretical framework introduced in this paper
     can be applied to all of those objects.}
}

@InProceedings{Loewe-KoenigH-Peters-Schulz-2006,
  author = {Michael L{\"o}we and Harald K{\"o}nig and Michael Peters and Christoph Schulz},
  title =        {Refactoring Informations Systems},
  booktitle = {Software Evolution through Transformations},
  year =      2006,
  volume =    3,
  series =    {Electronic communications of the EASST},
  URL = {http://journal.ub.tu-berlin.de/index.php/eceasst/article/view/43},
  WKloc =      {doc/pap/BIB},
  abstract = {We present our formal framework
     for the refactoring of complete information systems,
     i.e., the data model and the data itself.
     It is described using general and abstract notions of category theory
     and can handle addition, renaming and removal of model objects
     as well as folding and unfolding
     within complete and partial object compositions.},
  annote =    {contains pullback complement counterexamples}
}

@InProceedings{Loewe-Korff-Wagner-1993,
  author = {Michael L{\"o}we and Martin Korff and Annika Wagner},
  title = {An Algebraic Framework for the Transformation of Attributed
      Graphs},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  pages = {185--199}
}

@Article{Loewenheim-1908,
  author = {Leopold L{\"o}wenheim},
  title = {{\"Uber das Ausl\"osungsproblem
		im logische Klassenkalk\"ul}},
  journal = SITZ-B,
  year = 1908,
  pages = {89--94},
  bibliographies = {RelMiCS}
}

@Article{Loewenheim-1913,
  author = {Leopold L{\"o}wenheim},
  title = {{\"Uber Transformationen im Gebietekalk\"ul}},
  journal = MANN,
  year = 1913,
  volume = 73,
  pages = {245--272},
  bibliographies = {RelMiCS}
}

@Article{Loewenheim-1913a,
  author = {Leopold L{\"o}wenheim},
  title = {{Potenzen im Relativkalkul und Potenzen allgemeiner
		endlicher Transformationen}},
  journal = SITZ-B,
  year = 1913,
  pages = {65--71},
  note = {Published as appendix to Archiv der Mathematik und
		Physik, ser.\null{} 3, vol.\null{} 21, no.\null{} 1 (1913)},
  bibliographies = {RelMiCS}
}

@Article{Loewenheim-1915,
  author = {Leopold L{\"o}wenheim},
  title = {{\"Uber M\"oglichkeiten im Relativkalk\"ul}},
  note = {English translation in \cite{vanHeijenoort1967}},
  journal = MANN,
  volume = 76,
  year = 1915,
  pages = {447--470},
  bibliographies = {RelMiCS}
}

@Article{Loewenheim-1940,
  author = {Leopold L{\"o}wenheim},
  title = {{Einkleidung der Mathematik in Schr\"oderischen
		Relativkalk\"ul}},
  journal = JSYLO,
  volume = 5,
  year = 1940,
  pages = {1--15},
  bibliographies = {RelMiCS}
}

@Article{Loh-2010,
  author = {Loh, Eugene},
  title = {The Ideal {HPC} Programming Language},
  journal = CACM,
  volume = {53},
  number = {7},
  year = {2010},
  issn = {0001-0782},
  pages = {42--47},
  DOIURL = {http://doi.acm.org/10.1145/1785414.1785433},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {CAS706},
  abstract = 	 {Maybe it's Fortran. Or maybe it just doesn't matter.}
}

@Article{Loidl-Rubio-Scaife-Hammond-Horiguchi-Klusik-Loogen-Michaelson-Pena-Priebe-Rebon-Trinder-2003,
  author = {H.-W. Loidl and F. Rubio and N. Scaife and K. Hammond and S.
      Horiguchi and U. Klusik and R. Loogen and G.J. Michaelson and R.
      Pe{\~n}a and S. Priebe and {\'A}.J. Reb{\'o}n and P. W. Trinder},
  title = {Comparing Parallel Functional Languages: Programming and Performance},
  journal = {J. of Higher-order and Symbolic Computation},
  year = 2003,
  WKloc = {A-1445, doc/pap/BIB},
  bibliographies = {PMC}
}

@Misc{Long-1994,
  author =	 {David Long},
  title =	 {{The BDD Library of Model Checking @CMU}},
  howpublished = {\textsf{http://www.cs.cmu.edu/\~{}modelcheck/bdd.html}},
  URL = 	 {http://www.cs.cmu.edu/~modelcheck/bdd.html},
  year =	 1994
}

@InProceedings{LopezFraguas-RodriguezHortala-SanchezHernandez-2007,
  author    = {Francisco Javier L{\'o}pez-Fraguas and
               Juan Rodr{\'{\i}}guez-Hortal{\'a} and
               Jaime S{\'a}nchez-Hern{\'a}ndez},
  title     = {A simple rewrite notion for call-time choice semantics},
  booktitle = {Proceedings of the 9th International ACM SIGPLAN Conference
               on Principles and Practice of Declarative Programming, July
               14--16, 2007, Wroclaw, Poland},
  year      = {2007},
  pages     = {197--208},
  DOIURL = {http://dx.doi.org/10.1145/1273920.1273947},
  DOI = {10.1145/1273920.1273947},
  abstract = {Non-confluent and non-terminating rewrite systems
    are interesting from the point of view of programming.
    In particular, existing functional logic languages
    use such kind of rewrite systems
    to define possibly non-strict non-deterministic functions.
    The semantics adopted for non-determinism is call-time choice,
    whose combination with non-strictness is not a trivial issue
    that has been addressed from a semantic point of view
    in the Constructor-based Rewriting Logic (CRWL) framework.
    We investigate here how to express call-time choice and non-strict semantics
    from a point of view closer to classical rewriting.
    The proposed notion of rewriting
    uses an explicit representation for sharing with let-constructions
    and is proved to be equivalent to the CRWL approach.
    Moreover, we relate this let-rewriting relation
    (and hence CRWL) with ordinary rewriting,
    providing in particular soundness and completeness
    of let-rewriting with respect to rewriting for a class of programs
    which are confluent in a certain semantic sense.}
}

@Article{Lorenzen-1954,
  author = {Paul Lorenzen},
  title = {{\"Uber die Korrespondenzen einer Struktur}},
  journal = MATHZ,
  volume = 60,
  year = 1954,
  pages = {61--65},
  note = {Zbl.\null{} Mat.\null{} 55 23.},
  bibliographies = {RelMiCS}
}

@Article{Lotka-1926,
  author = 	 {A. J. Lotka},
  title = 	 {The Frequency Distribution of Scientific Productivity},
  journal = 	 {J.\null{} of the Washington Acad.\null{} of Sci.},
  year = 	 1926,
  volume =	 16,
  pages =	 {317-323},
  annote = {see also http://dx.doi.org/10.1002/asi.10025}
}

@TechReport{Loyall-1992,
  author = {Loyall, Joseph Patrick},
  title = {Specification of concurrent systems using graph grammars},
  institution = {University of Illinois, Dep. of Comp.Sc.},
  year = 1992,
  number = 1752,
  address = {Urbana, Illinois},
  abstract = {Existing textual programming language support sequential
               programming well because there is a correlation between
               the one-dimensional nature of text and the one-
               dimensional nature of sequential program,i.e, the single
               flow of control in a program. Textual notation does not
               support concurrent programming as wellm however, because
               concurrent programms have many threads of control, have
               a two-dimensional relationships between the flow of
               control in a processe and the flow of information
               between processes, and are often dynamic. Most existing
               models for the specification of concurrent systems
               support one two of these traits, but fall short
               supporting all three. Graph grammars are well suited for
               the specification of concurrent system because they are
               also inherently concurrent, two-dimensional and dynamic.
               We have developed a general rewriting model,(delta)-
               grams, based on the theory of graph grammars. Concurrent
               system are specified in (delta) by representing the
               state of the system as a graph and the state transitions
               as graph transformations. Previously, we have a formal
               semantics for grammars and demonstrated its usefulness
               for giving a graphical semantics of existing concurrent
               languages, such as and GARP. This thesis shows (delta)'s
               usefulness as a specification language. It describes how
               specifications are modularity and sysmatically designed
               in (delta). It is also described a technique for the
               static analysis of (delta)-specifications. Classes of
               (delta)-specification that are confluenct, terminating,
               deadlock-free, starvation-free, or efficient to execute
               are recognized by examining the structure of (delta)-
               productions and the way in which (delta)-productions
               interact, both with other (delta)production and with
               state graphs. The technique for static analysis can be
               used to analyze existing (delta)-specifications and as a
               guideline for the design of correct (delta)-
               specifications. The techniques are ilustrated throughout
               with examples and tutorial introduction to graph
               grammars and (delta)-grammars are includes.}
}

@Article{Loyall-Kaplan-Goering-1991,
  author = {Loyall, Joseph P. and Kaplan, Simon M. and Goering, Stephen K.},
  title = {Specification and Implementation of Actors with Graph
                   Rewriting},
  OPTcrossref = {},
  OPTkey = {},
  journal = {OOPS messenger},
  year = 1991,
  volume = 2,
  number = 2,
  pages = 73,
  month = APR,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Lu-1999,
  author = {Jian Lu},
  title = {Some Research on Componentware Frameworks Based On Mobile Agent Technology},
  year = 1999,
  month = FEB,
  WKloc = {A-0649}
}

@Article{Lu-Li-Ma-Cai-Tao-Zhang-Liu-2000,
  author = {Lu, Jian and Li, Yingjun and Ma, Xiaoxin and Cai, Min and
                  Tao, Xianping and Zhang, Guanqun and Liu, Jianzhong},
  title = {A Hierarchical Framework for Parallel Seismic Applications},
  journal = CACM,
  year = 2000,
  volume = 43,
  number = 10,
  pages = {55--59},
  month = OCT,
  WKloc = {A-1038}
}

@Misc{Lucanu-1995,
  author = {Dorel Lucanu},
  title = {Canonical Forms in Dag Algebras},
  OPThowpublished = {},
  month = FEB,
  year = 1995,
  WKloc = {A-1267},
  abstract = {In \cite{Lucanu-} a hiererchy of directed acyclic graph
      (dag) algebras is defined. The dags belonging to an algebra are
      defined up to isomorphism which can be expressed by a congruence over
      a free term algebra, i.e.\null{} a dag is described by a congruence
      class of terms. In this paper we deal with the following problem: can
      be defined a canonical form in every congruence class and a canonical
      (confluent and terminating) relation $\rightarrow$ such that for any
      canonical form $w_0$ we have $w \rightarrow! w_0$, for any $w$ in the
      class $[w_o]$?},
  annote = {vicinity of Stefanescu}
}

@Misc{Lucanu-1998,
  author = {Dorel Lucanu},
  OPTtitle = {Algebraic Structures of Directed Acyclic Graphs: Application to Concurrent Calculus},
  OPThowpublished = {},
  OPTmonth = NOV,
  OPTyear = 1998,
  WKloc = {A-1270},
  abstract = {The paper defines an algebraic tool for manipulating
      directed acyclic graphs (on short dags). The approach we propose
      combines two mathematical tools --- category theory and universal
      algebra --- and it allows an unitary treating of dag transformations.
      We show that the specification, implementation and programming of
      concurrent systems can be achieved by using this theory.},
  annote = {vicinity of Stefanescu}
}

@InProceedings{Lucas-1997,
  author = {Salvador Lucas},
  title = {Transformations for Efficient Evaluations in Functional
             Programming},
  pages = {127--141},
  year = 1997,
  crossref = {PLILP1997}
}

@PhDThesis{Lucas-1998,
  author = {S. Lucas},
  title = {Rewriting with replacement restrictions},
  school = {Departamento de Sistemas Inform\'aticos y Computaci\'on,
           Universidad Polit\'ecnica de Valencia},
  month = {Oct.},
  year = 1998,
  note = {In spanish},
  URL = {http://www.dsic.upv.es/users/elp/slucas/thesis.html},
  abstract = {The computational model of rewriting-based systems consists
      in reducing expressions to a canonical form (a head-normal form,
      (infinite) normal form or (infinite) value). In order to avoid
      useless or dangerous rewrite sequences it is necesary to restrict the
      reduction space associated to a given expression. We define a
      rewriting restriction, the context-sensitive rewriting (csr), which
      is obtained by imposing a particular syntactical replacement
      restriction. The replacement restriction is first associated to
      argument positions of symbols and then raised to arbitrary
      occurrences of terms.

      The thesis provides a complete analysis of such a simple syntactic
      replacement restriction. Our approach differs from the well-known
      strictness-based syntactic annotations which need some underlying
      reduction strategy which is eventually modified by taking the
      annotations into consideration. Instead, csr can be thought of as a
      mechanization of syntactic annotations themselves. We provide methods
      for proving the termination and the confluence of csr, we show how to
      define good context-sensitive rewriting strategies, and we give
      methods to automatically define the replacement restrictions which
      allow us to compute several classes of canonical forms. In this way,
      we believe that our work demonstrates the viability of csr as an
      alternative computational mechanism for rewriting-based systems which
      usually provides better results.}
}

@Article{Lucas-1998a,
  author = {Salvador Lucas},
  title = {Context-sensitive computations in functional and functional logic programs},
  journal = {Journal of Functional and Logic Programming},
  volume = 1998,
  number = 1,
  publisher = {The MIT Press},
  month = {January},
  year = 1998
}

@TechReport{Lucas-2000,
  author = {Salvador Lucas},
  title = {Context-Sensitive Rewriting Strategies},
  institution = {DSIC, Universidad Polit\'ecnica de Valencia},
  number = {DSIC-II/7/00},
  year = 2000,
  URL = {ftp://ftp.dsic.upv.es/pub/reports/elp/slucas-tr/tr2000.ps.gz}
}

@Article{Luccio-Pagli-1991,
  author = {F. Luccio and L. Pagli},
  title = {An Efficient Algorithm for Some Tree Matching Problems},
  year = 1991,
  volume = 39,
  pages = {51--57},
  journal = IPLET,
  nutshell = {I don't like their distance},
  bibliographies = {RelMiCS},
  abstract = {In this paper we consider ordered $h$-ary trees, that
		  is, trees whose nodes have exactly $h$ sons; and
		  ranked trees, where the number of sons depends on
		  the node label. We define the subtree distance
		  between two ordered $h$-trees $T_{1}$, $T_{2}$ as
		  the number of subtrees to be inserted or deleted in
		  $T_{1}$ to obtain $T_{2}$, and consider the problem
		  of finding all the occurrences with bounded distance
		  $k$, of an $h$-ary tree $P$ as a subtree of another
		  $h$-ary tree $T$. This problem is solved in time
		  $O(h \size{P} + max(h,k) \size{T})$. We then study
		  the classical problem of finding all the occurrences
		  of a ranked tree $P$ in another tree $T$, where the
		  two trees are labelled, and a special label $v$ in
		  the leaves of $P$ stands for any subtree in $T$. An
		  extension of the previous algorithm allows to solve
		  this problem in time $O(\size{P} + k \size{T})$,
		  where $k$ is the number of labels $v$ in $P$. We
		  also discuss some natural variants of the two
		  problems.}
}

@Article{Luce-1952,
  author = {R. D. Luce},
  title = {A Note on Boolean Matrix Theory},
  journal = PROAMS,
  volume = 3,
  year = 1952,
  pages = {382--388},
  bibliographies = {RelMiCS}
}

@Article{Luecke-Haque-1991,
  author = 	 {Glenn Luecke and Waqar Haque},
  title = 	 {Evaluation of {Fortran} Vector Compilers and Preprocessors},
  journal = 	 {Software Practice and Experience},
  year = 	 1991,
  volume = 	 21,
  number = 	 9,
  month = 	 SEP,
  bibliographies = {Coconut},
  annote = 	 {Cited by \cite{ShinJaewook-2005} for loop transformations aimed at exposing SIMD operations suitable for vectorisation.}
}

@Book{Luenberger-1979,
  ALTauthor = {David G. Luenberger},
  ALTeditor = {},
  title = {Introduction to Dynamic Systems: Theory, Models, and Applications},
  publisher = {Wiley},
  year = 1979,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  McMaster = {QA 402 .L84},
  pages = {xiv, 446},
  ISBN = 0471025941,
  keywords = {System analysis, Differential equations, Control theory}
}

@Article{Lueroth-1904,
  author = {J. L\"uroth},
  title = {{Aus der Algebra der Relative (nach dem dritten Bande von
		E. Schr\"oders Vorlesungen \"uber die Algebra der Logik)}},
  journal = JAHRDEUTS,
  volume = 13,
  year = 1904,
  pages = {73--111},
  bibliographies = {RelMiCS}
}

@Manual{Lueth-2002HTk,
  title = {A Short Introduction to {HTk}: Graphical User Interfaces for {Haskell}},
  author = {Christoph L{\"u}th},
  organization = {FB3 --- Mathematik und Informatik, Universit\"at Bremen},
  month = MAY,
  year = 2002,
  note = {Revision 1.3 of 2002/05/28},
  WKloc = {A-1443}
}

@InProceedings{Lueth-Ghani-1997,
  author = {Christoph L{\"u}th and Neil Ghani},
  title = {Monads and Modular Term Rewriting},
  crossref = {CTCS1997},
  pages = {69--86}
}

@InProceedings{Lueth-Ghani-2002,
  author = 	 {Christoph L{\"u}th and Neil Ghani},
  title = 	 {Composing Monads Using Coproducts},
  pages = 	 {133--144},
  crossref = {ICFP2002},
  OPTjournal = 	 {Intl. Conference on Functional Programming 2002},
  OPTyear = 	 {2002},
  OPTvolume = 	 {37},
  OPTnumber = 	 {9},
  WKloc = {doc/pap/BIB}
}

@Article{Lueth-Karlsen-Kolyang-Westmeier-Wolff-1998,
  author = {C. L{\"u}th and E. W. Karlsen and Kolyang and S. Westmeier and B. Wolff},
  title = {{HOL-Z} in the {UniForM-Workbench} --- {A} Case Study
                 in Tool Integration for {Z}},
  crossref = {ZUM1998},
  pages = {116--134},
  WKloc = {A-1330}
}

@Article{Lueth-Karlsen-Kolyang-Westmeier-Wolff-1998a,
  author = {C. L{\"u}th and E. W. Karlsen and Kolyang and S. Westmeier and B. Wolff},
  title = {Tool Integration in the {UniForM-Workbench}},
  crossref = {Berghammer-Lakhnech-1999},
  OPTpages = {},
  WKloc = {doc/pap/BIB}
}

@Article{Lueth-Wolff-199X,
  author = {C. L{\"u}th and B. Wolff},
  title = {Functional Design and Implementation of Graphical Interfaces for Theorem Provers},
  journal = {Journal of Functional Programming},
  year = {199?},
  volume = {??},
  OPTnumber = {},
  OPTmonth = {},
  pages = {??},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0645},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Lujan-Freeman-Gurd-2000,
  author = {Mikel Luj{\'a}n, T. L. Freeman, John R. Gurd},
  title = {OoLaLa: an Object Oriented Analysis and Design of Numerical Linear Algebra},
  crossref = {OOPSLA2000},
  OPTpages = {},
  URL = {http://www.cs.man.ac.uk/~lujanmx/research/docs/lujan2000.pdf},
  CiteSeer = {http://citeseer.nj.nec.com/luj00oolala.html},
  bibliographies = {PowerRel},
  WKloc = {doc/pap/BIB},
  abstract = {In this paper we review the design of a sequential object
      oriented linear algebra library, OoLaLa. Several designs are proposed
      and used to classify existing sequential object oriented libraries.
      The classification is based on the way that matrices and matrix
      operations are represented. OoLaLa's representation of matrices is
      capable of dealing with certain matrix operations that, although
      mathematically valid, are not handled correctly by existing
      libraries. OoLaLa also enables implementations...}
}

@InProceedings{Lukkien-1992,
  author = {Johan J. Lukkien},
  title = {An Operational Semantics for the Guarded Command Language},
  crossref = {MPC1992},
  pages = {233--249},
  abstract = {In [6], Dijkstra and Scholten present an axiomatic
		  semantics for Dijkstra's guarded command language
		  through the notions of weakest precondition and
		  weakest liberal precondition. The informal notion of
		  a computation is used as a justification for the
		  various definitions. In this paper we present an
		  operational semantics in which the notion of a
		  computation is made explicit. The novel contribution
		  is a generalization of the notion of weakest
		  precondition. This generalization supports reasoning
		  about general properties of programs (i.e, not just
		  termination in a certain state).}
}

@InProceedings{Lundgren-Steed-Barnes-,
  author = 	 {William I. Lundgren and James W. Steed and Kerry B. Barnes},
  title = 	 {A Demonstration of Model Driven Development with Automatic Implementation using {Gedae}},
  booktitle =	 {\unfinished},
  year =	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  URL = {gedae.com},
  WKloc = 	 {A-1675},
  bibliographies = {Coconut}
}

@Book{LuoZhaohui-1994,
  author =	 {Zhaohui Luo},
  title = 	 {Computation and Reasoning: A Type Theory for Computer Science},
  publisher = 	 OUP,
  year = 	 1994
}

@TechReport{LuoZhaohui-Pollack-1992,
  author = 	 {Zhaohui Luo and Robert Pollack},
  title = 	 {LEGO Proof Development System: User's Manual},
  institution =  {Laboratory for Foundations of Computer Science, University of Edinburgh},
  year = 	 1992,
  number =	 {ECS-LFCS-92-211}
}

@InProceedings{Luther-2001a,
   crossref = {IJCAR2001},
   author =   {Luther, Marko},
   title =    {More On Implicit Syntax},
   pages =    {386--400},
   URL =  {http://www.informatik.uni-ulm.de/ki/Typelab/ijcar01.html},
   PDF = {ftp://ftp.informatik.uni-ulm.de/pub/KI/papers/ijcar01.pdf},
   WKloc = {doc/pap/BIB},
   abstract = {Proof assistants based on type theories, such as Coq
      and LEGO, allow users to omit subterms on input that can be
      inferred automatically. While those mechanisms are well known,
      ad-hoc algorithms are used to suppress subterms on output. As a
      result, terms might be printed identically although they differ
      in hidden parts. Such ambiguous representations may confuse
      users. Additionally, terms might be rejected by the type checker
      because the printer has erased too much type information. This
      paper addresses these problems by proposing effective erasure
      methods that guarantee successful term reconstruction, similar
      to the ones developed for the compression of proof-terms in
      Proof-Carrying Code environments. Experiences with the
      implementation in Typelab proved them both efficient and
      practical.}
}

@PhDThesis{Luther-2003,
   author =  {Luther, Marko},
   title =   {Elaboration and Erasure in Type Theory},
   school =  {Universit{\"{a}}t Ulm},
   address = {Germany},
   year =    2003,
   PDF = {ftp://ftp.informatik.uni-ulm.de/pub/KI/papers/luther03-diss.pdf},
   URL = {http://www.informatik.uni-ulm.de/ki/Papers/luther-phd.html},
   WKloc = {doc/pap/BIB},

   abstract = {This thesis contributes to the construction of a
      convenient specification language on top of a type theoretic
      substrate. The subject arose in the context of the Typelab
      project that aimed at improving the machine assistance for the
      formal development of mathematics, software and hardware. Type
      theory was chosen as underlying theoretical framework, because
      it homogeneously comprises both the notion of computation and
      deduction. However, the price for its expressiveness is a
      verbose syntax. When I joined the Typelab group, my
      responsibility was to shape the external language of the Typelab
      system. Naturally, I first looked at related
      implementations. Most of them cope with the wordiness of type
      theory by allowing their users to omit on input redundant parts
      that can be inferred automatically through a process called
      elaboration. While the use of such a mechanism seems
      indispensable for serious verification tasks, I found the
      existing solutions unsatisfactory. Not only are the implemented
      algorithms seldom precisely documented and formally analyzed,
      they also lack strength. It is disappointing how much redundant
      information still has to be supplied on input. Furthermore, the
      ad hoc erasure algorithms, used to reduce the redundancy of
      expressions on output, often produce wordy or even ambiguous
      external representations. Such failures are especially fatal for
      interactive verification systems, where the output describing
      the actual system state is often the only hint for a user on how
      to proceed with a proof. To improve this situation, I developed
      the elaboration and erasure methods described in this thesis.

      The design of elaboration is inspired by the conciseness of
      functional programming languages and is formally grounded on
      type inference in the underlying type theory. To establish
      correctness, intermediate elaboration states are represented by
      open terms, adapting techniques recently developed for the
      representation of partial proofs. The erasure methods are based
      on estimations of the corresponding elaboration process and
      guarantee a successful reconstruction of the elided
      information. Experiments performed with Typelab proved the
      proposed methods both effective and efficient.}
}

@TechReport{Luther-Strecker-1998,
  author = 	 {Marko Luther and Martin Strecker},
  title = 	 {A guided tour through {\sc Typelab}},
  institution =  {Universit{\"a}t Ulm},
  year = 	 1998,
  number =	 {98-03},
  month =	 {January},
  URL = {http://www.informatik.uni-ulm.de/ki/Typelab/TR-98-03.html},
  WKloc = {A-1572, doc/pap/BIB},
  abstract = {This report gives a survey of Typelab, a
     specification and verification environment that integrates
     interactive proof development and automated proof search. Typelab
     is based on a constructive type theory, the Calculus of
     Constructions, which can be understood as a combination of a
     typed Lambda calculus and an expressive higher-order
     logic. Distinctive features of the type system are dependent
     function types for modeling polymorphism and dependent record
     types for encoding specifications and mathematical
     theories. After presenting an extended example which demonstrates
     how program development by stepwise refinement of specifications
     can be carried out, the theory underlying the prover component of
     Typelab is described in detail. A calculus with metavariables and
     explicit substitutions is introduced, and the meta-theoretic
     properties of this calculus are analyzed. Furthermore, it is
     shown that this calculus provides an adequate foundation for
     automated proof search in fragments of the logic.

     This technical report is an extended version of the book chapter
     ``Interactive and automated proof construction in type theory''
     \cite{Strecker-Luther-vonHenke-1998}.}
}

@InProceedings{Luttik-Visser-1997,
  author = {Bas Luttik and Eelco Visser},
  title = {Specifictaion of rewriting strategies},
  crossref = {ASF+SDF1997}
}

@Article{Lyndon-1950,
  author = {Roger C. Lyndon},
  title = {The Representation of Relational Algebras},
  journal = ANMA,
  volume = 51,
  year = 1950,
  pages = {707--729},
  bibliographies = {RelMiCS}
}

@Article{Lyndon-1956,
  author = {Roger C. Lyndon},
  title = {The Representation of Relation Algebras, II},
  journal = ANMA,
  volume = 63,
  year = 1956,
  pages = {294--307},
  bibliographies = {RelMiCS}
}

@Article{Lyndon-1961,
  author = {Roger C. Lyndon},
  title = {Relation Algebras and Projective Geometries},
  journal = MICH,
  volume = 8,
  year = 1961,
  pages = {21--28},
  bibliographies = {RelMiCS}
}

@InProceedings{MaQingMing-1992,
  author = {QingMing Ma},
  title = {Parametricity as Subtyping},
  crossref = {POPL1992},
  pages = {281--292},
  note = {Preliminary Report},
  authorsAddress = {Qingming.Ma\@cs.cmu.edu},
  URL = {http://www.acm.org/pubs/citations/proceedings/plan/143165/p281-ma/},
  WKloc = {A-0162},
  abstract = {A polymorphic function is parametric if it has
		  uniform behaviour for all type parameters. this
		  property is useful when writing, reasoning about, and
		  compiling functional programs.

                  We show how to syntactically define and reason about
		  parametricity in a language with intersection types
		  and bounded polymorphism. Within this framework,
		  parametricity is subtyping, and reasoning about
		  parametricity becomes reasoning about the
		  well-typedness of terms. This work also demonstrates
		  the expressiveness of languages that combine
		  intersection types and bounded polymorphism.}
}

@InProceedings{MacCaull-2001a,
  author = {Wendy MacCaull},
  title = {Relational Proof Systems},
  crossref = {RelMiCS2001-PP},
  pages = 10,
  note = {Invited talk},
  bibliographies = {RelMiCS}
}

@InProceedings{MacDonald-Hilfinger-Semenzato-1998,
  author = {Josh MacDonald and Paul N. Hilfinger and Luigi Semenzato},
  title = {{PRCS}: The Project Revision Control System},
  pages = {33--45},
  editor = {Boris Magnusson},
  booktitle = {8th International Symposium on System Configuration Management},
  month = JUL,
  year = 1998,
  series = LNCS,
  volume = 1439,
  publisher = Springer,
  URL = {http://www.xcf.berkeley.edu/~jmacd/prcs.html},
  WKloc = {A-1197}
}

@Article{MacLane-1963b,
  author = {Mac Lane, Saunders},
  title = {Natural Associativity and Commutativity},
  journal = {Rice University Studies},
  volume = 49,
  pages = {28--46},
  year = 1963,
  annote = {``categories with multiplication''
            --- alongside with \cite{Benabou-1963}
            original definition of monoidal categories.}
}

@Book{MacLane-1971,
  author = {Mac Lane, Saunders},
  title = {Categories for the Working Mathematician},
  publisher = {Springer-Verlag},
  year = 1971,
  McMaster = {QA 169 .M33 1998},
  WKloc = {B-0033}
}

@InProceedings{MacQueen-Sethi-1982,
  author = {D. B. MacQueen and Ravi Sethi},
  title = {A Semantic Model of Types for Applicative Languages},
  booktitle = {Conference Record of the 1982 {ACM} Symposium on Lisp
                 and Functional Programming},
  organization = {ACM},
  publisher = {ACM},
  month = aug,
  year = 1982,
  pages = {243--252},
  checked = 19940213,
  source = {dept. library},
  abstract = {If integer constants are added to the syntax of the
                 pure lambda calculus, then primitive integer values
                 have to be added to the underlying domain V of values.
                 Unlike functions, primitive values should not be
                 applied; we want a run-time error to occur if an
                 attempt is made to apply them as functions. Expressions
                 that might lead to run-time errors are separated out by
                 imposing a ``type'' structure on expressions. A
                 systematic model of types is developed, in which types
                 are formalized as ``ideals'' (sets with a certain
                 structure). Polymorphic functions are handled by
                 introducing a quantifier for taking conjunctions of
                 types. Operations for constructing new types from old
                 lead to the consideration of higher-order or meta
                 types, which are called ``kinds'' to avoid confusion
                 with types. Finally, the semantic model of types is
                 applied to show the soundness of a proof system for
                 inferring the types of expressions.},
  reffrom = {MacQueen:acm:lfp:1984},
  reffrom = {Reddy:acm:lfp:1988},
  reffrom = {Fagan:phd:1992}
}

@InProceedings{MacQueen-Tofte-1994,
  author = {David B. MacQueen and Mads Tofte},
  title = {A Semantics for Higher-Order Functors},
  crossref = {ESOP1994},
  pages = {409--424},
  WKloc = {A-0327},
  keywords = {SML},
  abstract = {$\ldots$}
}

@Article{Macfarlane-1880,
  author = {A. Macfarlane},
  title = {On a Calculus of Relationship},
  journal = PROEDIN,
  volume = 10,
  year = 1880,
  pages = {224--232},
  bibliographies = {RelMiCS}
}

@Article{Macfarlane-1882,
  author = {A. Macfarlane},
  title = {Algebra of Relationship -- {Part II}},
  journal = PROEDIN,
  volume = 11,
  year = 1882,
  pages = {5--13},
  bibliographies = {RelMiCS}
}

@Article{Macfarlane-1882a,
  author = {A. Macfarlane},
  title = {Algebra of Relationship -- {Part III}},
  journal = PROEDIN,
  volume = 11,
  year = 1882,
  pages = {162--163},
  bibliographies = {RelMiCS}
}

@Article{Mackenzie-2005,
  author = 	 {Dana Mackenzie},
  title = 	 {What in the Name of {Euclid} Is Going On Here?},
  journal = 	 {Science},
  year = 	 2005,
  volume =	 307,
  pages =	 {1402--1403},
  month =	 MAR,
  keywords = 	 {theorem prover, Ginthier, Coq, Isabelle},
  WKloc = 	 {A-1627}
}

@Misc{Mackie-1997,
  author = {Ian Mackie},
  title = {Static Analysis of Interaction Nets for Distributed Implementations},
  crossref = {SAS1997},
  pages={217--231},
  DOI={10.1007/BFb0032744},
  DOIURL={http://dx.doi.org/10.1007/BFb0032744},
  WKloc = {A-0592, doc/pap/BIB},
  abstract = {Interaction nets can be seen as both a programming language
    and an intermediate language
    for the implementation of other paradigms of computation.
    One of their principal advantages is
    that the reduction process is both local and confluent,
    thus being ideally suited for the development of parallel implementations.
    For distributed memory architectures, however,
    there is a need to know how the net should be distributed amongst processors,
    i.e. how do we break up the net so that the communication is kept to a minimum.
    The purpose of this paper is to suggest an analysis to solve this problem,
    and hint at other possible analyses for different applications,
    for example dynamic load balancing.}
}

@inproceedings{Mackie-1998,
 author = {Mackie, Ian},
 title = {{YALE}: Yet Another Lambda Evaluator Based on Interaction Nets},
 booktitle = {Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming},
 series = {ICFP '98},
 year = {1998},
 isbn = {1-58113-024-4},
 location = {Baltimore, Maryland, USA},
 pages = {117--128},
 numpages = {12},
 DOIURL = {http://doi.acm.org/10.1145/289423.289434},
 DOI = {10.1145/289423.289434},
 acmid = {289434},
 publisher = {ACM},
 address = {New York, NY, USA},
 abstract = {Interaction nets provide a graphical paradigm of computation based on net rewriting. They have proved most successful in understanding the dynamics of reduction in the &lambda;-calculus, where the prime example is the implementation of optimal reduction for the &lambda;-calculus (Lamping's algorithm), given by Gonthier, Abadi and L&eacute;vy. However, efficient implementations of optimal reduction have had to break away from the interaction net paradigm. In this paper we give a new efficient interaction net encoding of the &lambda;-calculus which is not optimal, but overcomes the inefficiencies caused by the bookkeeping operations in the implementations of optimal reduction. We believe that this implementation of the &lambda;-calculus could provide the basis for highly efficient implementations of functional languages.}
}

@InCollection{Mackie-2004,
  author={Mackie, Ian},
  title={Efficient $\lambda$-Evaluation with Interaction Nets},
  pages={155--169},
  crossref = {RTA2004},
  DOI={10.1007/978-3-540-25979-4_11},
  DOIURL={http://dx.doi.org/10.1007/978-3-540-25979-4_11}
}

@Article{Mackie-2005,
  title = "Towards a Programming Language for Interaction Nets",
  journal = ENTCS,
  volume = 127,
  number = 5,
  pages = "133--151",
  year = 2005,
  OPTnote = "Proceedings of the 2nd International Workshop on Term Graph Rewriting (TERMGRAPH 2004) Term Graph Rewriting 2004",
  note = "Proc.~TERMGRAPH 2004",
  issn = "1571-0661",
  DOIURL = "http://dx.doi.org/10.1016/j.entcs.2005.02.015",
  DOI = "10.1016/j.entcs.2005.02.015",
  DirectURL = "http://www.sciencedirect.com/science/article/pii/S1571066105050176",
  author = "Ian Mackie",
  keywords = "Interaction nets",
  keywords = "programming language design ",
  abstract = {Interaction nets were introduced almost 15 years ago.
    Since then they have been put forward as
    both a graphical programming paradigm
    and as an intermediate language into which we can compile other languages.
    Whichever way we use interaction nets,
    a problem remains in that the language is very primitive.
    Drawing an analogy with functional programming,
    we have the λ-calculus but we are missing
    the functional programming language: syntactic sugar,
    language constructs, data-structures, etc.
    The purpose of this paper is to make a first step
    towards defining such a programming language for interaction nets.}
}

@InCollection{Mackie-2008,
  author={Mackie, Ian},
  title={An Interaction Net Implementation of Closed Reduction},
  pages={43--59},
  year={2011},
  isbn={978-3-642-24451-3},
  LNCSbooktitle={IFL 2008},
  booktitle={Implementation and Application of Functional Languages},
  volume={5836},
  series=LNCS,
  editor={Scholz, Sven-Bodo and Chitil, Olaf},
  DOI={10.1007/978-3-642-24452-0_3},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-24452-0_3},
  publisher={Springer Berlin Heidelberg},
  language={English},
  abstract = {Closed reduction is a very efficient reduction strategy
    for the lambda calculus,
    which is explained using a simple form of explicit substitutions.
    This paper introduces this strategy,
    and gives an implementation as a system of interaction nets.
    We obtain one of the most efficient implementations of this kind to date.}
}

@inproceedings{Mackie-Sato-2015_GCM_Parallel,
  author    = {Ian Mackie and Shinya Sato},
  title     = {Parallel Evaluation of Interaction Nets: Some Observations and Examples},
  crossref = {GCM2015},
  pages     = {50--65},
  year      = {2015},
  url       = {http://ceur-ws.org/Vol-1403/paper5.pdf},
  timestamp = {Thu, 03 Sep 2015 17:01:02 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/conf/gg/MackieS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@Article{Mackworth-1977,
  author = {A. K. Mackworth},
  title = {Consistency in Networks of Relations},
  journal = AI,
  volume = 8,
  year = 1977,
  pages = {99--118},
  bibliographies = {RelMiCS}
}

@InCollection{Mackworth-1987,
  author = {A. K. Mackworth},
  title = {Constraint Satisfaction},
  booktitle = {Encyclopedia of Artificial Intelligence},
  note = {ed.\null{} S.\null{} Shapiro},
  publisher = Wiley,
  year = 1987,
  pages = {205--211},
  bibliographies = {RelMiCS}
}

@Article{Mackworth-Freuder-1985,
  author = {Alan K. Mackworth and Eugene C. Freuder},
  title = {The Complexity of some Polynomial Network Consistency
		Algorithms for Constraint Satisfaction Problems},
  journal = AI,
  volume = 25,
  pages = {65--74},
  year = 1985,
  bibliographies = {RelMiCS}
}

@Article{Maddux-1976,
  author = {Roger Duncan Maddux},
  title = {Some Nonrepresentable Relation Algebras},
  journal = NOTIC,
  year = 1976,
  volume = 23,
  pages = {A-431, A-557},
  bibliographies = {RelMiCS}
}

@Book{Maddux-1978,
  author = {Roger Duncan Maddux},
  title = {Topics in Relation Algebras},
  note = Doct,
  publisher = BERKELEY_P,
  year = 1978,
  pages = {iii+241},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1978a,
  author = {Roger Duncan Maddux},
  title = {Some Sufficient Conditions for the Representability of Relation
      Algebras},
  journal = ALGU,
  volume = 8,
  year = 1978,
  pages = {162--172},
  OPTnote = {MR 57\#205, Zbl 386.03033},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1980,
  author = {Roger Duncan Maddux},
  title = {{The Equational Theory of CA$_3$ is Undecidable}},
  journal = JSYLO,
  volume = 45,
  year = 1980,
  pages = {311--316},
  OPTnote = {MR 81e:03060, Zbl 435.03010},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1981,
  author = {Roger Duncan Maddux},
  title = {Embedding Modular Lattices into Relation Algebras},
  journal = ALGU,
  volume = 12,
  year = 1981,
  pages = {244--246},
  OPTnote = {MR 82d:06007, Zbl 415.06010, Zbl 452.06007},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1982,
  author = {Roger Duncan Maddux},
  title = {Some Varieties Containing Relation Algebras},
  journal = TRAMS,
  volume = 272,
  year = 1982,
  pages = {501--526},
  OPTnote = {MR 84a:03079, Zbl 515.03039},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1983,
  author = {Roger Duncan Maddux},
  title = {A Sequent Calculus for Relation Algebras},
  journal = ANPURE,
  volume = 25,
  year = 1983,
  pages = {73--101},
  OPTnote = {MR 85h:03067, Zbl 528.03016},
  bibliographies = {RelMiCS}
}

@InProceedings{Maddux-1985,
  author = {Roger Duncan Maddux},
  title = {Finite Integral Relation Algebras},
  booktitle = {Universal Algebra and Lattice Theory, Proc.\null{} of the Southeastern Conf.\null{} in Universal
		  Algebra and Lattice Theory, July 11--14, 1984},
  ANOTE = {MR 87d:03180, Zbl 583.03048, Zbl 563.00005},
  publisher = Springer,
  series = LNM,
  volume = 1149,
  year = 1985,
  pages = {175--197},
  bibliographies = {RelMiCS}
}

@Unpublished{Maddux-1987,
  author = {Roger Duncan Maddux},
  title = {Pair-dense Relation Algebras},
  note = {Draft paper, Iowa State Univ., Ames},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Article{Maddux-1989,
  author = {Roger Duncan Maddux},
  title = {Nonfinite Axiomatizability Results for Cylindric and
                Relation Algebras},
  journal = JSYLO,
  volume = 54,
  number = 3,
  year = 1989,
  pages = {951--974},
  OPTnote = {MR 90f:03099, Zbl 686.03035},
  month = SEP,
  bibliographies = {RelMiCS}
}

@Article{Maddux-1989a,
  author = {Roger Duncan Maddux},
  title = {Canonical Relativized Cylindric Set Algebras},
  journal = PROAMS,
  volume = 107,
  number = 2,
  year = 1989,
  month = OCT,
  pages = {465--478},
  OPTnote = {MR 90c:03064, Zbl 678.03029},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1989b,
  author = {Roger Duncan Maddux},
  title = {Finitary Algebraic Logic},
  journal = ZMALOG,
  volume = 35,
  pages = {321--332},
  year = 1989,
  OPTnote = {MR 90j:03111, Zbl 661.03052, Zbl 679.03029},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1990,
  author = {Roger Duncan Maddux},
  title = {A Relation Algebra which is not a Cylindric Reduct},
  journal = ALGU,
  volume = 27,
  year = 1990,
  pages = {279--288},
  OPTnote = {MR 91c:03048, Zbl 697.03036},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1990a,
  author = {Roger Duncan Maddux},
  title = {Necessary Subalgebras of Simple Nonintegral Semiassociative
      Relation Algebras},
  journal = ALGU,
  volume = 27,
  year = 1990,
  pages = {544--558},
  OPTnote = {Zbl 723.03038},
  bibliographies = {RelMiCS}
}

@InProceedings{Maddux-1991,
  author = {Roger D. Maddux},
  title = {Introductory Course on Relation Algebras,
		  Finite-dimensional Cylindric Algebras, and Their
		  Interconnections},
  crossref = {Andreka-Monk-Nemeti-1991},
  OPTpages = {361--392},
  OPTnote = {},
  OPTauthorsaddress = {},
  WKloc = {A-0128},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1991a,
  author = {Roger Duncan Maddux},
  title = {The Neat Embedding Problem and the Number of Variables Required
      in Proofs},
  journal = PROAMS,
  volume = 112,
  year = 1991,
  pages = {195--202},
  OPTnote = {MR 91h:03089, Zbl 717.03025},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1991b,
  author = {Roger Duncan Maddux},
  title = {Pair-dense Relation Algebras},
  journal = TRAMS,
  volume = 328,
  year = 1991,
  pages = {83--131},
  OPTnote = {MR 92c:03070},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1991c,
  author = {Roger Duncan Maddux},
  title = {The Origin of Relation Algebras in the Development and
      Axiomatization of the Calculus of Relations},
  journal = STUDLOG,
  volume = 50,
  number = {3/4},
  year = 1991,
  pages = {421--455},
  OPTnote = {MR~93d:03067 (Janos Cirulis), Zbl~754.03042 (J. Monk)},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1992,
  author = {Roger Duncan Maddux},
  title = {Relation Algebras of every Dimension},
  journal = JSYLO,
  volume = 57,
  number = 4,
  year = 1992,
  month = DEC,
  pages = {1213--1229},
  bibliographies = {RelMiCS}
}

@TechReport{Maddux-1992a,
  author = {Roger Duncan Maddux},
  title = {A working relational model: The derivation of the
		  {Dijkstra-Scholten}  predicate transformer semantics
		  from Tarski's axioms for the  {Peirce-Schr\"oder}
		  calculus of relations},
  institution = {Dept.\null{} of Mathematics, Iowa State Univ.},
  year = 1992,
  address = {Ames, Iowa 50011, USA},
  month = SEP,
  note = {Superseded by \cite{Maddux1993a}},
  bibliographies = {RelMiCS}
}

@InProceedings{Maddux-1993,
  author = {Roger Duncan Maddux},
  title = {Relation Algebras for Reasoning about Time and Space},
  crossref = {AMAST1993},
  pages = {27--44},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1993a,
  author = {Roger Duncan Maddux},
  title = {A working relational model: The derivation of the
		  {Dijkstra-Scholten}  predicate transformer semantics
		  from {Tarski}'s axioms for the  {Peirce-Schr\"oder}
		  calculus of relations},
  year = 1993,
  journal = South_African_Computer_Journal,
  volume = 9,
  pages = {92--130},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1994,
  author = {Roger Duncan Maddux},
  title = {A Perspective on the Theory of Relation Algebras},
  volume = 31,
  pages = {456--465},
  journal = ALGU,
  year = 1994,
  bibliographies = {RelMiCS}
}

@Article{Maddux-1994a,
  author = {Roger Duncan Maddux},
  title = {Undecidable semiassociative relation algebras},
  volume = 59,
  pages = {398--418},
  journal = JSYLO,
  year = 1994,
  bibliographies = {RelMiCS}
}

@InProceedings{Maddux-1995,
  author = {Roger Duncan Maddux},
  title = {On the Derivation of Identities Involving Projection Functions},
  booktitle = {Logic Colloquium '92},
  editor = {Csirmaz and Gabbay and de Rijke},
  publisher = {Center for the Study of Language and Information Publications},
  address = {Stanford},
  year = 1995,
  pages = {145--163},
  month = JAN,
  WKloc = {A-0455},
  abstract = {Then ``unsharpness problem'' is solved
    by construction of a finite nonintegral relation algebra with [\ldots].
    Although this equation fails in general,
    it does hold under slightly stronger hypotheses.},
  bibliographies = {RelMiCS}
}

@Article{Maddux-1996,
  year = 1996,
  volume = 160,
  title = {Relation-Algebraic Semantics},
  pages = {1--85},
  journal = TCS,
  author = {Roger D. Maddux},
  bibliographies = {RelMiCS}
}

@InCollection{Maddux-1997,
  author = {Roger Duncan Maddux},
  title = {Relation Algebras},
  chapter = 2,
  pages = {22--38},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Book{Maddux-2006,
  author = {Roger Duncan Maddux},
  title = {Relation Algebras},
  publisher = 	 {\unfinished},
  year = 	 {2006},
  OPTkey = 	 {},
  OPTvolume = 	 {150},
  OPTnumber = 	 {},
  series = {Studies in Logic and the Foundations of Mathematics},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  pages = 	 {758},
  ISBN = 	 {0-444-52013-9},
  abstract = {The modern theory of algebras of binary relations,
      reformulated by Tarski as an
      abstract, algebraic, equational theory of relation algebras,
      has considerable mathematical significance,
      with applications in various fields:
      e.g., in computer science --- databases, specification theory, AI ---
      and in anthropology, economics, physics, and philosophical logic.
      This comprehensive treatment of the theory of relation algebras and the
      calculus of relations is the first devoted to a systematic development of
      the subject.},
  contents = { Preface
               List of Figures
               List of Tables
               Chapter 1. Calculus of relations
               Chapter 2. Set theory
               Chapter 3. General algebra
               Chapter 4. Logic with equality
               Chapter 5. Boolean algebras
               Chapter 6. Relation algebras
               Chapter 7. Algebraic logic
               Chapter 8. 4329 finite integral relation algebras
               Bibliography
               Index}
}

@Article{Maddux-Tarski-1976,
  author = {Roger D. Maddux and Alfred Tarski},
  title = {A Sufficient Condition for the Representability of Relation
      Algebras},
  journal = NOTIC,
  volume = 23,
  year = 1976,
  pages = {A-447},
  note = {Reprinted in Alfred Tarski: Collected Papers (4 vols.), Birkh{\"a}user},
  bibliographies = {RelMiCS}
}

@InProceedings{Madsen-2000,
  author = {Ole Lehrmann Madsen},
  title = {Towards a UNified Programming Language},
  crossref = {ECOOP2000},
  pages = {1--26},
  WKloc = {A-1004},
  keywords = {BETA}
}

@TechReport{Maeda-1992,
  year = 1992,
  type = {Technical Memorandum},
  title = {Implementing a Process oriented debugger with
		  reflection and program transformation},
  number = {92-1168},
  institution = {Institute for New Generation Computer Technology
		  Tokyo, ICOT Research Center},
  author = {M. Maeda},
  annote = {tubibmue}
}

@Article{Maekinen-1989,
  year = 1989,
  volume = 32,
  title = {On the Subtree Isomorphism Problem for Ordered Trees},
  pages = {271--273},
  month = SEP,
  journal = IPLET,
  author = {E. M{\"a}kinen},
  bibliographies = {RelMiCS}
}

@InProceedings{Maessen-2002,
  author = {Jan-Willem Maessen},
  title = {Eager Haskell: resource-bounded execution yields efficient iteration},
  crossref = {Haskell2002},
  pages = {38--50},
  URL = {http://doi.acm.org/10.1145/581690.581694},
  WKloc = {doc/pap/BIB},
  abstract = {The advantages of the Haskell programming language are
      rooted in its clean equational semantics. Those advantages evaporate
      as soon as programmers try to write simple iterative computations and
      discover that their code must be annotated with calls to seq in order
      to overcome space leaks introduced by lazy evaluation. The Eager
      Haskell compiler executes Haskell programs eagerly by default, i.e.,
      bindings and function arguments are evaluated before bodies. When
      resource bounds are exceeded, computation falls back and is restarted
      lazily. By using a hybrid of eager and lazy evaluation, we preserve
      the semantics of Haskell and yet permit efficient iteration.}
}

@Book{Magee-Kramer-1999,
  author = {Jeff Magee and Jeff Kramer},
  title = {Concurrency: State Models \& Java Programs},
  publisher = {Wiley},
  year = 1999,
  note = {URL: \textsf{http://www-dse.doc.ic.ac.uk/concurrency/}},
  ISBN = {0-471-98710-7},
  bibliographies = {SE3B}
}

@InProceedings{MaggioloSchettini-Peron-1993,
  author = {Andrea MaggioloSchettini and Adriano Peron},
  title = {Semantics pf Full Statecharts Based on Graph Rewriting},
  crossref = {GTCS93},
  pages = {265--280},
  abstract = {A semantics of statecharts based on graph rewriting
		  is presented. Statecharts are formalized as graph
		  replacement rules. The graph of derivations gives a
		  sequential semantics which agrees with statechart
		  step semantics.}
}

@InProceedings{MaggioloSchettini-Winkowski-1992,
  abstract = {Transformations of hypergraphs by applying rewriting
		  rules are considered. An idea of progrmming such
		  transformations and a suitable language with a
		  denotational semantics is presented. It is shown
		  that in this language one can program sequential and
		  parallel processes of rewritimg as particular cases.},
  title = {A Programming Language for Deriving Hypergraphs},
  pages = {221--231},
  crossref = {CAAP92},
  author = {Andrea Maggiolo-Schettini and J\'ozef Winkowski},
  bibliographies = {RelMiCS}
}

@Book{Maier-1983,
  author = {D. Maier},
  title = {The Theory of Relational Databases},
  publisher = CompuSci,
  address = {Rockville, MD},
  year = 1983,
  bibliographies = {RelMiCS}
}

@InCollection{Maier-Mendelzon-Sadri-Ullman-1981,
  author = {D. Maier and A. O. Mendelzon and F. Sadri and J. D. Ullman},
  title = {Adequacy of Decompositions of Relational Databases},
  booktitle = {Advances in Database Theory},
  publisher = Plenum,
  year = 1981,
  address = {New York and London},
  editor = {Gallaire H. and J. Minker and J.M. Nicolas},
  volume = 28,
  number = 1,
  bibliographies = {RelMiCS}
}

@Book{Maier-Wildberger-1995,
  author = {Gunther Maier and Andreas Wildberger},
  title = {{In 8 Sekunden um die Welt: Kommunikation \"uber das
		  Internet}},
  publisher = {Addison-Wesley},
  year = 1995,
  edition = 4,
  note = {ISBN 3-89319-944-6},
  UniBwM = {INF190/YA9217},
  annote = {oriented towards PC users}
}

@Article{Major-Lapalme-Cedergren-1991,
  author = {Major, F. and Lapalme, G. and Cedergren, R.},
  journal = {J. Funct. Prog.},
  pages = {213--227},
  title = {Domain Generating Functions for Solving Constraint
                Satisfaction Problems},
  volume = 1,
  year = 1991
}

@TechReport{Makholm-Wells-2005,
  author = 	 {Henning Makholm and J. B. Wells},
  title = 	 {Type Inference and Principal Typings for
   Symmetric Record Concatenation and Mixin Modules},
  institution =  {},
  year = 	 {2005},
  URL = 	 {http://www.macs.hw.ac.uk/DART/software/Bowtini/},
  OPTtype = 	 {},
  OPTnumber = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  WKloc = {A-1608, doc/pap/BIB},
  bibliographies = {PMC},
  abstract = {In this report we present a new type system, Bowtie, for a lambda
calculus with records and a symmetric record concatenation operation.
The system has been designed to allow feasible \emph{type inference}
with a complexity of $O(nm*\alpha(n))$ where $n$ is the input size and $m$
the number of different field labels in the program, as well as
\emph{compositional type inference} and a notion of \emph{principal typings}.

(Because $\alpha(n) <= 4 for n < 10^{10^100}$ (a googolplex), one can
think of this factor as a constant for practical purposes.)

Most previous type systems for record concatenation have relied on
subtyping polymorphism.  Bowtie does not, and because subtyping is
absent, we have been able to straightforwardly extend Bowtie to a type
system, Martini, for \emph{mixin modules}.

Bowtie and Martini are both non-polymorphic type systems, but because
both have principal typings it is straightforward to extend either
with Milner's let-polymorphism, and we do so.

Apart from the definition of Bowtie and Martini we also present some
negative results (``things that don't work'').  Most notable among these
is a series of proofs that the straightforward typing rule for a
record concatenation operator in an SML-like type system leads to an
NP-hard typability problem --- \emph{even for the let-free (non-polymorphic)
fragment}, and \emph{no matter whether the concatenation operator is
symmetric or asymmetric/overwriting}.}
}

@Article{Makinen-1989,
  year = 1989,
  volume = 32,
  title = {On the Subtree Isomorphism Problem for Ordered Trees},
  pages = {271-273},
  month = SEP,
  journal = {Information Processing Letters},
  author = {E. M\"akinen}
}

@Article{Malcolm-1990,
  author = {Grant Malcolm},
  title = {Data Structures and Program Transformation},
  pages = {255--279},
  journal = SCICOP,
  year = 1990,
  volume = 14,
  WKloc = {A-0013},
  bibliographies = {RelMiCS},
  abstract = {The construction os structure-preserving maps,
		  ``homomorphisms'', is described for an arbitrary
		  data type, and a ``promotion'' theorem is derived
		  for proving equalities of homomorphisms. Examples
		  are given for finite lists, tree structures and
		  types defined by mutual induction; the construction
		  is then dualised to data types with infinite
		  objects, such as infinite lists. The promotion
		  theorem allows the development of concise,
		  calculational proofs: several examples are given of
		  its application to program transformation.},
  annote = {Homomorphisms and a promotion theorem for an arbitrary
		  data type. Dualised to infinite types (terminal
		  algebras, with destructors {\em from\/} the defined
		  type {\em to\/} the base types).
 		\begin{quotation}
                  Our starting point in this paper is the
		  `Bird-Meertens formalism', a mathematical framework
		  for program transformation currently being developed
		  by Richard Bird at Oxford and Lambert Meertens at Amsterdam.
                  The formalism comprises a concise functional
		  notation and a small number of remarkably powerful
		  theorems for proving equalities of functions. The
		  notation is based on a few elementary operations and
		  is so designed that it lends itself well to the
		  construction of elegant, calculational
		  transformations: from the basic operations, one
		  constructs a specification in algorithmic
		  form---there is no separate specification
		  language---and transforms the specification into a
		  more efficient program. A hallmark of the formalism
		  is that much of this transformation process can be
		  conducted as a linear, equational proof that the
		  original specification is extensionally equal to the
		  resulting more efficient version.
 		\end{quotation}}
}

@PhDThesis{Malcolm-1990a,
  year = 1990,
  title = {Algebraic Data Types and Program Transformation},
  school = {Rijksuniversiteit Groningen},
  author = {G. R. Malcolm},
  bibliographies = {RelMiCS}
}

@Misc{Malcolm-Goguen-199X,
  author = {Grant Malcolm and Joseph A. Goguen},
  title = {An Executable Course in the Algebraic Semantics of Imperative Programs},
  year = {199?},
  WKloc = {A-0668}
}

@PhdThesis{malecha2015thesis,
  author = "Gregory Michael Malecha",
  title = "Extensible Proof Engineering in Intensional Type Theory",
  school = "Harvard University",
  month = NOV,
  year = 2014,
  bibliographies = {RATH-Agda},
  url = {http://gmalecha.github.io/publication/2015/02/01/extensible-proof-engineering-in-intensional-type-theory.html},
  abstract = {We increasingly rely on large, complex systems in our daily lives ---
    from the computers that park our cars to the medical devices that regulate insulin levels
    to the servers that store our personal information in the cloud.
    As these systems grow, they become too complex for a person to understand,
    yet it is essential that they are correct.
    Proof assistants are tools that let us specify properties about complex systems
    and build, maintain, and check proofs of these properties in a rigorous way.
    Proof assistants achieve this level of rigor for a wide range of properties
    by requiring detailed certificates (proofs) that can be easily checked.

    In this dissertation, I describe a technique for compositionally building extensible automation
    within a foundational proof assistant for intensional type theory.
    My technique builds on computational reflection --- where properties are checked
    by verified programs --- which effectively bridges the gap
    between the low-level reasoning that is native to the proof assistant
    and the interesting, high-level properties of real systems.
    Building automation within a proof assistant provides a rigorous foundation
    that makes it possible to compose and extend the automation with other tools (including humans).
    However, previous approaches require using low-level proofs to compose different automation
    which limits scalability.
    My techniques allow for reasoning at a higher level about composing automation,
    which enables more scalable reflective reasoning.
    I demonstrate these techniques through a series of case studies
    centered around tasks in program verification.}
}

@InProceedings{Malik-Binford-1983,
  author = {J. Malik and T. O. Binford},
  title = {Reasoning in Time and Space},
  booktitle = {Proc.\null{} of the {{$8^{th}$} Internat.\null{} Joint
      Conf.\null{} on Artificial Intelligence, Karlsruhe, W.\null{}
      Germany, August 1983 (IJCAI)}},
  pages = {343--345},
  year = 1983,
  bibliographies = {RelMiCS}
}

@Article{Mallol-Olivier-Serrato-1985,
  author = {Cristian Mallol and Jean-Pierre Olivier and Dany Serrato},
  title = {Groupoids, Idempotents and Pointwise Inverses in
                  Relational Categories},
  journal = JPAA,
  year = 1985,
  volume = 36,
  pages = {23--51},
  bibliographies = {RelMiCS}
}

@InProceedings{Manber-Smith-Gopal-1997,
  author = {Udi Manber and Mike Smith and Burra Gopal},
  title = {{WebGlimpse} --- Combining Browsing and Searching},
  booktitle = {1997 Usenix Technical Conference},
  year = 1997,
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0594}
}

@InProceedings{Mancoridis-Holt-1996,
  author = 	 {Spiros Mancoridis and Richard C. Holt},
  title = 	 {Recovering the Structure of Software Systems Using Tube Graph Interconnection Clustering},
  crossref =	 {ICSM1996},
  pages =	 {23--},
  URL = 	 {http://doi.ieeecomputersociety.org/10.1109/ICSM.1996.564985},
  abstract = {An important product of the software design phase is
     the specification of the software structure at various levels of
     detail. Without reliable design documentation, significant
     software systems become less accessible to software engineers
     because structural information is buried in the intricate
     implementation source code. Reverse engineering techniques aim at
     recovering the structure of software systems, from the source
     code and mental models of developers, in order to make these
     systems more understandable to those maintaining them. Many
     reverse engineering techniques rely on creating a decomposition
     hierarchy by recursively clustering related software components
     (e.g., variables, procedures, classes, modules) into composite
     components (e.g., subsystems). Component clustering is necessary
     for managing complexity, and therefore is an important step in
     the reverse engineering process. In this paper, we argue that the
     clustering of interconnections is also necessary during reverse
     engineering. We propose an approach, based on a formalism called
     tube graphs, to specifying system structure that considers both
     composite software components and composite interconnections
     between these components. We present efficient algorithms for
     software interconnection clustering, which nicely complement
     existing software component clustering algorithms.}
}

@InProceedings{Mancoridis-Mitchell-Rorres-Chen-Gansner-1998,
  author = 	 {S. Mancoridis and B. Mitchell and C. Rorres and Y. Chen
and E. Gansner},
  title = 	 {Using Automatic Clustering to Produce High-Level System
Organizations of Source Code},
  booktitle = 	 {Proceedings of the IWPC},
  pages =	 {45--53},
  year =	 1998,
  bibliographies = {OPG}
}

@Book{Manes-1976,
  author =	 {Ernest G. Manes},
  title = 	 {Algebraic Theories},
  publisher = 	 Springer,
  series = {Graduate Texts in Mathematics},
  volume = 26,
  year = 	 1976,
  keywords = 	 {monads, term monad},
  McMaster = {QA 251 .M365}
}

@Book{Manes-Arbib-1986,
  author = {Ernest G. Manes and Michael A. Arbib},
  title = {Algebraic Approaches to Program Semantics},
  publisher = {Springer-Verlag},
  year = 1986,
  series = {Texts and Monographs in Computer Science},
  WKloc = {Q-006},
  bibliographies = {RelMiCS},
  annote = {Considers denotational semantics with ``multifunctions'',
    i.e., relations, as semantic domain.}
}

@MastersThesis{Mann-1999,
  author = {Matthias Mann},
  title = {{Gleichheitsanalyse von Ausdr{\"u}cken in nicht-strikten
           funktionalen Programmiersprachen unter Verwendung der
           Kontextanalyse}},
  school = {Universit\"at Frankfurt, Fachbereich Informatik},
  year = 1999,
  WKloc = {A-0897}
}

@Article{Manna-Ness-Vuillemin-1973,
  author = {Zohar Manna and S. Ness and J. Vuillemin},
  title = {Inductive Methods for Proving Properties of Programs},
  journal = {CACM},
  year = 1973,
  volume = 16,
  number = 8,
  OPTmonth = {},
  pages = {491--502},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0727},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Manna-Waldinger-1979,
  author = {Zohar Manna and Richard Waldinger},
  title = {Synthesis: Dreams {$\Rightarrow$} Programs},
  journal = {IEEE Transactions on Software Engineering},
  ISSN = {0098-5589},
  year = 1979,
  volume = {SE-5},
  number = 4,
  pages = {294--328},
  month = JUL
}

@Book{Manna-Waldinger-1993,
  year = 1993,
  title = {The deductive foundations of computer programming: a one-volume
      version of ``The logical basis for computer programming''},
  publisher = Addison,
  author = {Zohar Manna and Richard Waldinger},
  address = {Reading, MA},
  McMaster = {QA 76.6 .M35595 1993},
  bibliographies = {RelMiCS}
}

@Article{MannS-2000,
  author    = "Steve Mann",
  title     = "Comparametric Equations",
  journal   = "{IEEE} Trans. Image Proc.",
  year      = 2000,
  volume    = 9,
  number    = 8,
  note      = "ISSN 1057-7149",
  month     = "August",
  pages     = "1389--1406",
  URL = {http://wearcam.org/comparam.htm},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Mano-Ogawa-1996,
  author = {Ken Mano and Mizuhito Ogawa},
  title = {Unique Normal From Property of Higher-Order Rewriting Systems},
  crossref = {ALP-1996},
  pages = {269--283},
  OPTabstract = {},
  WKloc = {A-0450}
}

@Article{Maraist-Oderski-Wadler-1998,
  author =    {John Maraist and Martin Oderski and Philip Wadler},
  title =     {The Call-by-Need Lambda Calculus},
  journal =   JFP,
  year =      1998,
  volume =    8,
  number =    3,
  pages =     {275--317}
}

@InProceedings{Maraist-Silberman-1994,
  author = {J. Maraist and F. S. K. Silberman},
  title = {A Graph Reduction Technique with Sharing Across
		  Narrowings for Functional-Logic Languages},
  crossref = {PLILP1994},
  pages = {355--369},
  WKloc = {A-0308},
  keywords = {NG-machine (Narrowing G-Machine)},
  abstract = {We present a system for the efficient implementation
		  via graph reduction of functional-logic programming
		  languages. In contrast to existing systems, ours
		  allows sharing not only of fully common
		  subexpressions but also of partially common
		  expressions formed across narrowings, and allows
		  both lazy and eager evaluation semantics. This
		  sharing across narrowings is achieved by adapting a
		  technique described by Lamping for optimal reduction
		  of lambda terms.}
}

@Manual{Maranget-,
  title = {{HeVeA} User Documentation},
  URL = {http://para.inria.fr/~maranget/hevea},
  author = {Luc Maranget},
  organization = {INRIA},
  WKloc = {A-1155}
}

@InProceedings{Maranget-1991,
  author = {Luc Maranget},
  title = {GAML: A Parallel Implementation of Lazy ML},
  crossref = {FPCA-1991},
  pages = {102--123},
  refs = 14,
  abstract = {We present a new parallel implementation of lazy ML.  Our
		  scheme is a direct extension of the G-machine-based
		  implementation of lazy ML.  Parallelism is
		  introduced by {\em fork} annotations inserted by the
		  programmer.  We discuss the interference of such
		  user annotations with strictness annotations
		  generated by our compiler. The system has been
		  implemented on a Sequent Balance computer.  We also
		  address the main practical issues involved,
		  including stack and heap management.},
  sjb = {Gives performance figures for nfib30, primes, queens, and euler.},
  WKloc = {A-0662}
}

@TechReport{Maranget-1994,
  author = {Luc Maranget},
  title = {Two Techniques for Compiling Lazy Pattern Matching},
  year = 1994,
  month = OCT,
  institution = {INRIA},
  number = {RR 2385},
  WKloc = {A-0745},
  abstract = {In ML style pattern matching, pattern size is not
      constrained and ambiguous patterns are allowed. This generality leads
      to a clear and concise programming style but is challenging in the
      context of lazy evaluation. A first challenge concerns language
      designers: in lazy ML, the evaluation order of expressions follows
      actual data dependencies. That is, only the computations that are
      needed to produce the final result are performed. Once given a proper
      (that is, non-ambiguous) semantics, pattern matching should be
      compiled in a similar spirit: any value matching a given pattern
      should be recognized by performing only the minimal number of
      elementary tests needed to do so. This challenge was first met by
      A.~Laville. A second challenge concerns compiler designers. As it
      stands, Lavill?s compilation algorithm cannot be incorporated in an
      actual lazy ML compiler for efficiency and completeness reasons. As a
      matter of fact, Lavill?s original algorithm did not fully treat the
      case of integers in patterns and can lead to explosions both in
      compilation time and generated code size. This paper provides a
      complete solution to that second challenge. In particular, the
      well-known (and size-efficient) pattern matching compilation
      technique using backtracking automata is here introduced for the
      first time into the world of lazy pattern matching.}
}

@Article{Marar-1993,
  author = {Marar, W.L.},
  title = {Mapping fibrations},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Manuscripta mathematica},
  year = 1993,
  volume = 80,
  number = 3,
  pages = {273--},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Marche-1994,
  title = {Normalised Rewriting and Normalised Completion},
  author = {Claude March{\'e}},
  pages = {394--403},
  crossref = {LICS9},
  WKloc = {A-0362},
  abstract = {We introduce {\em normalised rewriting}, a new rewrite
		  relation.  It generalises former notions of
		  rewriting modulo~$E$, dropping some conditions
		  on~$E$.  For example, $E$~can now be the theory of
		  identity, idempotency, the theory of Abelian groups,
		  the theory of commutative rings. We give a new
		  completion algorithm for normalised rewriting. It
		  contains as an instance the usual $AC$ completion
		  algorithm, but also the well-known Buchberger's
		  algorithm for computing standard bases of polynomial ideals.

                  We investigate the particular case of completion of
		  ground equations, In this case we prove by a uniform
		  method that completion modulo~$E$ terminates, for
		  some interesting~$E$.  As a consequence, we obtain
		  the decidability of the word problem for some
		  classes of equational theories.

                  We give implementation results which shows the
		  efficiency of normalised completion with respect to
		  completion modulo~$AC$.}
}

@Article{Marche-1996,
  title = {Normalised Rewriting: an alternative to Rewriting module a Set of Equations},
  author = {Claude March{\'e}},
  journal = {J. Symbolic Computing},
  year = 1996,
  WKloc = {A-0423}
}

@InProceedings{Marchiori-1994,
  author = {Massimo Marchiori},
  title = {Logic Programs as Term Rewriting Systems},
  crossref = {ALP1994},
  pages = {223--241},
  abstract = {This paper studies the relationship between logic
		  programs and term rewriting systems (TRSs). A
		  compositional transform is defined which given a
		  logic program computes a TRS. For a relevant class
		  of logic programs, called Simply Well Moded (SWM),
		  there is a one-to-one correspondence  between
		  computed answer substitutions of the logic program
		  and normal forms of the corresponding TRS. Moreover
		  the transform preserves termination, i.e., a logic
		  program terminates iff the corresponding TRS
		  terminates. This transform is refined in such a way
		  that the above results hold for a relevant class of
		  unification free programs containing SWM, the class
		  of Flatly Well Moded (FWM) programs.}
}

@Article{Marciniec-1997,
  author = 	 {Jacek Marciniec},
  title = {Infinite Set Unification with Application to Categorial Grammar},
  journal = 	 {Studia Logica},
  year = 	 1997,
  volume = 	 58,
  pages = 	 {339--355},
  URL = 	 {http://www.springerlink.com/index/P255464H5423JG35.pdf},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {In this paper the notion of unifier is extended to
                  the infinite set case. The proof of existence of the
                  most general unifier of any infinite, unifiable set
                  of types (terms) is presented. Learning procedure,
                  based on in nite set unification, is described.}
}

@PhDThesis{Marichal-1998,
  author = {Marichal},
  title = {},
  school = {},
  year = 1998,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  annote = {PhD thesis with Marc Roubens:
   Justifies axiomatically, why Choquet integral and not Sugeno integral}
}

@InProceedings{Marinov-Khurshid-2001,
  author = {Darko Marinov and Sarfraz Khurshid},
  title = {TestEra: A Novel Framework for Automated Testing of Java Programs},
  crossref = {ASE2001},
  OPTpages = {},
  WKloc = {doc/pap/BIB},
  abstract = {We present TestEra, a novel framework for
    automated testing of Java programs. TestEra automatically generates
    all non-isomorphic test cases, within a given input size,
    and evaluates correctness criteria. As an enabling technology,
    TestEra uses Alloy, a first-order relational language,
    and the Alloy Analyzer. Checking a program with TestEra
    involves modeling the correctness criteria for the program in Alloy
    and specifying abstraction and concretization translations
    between instances of Alloy models and Java data structures.
    TestEra produces concrete Java inputs as counterexamples
    to violated correctness criteria. This paper discusses
    TestEra's analyses of several case studies:
    methods that manipulate singly linked lists and red-black trees,
    a naming architecture, and a part of the Alloy Analyzer.}
}

@PhDThesis{Marlow-1995,
  author = {Simon David Marlow},
  title = {Deforestation for Higher-Order Functional Programs},
  year = 1995,
  month = SEP,
  school = {University of Glasgow, Department of Computing Science},
  WKloc = {A-0830}
}

@Misc{Marlow-199X,
  author = {Simon David Marlow},
  title = {Update Avoidance Analysis by Abstract Interpretation},
  year = {199X},
  WKloc = {A-0485}
}

@Misc{Marlow-2000,
  author = {Simon David Marlow},
  title = {Writing High-Performance Server Applications in {Haskell} --- Case Study: A {Haskell} Web Server},
  WKloc = {A-1379}
}

@InProceedings{Marlow-2002,
  author = {Simon David Marlow},
  title = {{Haddock}, a {Haskell} documentation tool},
  crossref = {Haskell2002},
  pages = {78--89},
  URL = {http://doi.acm.org/10.1145/581690.581697},
  WKloc = {doc/pap/BIB},
  abstract = {This paper describes Haddock, a tool for automatically
      generating documentation from Haskell source code. Haddock's unique
      approach to source code annotations provides a useful separation
      between the implementation of a library and the interface (and hence
      also the documentation) of that library, so that as far as possible
      the documentation annotations in the source code do not affect the
      programmer's freedom over the structure of the implementation. The
      internal structure and implementation of Haddock is also discussed.}
}

@Manual{Marlow-2002a,
  title = {Haddock User Guide},
  author = {Simon Marlow},
  edition = {(version 0.4)},
  month = JUL,
  year = 2002,
  URL = {http://haskell.org/haddock/},
  WKloc = {A-1433}
}

@Misc{Marlow-PeytonJones-1998,
  author = {Simon David Marlow and Peyton Jones, Simon L.},
  title = {The new {GHC/Hugs} Runtime System},
  year = 1998,
  month = OCT,
  WKloc = {A-0834}
}

@InProceedings{Marlow-PeytonJones-2003,
  author = {Simon Marlow and Peyton Jones, Simon},
  title = {Making a fast curry: Push/enter vs eval/apply for higher-order languages},
  crossref = {ICFP2003?},
  OPTpages = {},
  year = 2003,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  month = APR,
  OPTorganization = {},
  WKloc = {A-1483, doc/pap/BIB},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Marlow-Wadler-1997,
  author = {Simon Marlow and Philip Wadler},
  title = {A Practical Subtyping System For Erlang},
  booktitle = {International Conference on Functional Programming, Amsterdam, June 1997},
  year = 1997,
  WKloc = {A-0473}
}

@inproceedings{Martel-2004,
 author = {Matthieu Martel},
 title = {Validation of Assembler Programs for {DSPs}: a Static Analyzer},
 booktitle = {Proceedings of the ACM-SIGPLAN-SIGSOFT workshop on Program analysis for software tools and engineering},
 year = {2004},
 isbn = {1-58113-910-1},
 pages = {8--13},
 location = {Washington DC, USA},
 doi = {http://doi.acm.org/10.1145/996821.996827},
 homepage = {http://www.enseignement.polytechnique.fr/profs/informatique/Matthieu.Martel/},
 publisher = {ACM Press},
 bibliographies = {Coconut},
 abstract = { Digital Signal Processors are widely used in
       critical embedded systems to pilot low-level, often critical
       functionalities. We describe a static analyzer based on
       abstract interpretation and designed to validate industrial
       assembler programs for a DSP. The validation consists of
       guaranteeing the absence of runtime errors such as incorrect
       memory accesses and of tracking the sources of inaccuracies
       introduced by floating-point computations. Our first
       contribution is a new static analysis for relocatable assembler
       programs able to cope with dynamically computed branching
       addresses. Our second contribution is the analyzer itself and
       its graphical interface which helps the user to understand the
       numerical inaccuracies.}
}

@Book{Martin-1911,
  author = {J. M. Martin},
  title = {Dictionary of Philosophy and Psychology},
  abstract = {second edition, edited by J.~M.~Martin},
  publisher = Macmillan,
  address = {New York},
  year = 1911,
  bibliographies = {RelMiCS}
}

@Article{Martin-1976,
  author = {Richard M. Martin},
  title = {Some Comments on {De Morgan}, {Peirce}, and the Logic of
      Relations},
  journal = PEIRCE,
  volume = 12,
  year = 1976,
  pages = {223--230},
  bibliographies = {RelMiCS}
}

@Article{Martin-1976a,
  author = {Richard M. Martin},
  title = {On Individuality and Quantification in {Peirce's}
		Published Logic Papers, 1867--1885},
  journal = PEIRCE,
  volume = 12,
  year = 1976,
  pages = {231--245},
  bibliographies = {RelMiCS}
}

@Article{Martin-1978,
  author = {Richard M. Martin},
  title = {Of servants, Lovers, and Benefactors: {Peirce's} Algebra of
      Relatives of 1870},
  journal = JPHIL,
  volume = 7,
  year = 1978,
  pages = {27--48},
  bibliographies = {RelMiCS}
}

@Booklet{Martin-1979,
  author = {Richard M. Martin},
  title = {{Peirce's} Logic of Relations and other Studies},
  publisher = RIDDER,
  address = {Lisse},
  year = 1979,
  bibliographies = {RelMiCS}
}

@Misc{Martin-Gibbons-2002,
  author = {Andrew Martin and Jeremy Gibbons},
  title = {A monadic interpretation of tactics},
  month = FEB,
  year = 2002,
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#tactics},
  WKloc = {A-1459, doc/pap/BIB},
  abstract = {Many proof tools use `tactic languages' as programs to
      direct their proofs. We present a simplified idealised tactic
      language, and describe its denotational semantics. The language has
      many applications outside theorem-proving activities. The semantics
      is parametrised by a monad (plus additional structure). By
      instantiating this in various ways, the core semantics of a number of
      different tactic languages is obtained.}
}

@InProceedings{Martin-Nipkow-1990,
  author = {Ursula Martin and Tobias Nipkow},
  title = {Automating Squiggol},
  crossref = {IFIP1990},
  pages = {233--246},
  WKloc = {Q-011}
}

@InProceedings{MartinLoef-1974,
  author =	 {Martin-L{\"o}f, Per},
  title =        {An intuitionistic theory of types: Predicative part},
  booktitle = {Logic Colloquium 73},
  pages =     {73--108},
  year =      1974,
  editor =    {H. Rose and J. Shepherdson},
  publisher = {North-Holland}
}

@Book{MartinLoef-1984,
  author =	 {Martin-L{\"o}f, Per},
  title = 	 {Intuitionistic Type Theory},
  publisher = 	 {Bibliopolis},
  year = 	 1984
}

@Article{MartinezLopez-Baum-1998,
  author = {Mart{\'{\i}}nez L{\'o}pez, Pablo E. and Gabriel A. Baum},
  title = {Fork Algebraic Datatypes},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 4,
  OPTmonth = {},
  pages = {531--543},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0567},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Maruyama-Matsuyama-Araki-1992,
  author = {H. Maruyama and M. Matsuyama and K. Araki},
  title = {Support Tool and Strategy for Type Error Correction
                 with Polymorphic Types},
  booktitle = {Proceedings; The Sixteenth Annual International
                 Computer Software and Applications Conference},
  pages = {287--93},
  publisher = {IEEE Computer Society Press},
  address = {Los Alamitos, CA},
  year = 1992,
  keywords = {functional},
  ISBN = {0-8186-3000-0},
  abstract = {The authors focus on strongly typed functional
                 programming languages with polymorphic types, type
                 inference facilities, and higher-order functions, i.e.
                 functional programming languages whose type checkers
                 infer types in programs including polymorphic types and
                 higher-order functions types from the context before
                 execution programs. They examine effective approaches
                 to support type error corrections in these languages
                 and two approaches to support them, following a general
                 framework for debugging. Using these two approaches as
                 case studies, systematic debugging processes based on
                 the general framework for debugging are developed.}
}

@InCollection{Marx,
  author = {Marx, M.},
  address = {Stanford},
  booktitle = {Arrow Logic and Multi-Modal Logic},
  editor = {Marx, M. and Polos, L.},
  publisher = CSLI_P,
  series = {Studies in Logic, Language and Information},
  title = {Dynamic arrow logic with pairs},
  year = {},
  note = {to appear},
  bibliographies = {RelMiCS}
}

@Article{Marx-1999,
  author = {Maarten Marx},
  title = {Relation Algebras Can Tile},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {173--191},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/28/abstract.html},
  abstract = {Undecidability of the equational theory of the class RA of
      relation algebras can easily be proved using the undecidability of
      the word-problem for semigroups. With some effort and ingenuity, one
      can push this proof through for the larger class SA. We provide
      another "cause" for undecidability which works for even larger
      classes than SA. The reason is that we can encode the tiling problem.
      In doing so we will meet very simple BAO-varieties with undecidable
      equational theories which might be useful in other undecidability
      proofs. Our work is part of the research project which tries to
      establish the border between undecidability and decidability in
      relational type algebras, cf. [15,16,12,1] and the references
      therein. The ultimate goal of this research is to come up with
      versions of relational algebra which are still suitable for modern
      dynamic applications but whose equational theory is decidable or even
      tractable.},
  bibliographies = {RelMiCS}
}

@Article{Marx-2001,
  author = {Maarten Marx},
  title = {Relation Algebra with Binders},
  journal = {Journal of Logic and Computation},
  year = 2001,
  volume = 11,
  number = 5,
  pages = {691--700},
  WKloc = {A-1298, doc/pap/BIB},
  bibliographies = {RelMiCS},
  URL = {http://www3.oup.co.uk/logcom/hdb/Volume_11/Issue_05/110691.sgm.abs.html},
  keywords = {Binary relations, relation algebras, modal logic, hybrid logic, fork algebras, interpolation, Beth definability},
  abstract = {The language of relation algebras is expanded with variables
      denoting individual elements in the domain and with the [darr] binder
      from hybrid logic. Every elementary property of binary relations is
      expressible in the resulting language, something which fails for the
      relation algebraic language. That the new language is natural for
      speaking about binary relations is indicated by the fact that both
      Craig's Interpolation, and Beth's Definability theorems hold for its
      set of validities. The paper contains a number of worked examples.}
}

@Book{Marx-Venema,
  author = {Marx, M. and Yde Venema},
  publisher = Kluwer,
  title = {Multi-Dimensional Modal Logic},
  year = {},
  note = {to appear},
  bibliographies = {RelMiCS}
}

@TechReport{Marzetta-1991,
  year = 1991,
  title = {A quantifier-free type inference system},
  number = {91-014},
  institution = {Bern University},
  author = {Markus A. Marzetta},
  bibliographies = {RelMiCS},
  filename = {iam.unibe.ch:TechReports/1991/iam-91-014.ps.Z},
  abstract = {Several kinds of logical system have been introduced
		  in order to establish properties of programs like
		  termination and correctness: second order typed
		  lambda calculus due to Girard and Reynolds',
		  Martin-L\"of's intuitionistic type theories, the
		  calculus of constructions due to Huet and Coquand,
		  etc.  These systems are mostly characterised by a
		  great proof-theoretical strength, which allows to
		  prove totality/ter-mination for a large class of
		  functions/programs, perhaps using essentially
		  impredicative methods, but also increases the
		  difficulty of finding such a proof (automatically).
		  Starting from this observation and from work of
		  Mitchell and Harper on the programming language ML,
		  Feferman has developed constructive (type) theories
		  ranging in strength from primitive recursive
		  arithmetic $\it(PRA)$ up to fairly strong subsystems
		  of analysis. Following these lines we present
		  quantifier-free type inference systems by means of
		  which types can be assigned to
		  $\lambda$-expressions. Formally these are deduction
		  systems for sequents of the form
       ${a_1{\mkern-2mu \mathrel{:}\mkern-2mu} A_1},
        {\allowbreak \ldots,{\allowbreak
       {a_n{\mkern-2mu \mathrel{:}\mkern-2mu} A_n}{\allowbreak
       {\mathrel\supset
       {t_1{\mkern-2mu \mathrel{:}\mkern-2mu} B}}}}}
       $\/ and
       $
       {a_1{\mkern-2mu \mathrel{:}\mkern-2mu} A_1},{\allowbreak
       \ldots,{\allowbreak
       {a_n{\mkern-2mu \mathrel{:}\mkern-2mu} A_n}{\allowbreak
       {\mathrel\supset
       {t_1=t_2{\mkern-2mu \mathrel{:}\mkern-2mu} B}}}}}
       $
                 where the $a_i$ are free variables, the $t_i$ are
		  individual terms of explicitly typed lambda calculus
		  and $A_i,B$ are type terms built up from the basic
		  type ${\hbox{\rlap{\hbox{\sf I}}{\hskip0.15em\sf
		  N}}}$, equational types and (optionally) sum and
		  product types.  Special interest is put on the
		  analysis of the proof-theoretical strength of these
		  systems.  Our basic system ${\tau{\it pr}^{-}}$ is
		  shown to be proof-theoretically equivalent to ${{\it
		  PRA}}$.  The extension ${\tau{\it pr}}$ includes new
		  type constructions, dependent sum and product, but
		  still has the same strength.  A stronger system
		  ${\tau{\it ha}}$ can indeed be obtained from
		  ${\tau{\it pr}}$ by varying the type constructions
		  for which induction/recursion is allowed.}
}

@InProceedings{Maslov-1994,
  author = {Vadim Maslov},
  title = {Lazy Array Data-Flow Analysis},
  crossref = {POPL1994},
  pages = {311--325},
  abstract = {Automatic parallelization of rela FORTRAN programs
		  does not live up to users expectations yet, $\ldots$}
}

@InProceedings{Massalin-1987,
  author = {Henry Massalin},
  title = {Superoptimizer: A Look at the Smallest Program},
  crossref = {ASPLOS1987},
  pages = {122--126},
  doi = {http://doi.acm.org/10.1145/36206.36194},
  WKloc = {A-1623, doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = {Given an instruction set, the superoptimizer finds
     the shortest program to compute a function. Startling programs
     have been generated, many of them engaging in convoluted
     bit-fiddling bearing little resemblance to the source programs
     which defined the functions. The key idea in the superoptimizer
     is a probabilistic test that makes exhaustive searches practical
     for programs of useful size. The search space is defined by the
     processor's instruction set, which may include the whole set, but
     it is typically restricted to a subset. By constraining the
     instructions and observing the effect on the output program, one
     can gain insight into the design of instruction sets. In
     addition, superoptimized programs may be used by peephole
     optimizers to improve the quality of generated code, or by
     assembly language programmers to improve manually written code.}
}

@Misc{MathML2.0,
  key = {MathML 2.0},
  editor =	 {David Carlisle and Patrick Ion and Robert Miner and and Nico Poppelier},
  title =	 {Mathematical Markup Language ({MathML}) version 2.0},
  howpublished = {W3C recommendation, World Wide Web Consortium},
  year =	 {2001},
  note =	 {Available at \url{http://www.w3.org/TR/MathML2/}},
  bibliographies = {OPG}
}

@PhDThesis{Matsushita-1998,
  author = {Tatsuru Matsushita},
  title = {On the Power of Declarative Programming},
  school = {University of York, Department of Computer Science},
  year = 1998,
  type = {D.Phil.\null{} thesis},
  month = OCT,
  URL = {ftp://ftp.cs.york.ac.uk/pub/tatsuru/thesis.ps.gz},
  WKloc = {doc/pap/BIB/Matsushita-1998_thesis*},
  annote = {Includes a translation scheme between logic programs
            in Prolog and functional programs in Haskell.
            The translation scheme is also fully implemented, and available from
            \utl{ftp://ftp.cs.york.ac.uk/pub/haskell/contrib/lp2fp.tar.gz}}
}

@InProceedings{Matthews-Moore-Ray-Vroon-2006,
  author       = "J. Matthews and J S. Moore and S. Ray and D. Vroon",
  title        = {Verification Condition Generation Via Theorem Proving},
  booktitle    = {Proceedings of the 13th International Conference on
                   Logic for Programming, Artificial Intelligence, and
                   Reasoning {(LPAR 2006)}},
  editor       = "M. Hermann and A. Voronkov",
  month        = NOV,
  year         = 2006,
  series       = LNCS,
  address      = "{Phnom Penh, Cambodia}",
  volume       = "4246",
  pages        = "362--376",
  publisher    = "Springer",
  WKloc = {doc/pap/BIB},
  AuthorURL = {http://www.cs.utexas.edu/~sandip/publications/symbolic-lpar/main.html},
  abstract = {We present a method to convert (i) an operational semantics for a given machine language, and (ii) an off-the-shelf theorem prover, into a high assurance verification condition generator (VCG). Given a program annotated with assertions at cutpoints, we show how to use the theorem prover directly on the operational semantics to generate verification conditions analogous to those produced by a custom-built VCG. Thus no separate VCG is necessary, and the theorem prover can be employed both to generate and to discharge the verification conditions. The method handles both partial and total correctness. It is also compositional in that the correctness of a subroutine needs to be proved once, rather than at each call site. The method has been used to verify several machine-level programs using the ACL2 theorem prover.}
}

@Book{Mattison-1996,
  author = {Rob Mattison},
  title = {Data Warehousing, Strategies, Technologies, and Techniques},
  publisher = {McGraw-Hill},
  year = 1996,
  keywords = {DW, data warehouse, data mining},
  ISBN = {0-07-041034-8},
  UniBwM = {BWL360/YC9849}
}

@PhDThesis{Mattson-1993,
  author = {James S. Mattson},
  title = {An Effective Speculative Evaluation Technique for Parallel Supercombinator Graph Reduction},
  year = 1993,
  institution = {University of California, San Diego},
  WKloc = {B-0041}
}

@Misc{Mattson-1993a,
  author = {James S. Mattson},
  title = {Performance of Parallel Schedulers for Distributed Graph Reduction},
  year = 1993,
  month = SEP,
  WKloc = {A-0661}
}

@Misc{Mattson-Griswold-19XX,
  author = {James S. Mattson and William G. Griswold},
  title = {Speculative Evaluation for Parallel Graph Reduction},
  year = {19??},
  WKloc = {A-0660}
}

@Article{Matula-1968,
  abstract = {Sketch of Reyners algorithm},
  year = 1968,
  volume = 10,
  title = {An Algorithm for Subtree Identification},
  pages = {273--274},
  note = {Abstract},
  journal = SIAMREV,
  author = {D. W. Matula},
  bibliographies = {RelMiCS}
}

@InProceedings{Matzke-1996,
  author = {Douglas J. Matzke},
  title = {information is Protophysical},
  crossref = {},
  pages = {223--225},
  OPTabstract = {},
  WKloc = {A-0590}
}

@InProceedings{Mauborgne-1999a,
  author = {Laurent Mauborgne},
  title = {Binary decision graphs},
  crossref = {SAS1999},
  pages = {101--116},
  URL = {http://www.di.ens.fr/~mauborgn/publi/sas99.html},
  abstract = {Binary Decision Graphs are an extension of Binary Decision
      Diagrams that can represent some infinite boolean functions. Three
      refinements of BDGs corresponding to classes of infinite functions of
      increasing complexity are presented. The first one is closed by
      intersection and union, the second one by intersection, and the last
      one by all boolean operations. The first two classes give rise to a
      canonical representation, which, when restricted to finite functions,
      are the classical BDDs. The paper also gives new insights in to the
      notion of variable names and the possibility of sharing variable
      names that can be of interest in the case of finite functions.},
  WKloc = {A-1282, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Article{Mauborgne-2000,
  author = {Laurent Mauborgne},
  editor = {Gert Smolka},
  title = {An Incremental Unique Representation for Regular Trees},
  journal = {Nordic Journal of Computing},
  year = 2000,
  volume = 7,
  number = 4,
  pages = {290--311},
  URL = {http://www.di.ens.fr/~mauborgn/publi/njc7.html},
  ACMcats = {D.1 [Programming Techniques]; E.1 [Data Structures]; G.2.2 [Discrete Mathematics]: Graph Theory; F.2 [Analysis of Algorithms and Problem Complexity]},
  keywords = {infinite trees, sharing, tree skeletons, cartesian approximation},
  WKloc = {A-1281, doc/pap/BIB},
  abstract = {In order to deal with infinite regular trees (or other
      pointed graph structures) efficiently, we give new algorithms to
      store such structures. The trees are stored in such a way that their
      representation is unique and shares substructures as much as
      possible. This maximal sharing allows substantial memory gain and
      speed up over previous techniques. For example, equality testing
      becomes constant time (instead of $O(n\log(n))$). The algorithms are
      incremental, and as such allow good reactive behavior. These new
      algorithms are then applied in a representation of sets of trees. The
      expressive power of this new representation is exactly what is needed
      by the original set-based analyses of Heintze and Jaffar [1990], or
      Heintze [1994].},
  bibliographies = {RelMiCS}
}

@Manual{Maude1999,
  title = {Maude: Specification and Programming in Rewriting Logic},
  author = {Manuel Clavel and Francisco Dur{\'a}n and Steven Eker and Patrick Linclon and Narciso Mart{\'\i}-Oliet and Jos{\'e} Meseguer and Jos{\'e} Quesada},
  organization = {Computer Science Laboratory, SRI International},
  month = JAN,
  year = 1999,
  WKloc = {B-0063},
  bibliographies = {SpecTech}
}

@InProceedings{Maurer-1994,
  author = {H. Maurer},
  title = {Learning Theoretical Aspects is Important but
		  (Sometimes) Dangerous},
  crossref = {Karhumaeki-Maurer-Rozenberg-1994},
  pages = {279--288}
}

@Article{Mauw-Reniers-1994,
  author = {S. Mauw and M. A. Reniers},
  title = {An Algebraic Semantics of Basic Message Sequence Charts},
  journal = {The Computer Journal},
  year = 1994,
  volume = 37,
  number = 4,
  OPTpages = {},
  WKloc = {A-1365, doc/pap/BIB},
  abstract = {Message Sequence Charts are a widely used technique
    for the visualization of the communications between system components.
    We present a formal semantics of Basic Message Sequence Charts,
    exploiting techniques from process algebra.
    This semantics is based on the semantics of the full language
    as being proposed for standardization in the
    International Telecommunication Union.}
}

@InCollection{May-1994,
  author = {David May},
  title = {How to Design a Parallel Computer},
  crossref = {Roscoe-1994},
  pages = {275--294},
  chapter = 17,
  OPTnote = {},
  OPTannote = {}
}

@MastersThesis{Mayr-1994,
  author = {Richard Mayr},
  title = {Konfluenz bei Termersetzungssystemen auf $\lambda$-Termen},
  school = {TU M\"unchen, Fakult\"at f\"ur Informatik},
  year = 1994,
  type = {Diplomarbeit}
}

@InProceedings{Mazanek-Minas-2008,
  author =       {Steffen Mazanek and Mark Minas},
  title =        {Functional-Logic Graph Parser Combinators},
  crossref =  {RTA2008},
  pages =     {261--275},
  WKloc = {doc/pap/BIB},
  OPTDOI =      {10.1007/978-3-540-70590-1_18},
  abstract =    {Parser combinators are a popular technique among functional programmers for writing parsers. They allow the definition of parsers for string languages in a manner quite similar to BNF rules. In recent papers we have shown that the combinator approach is also beneficial for graph parsing. However, we have noted as well that certain graph languages are difficult to describe in a purely functional way.

    In this paper we demonstrate that functional-logic languages can be used to conveniently implement graph parsers. Therefore, we provide a direct mapping from hyperedge replacement grammars to graph parsers. As in the string setting, our combinators closely reflect the building blocks of this grammar formalism. Finally, we show by example that our framework is strictly more powerful than hyperedge replacement grammars.

    We make heavy use of key features of both the functional and the logic programming approach: Higher-order functions allow the treatment of parsers as first class citizens. Non-determinism and logical variables are beneficial for dealing with errors and incomplete information. Parsers can even be applied backwards and thus be used as generators or for graph completion.}
}

@InProceedings{Mazurkiewicz-1986,
  author = 	 {A. Mazurkiewicz},
  title = 	 {Trace Theory},
  pages = 	 {279--324},
  crossref =  {Petri1986b},
  bibliographies = {Coconut}
}

@Misc{McAdam-1996,
  author = {Bruce J. McAdam},
  title = {Improving Type-Checkers},
  year = 1996,
  WKloc = {A-0468}
}

@Misc{McAdam-1996a,
  author = {Bruce J. McAdam},
  title = {Adding {BigTypes} to {ML}},
  year = 1996,
  WKloc = {A-0468}
}

@Article{McBride-2002,
  author = 	 {Conor McBride},
  title = 	 {{Faking It
                  (Simulating Dependent Types in Haskell)}},
  journal = 	 JFP,
  year = 	 2002,
  volume =	 12,
  number =	 {4\& 5},
  pages =	 {375--392},
  note =	 {Special Issue on Haskell},
  WKloc = {A-1691}
}

@Article{McBride-Paterson-2008,
  author = 	 {Conor McBride and Ross Paterson},
  title = 	 {Applicative Programming with Effects},
  journal = JFP,
  year = 2008,
  volume = 18,
  number = 1,
  pages = {1--13},
  URL = {http://www.soi.city.ac.uk/~ross/papers/Applicative.html},
  WKloc = {doc/pap/BIB},
  abstract = {In this paper, we introduce Applicative functors--an
                  abstract characterisation of an applicative style of
                  effectful programming, weaker than Monads and hence
                  more widespread. Indeed, it is the ubiquity of this
                  programming pattern that drew us to the
                  abstraction. We retrace our steps in this paper,
                  introducing the applicative pattern by diverse
                  examples, then abstracting it to define the
                  Applicative type class and introducing a bracket
                  notation which interprets the normal application
                  syntax in the idiom of an Applicative
                  functor. Further, we develop the properties of
                  applicative functors and the generic operations they
                  support. We close by identifying the categorical
                  structure of applicative functors and examining
                  their relationship both with Monads and with
                  Arrows.}
}

@Article{McColl-1878,
  author = {Hugh McColl},
  title = {The Calculus of Equivalent Statements and Integration Limits, I},
  journal = PROLMS,
  volume = 9,
  year = 1878,
  pages = {9--20},
  bibliographies = {RelMiCS}
}

@Article{McColl-1878a,
  author = {Hugh McColl},
  title = {The Calculus of Equivalent Statements, II},
  journal = PROLMS,
  volume = 9,
  year = 1878,
  pages = {177--186},
  bibliographies = {RelMiCS}
}

@Article{McColl-1879,
  author = {Hugh McColl},
  title = {The Calculus of Equivalent Statements, III},
  journal = PROLMS,
  volume = 10,
  year = 1879,
  pages = {16--28},
  bibliographies = {RelMiCS}
}

@Article{McColl-1880,
  author = {Hugh McColl},
  title = {The Calculus of Equivalent Statements, IV},
  journal = PROLMS,
  volume = 10,
  year = 1880,
  pages = {113--121},
  bibliographies = {RelMiCS}
}

@Misc{McCreary-Shieh-199Xa,
  author = {C. L. McCreary and Fwu-Shan Shieh},
  title = {Graph Parsing Techniques for Visualizing Directed Graphs},
  year = {199?},
  WKloc = {A-0511}
}

@Misc{McCreary-Shieh-199Xb,
  author = {C. L. McCreary and Fwu-Shan Shieh},
  title = {Using Graph Parsing for Automatic Graph Drawing},
  year = {199?},
  WKloc = {A-0512}
}

@UnPublished{McCune-2009,
  author = {William McCune},
  title = {{Prover9 and Mace4}},
  OPTnote = {\textsf{http://www.cs.unm.edu/\~{}mccune/prover9/}, version LADR-2009-11A},
  note = {\url{http://www.prover9.org/}, version LADR-2009-11A},
  year = {2009},
  DirectURL = {http://www.cs.unm.edu/~mccune/prover9/},
  FutureURL = {\url{http://www.prover9.org/}}
}

@Book{McEvoy-Tucker-1990,
  author = {K. McEvoy and J. V. Tucker},
  title = {Theoretical Foundations of {VLSI} Design},
  publisher = CambridgeUP,
  year = 1990,
  series = Cambridge,
  bibliographies = {RelMiCS}
}

@InProceedings{McGuffin-Scraefel-2004,
  author = 	 {McGuffin, M. J. and Schraefel, M. C.},
  title = 	 {A Comparison of Hyperstructures: {Zzstructures}, {mSpaces}, and Polyarchies},
  booktitle = {Proceedings of ACM Conference on Hypertext and Hypermedia, 2004, Santa Cruz, California, USA},
  pages = 	 {153--162},
  year = 	 2004,
  URL = 	 {http://eprints.ecs.soton.ac.uk/9230/},
  WKloc = 	 {A-1564, doc/pap/BIB}
}

@Article{McKay-1981,
  author =       {Brendan D. McKay},
  title =        {Practical Graph Isomorphism},
  journal =      {Congressus Numerantium},
  year =         1981,
  volume =    30,
  pages =     {45--87},
  bibliographies =     {RelMiCS},
  WKloc =      {doc/pap/BIB},
  URL =    {http://cs.anu.edu.au/~bdm/nauty/PGI/}
}

@Article{McKenzie-1965,
  author = {Ralph Nelson Whitfield McKenzie},
  title = {On Representing Relation Algebras in Groups},
  journal = NOTIC,
  volume = 12,
  year = 1965,
  pages = 821,
  bibliographies = {RelMiCS}
}

@Book{McKenzie-1966,
  author = {Ralph Nelson Whitfield McKenzie},
  title = {The Representation of Relation Algebras},
  note = Doct,
  publisher = COLORADO,
  address = {Boulder, USA},
  year = 1966,
  pages = {vi+128},
  bibliographies = {RelMiCS}
}

@Booklet{McKenzie-1966a,
  author = {Ralph Nelson Whitfield McKenzie},
  title = {A General Method for Constructing Elementary Axioms for Classes
      of Representable Structures},
  note = {Preprint, 1966, pp.\null{} 5},
  bibliographies = {RelMiCS}
}

@Article{McKenzie-1970,
  author = {Ralph Nelson Whitfield McKenzie},
  title = {The Representation of Integral Relation Algebras},
  journal = MICH,
  volume = 17,
  year = 1970,
  pages = {279--287},
  bibliographies = {RelMiCS}
}

@Article{McKinsey-1940,
  year = 1940,
  volume = 5,
  title = {Postulates for the Calculus of Binary Relations},
  pages = {85--97},
  number = 3,
  journal = JSYLO,
  author = {J. C. C. McKinsey},
  bibliographies = {RelMiCS}
}

@Article{McKinsey-1948,
  author = {J. C. C. McKinsey},
  title = {On the Representation of Projective Algebras},
  journal = AJM,
  volume = 70,
  year = 1948,
  pages = {375--384},
  bibliographies = {RelMiCS}
}

@Misc{McLaughlin-GesiBlanchard-Osani-1995,
  author = {Barry McLaughlin and Gesi Blanchard, Antoinette and Yuka Osani},
  title = {Assessing Language Development in Bilingual Preschool Children},
  howpublished = {NCBE Program Information Guide Series, Number 22, Summer 1995},
  year = 1995,
  WKloc = {A-0698}
}

@Manual{McMillan-1992,
  title = {The {SMV} System},
  author = {K. L. McMillan},
  organization = {Carnegie Mellon University},
  month = FEB,
  year = 1992,
  note = {Edited for SMV version 2.5.4 by Sergey Berezin, Nov.~2000},
  WKloc = {A-1204},
  bibliographies = {SpecTech}
}

@Book{McMillan-1993,
  author = {K. L. McMillan},
  title = {Symbolic Model Checking},
  publisher = {Kluwer Academic},
  year = 1993,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  bibliographies = {SpecTech},
  annote = {reference for the system SMV}
}

@Misc{McParland-199X,
  author = {P. J. McParland},
  title = {Evolving Software Using Automatic Program Transformation},
  year = {199X},
  WKloc = {A-0501}
}

@Misc{McParland-199Y,
  author = {P. J. McParland},
  title = {Using a Program Transformation System to Restructure {COBOL} Programs},
  year = {199X},
  WKloc = {A-0502}
}

@MastersThesis{McPhee-1995,
  author = {Richard McPhee},
  title = {Towards a Relational Programming Language},
  school = {Oxford University Computing Laboratory},
  type = {Qualifying dissertation for transfer to D.Phil.},
  year = 1995,
  month = SEP,
  WKloc = {B-0061},
  bibliographies = {RelMiCS}
}

@Article{McTaggart-1908,
  author = {J. M. E. McTaggart},
  title = {The Unreality of Time},
  journal = MIND,
  year = 1908,
  pages = {457--474},
  bibliographies = {RelMiCS}
}

@InProceedings{Mckinna-Pollack-1993,
  author = {James Mckinna and Robert Pollack},
  title = {Pure Type Systems Formalized},
  pages = {289--305},
  abstract = {This paper about our hobby. For us, machine-checked mathematics
             is a passion, and constructive type theory (in the broadest
             sense) is the way to this objective. Efficient and correct
             type-checking programs are necessary, so a formal theory of
             type systems leading to verified type synthesis algorithms is a
             natural goal. For over a year the second author has been
             developing a machine-checked presentation of the elementary
             meta-theory of Pure Type Systems (PTS) [Bar91], (formerly
             called Generalized Type Systems (GTS)). This project was
             blocked until the first author callaborated with a fresh idea.
             Here we describe the state of this ongoing project, presenting
             a completely formal, machine checked development of this basic
             meta-theory, including the underlying language of (explicitly
             typed) lambda calculus. We discuss some of the choices involved
             in formalization, some of the difficulties encountered, and
             techniques to overcome these difficulties.},
  crossref = {TLCA93},
  WKloc = {A-0186}
}

@Article{Mealy-1955,
  author = 	 {G.H. Mealy},
  title = 	 {A Method to Synthesizing Sequential Circuits},
  journal = 	 {Bell System Technical J.},
  year = 	 1955,
  pages = 	 {1045--1079},
  annote = 	 {see \textsf{http://en.wikipedia.org/wiki/Mealy_machine}}
}

@InProceedings{Medina-Immerman-1994,
  title = {A Syntactic Characterization of {NP}-Completeness},
  author = {J. Antonio Medina and Neil Immerman},
  pages = {241--250},
  crossref = {LICS9},
  abstract = {Fagin proved in 1974 that NP is equal to the set of problems
      expressible in second-order existential logic (SO$\exists$) [Fagin
      1974]. We consider problems that are NP-complete via first-order
      projections (fops). These low-level reductions are known to have nice
      properties, including the fact that every pair of problems that are
      NP-complete via fops are isomorphic via a first-order definable
      isomorphism [Allender et. al. 1993]. However, before this paper,
      fewer than five natural problems had actually been shown to be
      NP-complete via fops. \par We give a necessary and sufficient
      syntactic condition for an SO$\exists$ formula to represent a problem
      that is NP-complete via fops. Using this condition we prove
      syntactically that 29 natural NP-complete problems remain complete
      via fops.}
}

@InProceedings{Meertens-1986,
  author = {Lambert Meertens},
  title = {Algorithmics: Towards Programming as a Mathematical Activity},
  year = 1986,
  publisher = NoHo,
  pages = {289--334},
  editor = {J. W. de Bakker and M. Hazewinkel and J. K. Lenstra},
  booktitle = {Proc.\null{} {CWI} Symposium on Mathematics and Computer Science},
  abstract = {Of the various approaches to program correctness, that of
	``Transformational Programming'' appears to be the most helpful in
	constructing correct programs. The essence of the method is to start
	with an obviously correct ---but possibly hopelessly inefficient---
	algorithm, and to improve it by successively applying
	correctness-preserving transformations. The manipulations involved
	are akin to those used in mathematics. Two important impediments to
	this method are the verbosity of algorithmic notations, making the
	process cumbersome, and the semantic baroqueness of many primitives,
	making it hard to verify the validity of transformations. Computer
	Science can profit here from the lessons taught by the history of
	Mathematics. Another major step, comparable to one made long ago in
	Mathematics, is not to insist on the ``executability'' of algorithmic
	descriptions. This makes it possible to treat initial high-level
	specifications in the same framework as the final programs. Just as
	Mathematics evolved from ``Transformational Arithmetic'',
	Transformational Programming may come of age as ``Algorithmics''.},
  bibliographies = {RelMiCS}
}

@Unpublished{Meertens-1988,
  author = {Lambert Meertens},
  title = {First Steps Towards the Theory of Rose Trees},
  year = 1988,
  note = {Draft Report, CWI, Amsterdam}
}

@InProceedings{Meertens-1989,
  author = {Lambert Meertens},
  title = {Constructing a Calculus of Programs},
  crossref = {MPC1989},
  pages = {66--90},
  bibliographies = {RelMiCS}
}

@Article{Meertens-1992,
  title = {Paramorphisms},
  author = {Lambert Meertens},
  journal = FACOMP,
  volume = 4,
  number = 5,
  pages = {413--424},
  year = 1992,
  annote = {CWI Techreport CS-R9005},
  bibliographies = {RelMiCS}
}

@InProceedings{Meertens-1996,
  author = {Lambert Meertens},
  title = {Calculate Polytypically!},
  crossref = {PLILP1996},
  pages = {1--16},
  OPTabstract = {},
  WKloc = {A-0448}
}

@TechReport{Mehl-1991,
  abstract = {The goal of this work is to develop a formal logical
		  foundation of the  representation and the retrieval
		  of cases in CBR. An adequate basis therefor provides
		  the default logic with priorities. We present
		  transformations which construct defaults from the
		  memory of cases  such that the retrieval of
		  knowledge in CBR corresponds roughly  to the
		  preferred subtheory obtained by the defaults.

                  Die vorliegende Arbeit stellt einen Ansatz zur
		  logischen Fundierung  des fallbasierten Schlie\ss{}ens
		  vor. Aufbauend auf einer formalen Definition des
		  Fallspeichers, wird eine  Default-Theorie
		  entwickelt, die prinzipiell die gleichen
		  Informationen  enth"alt, wie der Fallspeicher. Die
		  Default-Theorie basiert auf der Default-Logik mit
		  Priorit"aten  nach Reiter mit der Erweiterung auf
		  priorisierte Defaults nach Brewka.},
  year = 1991,
  type = {SEKI Report},
  title = {Nichtmonotone Aspekte des fallbasierten Schlie{\ss}ens und
      deren Fundierung in Default-Logik mit Priorit{\"a}ten},
  number = {SWP-91-04 (SFB)},
  month = DEC,
  institution = KAIS,
  author = {Michael Mehl},
  address = {Postfach 3049, D-W 6750 Kaiserslautern}
}

@Book{Mehlhorn-Naeher-1999,
  author = {Kurt Mehlhorn and Stefan N{\"a}her},
  title = {{LEDA} --- A Platform for Combinatorial and Geometric Computing},
  year = 1999,
  publisher = {Cambridge University Press},
  ISBN = 0521563291,
  URL = {http://www.cup.cam.ac.uk/scripts/webbook.asp?isbn=0521563291}
}

@InProceedings{Mehlich-Baxter-1997,
  author = {Michael Mehlich and Ira D. Baxter},
  title = {Mechanical Tool Support for High  Integrity Software Development},
  booktitle = {Proc.\null{} High Integrity Software Conference, {Oct.\null{} 15--16, Albuquerque, new Mexico}},
  OPTpages = {},
  year = 1997,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = {IEEE},
  WKloc = {A-1013}
}

@Article{Meijer-2014,
  author =       {Erik Meijer},
  title =        {The Curse of the Excluded Middle --- ``Mostly functional'' programming does not work},
  journal =      {acm queue},
  year =         2014,
  volume =    12,
  number =    4,
  pages =     {20--29},
  month =     APR,
  URL = {https://queue.acm.org/detail.cfm?id=2611829},
  abstract =  {There is a trend in the software industry to sell
                  ``mostly functional'' programming as the silver
                  bullet for solving problems developers face with
                  concurrency, parallelism (manycore), and, of course,
                  Big Data. Contemporary imperative languages could
                  continue the ongoing trend, embrace closures, and
                  try to limit mutation and other side
                  effects. Unfortunately, just as ``mostly secure''
                  does not work, ``mostly functional'' does not work
                  either. Instead, developers should seriously
                  consider a completely fundamentalist option as well:
                  embrace pure lazy functional programming with all
                  effects explicitly surfaced in the type system using
                  monads.}
}

@Misc{Meijer-Claessen-199X,
  author = {Erik Meijer and Koen Claessen},
  title = {The Design and Implementation of Mondrian},
  year = {199?},
  WKloc = {A-0672}
}

@InProceedings{Meijer-Fokkinga-Paterson-1991,
  author = {Erik Meijer and Maarten Fokkinga and Ross Paterson},
  title = {Functional Programming with Bananas, Lenses,
		  Envelopes and Barbed Wire},
  pages = {124--144},
  crossref = {FPCA-1991},
  abstract = {We develop a calculus for lazy functional
		  programming based on recursion operators associated
		  with data type definitions. For these operators we
		  derive various algebraic laws that are useful in
		  deriving and manipulating programs. We shall show
		  that all example functions in Bird and Wadler's
		  ``Introduction to Functional Programming'' can be
		  expressed using these operators.},
  bibliographies = {RelMiCS}
}

@InProceedings{Meijer-Fokkinga-Paterson-1991-x,
  author = {Erik Meijer and Maarten Fokkinga and Ross Paterson},
  title = {Functional Programming with Bananas, Lenses,
		  Envelopes and Barbed Wire},
  pages = {124--144},
  abstract = {We develop a calculus for lazy functional
		  programming based on recursion operators associated
		  with data type definitions. For these operators we
		  derive various algebraic laws that are useful in
		  deriving and manipulating programs. We shall show
		  that all example functions in Bird and Wadler's
		  ``Introduction to Functional Programming'' can be
		  expressed using these operators.},
  UniBwM = {Z7648-5x-1, Handbib. WK},
  year = 1991,
  booktitle = {Functional Programming Languages and Computer Architecture, 5th {ACM} Conference},
  series = {LNCS},
  volume = 523,
  editor = {John Hughes}
}

@Unpublished{Meijer-Hutton-1995,
  author = {Erik Meijer and Graham Hutton},
  title = {Bananas in Space: Extending Fold and Unfold to
		  Exponential Types},
  note = {Submitted to: FPCA'95},
  year = 1995,
  WKloc = {A-0385},
  abstract = {Fold and unfold are general purpose functionals for
		  processing and constructing lists.  By using the
		  categorical approach of modelling recursive
		  datatypes as fixed points of functors, these
		  functionals --- together with their associated
		  algebraic properties --- were generalised from
		  simple lists to polynomial datatypes.  However, a
		  serious deficiency of this work is its limitation to
		  polynomial (sum-of-product) datatypes.  This
		  precludes the use of exponentials (function-spaces),
		  whereas it is central to functional programming that
		  functions are first-class values, and so
		  exponentials should be able to be freely used in
		  datatype definitions.  In this paper we solve this
		  problem, by showing how Freyds work on modelling
		  recursive datatypes as fixed points of difunctors
		  can be used to generalise the generic theory of fold
		  and unfold functionals from polynomial datatypes to
		  those involving arbitrary uses of exponentials.  For
		  concreteness, all concepts are implemented directly
		  in Gofer, by expoiting the constructor classes
		  extension of the standard type system.

                  {\em Note:} only elementary mathematical concepts
		  (functors, fixed points and algebras) are used in
		  this paper.}
}

@Unpublished{Meijer-Paterson-1995,
  author = {Erik Meijer and Ross Paterson},
  title = {Down With $\lambda$-Lifting!},
  note = {Unfinished draft},
  WKloc = {A-0386},
  year = 1995,
  abstract = {Simplifications of the Spineless Tagless G-Machine
		  and TIM are presented, which like the classic SECD
		  machine or the Krivine machine reduce
		  lambda-expressions to weak head normal form --- no
		  prior lambda-lifting is necessary. The machines are
		  at least as efficient as their combinator forebears
		  but more importantly they are simpler due to the
		  elimination of a translation step that obfuscates
		  programs without improving their efficiency.}
}

@Article{Meijer-vanDijk-199X,
  author = {Erik Meijer and van Dijk, Joost},
  title = {Internet Programming in Haskell},
  journal = {Journal of Functional Programming},
  year = {199X},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {B-0047},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Mellies-1999,
  author = "Paul-Andr{\'e} Melli{\'e}s",
  title = "On Double Categories and Multiplicative Linear Logic",
  url = "http://citeseer.ist.psu.edu/410937.html",
  WKloc = {doc/pap/BIB}
}

@InProceedings{Meinke-1991,
  author = {Karl Meinke},
  title = {Equational Specifications of Abstract Types and Combinators},
  crossref = {CSL91},
  pages = {257--271},
  note = {also as Report CSR 11-91, Dept. of Computer Science,
		  University College Swansea},
  WKloc = {A-0285},
  abstract = {We introduce an algebraic framework for the
		  equational specification of algebras of types and
		  combinators. A categorical semantics for type
		  specifications is given based on cofibrations of
		  categories and algebras. It is shown that each
		  equational type specification admits an initial
		  model semantics, and we present complete inference
		  systems for type assignbments and equations.}
}

@Article{Meinke-1992a,
  author = {Karl Meinke},
  title = {Universal Algebra in Higher Types},
  journal = {Theoretical Computer Science},
  year = 1992,
  OPTkey = {},
  volume = 100,
  OPTnumber = {},
  OPTmonth = {},
  pages = {385--417},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Meinke-1992b,
  author = {Karl Meinke},
  title = {Algebraic Semantics of Rewriting Terms and Types},
  crossref = {CTRS1992},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTyear = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTpages = {},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Meinke-Steggles-1993,
  author = {Karl Meinke and L.J. Steggles},
  title = {Specification and verification in higher-order algebra:
                  a case study of convolution},
  crossref = {HOA1993},
  pages = {189--222},
  abstract = {We present a case study of higher order algebraic
		  methods applied to the specification of convolution
		  as a second order transformation on streams. Two
		  systolic synchronous concurrent algorithms (SCAs)
		  for convolution are formally specified and verified
		  using higher order equational logic. We then study
		  the metamathematics of these verification proofs by
		  means of non-standard models.},
  bibliographies = {RelMiCS}
}

@Misc{Meloni-Santocanale-,
  author = {Gian Carlo Meloni and Luigi Santocanale},
  title = {Relational Semantics for Distributive Linear Logic},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1266},
  bibliographies = {RelMiCS},
  abstract = {Axioms ruling linear negation have been investigated in the
      context of the complete semantics for distributive intuitionistic
      linear logic. Among these are the condition of being a dualizing
      element and the one of being a cyclic element. The motivation for
      analyzing other syntactic constraints comes from the observation that
      groupoids are models for classical linear logic. The analysis
      proceeds also in the opposite way: given semantic conditions, which
      could possibly hold in the canonical model of prime filters,
      equivalent syntactic conditions are found. Last the relationships
      among analyzed axioms are investigated, counterexamples are provided
      whenever there is no provability dependence.}
}

@Article{Mendelzon-1979,
  author = {A. O. Mendelzon},
  title = {On Axiomatizing Multivalued Dependencies in Relational
      Databases},
  journal = JACM,
  volume = 26,
  number = 1,
  year = 1979,
  pages = {37--44},
  bibliographies = {RelMiCS}
}

@Article{Mendler-1991,
  UniBwM = {MAT/Z1756-51},
  keywords = {recursive types: inductive types and lazy types},
  abstract = {We add to the second-order lambda calculus the type
		  constructors $\mu$ and $\nu$, which give the least
		  and greatest solutions to positively defined type
		  expressions. Strong normalizability of typed terms
		  is shown using Girard's {\em candidat de
		  r\'eductibilit\'e} method. Using the same structure
		  built for that proof, we prove a necessary and
		  sufficient condition for determining when a
		  collection of equational type constraints admit the
		  typing of only strongly normalizable terms.},
  year = 1991,
  volume = 51,
  title = {Inductive Types and Type Constraints in the
		  Second-Order Lambda Calculus},
  pages = {159--172},
  journal = {Annals of Pure and Applied Logic},
  author = {Nax Paul Mendler}
}

@Misc{Menendez-2007m1,
  author = 	 {David Menendez},
  title = 	 {Re: Let's do {ListT} right, finally},
  howpublished = {Haskell mailing list message,
     \url{http://haskell.org/pipermail/libraries/2007-October/008313.html}},
  month = 	 OCT,
  year = 	 2007,
  WKloc = 	 {doc/pap/BIB}
}

@TechReport{Mens-1994,
  author = {Kim Mens},
  title = {An Introduction to Polymorphic Lambda Calculus with
		  Subtyping},
  institution = {Department of Computer Science, Vrije Universiteit Brussel},
  year = 1994,
  number = {vub-tinf-tr-94-01},
  month = OCT,
  WKloc = {A-0360}
}

@PhDThesis{Mens-1999,
  author = {Tom Mens},
  title = {A Formal Foundation for Object-Oriented Software Evolution},
  school = {Vrije Universiteit Brussel, Faculty of Sciences, Department of Computer Science, Programming Technology Lab},
  year = 1999,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  month = AUG,
  CiteSeer = {http://citeseer.nj.nec.com/mens99formal.html},
  WKloc = {doc/pap/BIB},
  abstract = {[This] PhD thesis claims that the principles behind
      object-oriented software evolution are independent of a particular
      domain or phase in the software lifecycle. To validate this claim, a
      formalism based on graphs and graph rewriting was developed and
      applied to a particular aspect of software evolution, namely the
      problem of software upgrading and software merging. When the same
      piece of software is modified in parallel by different software
      developers, unexpected inconsistencies can arise. Formal support can
      be provided to detect and resolve these inconsistencies in a general
      way.}
}

@Misc{Mens-Wermelinger-2001a,
  author = {Tom Mens and Michel Wermelinger},
  title = {Formal Foundations of Software Evolution: Workshop Report},
  howpublished = {Summary of the results of the discussions held during the workshop on Formal Foundations of Software Evolution in Lisbon on March 13, 2001},
  year = 2001,
  WKloc = {A-1090}
}

@TechReport{Merkel-1992,
  author = {M. Merkel},
  title = {Recurrent Patterns in Technical Documentation},
  keywords = {text, natural language, infosystems},
  year = 1992,
  type = {LiTH-IDA-Report},
  number = {92-31},
  institution = LINKOP,
  annote = {tubibmue},
  bibliographies = {RelMiCS}
}

@InProceedings{Merriam-Harrison-1997,
  author = {Nicholas A. Merriam and Michael D. Harrison},
  title = {What is Wrong with {GUI}s for Theorem Provers?},
  booktitle = {{Proc.\null{} UITP '97, INRIA Sophia Antipolis, France}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1997,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1100}
}

@Article{Merrill-1977,
  author = {Daniel D. Merrill},
  title = {On {De Morgan's} Argument},
  journal = NOTRE,
  volume = 18,
  year = 1977,
  pages = {133--139},
  bibliographies = {RelMiCS}
}

@Article{Merrill-1978,
  author = {Daniel D. Merrill},
  title = {{De Morgan}, {Peirce}, and the Logic of Relations},
  journal = PEIRCE,
  volume = 14,
  year = 1978,
  pages = {247--284},
  bibliographies = {RelMiCS}
}

@Booklet{Merrill-1984,
  author = {Daniel D. Merrill},
  title = {The 1870 Logic of Relatives Memoir},
  note = {{I}n \cite{Peirce-1984}, vol.\null{} 2, 1867-1871},
  pages = {xlii--xlviii},
  bibliographies = {RelMiCS}
}

@Article{Mertz-1979,
  author = {D. W. Mertz},
  title = {{Peirce}: Logic, Categories, and Triads},
  journal = PEIRCE,
  volume = 15,
  year = 1979,
  pages = {158--175},
  bibliographies = {RelMiCS}
}

@InProceedings{Merz-1997,
  author = {Stephan Merz},
  title = {Rules for Abstraction},
  series = {Lecture Notes in Computer Science},
  volume = 1345,
  pages = {32--??},
  year = 1997,
  WKloc = {A-0938},
  bibliographies = {SpecTech}
}

@InProceedings{Merz-1999,
  author = {Stephan Merz},
  title = {A More Complete {TLA}},
  editor = {Jeanette M. Wing and Jim Woodcock and Jim Davies},
  booktitle = {{FM}'99---Formal Methods, Volume~{II}},
  year = 1999,
  volume = 1709,
  series = {Lecture Notes in Computer Science},
  publisher = {Springer},
  pages = {1226--1244},
  WKloc = {A-0939},
  bibliographies = {SpecTech}
}

@InProceedings{Meseguer-1989,
  author = {J. Meseguer},
  title = {Relating Models of Polymorphism},
  crossref = {POPL1989},
  pages = {228--241},
  WKloc = {A-0207},
  OPTabstract = {A new general notion of model for the polymorphic
		  lambda calculus based on the simple idea of a {\em
		  universe}, is proposed. Although impossible in
		  nonconstructive set theory, the notion is
		  unproblematic for constructive sets, yields
		  completeness and initiality theorems, and can be
		  used to unify and relate many different notions of
		  model that have been proposed in the literature,
		  including thosethat extend the basic calculus with
		  additional features such as fixpoints or a type of
		  all types. Moreover, the polymorphic lambda calculus
		  and Martin-L\"of type theory are related by a map of
		  logics. A categorical and initial model semantics is
		  given for the basic calculus and for richer calculi
		  that extend the basic one with fixpoints or with a
		  type of all types.}
}

@InProceedings{Meseguer-1992,
  authorsAddress = {SRI International, Menlo Park and Center for
		  the Study of Language and Information, Stanford University},
  bibliographies = {RelMiCS},
  abstract = {A theory of general logics is outlined as a basis
		  for an axiomatic notion of ``logic programming
		  language''. It is shown that a wide variety of
		  logical programming languages are instances of the
		  general notion. The problem of designing
		  multiparadigm logic programming languages that
		  overcome the present limitations faced by relational
		  and functional logical languages in dealing with
		  state change and reactive systems is approached by a
		  method based on the use of the axiomatic notion of
		  logic programming language and of mappings between
		  logics to guide the search for a logic in which the
		  desired multiparadigm integration can be attained.
		  Following this method, rewriting logic is proposed
		  as a logic in which the functional, relational, and
		  concurrent object-oriented paradigms can be unified
		  in a simple and rigorous way. Two languages based on
		  this logic, Maude and MaudeLog, are briefly
		  described and illustrated with examples.},
  title = {Multiparadigm Logic Programming},
  pages = {158--200},
  note = {invited lecture},
  crossref = {MFCS1992},
  author = {Jos\'e Meseguer},
  annote = {?}
}

@InCollection{Meseguer-1998,
  year={1998},
  isbn={978-3-540-64299-2},
  booktitle={Recent Trends in Algebraic Development Techniques},
  volume={1376},
  series = LNCS,
  editor={Parisi Presicce, Francesco},
  DOI={10.1007/3-540-64299-4_26},
  title={Membership algebra as a logical framework for equational specification},
  DOIURL={http://dx.doi.org/10.1007/3-540-64299-4_26},
  publisher= Springer,
  author={Meseguer, Jos{\'e}},
  pages={18--61},
  WKloc = {doc/pap/BIB},
  abstract = {This paper proposes \emph{membership equational logic}
    --- a Horn logic in which the basic predicates are equations $t = t′$
    and membership assertions $t : s$
    stating that a term $t$ belongs to a sort $s$ ---
    as a logical framework in which
    a very wide range of total and partial equational specification formalisms
    can be naturally represented. Key features of this logic include:
    simplicity, liberality and equational character;
    generality and expressiveness in supporting subsorts, overloading,
    errors and partiality;
    and efficient implementability in systems such as Maude.
    The paper presents the basic properties of the logic and its models,
    and discusses in detail
    how many total and partial equational specification formalisms,
    including order-sorted algebra and \emph{partial} membership equational logic,
    can be represented in it,
    as well as the practical benefits in terms of tool reusability
    that this opens up for other languages, including CASL.}
}

@Misc{Meseguer-199X,
  author = {Jos\'e Meseguer},
  title = {Rewriting Logic as a Semantic Framework for Concurrency: A Progress Report},
  year = {199?},
  WKloc = {A-0724}
}

@Article{Meseguer-Goguen-Smolka-1989,
  author = 	 {Jos{\'e} Meseguer and Joseph A. Goguen and Gert Smolka},
  title = 	 {Order-sorted unification},
  journal = 	 JSYCO,
  year = 	 1989,
  volume = 	 8,
  number = 	 4,
  pages = 	 {383--413},
  McMaster = 	 {2nd floor QA 9 .A1 J68}
}

@Article{Metivier-Sopena-1997,
  author = {Yves M{\'e}tivier and Eric Sopena},
  title = {Graph Relabelling Systems: A General Overview},
  journal = {Computers and Artificial Intelligence},
  year = 1997,
  volume = 16,
  number = 2,
  OPTmonth = {},
  pages = {167--185},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {Z},
  abstract = {Graph relabelling systems have been introduced as a suitable
      model for expressing and studying distributed algorithms on a network
      of communicating processors. We recall the basic ideas underlying
      that model and we survey the main questions that have been considered
      and the main results that have been obtained in that framework.},
  keywords = {Distributed algorithm, election, $k$-covering, recognition, local computations in graphs, graph relabelling system},
  OPTannote = {}
}

@InProceedings{Metzemakers-Miniussi-Sherman-Strandh-1994,
  author = {T. Metzemakers and A. Miniussi and D. Sherman and
		  R. Strandh},
  title = {Improving Arithmetic Performance Using Fine-Grain Unfolding},
  crossref = {PLILP1994},
  pages = {324--339},
  keywords = {box/unbox, equational programming language}
}

@InProceedings{Meyer-Wand-1985,
  WKloc = {A-0075},
  keywords = {Continuation, Lambda-Calculi, Semantics, Types},
  abstract = {This paper reports preliminary work on the semantics
		  of the continuation transform. Previous work on the
		  semantics of continuations has concentrated on
		  untyped lanbda-calculi and has used primarily the
		  mechanism of inclusive predicates. Such predicates
		  are easy to understand on atomic values, but they
		  become obscure on functional values. In tha case of
		  the typed lambda-calculus, we show that such
		  predicates can be replaced by retractions. The main
		  theorem states that the meaning of a closed term is
		  a retraction of the meaning of the corresponding
		  continuationized term.},
  title = {Continuation Semantics in Typed Lambda-Calculi},
  pages = {219--224},
  crossref = {LOP1985},
  author = {Meyer, A.R. and Wand M.}
}

@Book{MeyerB-1990,
  UniBwM = {INF400/V6381},
  contents = {1 Basic concepts
	2 Mathematical background
	3 Syntax
	4 Semantics: the main approaches
	4.1 A toy language
	4.2 Attribute (affix) grammars
	4.3 Translational semantics
	4.4 Operational sematics
	4.5 Denotational semantics
	4.6 Axiomatic semantics
	4.7 Bibliographical notes
	5 Lambda calculus
	6 Denotational semantics: fundamentals
	7 Denotational semantics: language features
	7.1 Conventions
	7.2 Records
	7.3 Arrays
	7.4 Pointers
	7.5 Input and output
	7.6 Block structure
	7.7 Routines
	7.8 A peek at classes and inheritance
	7.9 Bibliographical notes
	8 The mathematics of recursion
	9 Axiomatic semantics
	10 The consistency of semantic definitions
	10.1 Comparing the two approaches
	10.2 Interpreting assertions
	10.3 Predicate semantics
	10.4 The consistency requirement
	10.5 The consistency proofs
	10.6 Discussion
	10.7 Bibliographical notes
	Bibliography
	Index},
  year = 1990,
  title = {Introduction to the Theory of Programming Languages},
  publisher = {Prentice Hall},
  author = {Bertrand Meyer}
}

@Article{MeyerJJCh-1986,
  year = 1986,
  volume = 45,
  title = {Merging Regular Processes by Means of Fixed-Point Theory},
  pages = {193-260},
  journal = {Theoretical Computer Science},
  author = {J.-J. Ch. Meyer}
}

@Article{Michael-1974,
  author = {Emily Michael},
  title = {{Peirce's} Early Study of the Logic of Relations, 1865--1867},
  journal = PEIRCE,
  volume = 10,
  year = 1974,
  pages = {63--75},
  bibliographies = {RelMiCS}
}

@Article{Michael-1979,
  author = {Emily Michael},
  title = {A Note on {Peirce} on {Boole's} Algebra of Logic},
  journal = NOTRE,
  volume = 20,
  year = 1979,
  pages = {636--638},
  bibliographies = {RelMiCS}
}

@Article{Michael-1979a,
  author = {Emily Michael},
  title = {An Examination of the Influence of {Boole's} Algebra on
		{Peirce's} Development in Logic},
  journal = NOTRE,
  volume = 20,
  year = 1979,
  pages = {801--806},
  bibliographies = {RelMiCS}
}

@TechReport{MichaelNG-Appel-2000,
  author = {Michael, Neophytos G. and Appel, Andrew W.},
  title = {Machine Instruction Syntax and Semantics in Higher Order Logic},
  institution =  {Princeton University, Department of Computer Science},
  year = 	 2000,
  URL = 	 {http://ncstrl.cs.princeton.edu/expand.php3?id=TR-619-00},
  number =	 {TR-619-00},
  pages = 	 18,
  month =	 APR,
  note =	 {A version of this paper is to appear at the 17th International Conference on Automated Deduction to be held between June 17-20, 2000 in Pittsburgh, Pennsylvania.},
  WKloc = {doc/pap/BIB},
  bibliographies = {OPG},
  abstract = {Proof-carrying code and other applications in
     computer security require machine-checkable proofs of properties
     of machine-language programs. These in turn require axioms about
     the opcode/operand encoding of machine instructions and the
     semantics of the encoded instructions. We show how to specify
     instruction encodings and semantics in higher-order logic, in a
     way that preserves the factoring of similar instructions in real
     machine architectures. We show how to automatically generate
     proofs of instruction decodings, global invariants from local
     invariants, Floyd-Hoare rules and predicate transformers, all
     from the specification of the instruction semantics. Our work is
     implemented in ML and Twelf, and all the theorems are checked in
     Twelf.}
}

@Book{Michaelson-1988,
  UniBwM = {INF400/S6274},
  year = 1988,
  WKloc = {A-0118 (nur Einleitung und Bibliographie)},
  title = {An Introduction to Functional Programming through Lambda Calculus},
  series = {International Computer Science Series},
  publisher = {Addison-Wesley},
  author = {Greg Michaelson}
}

@Book{Michalewicz-Fogel-2000,
  author = {Zbigniew Michalewicz and David B. Fogel},
  title = {How to Solve It: Modern Heuristics},
  publisher = Springer,
  year = 2000,
  ISBN = {3-540-66061-5},
  UniBwM = {MAT006/YG5830}
}

@Book{Michalski-,
  author = {Martin Michalski},
  title = {Das grosse Buch vom Zaubern},
  publisher = {Moewig},
  year = {???},
  ISBN = {3-8118-1024-3},
  note = {25.-DM},
  bibliographies = {Cynthia}
}

@Article{Michelbrink-2006,
  author = 	 {Markus Michelbrink},
  title = 	 {Interfaces as functors, programs as coalgebras ---
                  A final coalgebra theorem in intensional type theory},
  journal = 	 TCS,
  year = 	 {2006},
  OPTkey = 	 {},
  OPTvolume = 	 {360},
  OPTnumber = 	 {1--3},
  OPTpages = 	 {415--439},
  OPTmonth = 	 AUG,
  DOI = 	 {http://dx.doi.org/10.1016/j.tcs.2006.05.033},
  WKloc = {doc/pap/BIB},
  abstract = 	 {In [P. Hancock, A. Setzer, Interactive programs in dependent type theory, in: P. Clote, H. Schwichtenberg (Eds.), Proc. 14th Annu. Conf. of EACSL, CSL'00, Fischbau, Germany, 21--26 August 2000, Vol. 1862, Springer, Berlin, 2000, pp. 317--331, \url{citeseer.ist.psu.edu/article/hancock00interactive.html}; P. Hancock, A. Setzer, Interactive programs and weakly final coalgebras in dependent type theory, in: L. Crosilla, P. Schuster (Eds.), From Sets and Types to Topology and Analysis. Towards Practicable Foundations for Constructive Mathematics, Oxford Logic Guides, Clarendon Press, 2005, \url{www.cs.swan.ac.uk/~csetzer/}] Hancock and Setzer introduced rules to extend Martin-Löf's type theory in order to represent interactive programming. The rules essentially reflect the existence of weakly final coalgebras for a general form of polynomial functor. The standard rules of dependent type theory allow the definition of inductive types, which correspond to initial algebras. Coalgebraic types are not represented in a direct way. In this article we show the existence of final coalgebras in intensional type theory for these kind of functors, where we require uniqueness of identity proofs $...$ for the set of states $...$ and the set of commands $...$ which determine the functor. We obtain the result by identifying programs which have essentially the same behaviour, viz. are bisimular. This proves the rules of Setzer and Hancock admissible in ordinary type theory, if we replace definitional equality by bisimulation. All proofs [M. Michelbrink, Verifications of final coalgebra theorem in: Interfaces as Functors, Programs as Coalgebras --- A Final Coalgebra Theorem in Intensional Type Theory, 2005, \url{www.cs.swan.ac.uk/~csmichel/}] are verified in the theorem prover agda [C. Coquand, Agda, Internet, \url{www.cs.chalmers.se/~catarina/agda/}; K. Peterson, A programming system for type theory, Technical Report, S-412 96, Chalmers University of Technology, Göteborg, 1982], which is based on intensional Martin-Löf type theory.}
}

@InProceedings{Middeldorp-1997,
  author = {Aart Middeldorp},
  title = {Call by Need Computations to Root-Stable Form},
  URL = {http://www.acm.org:80/pubs/citations/proceedings/plan/263699/p94-middeldorp/},
  pages = {94--105},
  crossref = {POPL1997}
}

@Article{Middleton-1977,
  author = {A. G. Middleton},
  title = {A case for type and form flow analysis},
  journal = {The Computer Journal},
  volume = 20,
  number = 3,
  pages = {238--241},
  month = aug,
  year = 1977,
  coden = {CMPJA6},
  ISSN = {0010-4620},
  bibdate = {Tue Mar 25 13:51:56 MST 1997},
  acknowledgement = ack-nhfb,
  classcodes = {C4240 (Programming and algorithm theory)},
  classification = 723,
  corpsource = {Dept. of Computer Sci., Univ. Coll. of Swansea,
                 Swansea, UK},
  keywords = {computer programming; errors; flow analysis; form;
                 languages; programming theory; type},
  treatment = {T Theoretical or Mathematical}
}

@InProceedings{Migliorato-1988,
  author = {Renato Migliorato},
  title = {Isomorphisms of Finite Hypergroupoids},
  pages = {301--310},
  crossref = {IGCS1988},
  bibliographies = {RelMiCS}
}

@Article{Mikulas,
  title = {Taming First-Order Logic},
  author = {Szabolcs  Mikul\'as},
  pages = {305--316},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {In this paper we define computationally well-behaved versions of classical first-order logic and prove that the validity problem is decidable.},
  bibliographies = {RelMiCS}
}

@TechReport{Mikulas-1992,
  author = {Mikul\'{a}s, Szabolcs},
  title = {The completeness of the {Lambek} calculus
                  with respect to relational semantics},
  institution = {Inst.\null{} for Language, Logic and Information},
  year = 1992,
  type = {ITLI Prepublications},
  OPTnumber = {???},
  address = {Amsterdam},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@PhDThesis{Mikulas-1995,
  author = {Szabolcs Mikul\'as},
  school = {ILLC Diss.\null{} Series 1995--12},
  title = {Taming Logics},
  year = 1995,
  bibliographies = {RelMiCS}
}

@Article{Mikulas-Sain-Simon-1992,
  author = {Mikul\'{a}s, Szabolcs and Ildik{\'o} Sain and Andr\'as Simon},
  title = {Complexity of the Equational Theory of Relational
		  Algebras with Projection Elements},
  journal = Bulletin_of_the_Section_of_Logic_University_of_Lodz,
  year = 1992,
  volume = 21,
  number = 3,
  pages = {103--111},
  bibliographies = {RelMiCS}
}

@PhdThesis{Milanese-2003,
  author = 	 {Ulf Milanese},
  title = 	 {{Zur Implementierung eines ROBDD-basierten Systems f\"ur die Manipulation und Visualisierung von Relationen}},
  school = 	 {Institut f\"ur Informatik und Praktische Mathematik, Christian-Albrechts-Universit\"at Kiel},
  year = 	 2003,
  bibliographies = {RelMiCS}
}

@Misc{Milanese-2004KURE,
  author =	 {Ulf Milanese},
  title =	 {{KURE: Kiel University Relation Package, Release 1.0}},
  howpublished = {\textsf{http://www.informatik.uni-kiel.de/\~{}progsys/relview/kure}},
  year =	 2004,
  bibliographies = {RelMiCS}
}

@Article{Mili-1983,
  author = {Ali Mili},
  title = {A Relational Approach to the Design of Deterministic
                  Programs},
  journal = ACTIN,
  year = 1983,
  volume = 20,
  pages = {315--328},
  bibliographies = {RelMiCS}
}

@Article{Mili-1985,
  author = {Ali Mili},
  title = {Towards a Theory of Forward Error Recovery},
  journal = IEEETSE,
  volume = 11,
  year = 1985,
  pages = {735--748},
  bibliographies = {RelMiCS}
}

@Misc{Mili-1998,
  author = {Ali Mili},
  title = {The Long Story of a Short Theorem},
  year = 1998,
  month = OCT,
  WKloc = {A-0768},
  bibliographies = {RelMiCS}
}

@InProceedings{Mili-Desharnais-1984,
  author = {Ali Mili and Jules Desharnais},
  title = {A System for Classifying Program Verification
                  Methods: Assigning Meanings to Program Verification
                  Methods},
  booktitle = {{Proc.\null{} $7^{th}$ Internat.\null{} Conf.\null{}
                  on Software Engineering (ICSE 7), Orlando, FL}},
  year = 1984,
  pages = {499--509},
  month = MAR,
  bibliographies = {RelMiCS}
}

@InProceedings{Mili-Desharnais-1984a,
  author = {Ali Mili and Jules Desharnais},
  title = {Toward the Automatic Symbolic Execution of While
                  Statements},
  booktitle = {{Proc.\null{} $17^{th}$ Hawaii Internat.\null{}
                  Conf.\null{} on System Sciences, Honolulu, HI}},
  year = 1984,
  pages = {378--382},
  month = JAN,
  bibliographies = {RelMiCS}
}

@TechReport{Mili-Desharnais-Frappier-Mili-1996,
  author = {R. Mili and Jules Desharnais and Marc Frappier and Ali Mili},
  title = {Measures of Syntactic and Semantic Distance between Specifications},
  institution = {University of Texas at Dallas},
  year = 1996,
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = AUG,
  bibliographies = {RelMiCS},
  annote = {Original reference for \emph{specification difference},
      which is almost the same concept as the demonic implication in Kahl-2001a.
      Used in \cite{LabedJilani-Desharnais-Mili-2001}.
      Superseded by \cite{Mili-Desharnais-Frappier-Mili-2000}}
}

@Article{Mili-Desharnais-Frappier-Mili-2000,
  author = {Rym Mili and Jules Desharnais and Marc Frappier and Ali Mili},
  title = {Semantic Distance between Specifications},
  journal = TCS,
  year = 2000,
  volume = 247,
  number = {1--2},
  pages = {257--276},
  WKloc = {A-1055},
  bibliographies = {RelMiCS},
  annote = {Reference for \emph{specification difference},
      which is almost the same concept as the demonic implication in \cite{Kahl-2001a}.
      See also \cite{LabedJilani-Desharnais-Mili-2001}.}
}

@Article{Mili-Desharnais-Gagne-1985,
  author = {Ali Mili and Jules Desharnais and Jean-Raymond
                  Gagn\'{e}},
  title = {Strongest Invariant Functions: Their Use in the
                  Systematic Analysis of While Statements},
  journal = ACTIN,
  year = 1985,
  volume = 22,
  number = 1,
  pages = {47--66},
  month = APR,
  bibliographies = {RelMiCS}
}

@Article{Mili-Desharnais-Gagne-1986,
  author = {Ali Mili and Jules Desharnais and Jean-Raymond Gagn{\'e}},
  title = {Formal Models of Stepwise Refinement of Programs},
  journal = ACMCS,
  year = 1986,
  volume = 18,
  number = 3,
  pages = {231--276},
  month = SEP,
  bibliographies = {RelMiCS, RelMiS},
  URL = {http://doi.acm.org/10.1145/69610.357990},
  WKloc = {A-1291, doc/pap/BIB},
  abstract = {Of the many ways to express program specifications, three of
      the most common are: as a pair of assertions, an input assertion and
      an output assertion; as a function mapping legal inputs to correct
      outputs; or as a relation containing the input/output pairs that are
      considered correct. The construction of programs consists of mapping
      a potentially complex specification into a program by recursively
      decomposing complex specifications into simpler ones. We show how
      this decomposition takes place in all three modes of specification
      and draw some conclusions on the nature of programming.}
}

@Article{Mili-Desharnais-Gagne-1988,
  author = {Ali Mili and Jules Desharnais and Jean-Raymond Gagn\'e},
  title = {Formal Models of Stepwise Refinement of Programs},
  journal = BIT,
  year = 1988,
  pages = {79--108},
  month = MAY,
  note = {Japanese version of \cite{MiliDesharnaisGagne1986}},
  bibliographies = {RelMiCS}
}

@Article{Mili-Desharnais-Mili-1987,
  author = {Ali Mili and Jules Desharnais and Fatma Mili},
  title = {Relational Heuristics for the Design of
                  Deterministic Programs},
  journal = ACTIN,
  volume = 24,
  number = 3,
  year = 1987,
  pages = {239--276},
  bibliographies = {RelMiCS}
}

@Article{Mili-Mili-1992,
  author = {Fatma Mili and Ali Mili},
  title = {Heuristics for Constructing While Loops},
  journal = SCICOP,
  year = 1992,
  volume = 18,
  pages = {67--106},
  bibliographies = {RelMiCS}
}

@Article{Milius-2003,
  author =       {Stefan Milius},
  title =        {On Colimits in Categories of Relations},
  journal =      {Applied Categorical Structures},
  year =         2003,
  volume =    11,
  number =    3,
  pages =     {287--312},
  URL =     {http://www.iti.cs.tu-bs.de/~milius/research/colimits.ps},
  WKloc =      {doc/pap/BIB},
  bibliographies =    {RelMiCS}
}

@Article{Miller-1991,
  author = {Dale Miller},
  title = {A Logic Programming Language with Lambda
		  Abstraction, Function Variables, and Simple Unification},
  journal = {Journal of Logic and Computation},
  year = 1991,
  volume = 1,
  number = 4,
  OPTmonth = {},
  pages = {497--536},
  OPTnote = {},
  UniBwM = {INF/Z},
  OPTwkloc = {},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Miller-1991a,
  author = {Dale Miller},
  title = 	 {Uniform Proofs as a Foundation for Logic Programming},
  journal = 	 ANPURE,
  year = 	 1991,
  volume = 	 51,
  pages = 	 {125--127},
  WKloc = 	 {A-1702},
  keywords = 	 {hereditary Harrop formulae}
}

@InProceedings{Miller-1992,
  author = {Dale Miller},
  title = {Abstract Syntax and Logic Programming},
  pages = {322--337},
  WKloc = {A-0097},
  authorsAddress = {dale\@cis.upenn.edu},
  crossref = {LPAR-1990-1991},
  abstract = {When writing programs to manipulate structures such
		  as algebraic expressions, logical formulas, proofs,
		  and programs, it is highly desirable to take the
		  linear, human-oriented, concrete syntax of these
		  structures and parse them into a more computation
		  oriented syntax. For a wide variety of
		  manipulations, concrete syntax contains too much
		  useless information ({\em e.g.}, keywords and white
		  space) while important information is not
		  explicitely represented ({\em e.g.},
		  function-argument relations and the scope of
		  operators). In parse trees, much of the semantically
		  useless information is removed while other
		  relationships, such as between function and
		  argument, are made more explicit. Unfortunately,
		  parse trees do not adequately address important
		  notions of object-level syntax, such as bound and
		  free object-variables, scopes, alphabetic changes of
		  bound variables, and object-level substitution. I
		  will argue here that the {\em abstract syntax} of
		  such objects should be organized around
		  $\alpha$-equivalence classes of $\lambda$-terms
		  instead of parse trees. Incorporating this notion of
		  abstract syntax into programming languages is an
		  interesting challenge. This paper briefly describes
		  a logic programming language that directly supports
		  this notion of syntax. An example specifications in
		  this programming language is presented  to
		  illustrate its approach to handling object-level
		  syntax. A model-theoretic semantics for this logic
		  programming language is also presented.},
  bibliographies = {RelMiCS}
}

@InProceedings{Miller-1993,
  author = {Dale Miller},
  title = {A Proposal for Modules in Lambda-Prolog},
  crossref = {ELP93},
  pages = {206--221},
  WKloc = {A-0300},
  URL = {ftp://ftp.cis.upenn.edu/pub/papers/miller/welp93.ps.Z},
  abstract = {Higher-order hereditary Harrop formulas, the
		  underlying logical foundation of $\lambda$Prolog
		  \cite{Nadathur-Miller-1988}, are more expressive than
		  first-order Horn clauses, the logical foundation of
		  Prolog. In Particular, various formas of scoping and
		  abstraction are supported by the logic of
		  higher-order hereditary Harrop formulas while they
		  are not supported by first-order Horn
		  clauses. Various papers have argued that the scoping
		  and abstraction available in this richer logic can
		  be used to provide for modular programming [Mil89b],
		  abstract datra types [Mil89a], and state
		  encapsulation [HM90]. None of these papers, however,
		  have dealt with the problems of {\em
		  programming-in-the-large}, that is, the essentially
		  linguistic problems of putting together various
		  different textual sources of code found, say, in
		  different files on a persistent store into one logic
		  program. In this paper, I propose a module system
		  for $\lambda$Prolog and shall focus mostly on its semantics.}
}

@InProceedings{Miller-1994a,
  author = {Dale Miller},
  title = {Specifications Using Multiple-Conclusion Logic Programs},
  crossref = {ALP1994},
  pages = {3--4},
  note = {invited lecture},
  abstract = {Multiset rewriting has proved to be a usefule
		  presentation of process sunchronization
		  [1,2,3,6]. Since sequent calculus presentations of
		  logics that do not use the structural rules of
		  contractions and weakening are based on using
		  multisets of formulas as left and right contexts, it
		  is natural to identify processes with formulas,
		  multisets with sequent contexts, and multiset
		  rewriting as an inference rule. Given earlier work
		  on using sequent calculus to describe logic
		  programming as goal-directed search for proofs [8],
		  it is most natural to use right-hand contexts of
		  sequents to represent multisets of processes. This
		  choice requires the identification of the multiset
		  constructor and the empty multiset with the
		  multiplicative disjunction and false (the
		  $\backamb$ and $\perp$ of linear logic [4]), and
		  backchaining with a single step of multiset
		  rewriting. While the logic programming language
		  $\lambda$Prolog [10] and its linear logic refinement
		  Lolli \cite{Hodas-Miller-1994} contain rich sources of
		  abstraction (such as modular programming, abstract
		  data types, and higher-order programming), they
		  contain no primitives for specifying concurrency,
		  communications, or synchronization. If multiset
		  rewriting is added to Lolli via the logical
		  connectives $\backamp$ and $\perp$, the result is a
		  language that contains primitives for both
		  abstraction and concurrency. Surprisingly, the
		  resulting logic, called Forum \cite{Miller-1994b}, is
		  a presentation of all of linear logic in the sense
		  that all of the connectives of linear logic can be
		  defined via logical equivalences using only the
		  connectives of Forum. Thus the rich meta-theory of
		  linear logic, for example, the de Morgan dualities
		  and cut-elimination, can be applied to the analysis
		  of Forum programs. Several examples to illustrate
		  the expressiveness of this presentation of linear
		  logic will be given. These examples will involve a
		  specification of sequent calculi for object-level
		  logics, a specification of the $\pi$-calculus [9],
		  and a specification of a functional programming
		  language that contains side-effects and concurrency
		  operators. In each of these examples, we shall argue
		  that the specification is perspicuous and modular
		  and that the meta-theory of linear logic can be used
		  to derive properties of the specification.}
}

@InProceedings{Miller-1994b,
  title = {A Multiple-Conclusion Meta-Logic},
  author = {Dale Miller},
  pages = {272--281},
  crossref = {LICS9},
  WKloc = {A-0301},
  keywords = {Forum},
  URL = {ftp://ftp.cis.upenn.edu/pub/papers/miller/lics94.dvi.Z},
  annote = {see \cite{Miller-1994a}},
  abstract = {The theory of cut-free sequent proofs has been used to
		  motivate and justify the design of a number of logic
		  programming languages.  Two such languages,
		  $\lambda$Prolog and its linear logic refinement,
		  Lolli, provide for various forms of abstraction
		  (modules, abstract data types, higher-order
		  programming) but lack primitives for concurrency.
		  The logic programming language, LO (Linear Objects)
		  provides for concurrency but lacks abstraction
		  mechanisms.  In this paper we present Forum, a logic
		  programming presentation of all of linear logic that
		  modularly extends the languages $\lambda$Prolog,
		  Lolli, and LO.  Forum, therefore, allows
		  specifications to incorporate both abstractions and
		  concurrency.  As a meta-language, Forum greatly
		  extends the expressiveness of these other logic
		  programming languages. To illustrate its expressive
		  strength, we specify in Forum a sequent calculus
		  proof system and the operational semantics of a
		  functional programming language that incorporates
		  such non-functional features as counters and references.}
}

@Misc{Miller-1995a,
  OPTkey = {},
  OPTauthor = {Dale Miller},
  OPTtitle = {Logic Programming and Meta-Logic},
  OPThowpublished = {International Summer School Marktoberdorf,
		  Working Material},
  OPTyear = 1995,
  OPTmonth = JUL,
  OPTnote = {},
  OPTwkloc = {A-0413},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Unpublished{Miller-1996,
  author = {Dale Miller},
  title = {$\lambda$Prolog: An Introduction to the Language and its Logic},
  note = {Draft},
  month = JAN,
  year = 1996,
  keywords = {lambda-Prolog},
  WKloc = {B-0119}
}

@Article{Miller-1996a,
 author = {Dale Miller},
 title = {Forum: a multiple-conclusion specification logic},
 journal = TCS,
 volume = {165},
 number = {1},
 year = {1996},
 issn = {0304-3975},
 pages = {201--232},
 doi = {http://dx.doi.org/10.1016/0304-3975(96)00045-X},
 publisher = {Elsevier Science Publishers Ltd.},
 address = {Essex, UK},
 WKloc = {doc/pap/BIB},
 abstract = {The theory of cut-free sequent proofs has been used to
   motivate and justify the design of a number of logic programming
   languages. Two such languages, $\lambda$Prolog and its linear logic
   refinement, Lolli [15], provide for various forms of abstraction
   (modules, abstract data types, and higher-order programming) but
   lack primitives for concurrency. The logic programming language, LO
   (Linear Objects) [2] provides some primitives for concurrency but
   lacks abstraction mechanisms. In this paper we present Forum, a
   logic programming presentation of all of linear logic that
   modularly extends $\lambda$Prolog, Lolli, and LO. Forum, therefore,
   allows specifications to incorporate both abstractions and
   concurrency. To illustrate the new expressive strengths of Forum,
   we specify in it a sequent calculus proof system and the
   operational semantics of a programming language that incorporates
   references and concurrency. We also show that the meta theory of
   linear logic can be used to prove properties of the
   object-languages specified in Forum.}
}

@Misc{MillerP-1997,
  author = {Peter Miller},
  title = {Recursive Make Considered Harmful},
  year = 1997,
  WKloc = {A-0885}
}

@Article{MillerRJ-Gujarathi-1999,
  author = 	 {Renee J. Miller and A. Gujarathi},
  title = 	 {Mining for Program Structure},
  journal = 	 {International Journal on Software Engineering and
Knowledge Engineering},
  year = 	 1999,
  volume =	 9,
  number =	 5,
  pages =	 {499--517},
  bibliographies = {OPG}
}

@Article{Mills-1975,
  author = {H. D. Mills},
  title = {The New Math of Computer Programming},
  journal = CACM,
  volume = 18,
  number = 1,
  month = jan,
  year = 1975,
  pages = {43--48},
  URL = {http://doi.acm.org/10.1145/360569.360659},
  WKloc = {A-1289, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Structured programming has proved to be an important
      methodology for systematic program design and development. Structured
      programs are identified as compound function expressions in the
      algebra of functions. The algebraic properties of these function
      expressions permit the reformulation (expansion as well as reduction)
      of a nested subexpression independently of its environment, thus
      modeling what is known as stepwise program refinement as well as
      program execution. Finally, structured programming is characterized
      in terms of the selection and solution of certain elementary
      equations defined in the algebra of functions. These solutions can be
      given in general formulas, each involving a single parameter, which
      display the entire freedom available in creating correct structured
      programs.}
}

@Book{Mills-Basili-Gannon-Hamlet-1987,
  author = {H. D. Mills and V. R. Basili and J. D. Gannon and R. G. Hamlet},
  title = {Principles of Computer Programming. A Mathematical Approach},
  publisher = Allyn,
  year = 1987,
  bibliographies = {RelMiCS}
}

@Book{Mills-Linger-Hevner-1986,
  author = {H. D. Mills and R. C. Linger and A. R. Hevner},
  title = {Principles of Information Systems Analysis and Design},
  publisher = {Academic Press Inc.},
  year = 1986,
  address = {San Diego, California},
  annote = {Describes an approach to software specification
                  that uses black boxes (stimulus history and
                  response functions) instead of state machine models.}
}

@InProceedings{Milner-1985,
  author = {Robin Milner},
  title = {Lectures on a Calculus for Communicating Systems},
  pages = {205--228},
  crossref = {Marktoberdorf-1985}
}

@Misc{Milner-1991,
  author = {Robin Milner},
  title = {The Polyadic $\pi$-Calculus: A Tutorial},
  year = 1991,
  WKloc = {A-0638}
}

@InProceedings{Milner-1993,
  author = {Robin Milner},
  title = {Higher-order Action Calculi},
  crossref = {CSL93},
  pages = {238--260},
  WKloc = {A-0345},
  abstract = {$\ldots$}
}

@InProceedings{Milner-1994,
  author = {Robin Milner},
  title = {Pi-nets: A Graphical Form of the $\pi$-Calculus},
  crossref = {ESOP1994},
  pages = {26--42}
}

@TechReport{Milner-2001,
  author = {Robin Milner},
  title = {Bigraphical Reactive Systems: Basic Theory},
  institution = {Cambridge University},
  year = 2001,
  bibliographies = {RelMiCS}
}

@InProceedings{Milner-2002,
  author =       {Robin Milner},
  title =        {Bigraphs as a Model for Mobile Interaction},
  crossref =  {ICGT2002},
  DOI =       {10.1007/3-540-45832-8_3},
  pages =     {8--13},
  abstract =    {A bigraphical reactive system (BRS) involves bigraphs,
     in which the nesting of nodes represents locality,
     independently of the edges connecting them.
     BRSs represent a wide variety of calculi for mobility,
     including the $\pi$-calculus.
     This short essay explains how bigraphs compose,
     and uses the $\pi$-calculus to illustrate
     how they already provide elements of a unifying theory
     for calculi of mobile interactive processes.}
}

@Book{Milner-Tofte-1991,
  UniBwM = {INF400/V12697},
  contents = {1 Executing a simple program
	1.1 Execution; 1.2 Elaboration; 1.3 Evaluation
	2 Dynamic Semantics for the Core
	2.1 Semantic objects; 2.2 The initial dynamic basis;
	2.3 Evaluation order; 2.4 Function application; 2.5 Pattern matching
	2.6 Exceptions; 2.7 Constructors versus variables
	2.8 Evaluation theorems
	3 Dynamic Semantics for the Modules
	3.1 Structures and signatures; 3.2 Structures only;
	3.3 Signatures and interfaces; 3.4 Functors; 3.5 Alternative semantics
	4 Static Semantics for the Core
	4.1 Contexts, environments and scope; 4.2 Types and type schemes
	4.3 Closure of variable environments; 4.4 Explicit type variables
	4.5 Polymorphic references and exceptions
	5 Type Declarations and Principality
	5.1 Types, datatypes and type functions; 5.2 Equality
	5.3 Principal types and environments
	6 Static Semantics for the modules
	6.1 Structures; 6.2 Signatures; 6.3 Sharing;
	6.4 Coercive signature constraints; 6.5 Principal Signatures
	6.6 Summary
	7 Signature Matching
	7.1 Matching; 7.2 Realisation; 7.3 Instantiation; 7.4 Enrichment
	7.5 Discussion of matching; 7.6 Equality type specifications
	7.7 Type explication
	8 Elaboration of Functors
	8.1 Discussion; 8.2 Functor declaration; 8.3 Functor application
	8.4 Variations and extensions; 8.5 Higher-order functors
	9 Admissible Semantic Objects and Proofs
	9.1 Consistency; 9.2 Well-formed signatures; 9.3 Cycle-freedom
	9.4 Admissibility
	10 Elaboration of Signature Expressions
	10.1 The basis; 10.2 The rules; 10.3 The realisation theorem
	10.4 Admissification; 10.5 Checking admissibility
	11 Principal Signatures
	11.1 Bare principality; 11.2 Defective signatures
	11.3 Covering and principality; 11.4 Equality-principal signatures
	A Proof of Principality
	A.1 Structural contractions; A.2 The principality theorem
	A.3 Principal signatures
	B Identifier Status
	C Solutions to Exercises
	D Mistakes and Ambiguities
	Index},
  year = 1991,
  title = {Commentary on {Standard} {ML}},
  publisher = {MIT Press},
  author = {Robin Milner and Mads Tofte},
  McMaster = 	 {QA 76.73 .M6M548 1991}
}

@Book{Milner-Tofte-Harper-MacQueen-1997,
  ALTauthor = 	 {Robin Milner and Mads Tofte and Robert Harper and David MacQueen},
  ALTeditor = 	 {},
  title = 	 {The Definition of {Standard} {ML} (Revised)},
  publisher = 	 {MIT Press},
  year = 	 {1997},
  McMaster = 	 {QA 76.73 .M6D44 1997},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Milo-Suciu-1999,
  author = {Tova Milo and Dan Suciu},
  title = {Type Inference for Queries on Semistructured Data},
  editor = {{ACM}},
  booktitle = {Proceedings of the Eighteenth {ACM}
                 {SIGMOD-SIGACT-SIGART} Symposium on Principles of
                 Database Systems: {PODS} 1999: Philadelphia,
                 Pennsylvania, May 31--June 2, 1999},
  publisher = {ACM Press},
  address = {New York, NY 10036, USA},
  year = 1999,
  ISBN = {1-58113-062-7},
  pages = {215--226},
  year = 1999,
  bibdate = {Wed Oct 25 08:47:40 MDT 2000},
  URL = {http://www.acm.org/pubs/articles/proceedings/pods/303976/p215-milo/p215-milo.pdf;
                 http://www.acm.org/pubs/citations/proceedings/pods/303976/p215-milo/},
  acknowledgement = ack-nhfb,
  WKloc = {A-1230, doc/pap/BIB}
}

@Article{Milojicic-Douglis-Paindaveine-Wheeler-Zhou-2000,
  author = {Dejan S. Milo{\'\j}i{\caron{c}}i{\'c} and Fred Douglis and
            Yves Paindaveine and Richard Wheeler and Songnian Zhou},
  title = {Process migration},
  journal = ACMCS,
  year = 2000,
  volume = 32,
  number = 3,
  pages = {241--299},
  month = SEP,
  WKloc = {A-1238, doc/pap/BIB},
  abstract = {Process migration is the act of transferring a process
      between two machines. It enables dynamic load distribution, fault
      resilience, eased system administration, and data access locality.
      Despite these goals and ongoing research efforts, migration has not
      achieved widespread use. With the increasing deployment of
      distributed systems in general, and distributed operating systems in
      particular, process migration is again receiving more attention in
      both research and product development. As high-performance facilities
      shift from supercomputers to networks of workstations, and with the
      ever-increasing role of the World Wide Web, we expect migration to
      play a more important role and eventually to be widely adopted.This
      survey reviews the field of process migration by summarizing the key
      concepts and giving an overview of the most important
      implementations. Design and implementation issues of process
      migration are analyzed in general, and then revisited for each of the
      case studies described: MOSIX, Sprite, Mach, and Load Sharing
      Facility. The benefits and drawbacks of process migration depend on
      the details of implementation and, therefore, this paper focuses on
      practical matters. This survey will help in understanding the
      potentials of process migration and why it has not caught on.}
}

@Book{Milojicic-Douglis-Wheeler-2001,
  author = {Dejan S. Milojicic and Frederick Douglis and Richard G. Wheeler},
  title = {Mobility: Processes, Computers, and Agents},
  publisher = {Addison-Wesley},
  year = 1999,
  ISBN = 0201379287,
  month = JUN,
  URL = {http://www.amazon.com/exec/obidos/ASIN/0201379287/},
  abstract = {``This book grew out of a survey paper on process migration.
      After spending considerable time and effort collecting and sorting
      several hundred references for that work, we realized that we had
      almost enough material for a book. Since then, the scope of the book
      expanded to encompass broader issues in mobility. $\ldots$''},
  bibliographies = {ProcMig}
}

@InProceedings{Min-Rada-1993,
  author = {Zheng Min and Roy Rada},
  title = {SHyD-a Model for bridging Text and Hypermedia},
  abstract = {This paper persents a model of the document-hypermedia world
             which shows the bi-directional exchanges between the two worlds
             which are facilitated by international standards and a widely
             accepted hypertext reference model. In particular we show the
             role of the Dexter Model and the SGML/HyTime standards in
             supporting document-hypermedia transformations and hypermedia
             interchange. A first prototype -- the SGML_MUCH(SM) system --
             has been developed which follows the principles of the model.
             The SM system can accept different text and hypertext markup
             documents by going through a standard SGML representation,
             which can represent both text and hypermedia structure, and the
             SM system can then import these documents into a collaborative
             hypermedia system -- the MUCH system. On the other hand, a part
             of the MUCH database can be exported into a markup document
             which retains all the hypernedia information. This exported
             document then can be processed by a conventional document
             editor or used to interchange information with other hypermedia
             system.},
  crossref = {ACM1993}
}

@InProceedings{Minas-1999,
  author = {Mark Minas},
  title = {Creating Semantic Representations of Diagrams},
  booktitle = {Proc. of the Int'l Workshop on Applications of Graph
     Transformation with Industrial Relevance {(AGTIVE'99) at Monastery Rolduc, NL, September 1-3, 1999}},
  year = 2000,
  publisher = {Springer Verlag},
  series = {LNCS},
  OPTvolume = {},
  OPTpages = {},
  OPTabstract = {},
  URL = {http://www2.informatik.uni-erlangen.de:80/IMMD-II/Research/Activities/DiaGen/publ.html},
  WKloc = {A-0906}
}

@InProceedings{Minas-2000,
  author = {Mark Minas},
  title = {Hypergraphs as a Uniform Diagram Representation Model},
  booktitle = {Proc. 6th International Workshop on Theory and
     Application of Graph Transformations {(TAGT'98)} at
     {Paderborn, Germany, Nov. 1998}},
  year = 2000,
  publisher = {Springer Verlag},
  series = {LNCS},
  OPTvolume = {},
  OPTpages = {},
  OPTabstract = {},
  URL = {http://www2.informatik.uni-erlangen.de:80/IMMD-II/Research/Activities/DiaGen/publ.html},
  WKloc = {A-0907}
}

@InProceedings{Minas-2002,
  author = 	 {Mark Minas},
  title = 	 {Specifying graph-like diagrams with {DiaGen}},
  booktitle = {Proc. International Workshop on Graph-Based Tools (GraBaTs'02)},
  pages = 	 {16-25},
  year = 	 2002,
  volume = 	 72,
  number = 	 2,
  series = 	 {ENTCS},
  OPTURL = 	 {http://www2.cs.unibw.de/publ/minas/GraBaTs02.pdf}
}

@InProceedings{Minas-Koeth-1999,
  author = {Mark Minas and O. K{\"o}th},
  title = {Generating Diagram Editors with {DiaGen}},
  crossref = {AGTIVE1999},
  pages = {443--440},
  OPTabstract = {},
  OPTURL = {http://www2.cs.unibw.de/publ/minas/AGTIVE99-demo.pdf},
  WKloc = {A-0908}
}

@InProceedings{Minkwitz-1995,
  author = {Torsten Minkwitz},
  title = {Algorithms Explained by Symmetries},
  crossref = {STACS1995},
  pages = {157--180},
  authorsAddress = {Institut f\"ur Algorithmen und Kognitive
		  Systeme, U. Karlsruhe, minkwitz\@ira.uka.de},
  WKloc = {A-0409},
  abstract = {Many of the linear transforms that are used in
		  digital signal processing and other areas have a lot
		  of symmetry properties. The usual quadratic cost of
		  multiplying with the matrix is reduced, in some
		  cases to an almost linear complexity. Salient
		  examples are the Fourier transform, the Cosine
		  transform, linear maps with Toeplitz matrices or
		  convolutions. The article gives an exact definition
		  of the notion of symmetry that leads to fast
		  algorithms and presents a method to construct these
		  algorithms automatically in the case of an exiting
		  symmetry with a soluble group. The results may serve
		  to speed up the multiplication with a transform
		  matrix and also to solve a linear system of
		  equations with system. Even though the construction
		  is done at the level of abstract algebra, the
		  derived algorithms for many linear transforms
		  compare well with the best found in the literature
		  [CoTu65, ElRa82, Ra68, RaYi90]. In most cases, where
		  the new method was applicable, even the manually
		  optimized algorithms [Nu81] were not better, while
		  nothing more than the transform matrix and its
		  symmetry were provided here to obtain the results.}
}

@InCollection{Minsky-1956,
  author = 	 {M. L. Minsky},
  title = 	 {Some Universal Elements for Finite Automata},
  crossref =	 {Shannon-McCarthy-1956},
  pages =	 {117--128},
  WKloc = {A-1559}
}

@Article{Miraglia-Solitro-1998,
  author = 	 {Francisco Miraglia and Ugo Solitro},
  title = 	 {Sheaves over Right Sided Idempotent Quantales Keywords: },
  journal = 	 JIGPL,
  year = 	 {???},
  volume = 	 {6},
  number = 	 {4},
  pages = 	 {545--600},
  URL = {http://www3.oup.co.uk/igpl/Volume_06/Issue_04/pdf/miraglia.pdf},
  WKloc = {doc/pap/BIB},
  keywords = {quantales, Q-sets, sheaf, characteristic maps, first-order quantifiers},
  bibliographies = {RelMiCS},
  abstract = {We present a discussion of sheaves and presheaves
     over a right sided idempotent quantale in a fashion that is
     similar to the way that these objects are conceived over complete
     Heyting algebras by Fourman and Scott in [5].}
}

@Book{Miranda-Busta-1986,
  author = {S. Miranda and J. M. Busta},
  title = {L'art des Bases de Donn{\'e}es, Tome 2: les Bases de
      Donn{\'e}es Relationnelles},
  publisher = {Eyrolles},
  year = 1986,
  bibliographies = {RelMiCS}
}

@Article{Mirkowska-1977,
  author = {Mirkowska, G.},
  title = {Algorithmic logic and its application in the theory of programs},
  journal = FUNDI,
  year = 1977,
  OPTkey = {},
  volume = 1,
  OPTnumber = {},
  OPTmonth = {},
  pages = {1--17, 147--165},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Misc{Mislove-199X,
  author = {Michael Mislove},
  title = {Generalizing Domain Theory},
  year = {199?},
  WKloc = {A-0635}
}

@Article{Mislove-Hofmann-1977,
  year = 1977,
  volume = 154,
  title = {The Lattice of Kernel Operators and Topological Algebra},
  pages = {175--188},
  journal = MATHZ,
  author = {M. W. Mislove and K. H. Hofmann},
  bibliographies = {RelMiCS}
}

@InProceedings{Mislove-Oles-1992,
  author = {Michael W. Mislove and Frank J. Oles},
  title = {Full Abstraction and Unnested Recursion},
  crossref = {REX92},
  pages = {384--397},
  WKloc = {A-0257},
  abstract = {We begin with the assumption that we are given a
		  basic programming language $L$ without identifiers
		  (i.e., variables), which is, nonetheless, fairly
		  expressive. We also assume $L$ has been provided
		  with both an operational semantics and a
		  denotational semantics. Furthermore, the
		  denotational semantics is adequate and fully
		  abstract with respect to the operational
		  semantics. After clarifying exactly what these
		  assumptions entail,
                  \begin{enumerate}
                  \item we discuss what it means to extend $L$
		  algebraically to a language $L[X]$ by the addition
		  of identifiers,
                  \item we discuss the semantics of $L[X]$ in the
		  context of possibly self-referential environments
		  (i.e., systems of recursive definitions of the
		  identifiers), and
                  \item we show that extremely mild topological
		  assumptions about the operational model ensure that
		  adequacy and full abstraction of the denotational
		  semantics of $L[X]$ with respect to its operational
		  semantics follow automatically from the
		  corresponding results for $L$.
                  \end{enumerate}
                  Essentially, we will assume that the operational
		  model is a Haussdorff space, and the operational
		  process of unwinding a recursive program converges
		  to its operational meaning. Some work has been done
		  on the issue we are confronting, but from the
		  standpointof fair unwindings of recursive
		  constructs. That work indicates that full
		  abstraction results are not generally to be expected
		  in all situations. Our goal, however, is to
		  establish that full abstraction rtesults are indeed
		  available in a very general setting.},
  keywords = {Full abstraction, adequacy, algebraic semantics,
		  homomorphism, algebric poset}
}

@InCollection{Misra-1994,
  author = {Jayadev Misra},
  title = {Powerlist: A structure for parallel recursion},
  crossref = {Roscoe-1994},
  pages = {295--316},
  chapter = 18,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Mitchell-1991,
  title = {On Abstraction and the Expressive Power of Programming Languages},
  author = {John C. Mitchell},
  pages = {290--310},
  crossref = {TACS1991},
  abstract = {We present a tentative theory of programming language
		  expressiveness based on reductions (language
		  translations) that preserve observational
		  equivalence.  These are called
		  ``abstraction-preserving'' because of a connection
		  with a definition of ``abstraction'' or
		  ``information-hiding'' mechanism.  If there is an
		  abstraction-preserving reduction from one language
		  to another, then essentially every function on
		  natural numbers that is definable in the first is
		  also definable in the second.  Moreover, regardless
		  of the set of first-order functions definable in
		  either language, no programming language with an
		  abstraction mechanism can be reduced to a language
		  without.  Since Lisp with user-defined special forms
		  does not have an abstraction mechanism, it is
		  therefore not ``universal'' in this theory, in spite
		  of the ability to define every partial recursive
		  function on the natural numbers.  Several examples
		  and counter-examples to abstraction-preserving
		  reductions are given. We do not know whether there
		  is a natural universal language with respect to
		  abstraction-preserving reduction.}
}

@Book{Mitchell-1996,
  author = {John C. Mitchell},
  title = {Foundations for Programming Languages},
  publisher = {MIT Press},
  year = 1996,
  ISBN = 0262133210,
  McMaster = {QA 76.7 .M57 1996},
  bibliographies = {FP, ProgLang},
  errata = {
   p. 93: colon-separated --> semicolon-separated}
}

@Book{Mitchell-2003,
  author = {John C. Mitchell},
  title = {Concepts in Programming Languages},
  publisher = CambridgeUP,
  year = 2003,
  ISBN = {0-521-78098-5},
  McMaster = {QA 76.6 .M5293 2003},
  bibliographies = {FP, ProgLang}
}

@InProceedings{Mitchell-Honsell-Fisher-1993,
  author = {John C. Mitchell and Furio Honsell and Kathleen
                 Fisher},
  title = {A Lambda Calculus of Objects and Method
                 Specialization},
  pages = {26--38},
  abstract = {This paper presents an untyped lambda calculus,
                 extended with object primitives that reflect the
                 capabilities of so-called delegation-based
                 object-oriented languages. A type inference system
                 allows static detection of errors, such as message not
                 understood, while at the same time allowing the type of
                 an inherited method to be specialized to the type of
                 the inheriting object. Type soundness, in the form of a
                 ``subject-reduction'' theorem, is proved and examples
                 illustrating the expressiveness of the pure calculus
                 are presented.},
  booktitle = {Proceedings, Eighth Annual IEEE Symposium on Logic in
                 Computer Science},
  year = 1993,
  publisher = {IEEE Computer Society Press},
  month = JUN
}

@InProceedings{Mitchell-Meyer-1985,
  author = {John C. Mitchell and Albert R. Meyer},
  title = {Second-Order Logical Relations},
  pages = {225--236},
  note = {extended abstract},
  crossref = {LOP1985},
  WKloc = {A-0076},
  abstract = {Logical relations are a generalization of
		  homomorphisms between models of typed lambda
		  calculus. We define logical relations for
		  second-order typed lambda calculus and use these
		  relations to give a semantic characterization of
		  second-order lambda definability. Logical relations
		  are also used to state and prove a general
		  representation independence theorem. Representation
		  independence implies that the meanings of
		  expressions do not depend on whether $true$ is
		  represented by 1 and $false$ by 0, as long as all
		  the functions that manipulate truth values are
		  represented correctly.},
  bibliographies = {RelMiCS, LogRel}
}

@InProceedings{Mitchell-Plotkin-1985,
  author = {J. C. Mitchell and G. D. Plotkin},
  title = {Abstract types have existential type},
  booktitle = {Conference Record of the Twelfth Annual ACM Symposium
                 on Principles of Programming Languages},
  year = 1985,
  pages = {37--51}
}

@Misc{Mitchell-Scedrov-199X,
  author = {J. C. Mitchell and Andre Scedrov},
  title = {Notes on Sconing and Relators},
  year = {199?},
  WKloc = {A-0434},
  bibliographies = {LogRel}
}

@InProceedings{Mitchell-Viswanathan-1996,
  author = {John C. Mitchell and Ramesh Viswanathan},
  title = {Effective Models of Polymorphism, Subtyping and Recursion},
  crossref = {ICALP1996},
  pages = {170--181},
  OPTabstract = {},
  WKloc = {A-0454}
}

@Article{MitchellDG-,
  author = 	 {David G. Mitchell},
  title = 	 {A {SAT} Solver Primer},
  journal = 	 {},
  year = 	 {},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  WKloc = 	 {A-1639},
  keywords = 	 {SAT, DPLL}
}

@InCollection{MitchellOH-1883,
  author = {O. H. Mitchell},
  title = {On a New Algebra of Logic},
  crossref = {Peirce-1883},
  pages = {72--125},
  bibliographies = {RelMiCS}
}

@InCollection{Mitschke-1980,
  author = {Gerd Mitschke},
  title = {Infinite Terms and Infinite Reductions},
  crossref = {Seldin-Hindley-1980},
  pages = {243--257}
}

@Article{Miyoshi-1996,
  author = {Hiroyuki Miyoshi},
  title = {Modelling Conditional Rewriting Logic in Structured Categories},
  journal = ENTCS,
  year = 1996,
  volume = 4,
  WKloc = {A-1269},
  abstract = {We reformulate and generalize the functorial model of
      Meseguer's \emph{conditional} full rewriting logic by using inserter,
      a weighted limit in 2-categories. Indeed 2-categories are categories
      enriched in \textbf{Cat}. Therefore this method also can be extended
      to sesqui-categories and other enriched categories, with which we can
      model various aspects of rewritings and strategies.}
}

@Article{Mizoguchi-1993,
  author = {Yoshihiro Mizoguchi},
  title = {A Graph Structure over the Category of Sets and Partial Functions},
  journal = CTGDC,
  year = 1993,
  volume = 34,
  pages = {2--12},
  WKloc = {A-1080},
  bibliographies = {RelMiCS},
  abstract = {In 1984, Raoult proposed a formalization of graph rewritings
    using pushouts in the category of graphs and partial functions.
    This note generalizes his method and formulates algebraic graph structure
    to introduce a more general gframework for graph rewritings and to give
    a simple proof of existence theorem of pushouts using relational calculus.}
}

@Article{Mizoguchi-1999,
  author = {Yoshihiro Mizoguchi},
  title = {Properties of Graphs Preserved by Relational Graph Rewritings},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {289--299},
  abstract = {We formulate graphs and graph rewritings using binary
      relations and call them relational graphs and relational graph
      rewritings. In this framework, rewriting is defined using a pushout
      in a category of relational graphs. It is known that an important
      theorem of rewriting systems called critical pair's lemma can be
      proved using simple and clear categorical properties. In this paper,
      we construct treelike graphs and Raoult Graphs by some relational
      conditions. We give a sufficient condition for rewriting rules and
      matchings which guarantees the closedness of those graph rewritings.
      These results show that the critical pair's lemma also holds under
      some conditions for a graph rewriting system in which graphs are
      restricted to treelike graphs or Raoult Graphs.},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/35/abstract.html},
  bibliographies = {RelMiCS}
}

@Article{Mizoguchi-Kawahara-1995,
  author = {Yoshihiro Mizoguchi and Yasuo Kawahara},
  title = {Relational Graph Rewritings},
  journal = TCS,
  year = 1995,
  volume = 141,
  pages = {311--328},
  WKloc = {A-1079},
  bibliographies = {RelMiCS}
}

@Unpublished{Mizoguchi-Loucopoulos-2000,
  author = {Yoshihiro Mizoguchi and Pericles Loucopoulos},
  title = {Formalizing the Definition and Evolution of Models in a Repository using the Relational Graph Expressions},
  year = 2000,
  month = APR,
  note = {Unpublished draft},
  annote = {submitted to RelMiCS 5 proceedings},
  WKloc = {A-1023},
  bibliographies = {RelMiCS}
}

@Misc{Mizoguchi-Mori-2004,
  author = 	 {Yoshihiro Mizoguchi and Masao Mori},
  title = 	 {A labeled graph expression for a distribution set of {DNA} sequences},
  OPThowpublished = {},
  OPTmonth = 	 {},
  year = 	 {2004},
  OPTnote = 	 {},
  WKloc = 	 {A-1541}
}

@PhDThesis{Moebus-1981,
  title = {Relationale Algebren},
  author = {Axel M{\"o}bus},
  year = 1981,
  school = {Univ. D\"usseldorf, Mathematisch-Naturwissenschaftliche
      Fakult\"at},
  bibliographies = {RelMiCS}
}

@Article{Moeller-1985,
  year = 1985,
  volume = 22,
  title = {On the Algebraic Specification of Infinite Objects---Ordered and Continous Models of Algebraic Types},
  pages = {537--578},
  journal = ACTIN,
  author = {Bernd M{\"o}ller},
  bibliographies = {RelMiCS}
}

@InProceedings{Moeller-1991,
  author = {Bernhard M{\"o}ller},
  title = {Relations as Program Development Language},
  pages = {319--371},
  crossref = {IFIP1991},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings{Moeller-1993,
  author = {B. M{\"o}ller},
  title = {Ordered and continuous models of higher-order algebraic
                  specifications},
  crossref = {HOA1993},
  pages = {223--255},
  bibliographies = {RelMiCS},
  abstract = {We investigate the existence of continuous and
		  fixpoint models of higher-order
		  specifications. Particular attention is paid to the
		  question of extensionality. We use ordered
		  specifications, a particular case of Horn
		  specifications. The main tool for obtaining
		  continuous models is the ideal
		  completion. Unfortunately, it may destroy
		  extensionality. This problem is inherent: we show
		  that there is no completion method which is
		  guaranteed to preserve extensionality. To restore
		  it, generally a quotient has to be taken. It is
		  shown that under certain conditions this preserves
		  the existence of least fixpoints. Examples of the
		  specification method include the essential concepts
		  of Backus' FP and Hoare's CSP.}
}

@Article{Moeller-1993a,
  author = 	 {B. M{\"o}ller},
  title = 	 {Towards Pointer Algebra},
  journal = 	 SCICOP,
  year = 	 1993,
  volume =	 21,
  pages =	 {57--90},
  bibliographies = {RelMiCS}
}

@InProceedings{Moeller-1994,
  author = {B. M{\"o}ller},
  title = {Ideal Streams},
  crossref = {PROCOMET94},
  pages = {18--37},
  keywords = {Semantics of Program Languages; Mathematics of
		  Program Derivation},
  bibliographies = {RelMiCS}
}

@InProceedings{Moeller-1994a,
  author = {B. M{\"o}ller},
  title = {Algebraic Calculation of Graph and Sorting Algorithms},
  crossref = {Bjorner-Broy-Pottosin-1994},
  pages = {98--127},
  WKloc = {A-0999},
  bibliographies = {RelMiCS}
}

@Article{Moeller-1999,
  author = {Bernhard M{\"o}ller},
  title = {Calculating with Acyclic and Cyclic Lists},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {135--154},
  abstract = {We use a relational model of pointer structures to calculate
      a number of standard algorithms on singly linked lists, both acyclic
      and cyclic. This shows that our techniques are not just useful for
      tree-like structures, but apply to general pointer structures as
      well.},
  keywords = {Pointer structures; Sharing; Destructive updating; Program transformation; Relational calculus},
  DOI = "10.1016/S0020-0255(99)00011-0",
  DOIURL = "http://dx.doi.org/10.1016/S0020-0255(99)00011-0",
  DirectURL = "http://www.sciencedirect.com/science/article/pii/S0020025599000110",
  ElsevierURL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/26/abstract.html},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB}
}

@TechReport{Moeller-1999a,
  author = {Bernhard M{\"o}ller},
  title = {Typed {Kleene} algebras},
  institution = {Institut f\"ur Informatik, Universit\"at Augsburg},
  year = 1999,
  number = {1999-8},
  WKloc = {doc/pap/BIB},
  annote = {these are Kleene Algebras with Domain (KAD)},
  abstract = {Kleene algebras provide a convenient and powerful algebraic
      axiomatisation of a complete lattice that is endowed with a
      sequential composition operation. Models include formal languages
      under concatenation, relations under standard composition, sets of
      graph paths under path concatenation and sets of streams under
      concatenation. The least and greatest fixpoint operators of a
      complete lattice allow definitions of the finite and infinite
      iteration operators * and omega-iteration, resp. The abstract setting
      of Kleene algebras was used by Brunn/Möller/Russling (LNCS 1422) for
      the schematic derivation of a class of algorithms that abstracts
      layer-oriented graph traversals such as breadth-first search. It was
      also shown that the standard efficiency improvement for such
      algorithms, viz. carrying along a set of already visited vertices,
      can completely be transferred to the abstract level. The set of
      already visited vertices is used to restrict the underlying graph
      appropriately. So we need abstract counterparts of the notions of
      (sub)sets and restriction. The former role is taken over by elements
      of the Kleene algebra that are called types, whereas restriction can
      be modeled, as in the case of relations, by composition with a type.}
}

@Article{Moeller-2007,
  author = 	 {Bernhard M{\"o}ller},
  title = 	 {{Kleene} Getting Lazy},
  journal = 	 SCICOP,
  year = 	 2007,
  volume = 	 65,
  pages = 	 {195--214},
  URL = {http://dx.doi.org/10.1016/j.scico.2006.01.010},
  DOI = {10.1016/j.scico.2006.01.010},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  abstract = 	 {We propose a relaxation of Kleene algebra by giving
                  up strictness and right-distributivity of
                  composition. This allows the subsumption of
                  Dijkstra's computation calculus, Cohen's omega
                  algebra and von Wright's demonic refinement
                  algebra. Moreover, by adding domain and codomain
                  operators we can also incorporate modal
                  operators. We show that predicate transformers form
                  lazy Kleene algebras, the disjunctive and
                  conjunctive ones even lazy omega Kleene algebras. We
                  also briefly sketch two further applications: a
                  modal lazy Kleene algebra of commands modelling
                  total correctness and another one that abstractly
                  characterizes sets of trajectories as used in the
                  description of reactive and hybrid systems.}
}

@InProceedings{Moeller-Russling-1992,
  author = {Bernhard M\"oller and Martin Russling},
  title = {Shorter Paths to Graph Algorithms},
  crossref = {MPC1992},
  pages = {250--268},
  abstract = {We illustrate the use of formal languages and
		  relations in compact formal derivations of some
		  graph algorithms.},
  bibliographies = {RelMiCS}
}

@Article{Moerdijk-1995,
  author = {I. Moerdijk},
  title = {A Model for Intuitionistic Non-Standard Arithmetic},
  journal = ANPURE,
  year = 1995,
  volume = 73,
  pages = {37--51},
  bibliographies = {RelMiCS}
}

@Article{Moerdijk-Palmgren-1997,
  author = {I. Moerdijk and E. Palmgren},
  title = {Minimal Models of {Heyting} Arithmetic},
  journal = JSYLO,
  year = 1997,
  volume = 62,
  pages = {1448--1460},
  bibliographies = {RelMiCS}
}

@Manual{Moessenboeck-,
  title = {Object-Oriented Programming in {OBERON-2}},
  author = {Hanspeter M{\"o}ssenb{\"o}ck},
  organization = {ETH Z\"urich, Institut f\"ur Computersysteme},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1428}
}

@Manual{Moessenboeck-Wirth-1995,
  title = {The Programming Language {Oberon-2}},
  author = {Hanspeter M{\"o}ssenb{\"o}ck and N. Wirth},
  organization = {ETH Z\"urich, Institut f\"ur Computersysteme},
  month = MAR,
  year = 1995,
  WKloc = {A-1429}
}

@Article{Mogensen-1998,
  author = {Torben Mogensen},
  title = {Inherited Limits},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 5},
  WKloc = {A-0902, 17--20},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Mogensen-2005,
  author = 	 {Torben Mogensen},
  title = 	 {Semi-inversion of Guarded Equations},
  OPTcrossref =  {GPCE2005},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {189--204},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1650, doc/pap/BIB},
  OPTannote = 	 {}
}

@InProceedings{Moggi-1991,
  author = {Eugenio Moggi},
  title = {A Modular Approach to Denotational Semantics},
  crossref = {CTCS1991},
  pages = {138--139},
  WKloc = {A-0099},
  abstract = {We propose an incremental approach to the
		  denotational semantics of complex programming
		  languages based on the idea of {\em monad transformer}.},
  bibliographies = {RelMiCS}
}

@Article{Moggi-1991a,
  author = {Eugenio Moggi},
  title = {Notions of Computation and Monads},
  journal = {Information and Computation},
  year = 1991,
  volume = 93,
  WKloc = {A-1555},
  pages = {55--92}
}

@InProceedings{Moggi-1994,
  title = {A General Semantics for Evaluation Logic},
  author = {Eugenio Moggi},
  pages = {353--362},
  crossref = {LICS9},
  WKloc = {A-0374},
  abstract = {A previous semantics of Evaluation Logic relies on
		  additional properties of monads. This paper proposes
		  an alternative semantics, which drops all additional
		  requirement on monads, at the expense of stronger
		  assumptions on the underlying category. These
		  assumptions are satisfied by any topos, but not by
		  the category of cpos.  However, in the setting of
		  Synthetic Domain Theory it is possible to reconcile
		  the needs of Denotational Semantics with those of Logic.}
}

@Misc{Moggi-1994a,
  author = {Eugenio Moggi},
  title = {Metalanguages and Denotational Semantics},
  howpublished = {Talk at LMU, August 8, 1994},
  year = 1994,
  month = AUG,
  authorsAddress = {DISI, Univ. di Genova},
  abstract = {In this talk we discuss how metalanguages could be
		  used in semantics.  In particular, we will describe
		  their use for structuring and modularizing the
		  description of programming languages. Moreover, we
		  will suggest possible applications of Logical
		  Frameworks and Synthetic Domain Theory for providing
		  powerful metalanguages for the "semantic
		  description" of programming languages.}
}

@InProceedings{Mohan-1989,
  author = {Chilukuri K. Mohan},
  title = {Priority Rewriting: {Semantics}, Confluence, and
                 Conditionals},
  pages = {278--291},
  crossref = {RTA1989}
}

@PhDThesis{Mohnen-1997,
  author = {Markus Mohnen},
  title = {Optimising the memory Management of High-Order Functional Programs},
  school = {RWTH Aachen},
  year = 1997,
  month = NOV,
  WKloc = {A-0788}
}

@Misc{Mohnen-199X,
  author = {Markus Mohnen},
  title = {Context Patterns in {Haskell}},
  year = {199?},
  WKloc = {A-0686}
}

@Misc{Mohnen-199Y,
  author = {Markus Mohnen},
  title = {{Context Patterns, Part II}},
  year = {199?},
  WKloc = {A-0688}
}

@Misc{Mohnen-Tobies-199X,
  author = {Markus Mohnen and Stephan Tobies},
  title = {Implementing Context Patterns in the {Glasgow Haskell Compiler}},
  year = {199?},
  WKloc = {A-0687}
}

@Article{Mohr-Henderson-1986,
  author = {R. Mohr and T. C. Henderson},
  title = {Arc and Path Consistency Revisited},
  journal = AI,
  volume = 28,
  year = 1986,
  pages = {225--233},
  bibliographies = {RelMiCS}
}

@Book{Molzberger-Niegel-1992,
  authorsAddress = {inf32, inf2},
  year = 1992,
  title = {{Aspekte der Selbstorganisation}},
  series = {Springer Informatik-Fachbericht},
  number = 304,
  editor = {Molzberger, P. and Niegel, W.},
  address = {Berlin-Heidelberg}
}

@InProceedings{Monin-1995,
  author = {Jean-Fran{\c{c}}ois Monin},
  title = {Extracting Programs with Exceptions in an Impredicative Type System},
  crossref = {MPC1995},
  pages = {335--350},
  OPTabstract = {},
  WKloc = {A-0712}
}

@Book{Monin-2003,
  author =	 {Jean-Fran{\c{c}}ois Monin},
  title = 	 {Understanding Formal Methods},
  publisher = 	 Springer,
  year = 	 2003,
  ISBN = 	 {1-85233-247-6},
  WKloc = 	 {owned, \lent{Li Jingda}}
}

@Article{Monk-1961,
  author = {J. Donald Monk},
  title = {Relation Algebras and Cylindric Algebras},
  journal = NOTIC,
  volume = 8,
  year = 1961,
  pages = 358,
  bibliographies = {RelMiCS}
}

@PhDThesis{Monk-1961a,
  author = {J. Donald Monk},
  title = {Studies in cylindric algebra},
  note = Doct,
  school = BERKELEY,
  address = {Berkeley},
  year = 1961,
  pages = {vi+83},
  bibliographies = {RelMiCS}
}

@Article{Monk-1964,
  author = {J. Donald Monk},
  title = {On Representable Relation Algebras},
  journal = MICH,
  volume = 11,
  year = 1964,
  pages = {207--210},
  bibliographies = {RelMiCS}
}

@InProceedings{Monk-1965,
  author = {J. Donald Monk},
  title = {Model-theoretical Methods and Results in the Theory of
		Cylindric Algebras},
  crossref = {TM1965},
  pages = {238--250},
  bibliographies = {RelMiCS}
}

@Article{Monk-1969,
  author = {J. Donald Monk},
  title = {Nonfinitizability of Classes of Representable Cylindric
		Algebras},
  journal = JSYLO,
  volume = 34,
  year = 1969,
  pages = {331--343},
  bibliographies = {RelMiCS}
}

@Article{Monk-1970,
  author = {J. Donald Monk},
  title = {On an Algebra of Sets of Finite Sequences},
  journal = JSYLO,
  volume = 35,
  year = 1970,
  pages = {19--28},
  bibliographies = {RelMiCS}
}

@Article{Monk-1970a,
  author = {J. Donald Monk},
  title = {Completions of Boolean Algebras with Operators},
  journal = MATNACH,
  volume = 46,
  year = 1970,
  pages = {47--55},
  bibliographies = {RelMiCS}
}

@Article{Monk-1971,
  author = {J. Donald Monk},
  title = {Provability with Finitely Many Variables},
  journal = PROAMS,
  volume = 27,
  year = 1971,
  pages = {353--358},
  bibliographies = {RelMiCS}
}

@InCollection{Monk-1974,
  author = {J. Donald Monk},
  title = {Connections Between Combinatorial Theory
		and Algebraic Logic},
  crossref = {Daigneault-1974},
  pages = {58--91},
  bibliographies = {RelMiCS}
}

@InProceedings{Monnier-Haguenauer-2010,
  author =       {Stefan Monnier and David Haguenauer},
  title =        {Singleton Types Here, Singleton Types there, Singleton Types Everywhere},
  crossref =  {PLPV2010},
  WKloc =       {A-1735, doc/pap/BIB},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@Book{Montague-1974,
  author = {Richard Montague},
  title = {Formal Philosophy},
  year = 1974,
  address = {New Haven},
  publisher = YaleP,
  bibliographies = {RelMiCS}
}

@Article{Montanari-1974,
  author = {Ugo Montanari},
  title = {Networks of Constraints: Fundamental Properties and
		Applications to Picture Porcessing},
  journal = INFOSCI,
  volume = 7,
  pages = {95--132},
  year = 1974,
  bibliographies = {RelMiCS}
}

@InProceedings{Montanari-Degano-1985,
  author = {Ugo Montanari and Pierpaolo Degano},
  title = {Distributed Systems, Partial Orderings of Events, and Event Structures},
  pages = {7--106},
  abstract = {These lecture notes are divided in two parts,
                  dedicated to two models, concurrent histories and
                  Graphs for Distributed Systems (GDS), both based on
                  partial orderings. The models are largely consistent,
                  the latter being a richer version of the formaer,
                  conceived as a specification formalism for
                  distributed systems. The semantic aspects of the
                  first model are studied in finer detail, including
                  properties of non terminating omputations and the
                  definition of an observational equivalence.},
  crossref = {Marktoberdorf-1985}
}

@Misc{Montanari-Pistore-Rossi-199X,
  author = {Ugo Montanari and Marco Pistore and Francesca Rossi},
  title = {Modeling Concurrent, Mobile and Coordinated Systems via Graph Transformations},
  year = {199?},
  WKloc = {A-0845}
}

@Article{Montanari-Rossi,
  author = {Ugo Montanari and Francesca Rossi},
  title = {Fundamental Properties of Networks of Constraints:
		A New Formulation},
  journal = {},
  volume = {},
  pages = {426--449},
  year = {},
  note = {no Journal!!!!!},
  bibliographies = {RelMiCS}
}

@InProceedings{Montanari-Rossi-1991,
  title = {Perfect Relaxation in Constraint Logic Programming},
  pages = {223--237},
  crossref = {ICLP1991},
  author = {Montanari, Ugo and Rossi, Francesca},
  bibliographies = {RelMiCS}
}

@InProceedings{Montanari-Rossi-1991a,
  author = {Ugo Montanari and Francesca Rossi},
  title = {Graph Rewriting for a Partial Ordering Semantics of
		  Concurrent Constraint Programming},
  crossref = {GraTra91},
  pages = {225--256},
  abstract = {The concurrent constraint logic programming framework
               extends both logic programming and concurrent logic
               programming in that a program consists of the concurrent
               execution of agents which add and check constraints on a
               shared set of variables, and whose behavior is described
               by a set of clauses. This formulation is very general
               and can be seen as a concurrent logic programming shell
               which is parametrized w.r.t. the underlying constraint
               system. Graphs and graph grammars can be conveniently
               used to describe such a framework and the modelling is
               so elegant and expressive that they provide what we
               believe is the most natural abstract machine for
               concurrent constraint programming.},
  bibliographies = {RelMiCS}
}

@InProceedings{Montanari-Rossi-1993,
  author = {Ugo Montanari and Francesca Rossi},
  title = {Contextual Occurrence Nets and Concurrent Constraint
		  Programming},
  crossref = {GTCS93},
  pages = {280--295},
  abstract = {This paper proposes a new semantics for concurrent
		  constraint programs. The meaning of each program is
		  defined as a contextual net, which is just a usual
		  net where context conditions, besides pre- and
		  post-conditions, are allowed. Context conditions are
		  just items which have to be present in order for an
		  event to take place, but which are not affected by
		  the event. They are very useful for describing
		  situations where different events share a common
		  resource and want to read it simultaneously. In
		  fact, such events are concurrent in the net. The
		  causal dependency relation of the net induces a
		  partial order among objects in the same computation,
		  while its mutual exclusion relation provides a way
		  of expressing nondeterministic information. Such
		  information can be of great help to a scheduler
		  while trying to find an efficient execution of the
		  program, or also to a compile-time optimizer.},
  bibliographies = {RelMiCS}
}

@Misc{Montanari-Rossi-199X,
  author = {Ugo Montanari and Francesca Rossi},
  title = {Modeling Process Coordination via Tiles, Graphs, and Constraints},
  year = {199?},
  WKloc = {A-0842},
  bibliographies = {RelMiCS}
}

@Misc{Montanari-Rossi-199Y,
  author = {Ugo Montanari and Francesca Rossi},
  title = {Graph Rewriting, Constraint Solving and Tiles for Coordinating Distributed Systems},
  year = {199?},
  WKloc = {A-0844},
  bibliographies = {RelMiCS}
}

@InProceedings{Monti-Peron-1995,
  author = {A. Monti and A. Peron},
  title = {Systolic Tree $\omega$-Languages},
  crossref = {STACS1995},
  pages = {131--142},
  WKloc = {A-0408},
  abstract = {The class of $\omega$-languages recognized by
		  systolic tree automata is introduced. That class
		  extends the class of B\"uchi $\omega$-languages and
		  is closed under boolean operations. The emptiness
		  problem for systolic tree automata on infinite
		  sequences is decidable. A characterization of
		  systolic tree $\omega$-languages in terms of a
		  (suitable) concatenation of (finitary) systolic tree
		  languages is also provided.}
}

@InCollection{Moore-1956,
  author = 	 {Edward F. Moore},
  title = 	 {Gedanken-Experiments on Sequential Machines},
  crossref =	 {Shannon-McCarthy-1956},
  pages =	 {129--153},
  WKloc = {A-1560}
}

@Article{Moore-1980,
  author = {Gregory H. Moore},
  title = {Beyond First-order Logic: The Historical Interplay
		Between Mathematical Logic and Axiomatic Set Theory},
  journal = HIST,
  volume = 1,
  year = 1980,
  pages = {95--137},
  bibliographies = {RelMiCS}
}

@InCollection{Moore-1987,
  author = {Gregory H. Moore},
  title = {A House Divided Against Itself: The Emergence of
                First-order Logic as the Basis for Mathematics},
  booktitle = {Studies in the History of Mathematics},
  series = MAA_S,
  volume = 26,
  publisher = MAA,
  year = 1987,
  pages = {98--136},
  bibliographies = {RelMiCS}
}

@InCollection{Moore-1988,
  author = {Gregory H. Moore},
  title = {The Emergence of First-order Logic},
  booktitle = {History and Philosophy of Modern Mathematics},
  series = Minnesota,
  volume = {XI},
  publisher = MinnesotaUP,
  address = {Minneapolis},
  year = 1988,
  pages = {95--135},
  bibliographies = {RelMiCS}
}

@Misc{Moore-Crutchfield-199X,
  author = {Cristopher Moore and James P. Crutchfield},
  title = {Quantum Automata and Quantum Grammars},
  year = {199?},
  WKloc = {A-0430}
}

@InProceedings{MoralesGerman-1994,
  author = {D. Morales-German},
  title = {An {SGML-based} programming environment for literate
                 programming},
  crossref = {CASCON1994},
  pages = {42--49},
  month = {},
  year = 1994,
  WKloc = {A-0432},
  acknowledgement = {litprog.bib},
  abstract = {Literate programming is a documentation method that
                 attempts to maintain consistency among the various
                 design and program documents of a software system.
                 Unfortunately, the majority of the literate programming
                 tools do not have appropriate user interfaces and
                 require the users to learn complicated and cryptic
                 tagging languages. SGML is a metalanguage used to
                 specify markup or tagging languages that can be used to
                 encode the structure of documents. This paper describes
                 how a markup language defined using SGML can be used as
                 the basic method for structuring literate programming
                 documents and can be made independent of the
                 programming language. Furthermore, with SGML and tools
                 to browse and edit SGML documents, literate programs
                 can benefit from WYSIWYG editing and hypertext
                 capabilities and can even include pictures and other
                 graphics. In addition, syntax-directed editors that
                 support SGML can hide the markup tags and thus remove
                 the need to learn a markup language. Text databases
                 that use SGML can also be used to store literate
                 programs. As a result, literate programs can be browsed
                 and queried using complex search expressions, a
                 capability beyond most text editors. For example, the
                 searches can involve combinations of structural and
                 textual information. Because SGML is a popular and
                 emerging standard, we can expect to have more powerful
                 tools to manipulate many different forms of design and
                 program documentation. This paper describes the issues
                 involved in the development of a literate programming
                 environment that uses SGML as the storage model.},
  affiliation = {Dept. of Comput. Sci., Waterloo Univ., Ont., Canada},
  classification = {C6115 (Programming support); C6130D (Document
                 processing techniques); C6130M (Multimedia); C6140D
                 (High level languages)},
  keywords = {Complex search expressions; Consistency maintenance;
                 Design documents; Document browsing; Document editing;
                 Document structure encoding; Graphics; Hypertext;
                 Literate programming; Markup language; Program
                 documentation method; Program documents; SGML-based
                 programming environment; Storage model; Syntax-directed
                 editors; Tagging languages; Text databases; User
                 interfaces; WYSIWYG editing},
  thesaurus = {Hypermedia; ISO standards; Page description languages;
                 Programming environments; System documentation; Text
                 editing}
}


@Article{Moran-Sands-,
  author = 	 {Andrew Moran and David Sands},
  title = 	 {Improvement in a Lazy Context: An Operational Theory for Call-By-Need},
  journal = 	 {},
  year = 	 {},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  WKloc = 	 {A-1710},
  bibliographies = {PMC}
}

@TechReport{Moreau-1996,
  author = {Luc Moreau},
  title = {A Syntactic Theory of Dynamic Binding},
  year = 1996,
  month = MAY,
  institution = {Department of Electronics and Computer Science, University of Southampton},
  OPTtype = {},
  OPTnumber = {M96/4},
  OPTaddress = {},
  WKloc = {A-0739}
}

@Book{Mordeson-Malik-Malik-2002,
  author =	 {John N. Mordeson, Davender S. Malik, D. S. Malik},
  title = 	 {Fuzzy Automata and Languages: Theory and Application},
  publisher = 	 {Chapman \& Hall},
  year = 	 2002,
  ISBN = 	 1584882255,
  abstract = {The huge number and broad range of the existing and
     potential applications of fuzzy logic have precipitated a
     veritable avalanche of books published on the subject. Most,
     however, focus on particular areas of application. Many do no
     more than scratch the surface of the theory that holds the power
     and promise of fuzzy logic.Fuzzy Automata and Languages: Theory
     and Applications offers the first in-depth treatment of the
     theory and mathematics of fuzzy automata and fuzzy
     languages. After introducing background material, the authors
     study max-min machines and max-product machines, developing their
     respective algebras and exploring properties such as
     equivalences, homomorphisms, irreducibility, and minimality. The
     focus then turns to fuzzy context-free grammars and languages,
     with special attention to trees, fuzzy dendrolanguage generating
     systems, and normal forms. A treatment of algebraic fuzzy
     automata theory follows, along with additional results on fuzzy
     languages, minimization of fuzzy automata, and recognition of
     fuzzy languages. Although the book is theoretical in nature, the
     authors also discuss applications in a variety of fields,
     including databases, medicine, learning systems, and pattern
     recognition.Much of the information on fuzzy languages is new and
     never before presented in book form. Fuzzy Automata and Languages
     incorporates virtually all of the important material published
     thus far. It stands alone as a complete reference on the subject
     and belongs on the shelves of anyone interested in fuzzy
     mathematics or its applications.}
}

@Article{Morgan-1988,
  author = {Caroll Morgan},
  title = {Data Refinement by Miracles},
  journal = ACTIN,
  year = 1988,
  volume = 26,
  pages = {243--246},
  WKloc = {A-1311},
  bibliographies = {RelMiS}
}

@InCollection{Morgan-1994,
  author = {Caroll Morgan},
  title = {The Cuppiest Capjunctive Capping, and {Galois}},
  crossref = {Roscoe-1994},
  pages = {317--332},
  chapter = 19,
  OPTnote = {},
  OPTannote = {}
}

@Book{Morgan-1994b,
  author = {C. C. Morgan},
  title = {Programming from Specifications},
  publisher = {Prentice Hall International Series in Computer Science},
  edition = {2nd},
  ISBN = {13-12332274-6},
  length = 320,
  year = 1994,
  annote = {This book presents a rigorous treatment of most elementary
                  program development techniques, including iteration,
                  recursion, procedures, parameters, modules and data
                  refinement.},
  UniBwM = {INF400/T7919},
  bibliographies = {SpecTech}
}

@Article{Morgan-Robinson-1987,
  author = {C. Morgan and K. Robinson},
  title = {Specification Statements and Refinement},
  journal = IBMRD,
  year = 1987,
  volume = 31,
  number = 5,
  pages = {49--68},
  bibliographies = {SpecTech, RelMiCS}
}

@Book{MorganR-1998,
  ALTauthor = 	 {Robert Morgan},
  ALTeditor = 	 {},
  title = 	 {Building an Optimizing Compiler},
  publisher = 	 {Digital Press/Butterworth-Heinemann},
  year = 	 {1998},
  ISBN = 	 {155558179X},
  McMaster = 	 {QA 76.76 .C65 M67 1998},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Mori-Matsumoto-1994,
  author = {A. Mori and Y. Matsumoto},
  title = {Coherence for Cartesian Closed Categories: a Sequential Approach},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Kyoto University}
}

@Article{Mori-Mizoguchi-2003,
  author = 	 {Masao Mori and Yoshihiro Mizoguchi},
  title = 	 {Graph Transformation Approach for the Shortest Path Search and Length Calculation},
  journal = 	 {Bulletin of Informatics and Cybernetics, Research Association of Statistical Sciences},
  year = 	 2003,
  volume =	 35,
  number =	 {1--2},
  pages =	 {57--67},
  WKloc = 	 {A-1576}
}

@PhDThesis{Morris-1968,
  UniBwM = {INF400/R3058},
  year = 1968,
  title = {Lambda-Calculus Models of Programming Languages},
  school = MIT,
  month = DEC,
  WKloc = {Q-005 (nur bis Seite 59)},
  author = {James H. Morris},
  bibliographies = {RelMiCS}
}

@Article{Morris-1987,
  author = {J. M. Morris},
  title = {A Theoretical Basis for Stepwise Refinement and the
                   Programming Calculus},
  journal = SCICOP,
  year = 1987,
  volume = 9,
  pages = {287--306},
  bibliographies = {RelMiCS}
}

@Unpublished{Morriset-Felleisen-Harper-1995,
  author = {Greg Morrisett and Matthias Felleisen and Robert Harper},
  title = {Abstract Models of Memory Management},
  note = {Submitted to 1995 Conf. on Functional Programming
		  Languages and Computer Architecture},
  year = 1995,
  WKloc = {A-0389},
  keywords = {garbage collection},
  URL = {http://www.cs.cmu.edu:8001/afs/cs/user/jgmorris/web/my-papers.html}
}

@Article{Morrison-Atkinson-Brown-Dearle-1990,
  author = {Morrison and Atkinson and Brown and Dearle},
  title = {On the Classification of Binding Mechanisms},
  journal = {Information Processing Letters},
  volume = 34,
  year = 1990,
  pages = {51--55},
  number = 1,
  keywords = {R-value, L-value, static binding, dynamic binding}
}

@article{Morriset-Walker-Crary-Glew-1999,
  author = {Greg Morrisett and David Walker and Karl Crary and Neal Glew},
  title = {From System {F} to Typed Assembly Language},
  journal = ACM-TOPLAS,
  volume = {21},
  number = {3},
  year = {1999},
  issn = {0164-0925},
  pages = {527--568},
  doi = {http://doi.acm.org/10.1145/319301.319345},
  publisher = {ACM Press},
  keywords = {TAL},
  bibliographies = {Coconut}
}

@InProceedings{Morvan-2000,
  author = {Christophe Morvan},
  title = {On Rational Graphs},
  crossref = {FoSSaCS2000},
  pages = {252--266}
}

@Article{Moses-1971,
  author =       {Joel Moses},
  title =        {Algebraic Simplification: A Guide for the Perplexed},
  journal =      CACM,
  year =         1971,
  volume =    14,
  number =    8,
  pages =     {527--537},
  bibliographies =    {Coconut}
}

@Unpublished{Moshier-2013,
  author =       {M. Andrew Moshier},
  title =        {A Relational Category of Polarities},
  note =         {(unpublished draft)},
  WKloc =       {doc/pap/BIB},
  month =     NOV,
  year =      2013,
  bibliographies =    {RelMiCS}
}

@InProceedings{Moskewicz-Madigan-ZhaoYing-ZhangLintao-MalikSharad-2001,
  author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
  title = {Chaff: Engineering an Efficient {SAT} Solver},
 booktitle = {{DAC '01}: Proceedings of the 38th annual Design Automation Conference},
 year = {2001},
 isbn = {1-58113-297-2},
 pages = {530--535},
 location = {Las Vegas, Nevada, United States},
 DOIURL = {http://doi.acm.org/10.1145/378239.379017},
 publisher = {ACM},
 address = {New York, NY, USA},
 WKloc = {A-1727},
 abstract = {Boolean Satisfiability is probably the most studied
    of combinatorial optimization/search problems.
    Significant effort has been devoted
    to trying to provide practical solutions to this problem
    for problem instances encountered in a range of applications
    in Electronic Design Automation (EDA),
    as well as in Artificial Intelligence (AI).
    This study has culminated in the development of several SAT packages,
    both proprietary and in the public domain (e.g. GRASP, SATO)
    which find significant use in both research and industry.
    Most existing complete solvers
    are variants of the Davis-Putnam (DP) search algorithm.
    In this paper we describe the development of a new complete solver, Chaff,
    which achieves significant performance gains
    through careful engineering of all aspects of the search ---
    especially a particularly efficient implementation
    of Boolean constraint propagation (BCP)
    and a novel low overhead decision strategy.
    Chaff has been able to obtain
    one to two orders of magnitude performance improvement
    on difficult SAT benchmarks in comparison with other solvers (DP or otherwise),
    including GRASP and SATO.}
}

@Misc{Moshier-1995,
  author = {Moshier, M.A.},
  title = {Featureless {HPSG}},
  year = 1995,
  note = {Unpublished manuscript},
  bibliographies = {RelMiCS}
}

@Article{Moss-Johnson-1995,
  author = {Moss, L.S. and Johnson, D.E.},
  journal = JLLI,
  pages = {61--79},
  title = {Dynamic interpretations of constraint-based grammar formalisms},
  volume = 4,
  year = 1995,
  bibliographies = {RelMiCS}
}

@Misc{Moss-Runciman-199X,
  author = {Graeme E. Moss and Colin Runciman},
  title = {Auburn: A Kit for benchmarking Functional Data Structures},
  year = {199?},
  WKloc = {A-0720}
}

@InProceedings{Mosses-1981,
  author = {Mosses, Peter D.},
  title = {A Semantic Algebra for Binding Constructs},
  pages = {408--418},
  keywords = {Static and Dynamic Binding},
  booktitle = {Proceedings of International Colloquium on
		  Formalization of Programming Concepts},
  year = 1981,
  series = {LNCS},
  volume = 107,
  address = {Peniscola, Spain},
  month = apr
}

@InProceedings{Mosses-1989,
  author = {Peter D. Mosses},
  title = {Unified Algebras and Modules},
  crossref = {POPL1989},
  pages = {329--343},
  WKloc = {A-0211},
  abstract = {This paper concerns the algebraic specification of
		  abstract data types. It introduces and motivates the
		  recently-developed framework of unified algebras,
		  and provides a practical notation for their modular
		  specification. It also compares unified algebras
		  with the well-known framework of order-sorted
		  algebras, which underlies the {\sc Obj}
		  specification language.}
}

@InProceedings{Mosses-1989a,
  author = {Mosses, Peter D.},
  title = {Unified Algebras and Institutions},
  crossref = {LICS4},
  pages = {304--312},
  abstract = {A novel framework for algebraic specification of abstract
		  data types is introduced.  It involves so-called
		  ``unified algebras'', where sorts are treated as
		  values, so that operations may be applied to sorts
		  as well as to the elements that they classify.

                  An institution for unified algebras is defined, and
		  shown to be liberal.  However, the ordinary
		  ``forgetful'' functor does not forget any values in
		  unified algebras, so the usual data constraints do
		  not have any models.  A ``more forgetful'' functor
		  is introduced and used to define so-called
		  ``bounded'' data constraints, which have the
		  expected models.},
  annote = {see also \cite{Mosses-1992}}
}

@InProceedings{Mosses-1992,
  author = {P. Mosses},
  title = {Unified Algebras and Abstract Syntax},
  crossref = {SADT92},
  pages = {280--294},
  WKloc = {A-0334},
  abstract = {We consider the algebraic specification of abstract
		  syntax in the framework of unified algebras. We
		  illustrate the expressiveness of unified algebraic
		  specifications, and provide a grammar-like notation
		  for specifying abstract syntax, particularly
		  attractive for use in semantic descriptions of
		  full-scale programming languages.},
  annote = {see also \cite{Mosses-1989}}
}

@Book{Mosses-2004,
  editor =       {Peter D. Mosses},
  title =        {\textsc{Casl} Reference Manual:
    The Complete Documentation of the Common Algebraic Specification Language},
  year =         2004,
  publisher =    Springer,
  series =       LNCS # { (IFIP Series)},
  volume =       2960,
  note =         {With chapters by T. Mossakowski, D. Sannella,
                 and A. Tarlecki},
  URL = {http://link.springer.de/link/service/series/0558/tocs/t2960.htm},
  ISBN = {3-540-21301-5},
  keywords = {CASL, algebraic specification}
}

@PhDThesis{Mossin-1997,
  author = {Christian Mossin},
  title = {Flow Analysis of Typed Higher-Order Programs},
  school = {DIKU, Department of Computer Science, University of Copenhagen},
  year = 1997,
  note = {Tech.\null{} Report DIKU-TR-97/1},
  WKloc = {A-1162},
  bibliographies = {TermGraph}
}

@InProceedings{Mossin-1997a,
  author = {Christian Mossin},
  title = {Higher-Order Value Flow Graphs},
  crossref = {PLILP1997},
  pages = {???},
  WKloc = {A-1161},
  bibliographies = {TermGraph}
}

@InProceedings{Mossin-1997b,
  author = {Christian Mossin},
  title = {Exact Flow Analysis},
  crossref = {SAS1997},
  pages = {???},
  WKloc = {A-1163},
  bibliographies = {TermGraph}
}

@InProceedings{Moszkowski-1994,
  author = {B. Moszkowski},
  title = {Some very Compsitional Temporal Properties},
  crossref = {PROCOMET94},
  pages = {303--322},
  keywords = {Mathematical Logic; Deduction and Theorem Proving;
		  Compositionality in Concurrency}
}

@InProceedings{MuShinCheng-Bird-2002,
  author = {Shin-Cheng Mu and Richard Bird},
  title = {Functional Quantum Programming},
  OPTcrossref = {},
  OPTkey = {},
  booktitle = {APLAS 2002},
  OPTpages = {},
  year = 2002,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  URL = {http://ropas.kaist.ac.kr/aplas/},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1367}
}

@InProceedings{MuShinCheng-KoHsianShang-Jansson-2008,
  author =       {Shin-Cheng Mu and Hsiang-Shang Ko and Patrik Jansson},
  title =        {Algebra of Programming using Dependent Types},
  crossref =  {MPC2008},
  pages =     {268--283},
  SpringerURL =      {http://www.springerlink.com/content/3513x7q0117j667h/},
  DOI =    {10.1007/978-3-540-70594-9_15},
  abstract = {Dependent type theory is rich enough to express
    that a program satisfies an input/output relational specification,
    but it could be hard to construct the proof term.
    On the other hand, squiggolists know very well
    how to show that one relation is included in another
    by algebraic reasoning.
    We demonstrate how to encode functional and relational derivations
    in a dependently typed programming language.
    A program is coupled with an algebraic derivation from a specification,
    whose correctness is guaranteed by the type system.}
}

@Article{MuShinCheng-KoHsianShang-Jansson-2009,
  author =       {Shin-Cheng Mu and Hsiang-Shang Ko and Patrik Jansson},
  title =        {Algebra of Programming in {Agda}: Dependent Types for Relational Program Derivation},
  journal =      JFP,
  year =         2009,
  volume =    19,
  number =    5,
  pages =     {545--579},
  month =     SEP,
  DOI = {10.1017/S0956796809007345},
  DOIURL = {http://dx.doi.org.libaccess.lib.mcmaster.ca/10.1017/S0956796809007345},
  WKloc = {A-1720, doc/pap/BIB},
  bibliographies = {RelMiCS},
  ParentURL = {http://www.iis.sinica.edu.tw/pages/scm/publications_en.html},
  DirectURL = {http://www.iis.sinica.edu.tw/~scm/2009/algebra-of-programming-in-agda-dependent-types-for-relational-program-derivation/},
  CODEURL = {http://www.iis.sinica.edu.tw/~scm/2008/aopa/},
  note = {See also AoPA at \url{http://www.iis.sinica.edu.tw/~scm/2008/aopa/}},
  abstract =    {Relational program derivation is the technique of stepwise refining a relational specification to a program by algebraic rules. The program thus obtained is correct by construction. Meanwhile, dependent type theory is rich enough to express various correctness properties to be verified by the type checker.

We have developed a library, AoPA, to encode relational derivations in the dependently typed programming language Agda. A program is coupled with an algebraic derivation whose correctness is guaranteed by the type system.

Two non-trivial examples are presented: an optimisation problem, and a derivation of quicksort where well-founded recursion is used to model terminating hylomorphisms in a language with inductive types.

This article extends the paper we published in Mathematics of Program Construction 2008. Code accompanying the paper has been developed into an Agda library AoPA.}
}

@Article{MuShinCheng-Oliveira-2012,
  author =       {Shin-Cheng Mu and Jos{\'e} Nuno Oliveira},
  title =        {Programming from {Galois} Connections},
  journal =      JLAP,
  year =         2012,
  volume =    81,
  number =    6,
  pages =     {680--704},
  month =     AUG,
  DOI = {10.1016/j.jlap.2012.05.003},
  DOIURL = {http://dx.doi.org/10.1016/j.jlap.2012.05.003},
  JournalURL = {http://www.sciencedirect.com/science/article/pii/S1567832612000525},
  ParentURL = {http://www.iis.sinica.edu.tw/pages/scm/publications_en.html},
  DirectURL = {http://www.iis.sinica.edu.tw/papers/scm/13738-F.pdf},
  abstract =    {Problem statements often resort to superlatives such as in eg. “. . . the smallest
    such number”, “. . . the best approximation”, “. . . the longest such list” which
    lead to specifications made of two parts: one defining a broad class of solutions
    (the \emph{easy} part) and the other requesting one particular such solution, optimal
    in some sense (the \emph{hard} part).

    This article introduces a binary relational combinator which mirrors this
    linguistic structure and exploits its potential for calculating programs by optimization.
    This applies in particular to specifications written in the form of
    Galois connections, in which one of the adjoints delivers the optimal solution.\

    The framework encompasses re-factoring of results previously developed by
    by Bird and de Moor for greedy and dynamic programming, in a way which
    makes them less technically involved and therefore easier to understand and
    play with.}
}

@Book{Muchnick-1997,
  author = 	 {Steven S. Muchnick},
  title = 	 {Advanced Compiler Design and Implementation},
  publisher = 	 {Morgan Kauffmann},
  year = 	 1997,
  McMaster = 	 {QA 76.76 .C65M8 1997},
  ISBN = 	 {1-55860-320-4},
  bibliographies = {Coconut}
}

@InProceedings{Mueck-1992,
  authorsAddress = {mueck\@informatik.uni-muenchen.de},
  abstract = {In this paper we present a clean implementation
		  technique for functional/logic (or algebraic)
		  programming languages. First we define an
		  intermediate language to which a functional / logic
		  program is compiled. In order to implement this
		  intermediate language, we extend the Categorical
		  Abstract Machine (CAM) by an additional data
		  structure to handle logical variables and by a few
		  instructions covering unification and backtracking.
		  Finally, we show how the intermediate language is
		  compiled into the instruction set or our Categorical
		  Abstract Machine extension.},
  title = {{CAMEL}: An Extension of the Categorical Abstract
		  Machine to Compile Functional/Logical Programs},
  pages = {341--354},
  crossref = {PLILP1992},
  author = {Andy M{\"u}ck},
  bibliographies = {RelMiCS}
}

@Misc{Mueller-1996,
  author = {J\"urgen M{\"u}ller},
  year = 1996,
  title = {On Termination of Single-Pushout Graph Rewriting},
  WKloc = {A-0415}
}

@Article{Muller-1992,
  author = {Muller, F.},
  title = {Confluence of the lambda calculus with left-linear
                   algebraic rewriting},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Information processing letters},
  year = 1992,
  volume = 41,
  number = 6,
  pages = {293--},
  month = APR,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Muller-1994,
  author = {Robert Muller},
  title = {A Staging Calculus and its Application to the
		  Verification of Translators},
  crossref = {POPL1994},
  pages = {389--396},
  WKloc = {A-0399},
  abstract = {We develop a calculus in which the computation steps
		  required to execute a computer program can be
		  separated into discrete stages. The claculus,
		  denoted $\lambda_2$, is embedded within the pure
		  untyped $\lambda$-calculus. The main result of the
		  paper is a characterization of sufficient conditions
		  for confluence for terms in the calculus. The
		  condition can be taken as a correctness criterion
		  for translators that perform reductions in one stage
		  leaving residual redexes over for subsequent
		  computation stages. As an application of the theory, we verify
		  the correctness of a macro expansion algorithm. The
		  expansion algorithm is of some interest in its own
		  right since it solves the problem of desired
		  variable capture using only the familiar capture
		  avoiding substitutions.}
}

@Article{Mulvey-1986,
  author = 	 {Mulvey, C.},
  title = 	 {\&},
  journal = 	 {Rend.\null{} Circ.\null{} Mat.\null{} Palermo},
  year = 	 1986,
  volume =	 12,
  pages =	 {99--104},
  keywords = {quantale},
  URL = {http://www.maths.sussex.ac.uk/Staff/CJM/},
  annote = {cited by \cite{Desharnais-Moeller-Struth-2003}
            as source for quantales.},
  bibliographies = {RelMiCS}
}

@InProceedings{Mulvey-1986x,
  author =       "C. J. Mulvey",
  title =        "{\&}",
  booktitle =    "Second Topology Conference",
  series =       "Rendiconti del Circolo Matematico di Palermo, ser.2,
                 supplement no. 12",
  pages =        "99--104",
  year =         "1986",
}

@InProceedings{Mulvey-Pelletier-1991,
  author = 	 {C. J. Mulvey and J.W. Pelletier},
  title = 	 {A Quantisation of the Calculus of Relations},
  booktitle =	 {Category Theory 1991},
  pages =	 {345--360},
  year =	 1992,
  volume =	 13,
  series =	 {CMS Conference Proceedings},
  publisher =	 AMS,
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Article{Muraoka-Yasue-1996,
  author = {Muraoka, Yoichi and Yasue, T.},
  title = {A Compiling Technique for Dataflow Machines --- New Algorithm for Optimum Translation from Control Flow Graph into Dataflow Graph},
  journal = {Systems and Computers in Japan},
  ISSN = {0882-1666},
  year = 1996,
  volume = 27,
  number = 4
}

@Article{Murphy-1882,
  author = {J. J. Murphy},
  title = {On the Addition and Multiplication of Logical Relatives},
  journal = MANCHESTER,
  volume = 7,
  number = 27,
  year = 1882,
  pages = {201--224},
  bibliographies = {RelMiCS}
}

@Article{Murray-Rosenthal-1987,
  author = {Murray, Neil V. and Rosenthal, Erik},
  title = {Inference with Path Resolution and Semantic Graphs},
  journal = {Journal of the Association for Computing Machinery},
  year = 1987,
  volume = 34,
  number = 2,
  pages = {225--255},
  abstract = {A graphical representation of quantifier-free predicate
               calculus formulas in negation normal form and a new rule
               of inference that employs this representation are
               introduced. The new rule, path resolution, is an
               amalgamation of resolution and Prawitz analysis. The goal
               in the design of path resolution is to retain some of
               the advantages of both Prawitz analysis and resolution
               methods, and yet to avoid to some extent their
               disadvantages. Path resolution allows Prawitz analysis of
               an arbitrary subgraph of the graph representing a
               formula. If such a subgraph is not large enough to
               demonstrate a contradiction, a path resolvent of the
               subgraph may be generated with respect to the entire
               graph. This generalizes the notions of large inference
               present in hyperresolution, clash-resolution, NC-
               resolution, and UR-resoluton. A class of subgraphs is
               described for which deletion of some of the links
               resolved upon preserves the spanning property.}
}

@InProceedings{Muthukumar-Hermenegildo-1991,
  title = {Combined Determination of Sharing and Freeness of Program
		  Variables through Abstract Interpretation},
  pages = {49--63},
  crossref = {ICLP1991},
  author = {Muthukumar, K. and Hermenegildo, M.}
}

@InProceedings{Myreen-2010,
  author = 	 {Magnus O Myreen},
  title = 	 {Verified Just-In-Time Compiler on {x86}},
  crossref =  {POPL2010},
  PDFURL = 	 {http://www.cl.cam.ac.uk/~mom22/jit/jit.pdf},
  WKloc = {A-1723, doc/pap/BIB},
  OPTpages = 	 {},
  abstract = {This paper presents a method for creating
     formally correct just-in-time (JIT) compilers.
     The tractability of our approach is demonstrated through,
     what we believe is the first,
     verification of a JIT compiler
     with respect to a realistic semantics of self-modifying x86 machine code.
     Our semantics includes a model of the instruction cache.
     Two versions of the verified JIT compiler are presented:
     one generates all of the machine code at once,
     the other one is incremental i.e. produces code on-demand.
     All proofs have been performed inside the HOL4 theorem prover.}
}

@Misc{Naatz-,
  author = {Michael Naatz},
  title = {The Graph of Linear Extensions Revisited},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1219, doc/pap/BIB}
}

@InProceedings{Nadathur-Miller-1988,
  author = {Nadathur, Gopalan and Miller, Dale},
  title = {An Overview of {$\lambda$-Prolog}},
  booktitle = {Logic Programming: Proceedings of the 5th
		  International Conference and Symposium},
  year = 1988,
  editor = {Kowalski and Bowen},
  pages = {810--827},
  publisher = {MIT Press},
  address = {Cambridge, MA}
}

@Manual{Nagayama-Nishihara-Takeyama-2006,
  title = 	 {An {Agda} Tutorial},
  author =	 {Misao Nagayama and Hideaki Nishihara and Makoto Takeyama},
  month =	 {Jan.~19},
  year =	 2006,
  WKloc = 	 {A-1651}
}

@InProceedings{Nagl-1978,
  author = {Manfred Nagl},
  title = {A Tutorial and Bibliographical Survey on Graph Grammars},
  crossref = {GG1978},
  pages = {70--126},
  abstract = {In the first section we make some remarks (without
		  going into any details) about the main application
		  fields of graph grammars to motivate their
		  inveestigation. In section 2 and 3 we give a short
		  and informal overview on most of the approaches for
		  sequential and parallel graph grammars known in
		  literature. In the last part we introduce some of
		  the modifications and extensions enforced by
		  several applications and give some comments on
		  implementation of graph grammars realized so far.}
}

@Book{Nagl-1979,
  UniBwM = {INF700/G16023},
  keywords = {graph grammars, theory, implemetation, application},
  year = 1979,
  title = {Graph-Grammatiken, Theorie, Implementierung, Anwendungen},
  publisher = {Vieweg},
  author = {Manfred Nagl}
}

@InProceedings{Nagl-1993,
  author = {Manfred Nagl},
  title = {Uniform-Modelling in Graph Grammar Specifications},
  crossref = {GTCS93},
  pages = {296--311},
  abstract = {Buiding Integreted Environments in the context of
		  Software Development [Na94a], Data Specification
		  [Cs91], and Computer Integrated Manufacturing
		  [EWM92,SW92] we ideally proceed as follows: (1) We
		  specify the internal behaviour of tools by graph
		  grammars, (2) we edit, analyze, and (in the near
		  future) execute such specifications thereby
		  verifying and prototyping them, (3) we derive
		  efficient components for specified ones by a
		  generator machinery, and (4) we put them into a
		  framework architecture providing for the invariant
		  part of the environment. This paper sketches the
		  experiences we have got by specifying tools of
		  various environments. Doing so we have received
		  modelling knowledge which is not only useful for one
		  example but seems to be generally applicable. This
		  knowledge is on three levels, namely finding
		  reusable spec portions, learning how to write spec
		  portions, and structuring a spec consisting of
		  different components. We call such a generrally
		  applicable approach `uniform modelling'. The paper
		  sketches three ways of uniform modelling in graph
		  grammar specs. Whereas in the first approach we are
		  directly dealing with a gra gra spec, in the second
		  and third approach we are only arguing about
		  structuring gra gra specs. At the end of the paper
		  we give a summary and list some open problems.},
  keywords = {Software development environments, data modelling
		  environments, computer integrated manufacturing,
		  graph grammars, specifying abstract data types,
		  generators, IPSEN, SDE, CIM, PROGRES},
  ACMcats = {D.2.6, H.2, F.4.2, D.3.4}
}

@InProceedings{Nagle-Esch-Mineau-1992,
  author = {Timothy E. Nagle and John W. Esch and Guy Mineau},
  title = {A notation for conceptual structure graph matchers},
  booktitle = {Conceptual Strucutres: current Research and Practice},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1992,
  editor = {Timothy E. Nagle and Janice A. Nagle and Laurie L. Gerholz and Peter W. Eklund},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = {Ellis Horwood},
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{ Nanevski-Pfenning-Pientka-2005,
  author = 	 {Aleks Nanevski and Frank Pfenning and Brigitte Pientka},
  title = 	 {Contextual Modal Type Theory},
  crossref =	 {CMTT2005},
  URL = 	 {http://www.cs.cmu.edu/~fp/papers/cmtt05.pdf},
  WKloc = {doc/pap/BIB},
  abstract = 	 {The intuitionistic modal logic of
    necessity is based on the judgmental notion of categorical truth.  In
    this paper we investigate the consequences of relativizing these
    concepts to explicitly specified contexts.  We obtain contextual modal
    logic and its type-theoretic analogue.

    Contextual modal type theory provides an elegant, uniform foundation
    for understanding meta-variables and explicit substitutions.
    Moreover, it has applications to staged computation and run-time code
    generation where it serves as a foundation to justify and reason about
    open code.}
}

@Article{Narain-1986,
  year = 1986,
  volume = 3,
  title = {A Technique for Doing Lazy Evaluation in Logic},
  pages = {259--276},
  journal = {J. Logic Programming},
  author = {Sanjai Narain}
}

@InProceedings{Naumann-1994,
  author = {D.A. Naumann},
  title = {Predicate Transformer Semantics of an {Oberon}-like Language},
  crossref = {PROCOMET94},
  pages = {460--480},
  keywords = {Semantics of Programming Languages; Specifying and
		  Verifiying and Reasoning about Programs;
		  Object-oriented Programming}
}

@incollection {Naumowicz-Kornilowicz-2009,
   author = {Naumowicz, Adam and Korni{\l}owicz, Artur},
   affiliation = {University of Białystok Institute of Informatics Poland},
   title = {A Brief Overview of {{\sc Mizar}}},
   crossref = {TPHOL2009},
   pages = {67--72},
   DOIURL = {http://dx.doi.org/10.1007/978-3-642-03359-9_5},
   DOI = {10.1007/978-3-642-03359-9_5},
   abstract = {Mizar is the name of a formal language derived from informal mathematics and computer software that enables proof-checking of texts written in that language. The system has been actively developed since 1970s, growing into a popular proof assistant accompanied with a huge repository of formalized mathematical knowledge. In this short overview, we give an outline of the key features of the Mizar language, the ideas and theory behind the system, its main applications, and current development.},
   year = {2009}
}

@InProceedings{Naylor-Axelsson-Runciman-2007,
  author = 	 {Matthew Naylor and Emil Axelsson and Colin Runciman},
  title = 	 {A Functional-Logic Library for Wired},
  crossref =  {Haskell2007},
  pages = 	 {37--48},
  bibligraphies = {PMC},
  WKloc = {doc/pap/BIB},
  PDFURL = {http://tinyurl.com/2mw6jo},
  URL = {http://www.cs.chalmers.se/~emax/wired/},
  abstract = {We develop a Haskell library for functional-logic
                  programming, motivated by the implementation of
                  Wired, a relational embedded domain-specific
                  language for describing and analysing digital
                  circuits at the VLSI-layout level. Compared to a
                  previous library for logic programming by Claessen
                  and Ljungl\"of, we support residuation, easier
                  creation of logical data types, and pattern
                  matching. We discuss other applications of our
                  library, including test-data generation, and various
                  extensions, including lazy narrowing.}
}

@TechReport{Nebel-1991,
  title = {Belief Revision and Default Reasoning: Syntax-Based
		  Approaches},
  author = {Bernhard Nebel},
  year = 1991,
  type = {Research Report},
  number = {RR-91-11},
  note = {A shorter version of this paper was published in: J. A.
                  Allen, R. Fikes, and E. Sandewall (eds.), {\em
		  Principles of Knowledge Representation and
		  Reasoning:  Proceedings of the Second International
		  Conference}, Morgan Kaufmann, San Mateo, CA, 1991, 417--428.},
  month = APR,
  institution = DFKI,
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  bibliographies = {RelMiCS},
  abstract = {Belief revision leads to temporal nonmonotonicity,
		  i.e., the set of beliefs does not grow monotonically
		  with time.  Default reasoning leads to logical
		  nonmonotonicity, i.e., the set of consequences does
		  not grow monotonically with the set of premises. The
		  connection between these forms of nonmonotonicity
		  will be studied in this paper focusing on
		  syntax-based approaches.  It is shown that a general
		  form of syntax-based belief revision corresponds to
		  a special kind of partial meet revision in the sense
		  of the theory of epistemic change, which in turn is
		  expressively equivalent to some variants of logics
		  for default reasoning.  Additionally, the
		  computational complexity of the membership problem
		  in revised belief sets and of the equivalent problem
		  of derivability in default logics is analyzed, which
		  turns out to be located at the lower end of the
		  polynomial hierarchy.}
}

@TechReport{Nebel-Baeckstroem-1991,
  author = {Nebel, Bernhard and Christer B{\"a}ckstr{\"o}m},
  title = {On the Computational Complexity of  Temporal
		  Projection and some Related Problems},
  year = 1991,
  type = {Research Report},
  number = {RR-91-34},
  note = {Also published as Research Report LiTH-IDA-R-91-34,
		  Department of Computer and Information Science,
		  Link{\"o}ping University, Link{\"o}ping, Sweden},
  month = OCT,
  institution = DFKI,
  bibliographies = {RelMiCS},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  abstract = {One kind of temporal reasoning is temporal
		  projection--the computation of the consequences for
		  a set of events. This problem is related to a number
		  of other temporal reasoning tasks such as story
		  understanding, plan validation, and planning.  We
		  show that one particular simple case of temporal
		  projection on partially ordered events turns out to
		  be harder than previously conjectured. However,
		  given the restrictions of this problem, planning and
		  story understanding are easy.  Additionally, we show
		  that plan validation, one of the intended
		  applications of temporal projection, is tractable
		  for an even larger class of plans.  The incomplete
		  decision procedure for the temporal projection
		  problem that has been proposed by other authors,
		  however, fails to be complete in the case where we
		  have shown plan validation to be tractable.}
}

@TechReport{Nebel-Buerckert-1993,
  abstract = {We introduce a  new   subclass of  Allen's  interval
		  algebra  we  call "ORD-Horn subclass,"  which is a
		  strict superset of  the "pointisable subclass." We
		  prove that  reasoning   in the  ORD-Horn subclass is
		  a polynomial-time problem and show that  the
		  path-consistency method  is sufficient  for deciding
		  satisfiability.   Further, using an extensive
		  machine-generated case analysis, we show that the
		  ORD-Horn subclass is a maximal tractable  subclass
		  of the full algebra  (assuming P <> NP). In  fact,
		  it is the  unique  greatest tractable  subclass
		  amongst  the subclasses that contain all basic relations.},
  year = 1993,
  type = {Research Report},
  title = {Reasoning about Temporal Relations: A Maximal
		  Tractable Subclass of {A}llen's Interval Algebra},
  number = {RR-93-11},
  month = MAR,
  institution = DFKI,
  author = {Bernhard Nebel and Hans-J{\"u}rgen B{\"u}rckert},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Nebel-Koehler-1992,
  abstract = {The ability of a planner to modify a plan is considered
		  as a valuable tool for improving efficiency of
		  planning by avoiding the repetition of the same
		  planning effort.  From a computational complexity
		  point of view, however, it is by no means obvious
		  that modifying a plan is computationally as easy as
		  planning from scratch if the modification has to
		  follow the principle of ``conservatism,'' i.e., to
		  reuse as much of the old plan as possible.  Indeed,
		  considering propositional STRIPS planning, it turns
		  out that conservative plan modification is as hard
		  as planning and can sometimes be harder than plan
		  generation. Furthermore, this holds even if we
		  consider modification problems where the old and the
		  new goal specification are similar.  We put these
		  results into perspective and discuss the
		  relationship to existing plan modification systems.
		  Although sometimes claimed otherwise, these systems
		  do not address the modification problem, but use a
		  non-conservative form of plan modification as a
		  heuristic technique.},
  year = 1992,
  type = {Research Report},
  title = {Plan Modification versus Plan Generation: A
		  Complexity-Theoretic Perspective},
  number = {RR-92-48},
  month = NOV,
  institution = DFKI,
  author = {Nebel, Bernhard and Jana Koehler},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Nebel-Smolka-1991,
  abstract = {Research in knowledge representation has led to the
		  development of so-called terminological logics, the
		  purpose of which is to support the representation of
		  the conceptual and terminological part of Artificial
		  Intelligence applications.  Independently, in
		  computational linguistics, so-called feature logics
		  have been developed which are aimed at representing
		  the semantic and syntactic information natural
		  language sentences convey.  Since both of these
		  logics rely mainly on attributes as the primary
		  notational primitives for representing knowledge,
		  they can be jointly characterized as attributive
		  description formalisms.

                  Although the intended applications for
		  terminological logics and feature logics are not
		  identical, and the computational services of systems
		  based on the respective formalisms are quite
		  different for this reason, the logical foundations
		  turn out to be very similar -- as we pointed out
		  elsewhere.  In this paper, we will show how
		  attributive description formalisms relate to ``the
		  rest of the world.'' Recently, a number of formal
		  results in the area of attributive description
		  formalisms have been obtained by exploiting other
		  research fields, such as formal language theory,
		  automata theory, and modal logics.  This connection
		  between these different fields of formal research
		  will be highlighted in the sequel.},
  year = 1991,
  type = {Research Report},
  title = {Attributive Description Formalisms and the Rest of
		  the World},
  number = {RR-91-15},
  note = {Published in: O. Herzog and C.-R. Rollinger, Text
		  Understanding in {LILOG}, Springer-Verlag, Berlin,
		  Heidelberg, New York, 1991, 439--452},
  institution = DFKI,
  author = {Bernhard Nebel and Gert Smolka},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11,
		  Germany},
  bibliographies = {RelMiCS}
}

@InProceedings{Necula-1997,
  author = {George C. Necula},
  title = {Proof-Carrying Code},
  crossref = {POPL1997},
  pages = {106--119},
  DOI = {http://doi.acm.org/10.1145/263699.263712},
  abstract = {This paper describes Proof-Carrying Code from a more formal
      and language-theoretic perspective. On the application side, this
      paper describes the use of PCC as a basis for verifying the type
      safety of hand-optimized assembly language programs. This is useful
      for the safe interaction of native code libraries with code written
      in a type-safe language.},
  WKloc = {A-0667},
  bibliographies = {Coconut}
}

@InProceedings{Necula-2000,
  author = 	 {George C. Necula},
  title = 	 {Translation validation for an optimizing compiler},
  crossref =  {PLDI2000},
  pages = 	 {83--95},
  DOI = 	 {http://doi.acm.org/10.1145/349299.349314},
  abstract = 	 {We describe a translation validation infrastructure
                  for the GNU C compiler. During the compilation the
                  infrastructure compares the intermediate form of the
                  program before and after each compiler pass and
                  verifies the preservation of semantics. We discuss a
                  general framework that the optimizer can use to
                  communicate to the validator what transformations
                  were performed. Our implementation however does not
                  rely on help from the optimizer and it is quite
                  successful by using instead a few heuristics to
                  detect the transformations that take place. The main
                  message of this paper is that a practical
                  translation validation infrastructure, able to check
                  the correctness of many of the transformations
                  performed by a realistic compiler, can be
                  implemented with about the effort typically required
                  to implement one compiler pass. We demonstrate this
                  in the context of the GNU C compiler for a number of
                  its optimizations while compiling realistic programs
                  such as the compiler itself or the Linux kernel. We
                  believe that the price of such an infrastructure is
                  small considering the qualitative increase in the
                  ability to isolate compilation errors during
                  compiler testing and maintenance.}
}

@InProceedings{Necula-Lee-1996,
  author = {George C. Necula and Peter Lee},
  title = {Safe Kernel Extensions Without Run-Time Checking},
  booktitle = {Proc.\null{} Symposium on Operating System Design and Implementation, {OSDI '96}},
  year = 1996,
  month = OCT,
  abstract = {This paper received the "Best Paper Award" at OSDI'96. This
      is the gentler introduction to Proof-Carrying Code and its
      applications in systems. As a case study, we analyze in this paper
      the use of PCC for ensuring the safety of network packet filters. We
      show that PCC can be used even for hand-optimized packet filters
      written in assembly language. As a result the runtime performance of
      PCC packet filters is the best attainable on a given architecture.
      Measurements show that this approach yields filters that are about an
      order of magnitude faster than interpreted Berkeley Packet Filters
      and about 30\% faster than filters "sandboxed" filters using Software
      Fault Isolation. The cost of PCC filters lies in proof checking.
      Measurements show that proof-checking is fast and its one-time cost
      is usually amortized over a few thousand network packets.},
  WKloc = {A-1089, doc/pap/BIB},
  bibliographies = {Coconut}
}

@InProceedings{Necula-McPeak-Weimer-2002,
  author = {George C. Necula and Scott McPeak and Westley Weimer},
  title = {{CCured}: Type-Safe Retrofitting of Legacy Code},
  crossref = {POPL2002},
  OPTpages = {},
  WKloc = {A-1300, doc/pap/BIB},
  bibliographies = {SQRL},
  CiteSeer = {http://citeseer.nj.nec.com/necula02ccured.html},
  abstract = {In this paper we propose a scheme that combines type
      inference and run-time checking to make existing C programs type
      safe. We describe the CCured type system, which extends that of C by
      separating pointer types according to their usage. This type system
      allows both pointers whose usage can be veried statically to be type
      safe, and pointers whose safety must be checked at run time. We prove
      a type soundness result and then we present a surprisingly simple
      type inference algorithm that is able...},
  bibliographies = {Coconut, OPG}
}

@Book{Nederpelt-Geuvers-deVrijer-1994,
  editor =	 {R. P. Nederpelt and J. H. Geuvers and de Vrijer, R. C.},
  title = 	 {Selected Papers on {Automath}},
  publisher = 	 {Elsevier},
  year = 	 1994,
  volume =	 133,
  series =	 {Studies in logic and the foundations of mathematics},
  McMaster = 	 {QA 267.3 .S45 1994}
}

@Article{Negri-vonPlato-2004,
 author = {Negri, Sara and von Plato, Jan},
 title = {Proof systems for lattice theory},
 journal = MSCS,
 volume = {14},
 issue = {04},
 month = {8},
 year = {2004},
 issn = {1469-8072},
 pages = {507--526},
 numpages = {20},
 DOI = {10.1017/S0960129504004244},
 JournalURL = {http://journals.cambridge.org/article_S0960129504004244},
 abstract = {A formulation of lattice theory
   as a system of rules added to sequent calculus is given.
   The analysis of proofs for the contraction-free calculus
   of classical predicate logic known as G3c
   extends to derivations with the mathematical rules of lattice theory.
   It is shown that minimal derivations of quantifier-free sequents
   enjoy a subterm property:
   all terms in such derivations are terms in the endsequent.
   An alternative formulation of lattice theory
   as a system of rules in natural deduction style is given,
   both with explicit meet and join constructions
   and as a relational theory with existence axioms.
   A subterm property for the latter extends the standard decidable classes
   of quantificational formulas of pure predicate calculus to lattice theory.}
}

@Article{Nelson-1989,
  author = {G. Nelson},
  title = {A Generalization of {Dijkstra's} Calculus},
  journal = ACM-TOPLAS,
  year = 1989,
  volume = 11,
  number = 4,
  pages = {517--561},
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/69558.69559},
  abstract = {Dijsktra's calculus of guarded commands can be generalized
     and simplified by dropping the law of the excluded miracle.
     This paper gives a self-contained account of the
     generalized calculus from first principles through the
     semantics of recursion.
     The treatment of recursion uses the fixpoint method
     from denotational semantics.
     The paper relies only on the algebraic properties of predicates;
     individual states are not mentioned (except for motivation).
     To achieve this,
     we apply the correspondence between programs and predicates
     that underlies predicative programming.
     The paper is written from the axiomatic semantic point of view,
     but its contents can be described
     from the denotational semantic point of view roughly as follows:
     The Plotkin-Apt correspondence between wp semantics
     and the Smyth powerdomain is extended to a correspondence
     between the full wp/wlp semantics and the
     Plotkin powerdomain extended with the empty set.}
}

@Article{Nelson-1997,
  author = {Theodore Holm Nelson},
  title = {Embedded Markup Considered Harmful},
  journal = {World Wide Web Journal},
  year = 1997,
  volume = 2,
  number = 4,
  pages = {129--134},
  OPTnote = {},
  UniBwM = {INF600/YD5629},
  WKloc = {A-0678},
  OPTabstract = {},
  OPTcontents = {},
  annote = {ISBN: {1-56592-349-9}}
}

@Book{Nemeti-1985,
  author = {Istv\'an N{\'e}meti},
  title = {Logic with 3 variables has G\"odel's Incompleteness
		Property---Thus Free Cylindric Algebras are not Atomic},
  note = {Preprint No.\null{} 49/85},
  publisher = HUNGAR_MI,
  address = {Budapest},
  year = 1985,
  bibliographies = {RelMiCS}
}

@Book{Nemeti-1986,
  author = {Istv\'an N{\'e}meti},
  title = {Free Algebras and Decidability in Algebraic Logic},
  note = Doct,
  publisher = HUNGAR_AS,
  address = {Budapest},
  year = 1986,
  bibliographies = {RelMiCS}
}

@Article{Nemeti-1986a,
  author = {Istv\'an N{\'e}meti},
  title = {A Non-representable Cylindric Algebra with
		Pairing Functions},
  journal = ALGU,
  year = 1986,
  volume = 22,
  pages = {117--119},
  bibliographies = {RelMiCS}
}

@Article{Nemeti-1987,
  author = {Istv\'an N{\'e}meti},
  title = {Decidability of Relation Algebras with Weakened
		Associativity},
  journal = PROAMS,
  volume = 100,
  number = 2,
  year = 1987,
  month = JUN,
  pages = {340--344},
  bibliographies = {RelMiCS}
}

@Article{Nemeti-1987a,
  author = {Istv\'an N{\'e}meti},
  title = {On Varieties of Cylindric Algebras
		with Applications to Logic},
  journal = ANPURE,
  volume = 36,
  year = 1987,
  pages = {235--277},
  bibliographies = {RelMiCS}
}

@Unpublished{Nemeti-1991,
  author = {Istv\'an N{\'e}meti},
  title = {Algebraizations of Quantifier Logics, an Introductory Overview},
  note = {{{$10^{th}$} Version}},
  OPTkey = {},
  year = 1991,
  month = OCT,
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Unpublished{Nemeti-199x,
  author = {Istv\'an N{\'e}meti},
  title = {On Cylindric Algebraic Model Theory},
  note = {?},
  OPTkey = {},
  year = {199x},
  OPTmonth = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Nentwich-Capra-Emmerich-Finkelstein-2003,
  author = {Christian Nentwich and L. Capra and Wolfgang Emmerich and Anthony Finkelstein},
  title = {xlinkit: A Consistency Checking and Smart Link Generation Service},
  journal = {ACM Transactions on Internet Technology},
  year = 2003,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {to appear},
  WKloc = {A-1440, doc/pap/BIB},
  OPTannote = {}
}

@InProceedings{Nentwich-Emmerich-Finkelstein-2001,
  author = {Christian Nentwich and Wolfgang Emmerich and Anthony Finkelstein},
  title = {Static Consistency Checking for Distributed Specifications},
  crossref = {ASE2001},
  OPTpages = {},
  URL = {http://www.xlinkit.com/},
  WKloc = {A-1439, doc/pap/BIB},
  abstract = {Software engineers building a complex system
     make use of a number of informal and semi-formal notations.
     We describe a framework, xlinkit, for managing the consistency
     of development artifacts expressed in such notations.
     xlinkit supports distributed software engineering
     by providing a distribution-transparent language
     for expressing constraints between specifications.
     It specifies a semantics for those constraints that permits
     the generation of hyperlinks between inconsistent elements.
     We give a formal semantics for link generation,
     and show how we expressed the rules of the UML Foundation/Core modules
     in our language. We outline how we implemented xlinkit
     as a light-weight web service using open standard technology
     and present the results of an evaluation
     against several sizeable UML models provided by industrial partners.}
}

@Article{Nentwich-Emmerich-Finkelstein-Ellmer-2003,
  author = {Christian Nentwich and Wolfgang Emmerich and Anthony Finkelstein and Ernst Ellmer},
  title = {Flexible Consistency Checking},
  journal = {???},
  year = 2003,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {submitted},
  WKloc = {A-1441, doc/pap/BIB},
  OPTannote = {}
}

@InProceedings{Nesi-dePaiva-Ritter-1993,
  author = {M. Nesi and de Paiva, V. and E. Ritter},
  title = {Rewriting properties of combinators for
		  intuitionistic linear logic},
  crossref = {HOA1993},
  pages = {256--275},
  WKloc = {A-0115},
  abstract = {In this paper we investigate the possibility of
		  developing a (semi-)automatic rewriting tool for
		  manipulating and reasoning about combinators for
		  intuitionistic linear logic. In particular, we
		  develop a canonical (i.e.\ confluent and
		  terminating) term rewriting system associated to a
		  theory of categorical combinators for (rudimentary)
		  linear logic. In order to do that, we make use of
		  the Knuth-Bendix completion algorithm to transform
		  the equational theory for the combinators into an
		  equivalent canonical rewrite system. This means that
		  a set of categorical combinators for linear logic
		  has first to be derived, and then the resulting
		  system of combinators can be checked for rewriting
		  properties using rewriting techniques.},
  bibliographies = {RelMiCS}
}

@InProceedings{Neubauer-Thiemann-2003,
  author = {Matthias Neubauer and Peter Thiemann},
  title = {Discriminative Sum Types Locate the Source of Type Errors},
  crossref = {ICFP2003},
  OPTpages = {},
  WKloc = {A-1476, doc/pap/BIB},
  URL = {http://www.informatik.uni-freiburg.de/~neubauer/papers/icfp03.pdf},
  abstract = {We propose a type system for locating the source of type
    errors in an applied lambda calculus with ML-style polymorphism. The
    system is based on discriminative sum types-known from work on soft
    typing-with annotation subtyping and recursive types. This way, type
    clashes can be registered in the type for later reporting. The
    annotations track the potential producers and consumers for each value
    so that clashes can be traced to their cause.

    Every term is typeable in our system and type inference is
    decidable. A type derivation in our system describes all type errors
    present in the program, so that a principal derivation yields a
    principal description of all type errors present. Error messages are
    derived from completed type derivations. Thus, error messages are
    independent of the particular algorithm used for type inference,
    provided it constructs such a derivation.}
}

@InCollection{Neumann-1977,
  author = {Peter M. Neumann},
  title = {Finite Permutation Groups, Edge-colored Graphs and Matrices},
  booktitle = {Topics in Group Theory and Computation},
  note = {edited by Michael P. J. Curran},
  publisher = Academic,
  year = 1977,
  bibliographies = {RelMiCS}
}

@Article{Newman-1942,
  author = {M.H.A. Newman},
  title = {On Theories with a Combinatorial Definition of
		  ``Equivalence''},
  journal = {Annals of Mathematics},
  year = 1942,
  volume = 43,
  number = 2,
  pages = {223--243},
  bibliographies = {FP}
}

@Book{Newton-1980,
  author = {W. H. Newton-Smith},
  title = {The Structure of Time},
  publisher = Routledge,
  year = 1980,
  bibliographies = {RelMiCS}
}

@Article{Ng-1977,
  author = {Kan Ching Ng},
  title = {The {Cantor-Bernstein} Theorem and Related Results in a
		Relation Algebraic Setting},
  journal = NOTIC,
  volume = 24,
  year = 1977,
  pages = {A-30, A-304},
  bibliographies = {RelMiCS}
}

@PhDThesis{Ng-1984,
  author = {Kan Ching Ng},
  title = {Relation Algebras with Transitive Closure},
  note = Doct,
  school = BERKELEY,
  address = {Berkeley},
  year = 1984,
  pages = {iv+157},
  bibliographies = {RelMiCS}
}

@Article{Ng-Tarski1977,
  author = {Kan Ching Ng and Alfred Tarski},
  title = {Relation Algebras with Transitive Closure},
  journal = NOTIC,
  volume = 24,
  year = 1977,
  pages = {A-29},
  bibliographies = {RelMiCS}
}

@PhDThesis{Nguyen-1988,
  author = {Thanh Tung  Nguyen},
  title = {Multi-Valued Function Theory for Computer Programming},
  school = {Univ.\ Cath.\ de Louvain},
  year = 1988,
  address = {Belgium},
  bibliographies = {RelMiCS}
}

@Article{Nguyen-1991,
  author = {Thanh Tung Nguyen},
  title = {A Relational Model of Demonic Nondeterministic Programs},
  journal = IJFCS,
  volume = 2,
  number = 2,
  year = 1991,
  pages = {101--131},
  bibliographies = {RelMiCS}
}

@TechReport{Nguyen-1992,
  author = {Thanh Tung Nguyen},
  title = {The Connection between Predicate Logic and Demonic
                  Relation Calculus},
  institution = {Centre de Recherche en Informatique de Nancy},
  year = 1992,
  number = {CRIN 92-R-187},
  month = NOV,
  bibliographies = {RelMiCS}
}

@TechReport{Nguyen-1995,
  author = {Thanh Tung Nguyen},
  title = {Duality between Relations and Predicate Transformers},
  institution = {SIGRAPA},
  year = 1995,
  number = {SIGRAPA/INFO/RR.95-01},
  address = {Kraainen, Belgium},
  month = MAY,
  bibliographies = {RelMiCS}
}

@Book{Nicholls-Martin-Wallace-1995,
  author = {John G. Nicholls and A. Robert Martin and Bruce G. Wallace},
  title = {{Vom Neuron zum Gehirn}},
  publisher = {Gustav Fischer Verlag},
  year = 1995,
  address = {Stuttgart, Jena, New York},
  note = {Orig.: ``From Neuron to Brain, A Cellular and
		  Molecular Approach to the Function of the Nervous
		  System'' Third Edition, 1992, Sinauer Associates,
		  Inc., Sunderland, Massachussetts 01375},
  UniBwM = {MED100/YA11960}
}

@Book{Nichols-Buttlar-Farrell-1998,
  author = {Bradford Nichols and Dick Buttlar and Jacqueline Proulx Farrell},
  title = {Pthreads Programming},
  publisher = {O'Reilly},
  year = 1998,
  ISBN = {1-56592-115-1},
  UniBwM = {INF400/YD5483}
}

@InProceedings{Nickel-Niere-Zuendorf-2000,
  author = {Nickel, Ulrich and Niere, J\"{o}rg and Z\"{u}ndorf, Albert},
  title = {The {FUJABA} environment},
  OPTbooktitle = {Proceedings of the 22nd International Conference on Software Engineering},
  booktitle = {ICSE 2000},
  OPTseries = {ICSE '00},
  year = {2000},
  isbn = {1-58113-206-9},
  location = {Limerick, Ireland},
  pages = {742--745},
  numpages = {4},
  DOIURL = {http://doi.acm.org/10.1145/337180.337620},
  DOI = {10.1145/337180.337620},
  publisher = {ACM},
  address = {New York, NY, USA},
  abstract = {However, a single collaboration diagram is usually not expressive enough to model complex operations performing several modifications at different parts of the overall object structure. Such series of modifications need several collaboration diagrams to be modeled. In addition, there may be different situations where certain collaboration diagrams should be executed and others not. Thus, we need additional control structures to control the execution of collaboration diagrams. In our approach we combine collaboration diagrams with statecharts and activity diagrams for this purpose. This means, instead of just pseudo code, any state or activity may contain a collaboration diagram modeling the do-action of this step.Figure 1 illustrates the main concepts of Fujaba. Fujaba uses a combination of statecharts and collaboration diagrams to model the behavior of active classes. A combination of activity diagrams and collaboration diagrams models the bodies of complex methods. This integration of class diagrams and UML behavior diagrams enables Fujaba to perform a lot of static analysis work facilitating the creation of a consistent overall specification. In addition, it turns these UML diagrams into a powerful visual programming language and allows to cover the generation of complete application code. During testing and maintenance the code of an application may be changed on the fly, e.g. to fix small problems. Some application parts like the graphical user interface or complex mathematical computations may be developed with other tools. In cooperative (distributed) software development projects some developers may want to use Fujaba, others may not. Code of different developers may be merged by a version management tool. There might already exist a large application and one wants to use Fujaba only for new parts. One may want to do a global search-and-replace to change some text phrases. One may temporarily violate syntactic code structures while she or he restructures some code. For all these reasons, Fujaba aims to provide not just code generation but also the recovery of UML diagrams from Java code. One may analyse (parts of) the application code, recover the corresponding UML diagram (parts), modify these diagram (parts), and generate new code (into the remaining application code). So far, this works reasonable for class diagrams and to some extend for the combination of activity and collaboration diagrams. For statecharts this is under development.The next chapters outline the (forward engineering) capabilities of Fujaba with the help of an example session.}
}

@InProceedings{Nicolas-1978,
  author = {J. M. Nicolas},
  title = {Mutual Dependencies and some Results on Undecomposable
      Relations},
  booktitle = {{$4^{th}$} Internat.\null{}  Conf.\null{} on Very Large Data Bases},
  year = 1978,
  month = sep,
  address = {Berlin},
  pages = {360--367},
  bibliographies = {RelMiCS}
}

@TechReport{Niehren-Podelski-Treinen-1993,
  author = {Joachim Niehren and Andreas Podelski and Ralf Treinen},
  title = {Equational and Membership Constraints for Infinite Trees},
  year = 1993,
  type = {Research Report},
  number = {RR-93-14},
  month = APR,
  institution = DFKI,
  filename = {RR-93-14.ps.Z},
  DIRECTORY = {dfki-saarbruecken},
  abstract = {We  present  a new constraint system with equational
		  and membership constraints over infinite trees.  It
		  provides for complete and correct satisfiability and
		  entailment tests and  is therefore suitable for the
		  use in concurrent constraint programming   systems
		  which are based  on cyclic data structures.

                  Our  set defining devices  are  greatest  {\em
		  fixpoint solutions}  of regular systems of
		  equations with a deterministic  form of union.  As
		  the main technical particularity of the algorithms
		  we present a novel memorization   technique.  We
		  believe  that  both satisfiability  and entailment
		  tests can be   implemented in an efficient  and
		  incremental manner.},
  bibliographies = {RelMiCS}
}

@InProceedings{Niehren-Smolka-1994,
  author = {Joachim Niehren and Gert Smolka},
  title = {A Confluent Relational Calculus for Higher-Order
		  Programming with Constraints},
  crossref = {CCL94},
  pages = {89--104},
  abstract = {We present the $\rho$-calculus, a relational
		  calculus parameterized with a logical constraint
		  system. The $rho$-calculus provides for higher-order
		  relational programming with first-order constraints,
		  and subsumes higher-order functional programming as
		  a special case. It captures important aspects of the
		  concurrent constraint programming language Oz. $\ldots$},
  bibliographies = {RelMiCS}
}

@Article{Nielson-Nielson-1990,
  WKloc = {A-0053},
  year = 1990,
  volume = 70,
  title = {Functional Completeness of the Mixed $\lambda$-Calculus and Combinatory Logic},
  pages = {99-126},
  journal = {Theoretical Computer Science},
  author = {Hanne Riis Nielson and Flemming Nielson}
}

@InProceedings{Nielson-Nielson-1990a,
  author = {Hanne Riis Nielson and Flemming Nielson},
  title = {Eureka Definitions for Free! {\small or}
		  Disagreement Points for Fold/Unfold Transformations},
  crossref = {ESOP1990},
  pages = {291--305},
  abstract = {The fold/unfold framework of Burstall and Darlington
		  is a very powerful framework for transforming
		  function definitions in the form of recursive
		  equation schemes. This may be used to transform a
		  function so as to improve the efficiency of its
		  implementation. However, for this to work the user
		  must supply so-called Eureka definitions and it may
		  require some ingenuity to construct these. This
		  paper shows that a class of these Eureka definitions
		  can be derived in a rather systematic way.},
  annote = {--- HOPSnotes ---
                  should be accessible to transformation programming}
}

@InProceedings{Nielson-Nielson-1992,
  author = {Flemming Nielson and Hanne Riis Nielson},
  title = {The Tensor Product in Wadler's Analysis of Lists},
  crossref = {ESOP1992},
  pages = {351--370},
  authorsAddress = {Aarhus University, fnielson\@daimi.aau.dk},
  abstract = {We consider abstract interpretation (in particular
		  strictness analysis) for pairs and lists. We begin
		  by reviewing the well-known fact that the best known
		  description of a pair of elements is obtained using
		  the tensor product rather than the cartesian
		  product. We next present a generalisation of
		  Wadler's strictness analysis for lists using the
		  notion of open set. Finally, we illustrate the
		  intimate connection between the case analysis
		  implicit in Wadler's strictness analysis and the
		  precision that the tensor product allows for
		  modelling the inverse cons operation.}
}

@Book{Nielson-Nielson-1992a,
  ISBN = {0-521-40384-7},
  year = 1992,
  volume = 34,
  title = {Two-Level Functional Languages},
  series = {Cambridge tracts in theoretical computer science},
  publisher = {Cambridge University Press},
  author = {Flemming Nielson and Hanne Riis Nielson},
  annote = {tubibmue}
}

@InProceedings{Nielson-Nielson-1992b,
  author = {Hanne Riis Nielson and Flemming Nielson},
  title = {Bounded Fixed Point Iteration},
  crossref = {POPL1992},
  pages = {71--82},
  note = {(extended abstract)}
}

@InProceedings{Nielson-Nielson-1992c,
  author = {Flemming Nielson and Hanne Riis Nielson},
  title = {Layered Predicates},
  crossref = {REX92},
  pages = {425--456},
  abstract = {We review the concept of logical relations and how
		  they interact with structural induction; furthermore
		  we give examples of their use, and of particular
		  interest is the combination with the PER-idea
		  (partial equivalence relations). This is then
		  generalized to Kripke-logical relations; the major
		  application is to show that in combination with the
		  PER-idea this solves the problem of establishing a
		  substitution property in a manner conducive to
		  structural induction. Finally we introduce the
		  concept of Kripke-layered predicates; this allows a
		  modular definition of predicates and supports a
		  methodology of ``proof in stages'' where each stage
		  focuses on only one aspect and thus is more
		  manageable. All of these techniques have been tested
		  and refined in ``realistic applications'' that have
		  been documented elsewhere.},
  keywords = {logical relations, partial equivalence relations,
		  Kripke-logical relations, layered predicates,
		  Kripke-layered predicates, substitution properties,
		  well-structured proofs, denotational semantics,
		  correctness of code generation, proof principles},
  bibliographies = {RelMiCS, LogRel}
}

@Book{Nielson-Nielson-1992d,
  author = {Hanne Riis Nielson and Flemming Nielson},
  title = {Semantics with Application: A Formal Introduction},
  year = 1992,
  ISBN = {0 471 92980 8},
  publisher = {John Wiley and Sons},
  note = {Revised edition available at
          \textsf{URL: http://www.daimi.au.dk/~hrn/}},
  URL = {http://www.daimi.au.dk/~hrn/},
  WKloc = {A-0815, doc/pap/BIB}
}

@Book{Nielson-Nielson-1999,
  author = {Hanne Riis Nielson and Flemming Nielson},
  title = {Semantics with Application: A Formal Introduction},
  year = 1999,
  edition = {revised edition},
  note = {Available at \textsf{URL: http://www.daimi.au.dk/~hrn/};
       original edition published 1992 by John Wiley and Sons},
  URL = {http://www.imm.dtu.dk/~riis/Wiley_book/wiley.html},
  WKloc = {A-0815, doc/pap/BIB}
}

@Book{Nielson-Nielson-2007,
  author    = {Hanne Riis Nielson and Flemming Nielson},
  title     = {Semantics with Applications: An Appetizer},
  year      = 2007,
  publisher = Springer,
  address   = {London},
  series    = {Undergraduate Topics in Computer Science},
  DOIURL    = {http://dx.doi.org/10.1007/978-1-84628-692-6}
}

@Misc{Niemelae-1998,
  author = {Ilkka Niemel{\"a}},
  title = {Logic Programs with Stable Model Semantics as a Constraint
      Programming Paradigm},
  year = 1998,
  WKloc = {A-0643}
}

@Misc{Niemelae-199X,
  author = {Ilkka Niemel{\"a}},
  title = {Declarative Rule-Based Constraint Programming},
  year = {199?},
  note = {slides},
  WKloc = {A-0643}
}

@InProceedings{Niere-Zuendorf-1999,
  author = {J{\"o}rg Niere and Albert Z{\"u}ndorf},
  title = {Using {FUJABA} for the development of production control systems},
  crossref = {AGTIVE1999},
  pages = {181--191}
}

@Misc{NilssonMarcus-gbdd,
  author =	 {Marcus Nilsson},
  title =	 {{GBDD} --- A package for representing relations with {BDDs}},
  howpublished = {Available from \textsf{http://www.regularmodelchecking.com/}},
  edition =	 {version 0.10},
  year =	 2004
}

@InProceedings{Nilsson-2005,
  author = 	 {Henrik Nilsson},
  title = 	 {Dynamic Optimization for Functional Reactive Programming using Generalized Algebraic Data Types},
  crossref =	 {ICFP2005},
  pages =	 {54--65}
}

@InProceedings{Nilsson-Courtney-Peterson-2002,
  author = {Henrik Nilsson and Antony Courtney and John Peterson},
  title = {Functional reactive programming, continued},
  crossref = {Haskell2002},
  pages = {51--64},
  DOI = {http://doi.acm.org/10.1145/581690.581695},
  URL = {http://www.haskell.org/yale/papers/haskellworkshop02/index.html},
  WKloc = {doc/pap/BIB},
  abstract = {Functional Reactive Programming (FRP) extends a host
      programming language with a notion of time flow. Arrowized FRP (AFRP)
      is a version of FRP embedded in Haskell based on the arrow
      combinators. AFRP is a powerful synchronous dataflow programming
      language with hybrid modeling capabilities, combining advanced
      synchronous dataflow features with the higher-order lazy functional
      abstractions of Haskell. In this paper, we describe the AFRP
      programming style and our Haskell-based implementation. Of particular
      interest are the AFRP combinators that support dynamic collections
      and continuation-based switching. We show how these combinators can
      be used to express systems with an evolving structure that are
      difficult to model in more traditional dataflow languages.}
}

@InProceedings{Nilsson-Fritzson-1992,
  WKloc = {A-0035, ~kahl/doc/pap/ladb.ps},
  authorsAddress = {Link\"oping, henni\@ida.liu.se},
  abstract = {Lazy functional languages have non-strict semantics
		  and are purely declarative, i.e. they support the
		  notion of referential transparency and are devoid of
		  side effects. Traditional debugging techniques are,
		  however, not suited for lazy functional languages
		  since computations generally do not take place in
		  the order one might expect. Since
		  {\em algorithmic debugging} allows the user to
		  concentrate on the declarative aspects of program
		  semantics, and will semi-automatically find
		  functions containing bugs,
		  we propose to use this technique for debugging lazy
		  functional programs. In this paper we present an
		  algorithmic debugger for a lazy functional language
		  and some experience in using it. Because of the
		  non-strict semantics of lazy functional languages,
		  arguments to functions are in general partially
		  evaluated expressions. The user is, however, usually
		  more concerned with the values that these
		  expressions represent. We address this problem by
		  providing the user with a {\em strictified} view of
		  the execution trace whenever possible.},
  title = {Algorithmic Debugging for Lazy Functional Languages},
  pages = {385--399},
  crossref = {PLILP1992},
  author = {Henrik Nilsson and Peter Fritzson}
}

@Article{Nipkow-1986,
  author = {Tobias Nipkow},
  title = {Non-deterministic Data Types: Models and Implementations},
  journal = {Acta Informatica},
  year = 1986,
  volume = 22,
  pages = {629--661},
  WKloc = {A-1306},
  abstract = {The model theoretic basis for (abstract) data types is
               generalized from algebras to multi-algebras in order to
               cope with non-deterministic operations. A programming oriented
               \emph{definition} and a model theoretic \emph{criterion}
               (called simulation) for implementation of data types are given.
               To justify the criterion w.r.t.\null{} the definition,
               an abstract framework linking denotational semantics of
               programming languages and model theory of data types is set up.
               A set of constraints on a programming language semantics
               are derived which guarantee that simulation implies
               implementation. It is argued that any language supporting
               data abstraction does fullfill these constraints.
               As an example a simple but expressive language $L$ is defined
               and it is formally proved that $L$ does conform to these
               restrictions},
  annote = {Independent version of LD-relations.}
}

@InProceedings{Nipkow-1990,
  author = {Tobias Nipkow},
  title = {Higher-Order Unification, Polymorphism, and Subsorts},
  crossref = {CTRS1990},
  pages = {437--447},
  WKloc = {doc/pap/BIB},
  abstract = {This paper analyzes the problems that arise in
		  extending Huet's higher-order unificatin algorithm
		  from the simply typed $\lambda$-calculus to one with
		  type variables. A simple, incomplete, but in
		  practice very useful extension to Huet's algorithm
		  is discussed. This extension takes an abstract view
		  of types. As a particular instance we explore a type
		  system with ML-style polymorphism enriched with a
		  notion of {\em sorts}. Sorts are partially ordered
		  and classify types, thus giving rise to an
		  order-sorted algebra of types. {\em Type classes} in
		  the functional language Haskell can be understood as
		  sorts in this sense. Sufficient conditions on the
		  sort structure to ensure the existence of principal
		  types are discussed. Finally we suggest a new type
		  system for the $\lambda$-calculus which may pave the
		  way to a complete unification algorithm for
		  polymorphic terms.}
}

@InProceedings{Nipkow-1991,
  author = {Tobias Nipkow},
  title = {Higher-Order Critical Pairs},
  crossref = {LICS6},
  pages = {342--349},
  CiteSeer = {http://citeseer.nj.nec.com/context/59649/0}
}

@InProceedings{Nipkow-1991-x,
  author = {Tobias Nipkow},
  title = {Higher-Order Critical Pairs},
  pages = {342--349},
  booktitle = {Logics in Computer Science},
  year = 1991
}

@Unpublished{Nipkow-1992,
  author = {Tobias Nipkow},
  title = {Orthogonal Higher-Order Rewrite Systems are Confluent},
  year = 1992,
  month = {aug},
  note = {Available by ftp, announced in rewriting-list},
  WKloc = {A-0022, ~kahl/doc/pap/nipkow/ohrs.dvi},
  abstract = {Higher-order rewrite systems (HRSs) are rewrite
		  systems over simply typed lambda-terms with
		  restricted left-hand sides. They were introduced in
		  \cite{Nipkow-1991} where it was shown that the notion
		  of critical pairs generalizes to HRSs. This paper is
		  a second step towards a general theory of
		  higher-order rewrite systems. It considers the
		  complementary case, when there are no critical
		  pairs, all rules are left-linear (no free variable
		  appears twice on the left-hand side), and
		  termination is immaterial. Such systems are usually
		  called {\em orthogonal}. The main result of this
		  paper is that all orthogonal HRSs are confluent.}
}

@InProceedings{Nipkow-1995,
  author = {Tobias Nipkow},
  title = {Higher-Order Rewrite Systems},
  pages = {256--256},
  crossref = {RTA95}
}

@Misc{Nipkow-1995a,
  author = {Tobias Nipkow},
  title = {Equational Reasoning},
  howpublished = {ftp://ftp.informatik.tu-muenchen.de/pub/??/nipkow/er.dvi},
  year = 1995,
  month = APR,
  WKloc = {B-0038 (also 1994 version)}
}

@InProceedings{Nipkow-199X,
  author = {Tobias Nipkow},
  title = {Higher-Order Unification, Polymorphism, and Subsorts},
  crossref = {??},
  pages = {436--447},
  OPTabstract = {},
  WKloc = {A-0489}
}

@InProceedings{Nipkow-2002,
  author = {Tobias Nipkow},
  title = {Structured Proofs in {Isar/HOL}},
  crossref = {TYPES2002},
  pages = {259--278},
  abstract = {Isar is an extension of the theorem prover Isabelle with a
      language for writing human-readable structured proofs. This paper is
      an introduction to the basic constructs of this language.},
  WKloc = {doc/pap/BIB}
}

@Manual{Nipkow-2012,
  title =     {Programming and Proving in {Isabelle/HOL}},
  author =    {Tobias Nipkow},
  month =     {May 22},
  year =      2012,
  WKloc =     {A-1746},
  annote =    {distributed by Lawrence Paulson at RAMiCS 2012}
}

@Manual{Nipkow-Berghofer-2002a,
  author = {Tobias Nipkow and Stefan Berghofer},
  title = {Fundamental Properties of Lambda Calculus},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1392}
}

@Book{Nipkow-Paulson-Wenzel-2002,
  author = {Tobias Nipkow and Lawrence C. Paulson and Markus Wenzel},
  title = {{Isabelle/HOL} --- A Proof Assistant for Higher-Order Logic},
  publisher = Springer,
  year = 2002,
  volume = 2283,
  series = LNCS,
  WKloc = {A-1382},
  bibliographies = {HHOL}
}

@Manual{Nipkow-Paulson-Wenzel-2002a,
  author = {Tobias Nipkow and Lawrence C. Paulson and Markus Wenzel},
  title = {{Isabelle}'s Logics: {HOL}},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1387},
  bibliographies = {HHOL}
}

@InProceedings{Nipkow-Prehofer-1993,
  author = {Tobias Nipkow and Christian Prehofer},
  title = {Type Checking Type Classes},
  pages = {409--418},
  abstract = {We study the type inference problem for a system with type
             classes as in the functional programming language Haskell. Type
             classes are an extension of ML-style polymorphism with
             overloading. We generalize Milner's work on polymorphism by
             introducing a separate context constraining the type variables
             in a typing judgement. This leads to simple type inference
             systems and algorithms which closely resemble those for ML. In
             particular we present a new unification algorithm which is an
             extension of syntactic unification with constraint solving. The
             existence of principal types follows from an analysis of this
             unification algorithm.},
  crossref = {POPL1993},
  WKloc = {A-0200}
}

@InProceedings{Nishizaki-1991,
  title = {Programs with Continuations and Linear Logic},
  author = {Shin-ya Nishizaki},
  pages = {513--531},
  crossref = {TACS1991},
  abstract = {A programming language with continuations is studied in
		  the framework of Girard's linear logic.  The
		  execution of a program with continuations is in
		  general {\em non-deterministic\/}: the result of
		  computation depends on the evaluation strategy,
		  e.g., call-by-value evaluation, call-by-name
		  evaluation, \dots, etc.  In this paper, we first
		  introduce $\lambda^\rightarrow_c$, a programming
		  language with continuations, and then define the
		  translation from $\lambda^\rightarrow_c$ to linear
		  logic, which eleminates the non-determinism of
		  $\lambda^\rightarrow_c$.  The relation between
		  computation of $\lambda^\rightarrow_c$ and
		  normalization of linear logic is also shown.}
}

@Booklet{Nissen-1991,
  year = 1991,
  title = {Formrepr"asentation von {O}bjekten
           mit gekr"ummten {K}anten},
  note = {UniBwM ID 29/91},
  month = {Oktober},
  howpublished = {Diplomarbeit},
  author = {Nissen, Oliver},
  annote = {Kurven 2. Ordnung mit CAD-Editor},
  address = {Neubiberg}
}

@InProceedings{Nitta-Seki-2003,
  author = 	 {Naoya Nitta and Hiroyuki Seki},
  title = 	 {An Extension of Pushdown System and Its Model Checking Method},
  crossref =  {CONCUR2003},
  pages =	 {281--295},
  WKloc = 	 {doc/pap/BIB},
  abstract = {In this paper, we present a class of infinite
     transition systems which is an extension of pushdown systems
     (PDS), and show that LTL (linear temporal logic) model checking
     for the class is decidable. Since the class is defined as a
     subclass of term rewriting systems, pushdown stack of PDS is
     naturally extended to tree structure. By this extension, we can
     model recursive programs with exception handling.}
}

@InProceedings{Nivat-1975,
  year = 1975,
  booktitle = {Convegni del Feb.\null{} e dell` Apr. del 1973},
  title = {On the Interpretation of Recursive Polyadic Program Schemes},
  publisher = Academic,
  pages = {255--281},
  organization = {Istituto Na\-zi\-o\-na\-le di Alta Matematica},
  series = SymposiaMat,
  volume = 15,
  author = {Maurice Nivat},
  address = {London},
  bibliographies = {RelMiCS}
}

@Misc{Niwinski-Walukiewicz-199X,
  author = {Damian Niwi{\'n}ski and Igor Walukiewicz},
  title = {Games for the $\mu$-calculus},
  year = {199?},
  WKloc = {A-0641}
}

@TechReport{Noble-Runciman-1994,
  author = {Rob Noble and Colin Runciman},
  title = {Functional Languages and Graphical User Interfaces
		  --- a review and a case study},
  institution = {York University},
  year = 1994,
  file = {~kahl/doc/pap/york/YCS-94-223.ps},
  number = {YCS-94-223},
  month = FEB,
  WKloc = {B-0021},
  abstract = {At first sight, I/O in a pure functional language is
		  not as straightforward as in imperative
		  languages. For some years work has been going on to
		  alleviate these problems, and there are now a number
		  of different approaches. The purpose of this report
		  is twofold --- firstly we shall review the problems
		  encountered in performing I/O in a functional
		  language and look at some of the ways these might be
		  conquered, and secondly we shall look at some more
		  recent solutions to I/O whihc encompass graphical
		  interfaces.}
}

@InProceedings{Noecker-Smetsers-vanEekelen-Plasmeijer-1991,
  WKloc = {?},
  abstract = {?},
  title = {Concurrent {Clean}},
  pages = {202--219},
  crossref = {PARLE91b},
  author = {E.G.J.M.H. N{\"o}cker and J.E.W. Smetsers and van
		  Eekelen, M.C.J.D. and M.J. Plasmeijer91},
  annote = {--- PLGnotes ---}
}

@InProceedings{Noecker-Smetsers-vanEekelen-Plasmeijer-1991-x,
  WKloc = {?},
  abstract = {?},
  year = 1991,
  volume = {506-II},
  title = {Concurrent {Clean}},
  series = {LNCS},
  publisher = {Springer-Verlag},
  pages = {202--219},
  booktitle = {Proceedings of PARLE '91, Parallel Architectures and
		  Languages {Europe}},
  author = {E.G.J.M.H. N{\"o}cker and J.E.W. Smetsers and van
		  Eekelen, M.C.J.D. and M.J. Plasmeijer91},
  annote = {--- PLGnotes ---},
  address = {Eindhoven, The Netherlands}
}

@Book{Noeth-2000,
  author = {Winfried N{\"o}th},
  title = {{Handbuch der Semiotik}},
  publisher = {J. B. Metzler},
  year = 2000,
  address = {Stuttgart, Weimar},
  edition = {2. Auflage},
  UniBwM = {ASL070/YF32}
}

@InProceedings{Noll-1994,
  author = {Thomas Noll},
  title = {On the First-Order Equivalence of Call-by-Name and
		  Call-by-Value},
  crossref = {CAAP94},
  pages = {246--260},
  authorsAddress = {Aachen}
}

@TechReport{Nonnengart-1992,
  author = {Andreas Nonnengart},
  title = {First-Order Modal Logic Theorem Proving and Standard PROLOG},
  institution = {Max-Planck-Institut f\"ur Informatik},
  year = 1992,
  number = {MPI-I-92-228},
  address = {Im Stadtwald, Saarbr\"ucken},
  month = JUL,
  authorsAddress = {nonnenga@mpi-sb.mpg.de},
  WKloc = {C-0005}
}

@Misc{Nordlander-Carlsson-199X,
  author = {Johan Nordlander and Magnus Carlsson},
  title = {Reactive Objects in a Functional Language --- An Escape from the Evil ``I''},
  year = {199?},
  WKloc = {A-0790},
  abstract = {We present an extension to Haskell which supports reactive,
      concurrent programming with objects, {\sl sans} the problematic
      blocking input. We give a semantics together with a number of
      programming examples, and show an implementation based on a
      preprocessor and a library implementing seven monadic constants.}
}

@Misc{Nordstroem-2002,
  author =	 {Bengt Nordstr{\"o}m},
  title =	 {Constructivism. A {Computer Science} Perspective},
  howpublished = {Notes for the meeting on Foundations and the Ontological Quest in Pontificial Lateran University, Vatican City},
  month =	 JAN,
  year =	 2002,
  URL = 	 {http://www.cs.chalmers.se/~bengt/papers/vatican.ps},
  WKloc = 	 {A-1496}
}

@Book{Nordstroem-Petersson-Smith-1990,
  author = {Bengt Nordstr{\"o}m and Kent Petersson and Jan M. Smith},
  title = {Programming in {Martin-L\"of's} Type Theory},
  publisher = OUP,
  year = 1990,
  URL = {http://www.cs.chalmers.se/Cs/Research/Logic},
  WKloc = {B-0050}
}

@InCollection{Nordstroem-Petersson-Smith-2000,
  author = {Bengt Nordstr{\"o}m and Kent Petersson and Jan M. Smith},
  title = {Programming in {Martin-L\"of's} Type Theory},
  crossref = {HBLCS-V},
  pages = {\unfinished},
  URL = {http://www.cs.chalmers.se/Cs/Research/Logic},
  WKloc = {A-1753 preliminary, doc/pap/BIB}
}

@PhdThesis{Norell-2007,
  author = 	 {Ulf Norell},
  title = 	 {Towards a Practical Programming Language Based on Dependent Type Theory},
  OPTschool  = {Department of Computer Science and Engineering, Chalmers University of Technology},
  school  = {Dept.\null{} Comp.\null{} Sci.\null{} and Eng., Chalmers Univ.\null{} of Technology},
  year = 	 2007,
  month = SEP,
  DirectURL = 	 {http://www.cs.chalmers.se/~ulfn/papers/thesis.html},
  WKloc = 	 {doc/pap/BIB},
  note = {See also \url{http://wiki.portal.chalmers.se/agda/pmwiki.php}},
  abstract = 	 {Dependent type theories have a long history
    of being used for theorem proving.
    One aspect of type theory
    which makes it very powerful as a proof language
    is that it mixes deduction with computation.
    This also makes type theory a good candidate for programming ---
    the strength of the type system allows properties of programs
    to be stated and established,
    and the computational properties provide semantics for the programs.

    This thesis is concerned with bridging the gap
    between the theoretical presentations of type theory
    and the requirements on a practical programming language.
    Although there are many challenging research problems
    left to solve before we have
    an industrial scale programming language based on type theory,
    this thesis takes us a good step along the way.

    In functional programming languages
    pattern matching provides a concise notation for defining functions.
    In dependent type theory, pattern matching becomes even more powerful,
    in that inspecting the value of a particular term
    can reveal information about the types and values of other terms.
    In this thesis we give a type checking algorithm
    for definitions by pattern matching in type theory,
    supporting overlapping patterns,
    and pattern matching on intermediate results using the with rule.

    Traditional presentations of type theory
    suffers from rather verbose notation,
    cluttering programs and proofs with,
    for instance, explicit type information.
    One solution to this problem is to allow
    terms that can be inferred automatically to be omitted.
    This is usually implemented
    by inserting metavariables in place of the omitted terms
    and using unification to solve these metavariables during type checking.
    We present a type checking algorithm for a theory with metavariables
    and prove its soundness
    independent of whether the metavariables are solved or not.

    In any programming language it is important
    to be able to structure large programs into separate units or modules
    and limit the interaction between these modules.
    In this thesis we present a simple, but powerful module system
    for a dependently typed language.
    The main focus of the module system
    is to manage the name space of a program,
    and an important characteristic
    is a clear separation between the module system and the type checker,
    making it largely independent of the underlying language.

    As a side track,
    not directly related to the use of type theory for programming,
    we present a connection between type theory
    and a first-order logic theorem prover.
    This connection saves the user the burden of proving simple,
    but tedious first-order theorems by leaving them for the prover.
    We use a transparent translation to first-order logic
    which makes the proofs constructed by the theorem prover human readable.
    The soundness of the connection is established by a general metatheorem.

    Finally we put our work into practise
    in the implementation of a programming language, Agda,
    based on type theory.
    As an illustrating example we show how to program
    a simple certfied prover for equations in a commutative monoid,
    which can be used internally in Agda.
    Much more impressive examples have been done by others,
    showing that the ideas developed in this thesis are viable in practise. }
}

@InCollection{Norvell-Hehner-93,
  title = {Logical Specifications for Functional Programs},
  author = {Theodore S. Norvell and Eric C.R. Hehner},
  crossref = {MPC1992},
  pages = {269--290},
  WKloc = {A-0034, ~kahl/doc/pap/lsfp.tex},
  abstract = {We present a formal method of functional program
		development based on step-by-step
		transformation.

	In their most abstract form, specifications are
		essentially predicates that relate
		the result of the specified program
		to the free variables of that program.
	In their most concrete form, specifications are
		simply programs in a functional programming language.
	Development from abstract specifications to programs is
		calculational.

	Using logic in the specification language has many
		advantages.
	Importantly it allows nondeterministic specifications to be given,
		and thus does not force overspecification.}
}

@Article{Novack-1995,
  author = {Steven Novack and Alexandru Nicolau},
  title = {A Hierarchical Approach to Instruction-level
                 Parallelization},
  journal = IJPP,
  volume = 23,
  number = 1,
  pages = {35--62},
  month = feb,
  year = 1995,
  CODEN = {IJPPE5},
  ISSN = {0885-7458},
  bibdate = {Sat Apr 26 11:36:49 MDT 1997},
  bibsource = {Compendex database},
  acknowledgement = ack-nhfb,
  affiliation = {Univ of California},
  affiliationaddress = {Irvine, CA, USA},
  classification = {722.4; 723.1; 723.2; 731.1; C6110P (Parallel
                 programming); C6140B (Machine-oriented languages);
                 C6150C (Compilers, interpreters and other processors);
                 C6150N (Distributed systems software)},
  corpsource = {Dept. of Inf. and Comput. Sci., California Univ.,
                 Irvine, CA, USA},
  journalabr = {Int J Parallel Program},
  keywords = {Code explosion; code explosions; Codes (symbols);
                 compiler efficiency; constant time; control flow graph;
                 Control flow graph; Critical path analysis; data flow
                 graphs; Flowcharting; hierarchical approach;
                 Hierarchical systems; Hierarchical task graphs;
                 Instruction level parallelism; instruction sets;
                 instruction-level parallelization; multiple control
                 paths; nonincremental code motions; Parallel processing
                 systems; parallel programming; Percolation scheduling;
                 percolation scheduling; processor scheduling; program
                 compilers; PS transformations; Scheduling; simulation
                 results; single-entry/single-exit regions; VLIW
                 program},
  treatment = {P Practical}
}

@Manual{NuSMV2.2Tut,
  title = 	 {{NuSMV 2.2 Tutorial}},
  author =	 {Roberto Cavada and Alessandro Cimatti and Gavin Keighren and Emmanuele Olivetti and Marco Pistore and Marco Roveri},
  organization = {ITC-irst},
  address =	 {Povo (Trento), Italy},
  year =	 {200?},
  WKloc = 	 {A-1699}
}

@InProceedings{Nutt-1990,
  author = 	 {Werner Nutt},
  title = 	 {Unification in monoidal theories},
  crossref =  {CADE1990},
  DOI = {10.1007/3-540-52885-7_118},
  URL = 	 {http://www.springerlink.com/content/7777h726818w3u41/},
  pages = 	 {618--632},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {We study the unification problem for a class of
                  equational theories that comprises important
                  examples like abelian monoids (AC), idempotent
                  abelian monoids (ACI), and abelian groups. Monoidal
                  theories have the common characteristic that
                  unification algorithms are based on solving linear
                  equation systems over a semiring.  The close
                  correspondence between unification and linear
                  algebra can be used to characterize the unification
                  type of monoidal theories in purely algebraic terms,
                  and an application of Hilbert's Basis Theorem gives
                  a sufficient criterion for a monoidal theory to be
                  unitary.}
}

@InProceedings{Nuzman-etal-2006,
  author = 	 {Nuzman and others},
  title = 	 {},
  crossref =  {PLDI2006},
  bibliographies = {Coconut},
  OPTpages = 	 {},
  OPTnote = 	 {},
  annote = 	 {sharing different computations in SIMD}
}

@Misc{OCL-FAQ,
  key = {OCL-FAQ},
  title = {OCL Frequently Asked Questions},
  year = 1999,
  WKloc = {A-0993}
}

@inproceedings{ODonnell-1993,
  author = {O'Donnell, John T.},
  title = {Generating Netlists from Executable Circuit Specifications},
  booktitle = {Proceedings of the 1992 Glasgow Workshop on Functional Programming},
  year = {1993},
  isbn = {3-540-19820-2},
  pages = {178--194},
  publisher = Springer,
  address = {London, UK},
  WKloc = {doc/pap/BIB},
  bibliographies = {Coconut}
}

@Article{OHearn-Pym-1999,
  author = 	 {O'Hearn, Peter W. and David J. Pym},
  title = 	 {The Logic of Bunched Implications},
  journal = 	 {Bulletin of Symbolic Logic},
  year = 	 1999,
  volume = 	 5,
  number = 	 4,
  pages = 	 {215--244},
  month = 	 JUN,
  WKloc = 	 {A-1685, doc/pap/BIB},
  abstract = 	 {We introduce a logic \textbf{BI}
    in which a multiplicative (or linear) and an additive (or intuitionistic) implication
    live side-by-side.
    The propositional version of \textbf{BI} arises from an analysis
    of the proof-theoretic relationship between conjunction and implication;
    it can be viewed as a merging of intuitionistic logic
    and multiplicative intuitionistic linear logic.
    The naturality of \textbf{BI} can be seen categorically:
    models of propositionsl \textbf{BI}'s proofs
    are given bi bicartesian doubly closed categories,
    \textsl{i.e.}, categories which freely combine
    the semantics of propositional intuitionistic logic
    and propositional multiplicative intuitionistic linear logic.
    The predicate version of \textbf{BI} includes,
    in addition to standard additive quantifiers,
    multiplicative (or intensional) quantifiers
    $\forall_{\mathbf{new}}$ and $\exists_{\mathbf{new}}$
    which arise from observing restrictions on structural rules
    on the level of terms as well as propositions.
    We discuss computational interpretations,
    based on sharing,
    at both the propositional and predicate levels.}
}

@InProceedings{OHearn-Riecke-94,
  author = {P.W. O'Hearn and Jon G. Riecke},
  title = {Fully Abstract Translations and Parametric Polymorphism},
  crossref = {ESOP1994},
  pages = {454--468}
}

@InProceedings{OHearn-Tennent-93,
  author = {O'Hearn and Tennent},
  title = {Relational Parametricity and Local Variables},
  pages = {171--184},
  crossref = {POPL1993},
  abstract = {J. C. Reynolds suggested that Strachey's intuitive concept of
           ``parametric''(i.e., uniform) polymorphism is closely linked to
           {R-representation independece}, and used logical relations to
           formalize this principle in languages with type variables and
           user-defined types. Here, we use relational parametricity to
           address long-standing problems with the semantics of
           local-variable declarations, by showing that interactions between
           local and nonlocal entities satisfy certain relational
           criteria. The new model is based on a cartesian closed category of
           ``relation-preserving'' functors and natural transformations which
           is induced by a suitable category of ``possible worlds'' with
           relations assigned to its objects and morphisms. The semantic
           interpretation supports straightforward validations of all the
           test equivalences that have been proposed in the literature, and
           encompasses standard methods of reasoning about data
           representations; however, it is not known whether it is
           fully abstract.},
  WKloc = {A-0194},
  bibliographies = {RelMiCS, LogRel}
}

@Article{OKeefe-2004,
  author =       {Greg O'Keefe},
  title =        {Towards a Readable Formalisation of Category Theory },
  journal =      ENTCS,
  year =         2004,
  volume =    91,
  pages =     {212--228},
  DOI =      {10.1016/j.entcs.2003.12.014},
  DOIURL =      {http://dx.doi.org/10.1016/j.entcs.2003.12.014},
  annote =    {Proceedings of Computing: The Australasian Theory Symposium (CATS) 2004},
  abstract = {We formally develop category theory up to Yoneda's lemma,
     using Isabelle/HOL/Isar, and survey previous formalisations.
     By using recently added Isabelle features,
     we have produced a formal text
     that more closely approximates informal mathematics.}
}

@Book{OSF-1993,
  organization = {Open Software Foundation},
  title = {Design of the {OSF/1} Operating System, Release 1.2},
  publisher = {Prentice Hall},
  year = 1993,
  McMaster = {QA 76.76 .O63 D475 1993},
  bibliographies = {SE3B},
  keywords = {Mach}
}

@Article{Ochs-2010,
  author = 	 {Eduardo Ochs},
  title = 	 {Internal Diagrams in Category Theory},
  OPTjournal = 	 {Logical Universalis},
  OPTyear = 	 {2010},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  keywords = 	 {archetypal language},
  note = 	 {submitted 2010},
  WKloc = 	 {A-1740, doc/pap/BIB}
}

@TechReport{Odersky-1991,
  WKloc = {~kahl/doc/pap/funoop},
  abstract = {Object-oriented programming languages with subtyping
		  facilitate the reuse of code which forms the top
		  part of a call graph. This style of code reuse,
		  sometimes called inverted programming, cannot be
		  simulated easily in traditional languages.  We
		  present a technique for achieving the same effect in
		  the framework of a higher-order, polymorphic
		  functional language without subtyping. Our method
		  establishes a close relationship between the
		  object-oriented concepts of class, ``self'', and
		  subtyping, and the functional concepts of algebraic
		  data types and abstraction functions. This
		  relationship can form the basis of a
		  transformational semantics of object-oriented
		  language elements.},
  year = 1991,
  type = {Research Report},
  title = {Objects and Subtyping in a Functional Perspective},
  number = {RC 16423},
  month = JAN,
  institution = {IBM Research, Thomas J. Watson Research Center},
  author = {Martin Odersky}
}

@InProceedings{Odersky-1992,
  author = {Martin Odersky},
  title = {Observers for Linear Types},
  crossref = {ESOP1992},
  pages = {390--407},
  authorsAddress = {Yale University, odersky\@cs.yale.edu},
  abstract = {Linear types provide the framework for a safe
		  embedding of mutable state in functional languages
		  enforcing the principle that variables of linear
		  type must be used exactly once. A potential
		  disadvantage of this approach is that it places read
		  accesses to such variables under the same
		  restriction as write accesses, and thus prevents
		  reads to proceed in parallel. We present here an
		  extension of linear types which augments the usual
		  distinction between linear and non-linear by a third
		  state, {\em observers} of linear variables. Since,
		  unlike linear variables, observers can be
		  duplicated, multiple concurrent reads are made
		  possible. On the other hand, observers must be
		  short-lived enough to never overlap with mutations.
		  The resulting type system is in many aspects similar
		  to the one of ML: It is polymorphic, has principal
		  types, and admits a type reconstruction algorithm.}
}

@TechReport{Odersky-1993,
  author = {Martin Odersky},
  title = {A Syntactic Theory of Local Names},
  institution = {Yale University, Department of Computer Science},
  year = 1993,
  type = {Research Report},
  number = {YALEU/DCS/RR-965},
  month = MAY,
  file = {~kahl/doc/pap/yale/reports/RR-965.dvi},
  WKloc = {A-0263},
  abstract = {$\lambda\nu$ is an extension of the
		  $\lambda$-calculus with a binding construct for
		  local names. The extension is symmetric in
		  identifiers and names, it has properties analogous
		  to classical $\lambda$-calculus, and it preserves
		  all observational equivalences of $\lambda$. The
		  calculus is useful as a basis for modeling
		  wide-spectrum languages that build on a functional core.}
}

@InProceedings{Odersky-1994,
  author = {Martin Odersky},
  title = {A Functional Theory of Local Names},
  crossref = {POPL1994},
  pages = {48--59},
  abstract = {$\lambda\nu$ is an extension of the
                  $\lambda$-calculus with a binding construct for
                  local names. The extension has properties analogous
                  to classical $\lambda$-calculus and preserves
                  all observational equivalences of $\lambda$. It
                  is useful as a basis for modeling
                  wide-spectrum languages that build on a functional core.},
  annote = {see \cite{Odersky-1993}, A-0263}
}

@Misc{Odersky-1999,
  author = {Martin Odersky},
  title = {Functional Nets},
  year = 1999,
  URL = {http://lampwww.epfl.ch/papers/functional-nets.ps},
  note = {Draft},
  WKloc = {A-0912}
}

@InProceedings{Odersky-2000,
  author = {Martin Odersky},
  title = {Functional Nets},
  crossref = {ESOP2000},
  pages = {1--25},
  WKloc = {A-1000}
}

@InProceedings{Odersky-Laeufer-1996,
  author = {Martin Odersky and Konstantin L{\"a}ufer},
  title = {Putting Type Annotations To Work},
  booktitle = {POPL1996},
  pages = {54--67},
  month = JAN,
  abstract = {We study an extension of the Hindley/Milner system
    with explicit type scheme annotations and type declarations.
    The system can express polymorphic function arguments,
    user-defined data types with abstract components,
    and structure types with polymorphic fields.
    More generally, all programs of the polymorphic lambda calculus
    can be encoded by a translation between typing derivations.
    We show that type reconstruction in this system
    can be reduced to the decidable problem of
    first-order unification under a mixed prefix.},
  URL = {http://lampwww.epfl.ch/~odersky/papers/},
  WKloc = {A-1193}
}

@Unpublished{Odersky-Rabin-Hudak-1992,
  WKloc = {A-0002,~kahl/doc/pap/lamvar.dvi},
  abstract = {We augment functional programming with assignment
		  while maintaining the algebraic laws of the
		  functional subset. We define an extension of the
		  call-by-name lambda calculus with additional
		  constants and reduction rules for mutable variables
		  and assignments. The extended calculus has neither a
		  concept of an explicit store nor a concept of
		  evaluation order; nevertheless, we show that
		  programs in the calculus can be implemented using a
		  single threaded store. We also show that the new
		  calculus has the Church-Rosser property and that it
		  is a conservative extension of classical lambda
		  calculus with respect to operational equivalence.},
  year = 1992,
  title = {Call by Name, Assignment, and the $\lambda$-Calculus},
  note = {Technical Summary},
  month = JUL,
  author = {Martin Odersky and Dan Rabin and Paul Hudak}
}

@InProceedings{Odersky-Wadler-1997,
  title = {Pizza into {Java}: Translating theory into practice},
  author = {Martin Odersky and Philip Wadler},
  pages = {146--159},
  crossref = {POPL1997},
  references = {\cite{IC::Breazu-TannenCGS1991}
                 \cite{ACMCS::Bruce1997} \cite{ACMCS::CardelliW1985}
                 \cite{POPL::DamasM1982} \cite{TOPLAS::LauferO1994}
                 \cite{TOPLAS::MitchellP1988} \cite{POPL::Ohori1992}
                 \cite{POPL::WadlerB1989}},
  URL = {http://homepages.inf.ed.ac.uk/wadler/pizza/},
  WKloc = {B-0111}
}

@InProceedings{Odersky-Wadler-1998,
  author = {Martin Odersky and Philip Wadler},
  title = {Leftover {Curry} and reheated {Pizza}: How functional programming nourishes software reuse},
  booktitle = {IEEE Fifth International Conference on Software Reuse},
  year = 1998,
  WKloc = {A-0717}
}

@InProceedings{Odersky-Zenger-2001,
  author = {Martin Odersky and Christoph Zenger},
  title = {Nested Types},
  crossref = {FOOL2001},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1084, doc/pap/BIB}
}

@MastersThesis{Offermann-1998,
  author = {Eric Offermann},
  title = {Programming Term Graph Layout Algorithms in {HOPS}},
  year = 1998,
  type = {Trimesterarbeit},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  month = NOV,
  WKloc = {A-0859}
}

@PhdThesis{Offermann-2003,
  author = 	 {Eric Offermann},
  title = 	 {Konstruktion Relationaler Kategorien},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 	 {2003},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Ogata-Futatsugi-1997,
  author = {Kazuhiro Ogata and Kokichi Futatsugi},
  title = {Implementation of Term Rewritings with the Evaluation Strategy},
  crossref = {PLILP1997},
  pages = {225--239},
  WKloc = {A-0976}
}

@InProceedings{OhTaewook-Egger-ParkHyunchul-Mahlke-2009,
  author = 	 {Taewook Oh and Bernhard Egger and Hyunchul Park and Scott Mahlke},
  title = 	 {Recurrence cycle aware modulo scheduling for coarse-grained reconfigurable architectures},
  OPTcrossref =  {LCTES2009},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {21--30},
  DOIURL = 	 {http://doi.acm.org/10.1145/1543136.1542456},
  bibliographies = {Coconut},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  abstract = {In high-end embedded systems,
     coarse-grained reconfigurable architectures (CGRA)
     continue to replace traditional ASIC designs.
     CGRAs offer high performance at a low power consumption,
     yet provide flexibility through programmability.
     In this paper we introduce
     a recurrence cycle-aware scheduling technique for CGRAs.
     Our modulo scheduler
     groups operations belonging to a recurrence cycle into a clustered node
     and then computes a scheduling order for those clustered nodes.
     Deadlocks that arise when two or more recurrence cycles depend on each other
     are resolved by using heuristics
     that favor recurrence cycles with long recurrence delays.
     While with previous work one had to sacrifice
     either a fast compilation speed in order to get good quality results,
     or vice versa, this is not necessary anymore
     with the proposed recurrence cycle-aware scheduling technique.
     We have implemented the proposed method
     into our in-house CGRA chip and compiler solution
     and show that the technique achieves better quality schedules
     than schedulers based on simulated annealing at a 170-fold speed increase.}
}

@Unpublished{Ohlbach-1994,
  author = {Hans J\"urgen Ohlbach},
  title = {Boolean Algebras with Functions Correspondence,
		  Completeness and Quantifier Elimination},
  note = {submitted to IJCAI},
  year = 1994,
  abstract = {It is shown how axiomatic specifications of Boolean
		  Algebras with extra functions as well as
		  propositional extension of standard propositional
		  logic can be transformed and simplified using
		  syntactic methods, in particular quantifier
		  elimination algorithms for second-order predicate
		  logic.

                  This enables us to exploit representation theorems
		  and model theoretic semantics for these algebras and
		  logics in such a way that for special instances of
		  these systems, i.e.\null{} particular algebras and
		  particular logics the corresponding specializations
		  on the semantic side can be computed automatically.

                  Special cases of the results of this paper are the
		  theorem proving aspects of J\'onsson and Tarski's
		  representation theorem for Boolean Algebras with
		  operators, completeness of different possible worlds
		  semantics for modal logics and a clarification of
		  the correlation between correspondence and
		  completeness in modal logic.},
  keywords = {Knowledge Representation, Logics for Knowledge,
		  Theorem Proving in Nonclassical Logics, Jonsson},
  WKloc = {A-0383}
}

@InProceedings{Ohlbach-1998,
  author = {Hans J\"urgen Ohlbach},
  title = {Combining Hilbert Style and Semantic Reasoning in a Resolution Framework},
  crossref = {CADE1998},
  pages = {205--219},
  OPTabstract = {},
  WKloc = {A-0613 (with PDF faults)}
}

@TechReport{Ohlbach-Schmidt-1995,
  author = {Hans J\"urgen Ohlbach and Renate Schmidt},
  title = {Functional translation and second-order frame properties of
      modal logics},
  institution = {Max-Planck-Inst.},
  year = 1995,
  number = {MPI-I-95-2-002},
  address = {Stuttgart},
  bibliographies = {RelMiCS}
}

@InProceedings{Ohlebush-1994,
  author = {Enno Ohlebush},
  title = {On the Modularity of Confluence of
		  Constructor-Sharing Term Rewriting Systems},
  crossref = {CAAP94},
  pages = {261--276},
  authorsAddress = {Bielefeld}
}

@TechReport{Ohsaki-2004,
  author =	 {Hitoshi Ohsaki},
  title =	 {System Verification and Tree Automata},
  institution =  {National Institute of Advanced Industrial Science and Technology (AIST), Laboratory for Verification and Semantics},
  year = 	 2004,
  month =	 JAN,
  note =	 {Lecture notes},
  WKloc = 	 {C-0023}
}

@InProceedings{Ohsaki-Takai-2002,
  author = 	 {Hitoshi Ohsaki and Toshinori Takai},
  title = 	 {Decidability and Closure Properties of Equational Tree Languages},
  crossref =	 {RTA2002},
  pages =	 {114--128},
  WKloc = 	 {A-1549, also in \cite{Ohsaki-2004}},
  bibliographies = {RelMiCS},
  abstract = {Equational tree automata provide a powerful tree
     language framework that facilitates to recognize congruence
     closures of tree languages. In the paper we show the emptiness
     problem for AC-tree automata and the intersection-emptiness
     problem for regular AC-tree automata, each of which was open in
     our previous work [20], are decidable, by a straightforward
     reduction to the reachability problem for ground AC-term
     rewriting. The newly obtained results generalize decidability of
     so-called reachable property problem of Mayr and Rusinowitch
     [17]. We then discuss complexity issue of AC-tree
     automata. Moreover, in order to solve some other questions about
     regular A- and AC-tree automata, we recall the basic connection
     between word languages and tree languages.}
}

@TechReport{Ohsaki-Takai-2002a,
  author = 	 {Hitoshi Ohsaki and Toshinori Takai},
  title = 	 {A Tree Automata Theory for Unification Modulo Equational Rewriting},
  institution =  {National Institute of Advanced Industrial Science and Technology (AIST), Laboratory for Verification and Semantics},
  year = 	 2002,
  type =	 {AIST Programming Science Group Technical Report},
  number =	 {AIST-PS-2002-006},
  month =	 JUL,
  bibliographies = {RelMiCS},
  WKloc = 	 {A-1550}
}

@InProceedings{Ohsuga-Sakai-1990,
  author = {Akihiko Ohsuga and K{\^o} Sakai},
  title = {Complete Equational Unification Based on an
		  Extension of the Knuth-Bendix Completion Procedure},
  pages = {197--209},
  crossref = {IWWERT90},
  WKloc = {A-0156},
  abstract = {A unifier is a substitution that makes two terms
		  syntactically equal. In this paper, we discuss a
		  more semantical unifier: an equational unifier,
		  which is a substitution that makes two terms equal
		  modulo a congruence relation. As a result we will
		  give a general procedure that enumerates a complete
		  set of equational unifiers for a given pair of terms
		  under a given congruence.},
  bibliographies = {RelMiCS}
}

@TechReport{Ohsuga-Sakai-1992,
  year = 1992,
  type = {Technical Report},
  title = {Metis: A term rewriting system generator},
  number = {92--753},
  institution = ICOT,
  author = {Akihiko Ohsuga and K{\^o} Sakai},
  annote = {tubibmue {--- HOPSnotes ---}},
  bibliographies = {RelMiCS}
}

@Article{Okada-Hayashi-1993,
  author = {Okada, Yasuyoshi and Hayashi, Masahiro},
  title = {Graph Rewriting Systems and Their Application to Network
                   Reliability Analysis},
  OPTcrossref = {},
  OPTkey = {},
  journal = {IEICE transactions on information and systems},
  year = 1993,
  volume = 76,
  number = 2,
  pages = {154--},
  month = FEB,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Okasaki-1994,
  author = {Chris Okasaki},
  title = {Simple and Efficient Purely Functional Queues and Deques},
  journal = {Journal of Functional Programming},
  year = 1994,
  URL = {ftp://ftp.cs.cmu.edu/afs/cs/project/fox/ftp/queue.tar.Z},
  directory = {~kahl/doc/pap/cmu/queue},
  note = {to appear},
  abstract = {We present purely functional implementations of
		  queues and double-ended queues (deques) requiring
		  only O(1) time per operation in the worst case. Our
		  algorithms are considerably simpler than previous
		  designs with the same bounds. The inspiration for
		  our approach is the incremental behavior of certain
		  functions on lazy lists.},
  WKloc = {A-0262}
}

@Article{Okasaki-1995,
  author = {Chris Okasaki},
  title = {Simple and Efficient Purely Functional Queues and Deques},
  journal = JFP,
  volume = 5,
  number = 4,
  pages = {583--592},
  month = OCT,
  year = 1995,
  keywords = {purely functional implementation of queues and
                 double-ended queues; constant worst-case time insertion
                 and removal of elements functional programming,
                 functional pearls,},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#jfp95},
  WKloc = {B-0067}
}

@InCollection{Okasaki-1995a,
  author = {Chris Okasaki},
  title = {Purely Functional Random-Access Lists},
  booktitle = {Functional Programming \& Computer Architecture},
  pages = {86--95},
  publisher = {ACM Press},
  month = JUN,
  year = 1995,
  keywords = {data structures, purely functional, O(1) list
                 operations, O(log n) random-access, functional
                 programming, functional pearls,},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#fpca95},
  WKloc = {B-0067}
}

@InProceedings{Okasaki-1995b,
  author = {Chris Okasaki},
  title = {Amortization, Lazy Evaluation, and Persistence: Lists
                 with Catenation via Lazy Linking},
  year = 1995,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#catenation},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/catenation.ps},
  keywords = {amortization, lazy evaluation, functional data
                 structures, lists with O(1) catenation},
  month = oct,
  organization = {IEEE},
  scope = {pearls},
  booktitle = {Symposium on Foundations of Computer Science},
  WKloc = {B-0067}
}

@Article{Okasaki-1995c,
  author = {Chris Okasaki},
  journal = JFP,
  title = {Simple and Efficient Purely Functional Queues and
                 Deques},
  year = 1995,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#jfp95},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/jfp95.ps},
  keywords = {purely functional implementation of queues and
                 double-ended queues; constant worst-case time insertion
                 and removal of elements},
  month = oct,
  number = 4,
  pages = {583--592},
  volume = 5
}

@InProceedings{Okasaki-1996,
  author = {Chris Okasaki},
  title = {The Role of Lazy Evaluation in Amortized Data
                 Structures},
  year = 1996,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#icfp96},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/icfp96.ps},
  keywords = {functional data structures, lazy evaluation, time
                 analysis},
  month = may,
  pages = {62--72},
  scope = {pearls},
  booktitle = {International Conference on Functional Programming},
  WKloc = {B-0067}
}

@InProceedings{Okasaki-1996a,
  author = {Chris Okasaki},
  title = {Functional Data Structures},
  year = 1996,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#ssafp96},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/ssafp96.ps},
  keywords = {tutorial on functional data structures, including FIFO
                 queues, catenable lists, and meldable priority queues},
  crossref = {AFP1996},
  WKloc = {B-0067}
}

@Article{Okasaki-1997a,
  author = {Chris Okasaki},
  journal = {Journal of Functional Programming},
  title = {Three Algorithms on Braun Trees},
  year = 1997,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#jfp97},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/jfp97.ps},
  keywords = {Functional Pearl column, efficient algorithms on a
                 class of balanced trees},
  month = nov,
  number = 6,
  volume = 7,
  scope = {pearls},
  WKloc = {B-0067}
}

@InProceedings{Okasaki-1997b,
  author = {Chris Okasaki},
  title = {Catenable Double-Ended Queues},
  year = 1997,
  abstract-url = {http://foxnet.cs.cmu.edu/people/cokasaki/papers.html#icfp97},
  URL = {http://foxnet.cs.cmu.edu/people/cokasaki/icfp97.ps},
  keywords = {double-ended queues with O(1) catenation (i.e.,
                 append), lazy evaluation, polymorphic recursion,
                 views},
  crossref = {ICFP1997},
  WKloc = {B-0067}
}

@Book{Okasaki-1998,
  author = {Chris Okasaki},
  title = {Purely Functional Data Structures},
  publisher = CambridgeUP,
  year = 1998,
  annote = {Data structures and efficiency analysis for functional
                 programming. Code in ML and Haskell. Many references.}
}

@Article{Okasaki-1998b,
  author = {Chris Okasaki},
  title = {Even higher-order functions for parsing or Why would
                 anyone ever want to use a sixth-order function?},
  journal = JFP,
  year = 1998,
  volume = 8,
  number = 2,
  month = MAR,
  pages = {195--199},
  annote = {Parsing combinators for ML.},
  WKloc = {B-0067}
}

@InProceedings{Okasaki-1999,
  author = {Chris Okasaki},
  title = {From Fast Exponentiation to Square Matrices: An Adventure in Types},
  crossref = {ICFP1999},
  WKloc = {A-1272}
}

@InProceedings{Okasaki-2000,
  author = {Chris Okasaki},
  title = {Breadth-first numbering: lessons from a small exercise
                 in algorithm design.},
  pages = {131--136},
  crossref = {ICFP2000}
}

@Article{Okasaki-2001,
  author =       {Chris Okasaki},
  title =        {An Overview of {Edison}},
  journal =      ENTCS,
  year =         2001,
  volume =    41,
  number =    1,
  pages = {60--73},
  DOIURL = {http://dx.doi.org/10.1016/S1571-0661(05)80546-8},
  DOI = {10.1016/S1571-0661(05)80546-8},
  note =      {(Original version in Haskell Workshop, September 2000, pp.\null{} 34--45)},
  abstract =    {Edison is a library of functional data structures implemented in Haskell. It supports three main families of abstractions: sequences, collections (e.g., sets and priority queues), and associative collections (e.g., finite maps). This paper summarizes the design of Edison, with particular attention to how that design is influenced by details of Haskell. },
  WKloc = {doc/pap/BIB},
  URL = {http://www.eecs.usma.edu/webs/people/okasaki/pubs.html#hw00}
}

@InProceedings{Okasaki-2002,
  author = {Chris Okasaki},
  title = {Techniques for embedding postfix languages in Haskell },
  crossref = {Haskell2002},
  pages = {105--113},
  URL = {http://doi.acm.org/10.1145/581690.581699},
  WKloc = {doc/pap/BIB},
  abstract = {One popular use for Haskell in recent years has been as a
      host language for domain-specific embedded languages. But how can one
      embed a postfix language in Haskell, given that Haskell only supports
      prefix and infix syntax? This paper describes several such embeddings
      for increasingly complex postfix languages.}
}

@Article{Okasaki-Lee-Tarditi-1994,
  author = {Chris Okasaki and Peter Lee and David Tarditi},
  title = {Call-by-need and Continuation-passing Style},
  journal = {Lisp and Symbolic Computation},
  year = 1994,
  volume = 7,
  number = 1,
  pages = {57--81},
  URL = {ftp://ftp.cs.cmu.edu/usr/anon/user/cokasaki/lasc94.dvi.gz},
  WKloc = {B-0067}
}

@MastersThesis{Okuma-1998,
  author = {Hitomi Okuma},
  title = {{Hoare}'s Semantics in {Dedekind} Categories},
  school = {Department of Informatics, Kyushu University},
  year = 1998,
  note = {Japanese}
}

@Article{Okuma-Kawahara-2000,
  author = {Hitomi Okuma and Yasuo Kawahara},
  title = {Relational Aspects of Relational Database Dependencies},
  journal = BUIC,
  year = 2000,
  volume = 32,
  number = 2,
  pages = {91--104},
  month = DEC,
  bibliographies = {RelMiCS},
  WKloc = {A-1318},
  abstract = {This paper presents a relational treatment of
     inference rules for functional and multivalued dependencies
     in relational databases, to show the soundness and the
     completeness of the inference rules in Dedekind categories,
     which also cover the fuzzy case.}
}

@InProceedings{Oliart-Snyder-1998,
  author = {Alberto Oliart and Wayne Snyder},
  title = {A Fast Algorithm for Uniform Semi-Unification},
  crossref = {CADE1998},
  pages = {239--253},
  OPTabstract = {},
  WKloc = {A-0614 (with PDF faults)}
}

@InProceedings{Oliveira-Cavalcanti-Woodcock-2004,
  author = 	 {Marcel Oliveira and Ana Cavalcanti and Jim Woodcock},
  title = 	 {Laws of Refinement for \textsf{\itshape{Circus}}},
  OPTcrossref =  {ICTAC2004},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1557},
  annote = 	 {submitted},
  abstract = {Abstract. Among the existing formalisms that bring
      together notations to capture both data and behavioural features
      of a system, \textsf{\itshape{Circus}} is the only one with an associated
      refinement calculus. A refinement strategy that unifies theories
      of refinement for processes and their constituent actions has
      already been proposed. Throughout a development of an
      industrial-scale case study, we faced the need for many new
      refinement laws: this paper presents these laws and some of the
      soundness proofs.},
  keywords = {Program development, concurrency, Circus, refinement}
}

@article{Oliveira-Ferreira-2013,
  author = {Oliveira, Jose N. and Ferreira, Miguel A.},
  title = {Alloy Meets the {Algebra of Programming}: A Case Study},
  journal = {IEEE Trans. Softw. Eng.},
  issue_date = {March 2013},
  volume = {39},
  number = {3},
  month = MAR,
  year = {2013},
  issn = {0098-5589},
  pages = {305--326},
  numpages = {22},
  DOIURL = {http://dx.doi.org/10.1109/TSE.2012.15},
  DOI = {10.1109/TSE.2012.15},
  acmid = {2478709},
  publisher = {IEEE Press},
  OPTaddress = {Piscataway, NJ, USA},
  WKloc = {A-1747, doc/pap/BIB},
  keywords = {Software, Programming, Matrices, grand challenges in computing, model checking, algebra of programming, software verification},
  abstract = {Relational algebra offers to software engineering the
                  same degree of conciseness and calculational power
                  as linear algebra in other engineering
                  disciplines. Binary relations play the role of
                  matrices with similar emphasis on multiplication and
                  transposition. This matches with Alloy's lemma
                  “everything is a relation” and with the relational
                  basis of the Algebra of Programming
                  (AoP). Altogether, it provides a simple and coherent
                  approach to checking and calculating programs from
                  abstract models. In this paper, we put Alloy and the
                  Algebra of Programming together in a case study
                  originating from the Verifiable File System
                  mini-challenge put forward by Joshi and Holzmann:
                  verifying the refinement of an abstract file store
                  model into a journaled (Flash) data model catering
                  to wear leveling and recovery from power loss. Our
                  approach relies on diagrams to graphically express
                  typed assertions. It interweaves model checking (in
                  Alloy) with calculational proofs in a way which
                  offers the best of both worlds. This provides ample
                  evidence of the positive impact in software
                  verification of Alloy's focus on relations,
                  complemented by induction-free proofs about data
                  structures such as stores and lists.}
}

@Article{Olivier-1982,
  author = {Jean-Pierre Olivier},
  title = {Liaisons entre les {S}-Relations et les Relations de
                  {F}errers. {R}epresentations},
  journal = Math_Sci_hum,
  year = 1982,
  volume = 20,
  number = 80,
  pages = {67--82},
  bibliographies = {RelMiCS}
}

@Unpublished{Olivier-1994,
  author = {Jean-Pierre Olivier},
  title = {Binary and Oriented Trees in Distribitive Allegories},
  note = {unpublished?},
  OPTkey = {},
  year = 1994,
  month = SEP,
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Article{Olivier-Serrato-1980,
  author = {Jean-Pierre Olivier and Dany Serrato},
  title = {Cat\'egories de {Dedekind}. {Morphismes} transitifs dans les cat\'egories de {Schr\"oder}},
  OPTtitle = {{Dedekind} Categories. Transitive Morphisms in the {Schr\"oder} Categories},
  year = 1980,
  pages = {939--941},
  number = {20},
  volume = 290,
  journal = CRPARIS,
  WKloc = {A-1734, doc/pap/BIB},
  URL = {http://visualiseur.bnf.fr/CadresFenetre?O=NUMM-5813085&I=21&M=tdm},
  bibliographies = {RelMiCS},
  abstract = {In this Note, we introduce the notion of Dedekind's categories.
    This notion performs very well
    in the study of identities of binary relations,
    in classical categories as well as in many others.
    We specify this concept to the boolean case
    in which we give results on the transitive morphisms.
    We also introduce an important notion:
    the Schr\"oder category $\mathrm{G}(a)$ associated with the morphism $a$.
    This category from the point of view of measurement theory
    may be interpreted as the set of measure instrument
    logically derived from a measure instrument $a$.
    We derive the structure of $\mathrm{G}(a)$ when $a$ is an equivalence.},
  annote = {``Rappelons que les morphismes idempotents et sym\'etriques
              sont appel\'es des \'equivalences.'' --- Reflexivity is missing.}
}

@InProceedings{Olivier-Serrato-1982,
  author = {Jean-Pierre Olivier and Dany Serrato},
  title = {Approach to an Axiomatic Study on the Fuzzy
                  Relations on Finite Sets},
  booktitle = {Fuzzy Information and Decision Processes},
  editor = {Madan M. Gupta and Elie Sanchez},
  McMaster = {QA 279.4 .F89 1982},
  McMasterID = {39005040690202},
  year = 1982,
  pages = {111--116},
  publisher = NoHo,
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {This paper deals with the notion of Caratheodory algebra
    which is thought to be a well adapted axiomatic structure in order
    to set up properties of fuzzy or classical relations on finite sets.
    This leads to establish old and new properties and concepts.
    Some results concerning the mono\"{\i}d of powers of a single element,
    as well as the quasi-primitive decomposition of a morphism, are given.},
  annote = {Dedekind categories are defined in the appendix
    as lower semi-lattice categories with converse and Dedekind property,
    that is, as allegories.}
}

@Article{Olivier-Serrato-1995,
  author = {Jean-Pierre Olivier and Dany Serrato},
  title = {Squares and Rectangles in Relation Categories -- Three Cases:
      Semilattice, Distributive Lattice and Boolean Non-unitary},
  year = 1995,
  pages = {167--178},
  number = {2},
  volume = 72,
  DOI = {10.1016/0165-0114(94)00349-C},
  DOIURL = {},
  journal = {Fuzzy Sets and Systems},
  bibliographies = {RelMiCS},
  abstract = {In this paper we investigate rectangle morphisms of
    Dedekind categories
    (or semilattice relation categories, or unitary allegories)
    which appear to be a basic structure for relation categories.
    In order to reconstruct the Dedekind category generated by
    all the rectangle morphisms, we are obliged to clarify the role
    of both the square morphisms and the functions $s$
    which assign to every square morphism $c$ the ideal morphism $cs = 1c1$.
    We translate this observations into adapted new objects
    that we call the ``MP-families''.
    In this way, the reconstruction problem becomes an adjunction problem.
    To each Dedekind category we associate a MP-family;
    to each functor between two Dedekind categories
    we associate a morphism between two MP-families.
    This association is a functor.
    This functor has a left adjoint.
    This result is also true for other universes:
    distributive lattices, Boolean algebras.}
}

@article{Olivier1997249,

  author = {Jean-Pierre Olivier and Dany Serrato},
  title = {{Peirce} allegories. {Identities} involving transitive elements and symmetrical ones},
  bibliographies = {RelMiCS},
  journal = JPAA,
  volume = 116,
  number = {1--3},
  pages = {249--271},
  year = 1997,
  issn = {0022-4049},
  DOI = {http://dx.doi.org/10.1016/S0022-4049(96)00167-3},
  DirectURL = {http://www.sciencedirect.com/science/article/pii/S0022404996001673},
  abstract = {Peirce allegories are defined.
    They mix two dual allegory structures, as fuzzy matrix theory does.
    Sequential applications of the transitive closure,
    the transitive interior, the symmetric closure
    and the symmetric interior are studied in Peirce allegories.
    Semi-direct product of Peirce algebras by finite acting groups are defined.
    Internal characterization of such algebras is established.
    Properties of the coimage functor are given.
    Conversely, a Peirce semi-allegory is associated to a functor
    (on a suitable category) which resembles a coimage functor.}
}


@InProceedings{Ollongren-1978,
  author = {A. Ollongren},
  title = {On Multilevel Graph Grammars},
  crossref = {GG1978},
  pages = {341--349},
  abstract = {The so-called operational definition of the
		  semantics of programming languages is often based
		  upon the idea that the meaning of a program is
		  determined by its interpretation by an abstract
		  machine. During interpretation the machine carries
		  out state transitions, the states being vectors in
		  some appropriate set. In this spirit the Vienna
		  definition method and the Vienna definition
		  languange for the formal semantics of procedural
		  programming languages have been developed, and to a
		  certain extent also the Vienna development method
		  for ptogramming concepts. The present author has
		  worked on the Vienna method a number of years; at
		  first the Vienna interpreting machine was used,
		  later it was modified [1, chapter 8], but still
		  using the selector-labelled trees as basic data
		  objects. In more recent work our group has
		  generalised both the abstract machine [2], and the
		  basic data objects [3,4,5]. The latter have been
		  replaced by abstract objects defined over a set of
		  selectors and a set of elementary objects, and
		  representable as multi-level selector-labelled
		  rooted digraphs. In the present note I give the
		  general definition of the abstract machine, a
		  definition of the abstract objects and abstract
		  grammars over them, and, finally, an example (the
		  interpretation of binary expressions) to show
		  possible uses of the multi-level graphs.}
}

@InProceedings{Ong-1988,
  UniBwM = {INF700/Z4356-29},
  WKloc = {A-0039},
  abstract = {Much of what is known about the model theory and proof
	theory of the $\lambda$-calculus is {\em sensible} in nature,
	i.e.\ only {\em head normal forms} are semantically meaningful.
	However, most functional languages are {\em lazy}, i.e.\ Programs
	are evaluated in normal order to {\em weak head normal forms}. In
	this paper, we seek to develop a theory of {\em lazy} or {\em
	stronly sensible} $\lambda$-calculus that corresponds to practice.
	A pure lazy language $\lambda l$ is defined in which the only
	computational observable is convergence to abstraction. $\lambda l$
	is not fully abstract w.r.t.\ $D$, the initial sulution of the
	domain equation $D \cong [D \tfun D]_{\bottom}$ --- the canonical
	model; however, $\lambda l_P$ which is $\lambda l$ augmented with a
	{\em parallel convergence} construct P is. Two more languages
	$\lambda l_C$ ($\lambda l$ with {\em convergence testing}) and
	$\lambda l_{\omega}$ ($\lambda l$ with {\em projections}) with
	expressive powers between those of $\lambda l$ and $\lambda l_P$
	are introduced and their full abstraction properties w.r.t.\ $D$
	are studied. A general method for constructing fully abstract
	models for a class of lazy languages, including $\lambda l_C$ and
	$\lambda l_{\omega}$, is illustrated. A new formal system
	$\lambda \beta C$ ($\lambda \beta$-calculus with convergence
	testing $C$) is introduced and its properties investigated.},
  year = 1988,
  title = {Fully Abstract Models of the Lazy Lambda Calculus},
  pages = {368--377},
  organization = {IEEE},
  month = OCT,
  booktitle = {Symposium on Foundations of Computer Science},
  author = {C.-H. Luke Ong},
  address = {Washington, D.C.}
}

@Misc{Ong-1997,
  author =	 {C.-H. L. Ong},
  title =	 {Lambda Calculus},
  howpublished = {OUCL PRG Lecture Notes},
  year =	 1997,
  WKloc = 	 {A-1713},
  bibliographies = {FP}
}

@InProceedings{Ong-Ritter-1993,
  author = {C.-H. Ong and E. Ritter},
  title = {A Generic Strong Normalisation Argument: Application
		  to the Calculus of Constructions},
  crossref = {CSL93},
  pages = {261--279},
  WKloc = {A-0344},
  abstract = {$\ldots$},
  annote = {mostly appendices}
}

@Article{Ono-1957,
  author = {K. Ono},
  title = {On some Properties of Binary Relations},
  journal = NAGOYA,
  volume = 12,
  year = 1957,
  pages = {161--170},
  bibliographies = {RelMiCS}
}

@Article{Ore-1942,
  author = {Oystein Ore},
  title = {Theory of Equivalence Relations},
  journal = DUKE,
  volume = 9,
  year = 1942,
  pages = {573--627},
  bibliographies = {RelMiCS}
}

@InProceedings{Orejas-2008,
  author = 	 {Fernando Orejas},
  title = 	 {Graph Constraints},
  crossref =  {FASE2008},
  pages = 	 {\unfinished},
  bibliographies = {TGV},
  annote = 	 {Sound and complete decision procedures for several classes of graph constraints

      $\exists C$ iff there is a monomorphism from $C$ to $G$

      $\forall c : X \tfun C$ iff
      for each $h : X \tfun G$ there is $f : C \tfun G$ such that $h = c \RELcmp f$.

      $\forall$ could emulate typing elements.}
}

@Article{Orejas-2011,
 author = {Orejas, Fernando},
 title = {Symbolic Graphs for Attributed Graph Constraints},
 journal = JSYCO,
 issue_date = {March, 2011},
 volume = {46},
 number = {3},
 month = MAR,
 year = {2011},
 issn = {0747-7171},
 pages = {294--315},
 numpages = {22},
 DOIURL = {http://dx.doi.org/10.1016/j.jsc.2010.09.009},
 DOI = {10.1016/j.jsc.2010.09.009},
 acmid = {1937400},
 publisher = {Academic Press, Inc.},
 address = {Duluth, MN, USA},
 keywords = {Attributed graphs, Graph constraints, Symbolic graphs},
 abstract = {In this paper we present a new class of graphs,
   called symbolic graphs,
   to define a new class of constraints on attributed graphs.
   In particular, in the first part of the paper,
   we study the category of symbolic graphs
   showing that it satisfies some properties,
   which are the basis for the work that we present in the second part of the paper,
   where we study how to reason with attributed graph constraints.
   More precisely, we define a set of inference rules,
   which are the instantiation of the inference rules defined in a previous paper,
   for reasoning about constraints on standard graphs,
   showing their soundness and (weak) completeness.
   Moreover, the proof of soundness and completeness is also an instantiation
   of the corresponding proof for standard graph constraints,
   using the categorical properties studied in the first part of the paper.
   Finally, we show that adding a new inference rule
   makes our system sound and strongly complete.}
}

@InProceedings{Orejas-Lambers-2010,
  author =       {Fernando Orejas and Leen Lambers},
  title =        {Delaying Constraint Solving in Symbolic Graph Transformation },
  crossref =  {ICGT2010},
  DOI = {10.1007/978-3-642-15928-2_4},
  pages =     {43--58},
  abstract =    {Applying an attributed graph transformation rule
    to a given object graph always implies some kind of constraint solving.
    In many cases, the given constraints are almost trivial to solve.
    For instance, this is the case when a rule describes
    a transformation $G \Rightarrow H$,
    where the attributes of $H$ are obtained by some simple computation
    from the attributes of $G$.
    However there are many other cases where the constraints to solve
    may be not so trivial and, moreover, may have several answers.
    This is the case, for instance,
    when the transformation process includes some kind of searching.
    In the current approaches to attributed graph transformation
    these constraints must be completely solved
    when defining the matching of the given transformation rule.
    This kind of \emph{early binding} is well-known
    from other areas of Computer Science to be inadequate.
    For instance, the solution chosen for the constraints
    associated to a given transformation step may be not fully adequate,
    meaning that later, in the search for a better solution,
    we may need to backtrack this transformation step.

    In this paper, based on our previous work on the use of \emph{symbolic graphs}
    to deal with different aspects related with attributed graphs,
    including attributed graph transformation,
    we present a new approach that allows us
    to delay constraint solving when doing attributed graph transformation.
    In particular we show that the approach is
    sound and complete with respect to standard attributed graph transformation.
    A running example, where a graph transformation system
    describes some basic operations of a travel agency,
    shows the practical interest of the approach.}
}

@Article{Orejas-Lambers-2010b,
  author =       {Fernando Orejas and Leen Lambers},
  title =        {Symbolic Attributed Graphs for Attributed Graph Transformation},
  journal =      ECEASST,
  year =         2010,
  volume =    30,
  number =    {{Graph and Model Transformation 2010}},
  pages =     {2.1--2.25},
  abstract = {In this paper we present a new approach
    to deal with attributed graphs and attributed graph transformation.
    This approach is based on working with what we call symbolic graphs,
    which are graphs labelled with variables together with a formula that
    constrains the possible values that we may assign to these variables.
    In particular, in this paper we will compare in detail this new approach
    with the standard approach to attributed graph transformation.}
}

@InProceedings{Orejas-Pino-Ehrig-1994,
  author = {Fernando Orejas and Elvira Pino and Hartmut Ehrig},
  title = {Algebraic Methods in the Compositional Analysis of
		  Logic Programs},
  crossref = {MFCS94},
  pages = {112--126},
  authorsAddress = {Barcelona},
  WKloc = {A-0310},
  abstract = {The compositionality of the semantics of logic
		  programs with respect to (different varieties of)
		  program union has been studied recently by a number
		  of researchers. The approaches used can be
		  considered quite ad-hoc in the sense that they
		  provide, from scratch, the semantic constructions
		  needed to ensure compositionality and, in some
		  cases, full abstraction in the given framework. In
		  this paper, we study the application of general
		  algebraic methods for obtaining, systematically,
		  this kind of results. In particular, the method
		  proposed consists in defining the adequate
		  inststution for describing the given class of logic
		  programs and, then, in using general
		  inststution-independent results to prove
		  compositionality and full abstraction. This is done
		  in detail for the class of definite logic programs,
		  where the associated inststution is defined in such
		  a way that initial algebra semantics is equivalent
		  to computed answer substitution semantics. Then a
		  similar solution is sketched for definite logic
		  programs with constraints and equality and for
		  normal logic programs with constructive negation.},
  annote = {Cites \cite{Burstall-Goguen-1980} as source for institutions.}
}

@InProceedings{Orlowska-1983,
  author = {Orlowska, Ewa},
  title = {Semantics of vague concepts},
  booktitle = {Foundations of Logic and Linguistics. Problems and Solutions.
                  Selected contributions to the {{$7^{th}$} Internat.\null{} Congress
                  of Logic, Methodology, and Philosophy of Science,
                  {Salzburg} 1983}},
  editor = {Dorn, G. and Weingartner, P.},
  year = 1983,
  publisher = Plenum,
  address = {London, New York},
  pages = {465--482},
  bibliographies = {RelMiCS}
}

@Book{Orlowska-1984,
  author = {Ewa Orlowska},
  title = {Reasoning About Database Constraints},
  publisher = PolishC,
  series = {PAS Reports},
  volume = 543,
  address = {Warsaw},
  year = 1984,
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1984a,
  author = {Ewa Or{\l}owska},
  title = {A logic of indiscernibility relations},
  pages = {177--186},
  ISBN = {3-540-16066-3},
  editor = {Andrzej Skowron},
  booktitle = {Proc.\null{} of the {{$5^{th}$} Sympos.\null{}  on Computation
                 Theory}},
  address = {Zabor{\'o}w, Poland},
  month = dec,
  year = 1984,
  series = LNCS,
  volume = 208,
  publisher = Springer,
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1985,
  author = {Ewa Orlowska},
  title = {Logic of nondeterministic information},
  journal = STUDLOG,
  year = 1985,
  volume = 44,
  pages = {93--102},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1987,
  author = {Orlowska, Ewa},
  title = {Algebraic Approach to Database Constraints.},
  journal = FUNDI,
  volume = {X},
  year = 1987,
  pages = {57--68},
  annote = {Database constraints are shown to be representable in terms of
      some binary relations derived from relational databases.},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1988,
  author = {Ewa Orlowska},
  title = {Proof System for Weakest Prespeficiation},
  journal = IPLET,
  volume = 27,
  year = 1988,
  pages = {309--313},
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1988a,
  author = {Ewa Orlowska},
  title = {Kripke models with relative accessibility and their
                  application to inferences from incomplete information},
  booktitle = {Mathematical Problems in Computation Theory},
  editor = {Mirkowska, G. and Rasiowa, H.},
  volume = 21,
  series = {Banach Center Publications},
  year = 1988,
  pages = {329--339},
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1988b,
  author = {Ewa Orlowska},
  title = {Relational interpretation of modal logics},
  pages = {443--471},
  crossref = {AL1991},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1989,
  author = {Orlowska, Ewa},
  title = {Interpretation of Dynamic Logic and its Extensions in the
      Relational Calculus.},
  journal = BUPOLLOG,
  volume = 18,
  number = 4,
  year = 1989,
  pages = {132--137},
  annote = {Methodology from \cite{Orlowska1987} is applied to dynamic
      logic.},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1989a,
  author = {Ewa Orlowska},
  title = {Logic for reasoning about knowledge},
  journal = ZMALOG,
  year = 1989,
  volume = 35,
  pages = {559--572},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1990,
  author = {Orlowska, Ewa},
  title = {Interpretation of Relevant Logics in a Logic of Ternary
      Relations.},
  journal = BUPOLLOG,
  volume = 19,
  number = 2,
  year = 1990,
  pages = {39--48},
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1991,
  author = {Ewa Orlowska},
  title = {Relational Interpretation of Modal Logics},
  crossref = {AL1991},
  annote = {Abstract in the Journal of Symbolic Logic 57, 1992, p.322.
      Relational semantics and relational proof system for modal logics is
      developed.},
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1991a,
  author = {Orlowska, Ewa},
  title = {Semantics of Relevant Logics Based on Relation Algebras.},
  booktitle = {Abstracts of the {$9^{th}$} Internat.\null{} Congress of
      Logic, Methodology and Philosophy of Science},
  volume = 1,
  address = {Uppsala, Sweden},
  year = 1991,
  pages = 152,
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1991b,
  author = {Orlowska, Ewa},
  title = {{Relational Proof Systems for some AI Logics.}},
  booktitle = {Proc.\null{} of the Internat.\null{} Conf.\null{} on
      Fundamentals of Artificial Intelligence Research},
  publisher = Springer,
  volume = 535,
  series = LNAI,
  year = 1991,
  pages = {33--47},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1992,
  author = {Orlowska, Ewa},
  title = {Relational Proof Systems for Relevant Logics.},
  journal = JSYLO,
  volume = 57,
  year = 1992,
  pages = {1425--1440},
  bibliographies = {RelMiCS}
}

@Article{Orlowska-1993,
  author = {Orlowska, Ewa},
  title = {Dynamic Logic with Program Specifications and its Relational
      Proof System.},
  journal = JANCL,
  volume = 3,
  year = 1993,
  pages = {147--171},
  bibliographies = {RelMiCS}
}

@InCollection{Orlowska-1994,
  author = {Orlowska, Ewa},
  title = {Relational Semantics for Non-classical Logics: Formulas are
      Relations.},
  booktitle = {Philosophical Logic in Poland.},
  editor = {Wolenski, J.},
  publisher = Kluwer,
  year = 1994,
  pages = {167--186},
  bibliographies = {RelMiCS}
}

@InCollection{Orlowska-1995,
  author = {Orlowska, Ewa},
  title = {Temporal Logics --- in a Relational Framework},
  booktitle = {Time and Logic --- A Computational Approach.},
  editor = {Bolc, L. and Szalas,A.},
  publisher = {Univ.\null{} College London Press},
  year = 1995,
  pages = {249--277},
  bibliographies = {RelMiCS}
}

@InProceedings{Orlowska-1995a,
  author = {Ewa Orlowska},
  title = {Relational proof systems for modal logics},
  booktitle = {Proof Theory of Modal Logic},
  editor = {Wansing, H.},
  year = 1995,
  publisher = Kluwer,
  pages = {55--77},
  bibliographies = {RelMiCS}
}

@InCollection{Orlowska-1997,
  author = {Ewa Orlowska},
  title = {Relational Formalisation of Nonclassical Logics},
  chapter = 6,
  pages = {90--105},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Book{Orlowska-GolinskaPilarek-2011,
  ALTauthor =    {E. Or{\l}owska and J. Goli{\'n}ska-Pilarek},
  ALTeditor =    {},
  title =        {Dual Tableaux: Foundations, Methodology, Case Studies},
  publisher =    Springer,
  year =         {2011},
  OPTkey =       {},
  OPTvolume =    {36},
  OPTnumber =    {},
  OPTseries =    {Trends in Logic},
  DOI =   {10.1007/978-94-007-0005-5_1},
  bibliographies =   {RelMiCS},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@Article{Orlowska-Pawlak-1984,
  author = {Ewa Orlowska and Pawlak, Z.},
  title = {Representation of nondeterministic information},
  journal = TCS,
  year = 1984,
  volume = 29,
  pages = {27--39},
  bibliographies = {RelMiCS}
}

@InProceedings{Otth-1993,
  author = {Daniel F. Otth},
  title = {Monotonic versus Antimonotonic Exponentiation},
  pages = {318--327},
  abstract = {We investigate the relationship between the monotonic
             ($\rightarrow$) and the antimonotonic exponentiation
             ($\rightarroW$) in a type system with subtyping. We present a
             model in which we can develop both exponentiations at the same
             time. In this model the monotonic and the antimonotonic
             exponentiation enjoy a duality, namely $\alpha \rightarrow
             \beta ={\sf C}(\alpha \rightarrow {\sf C} \beta$)
             where ${\sf C}$ is the type
             constructor complement. We give a sound and complete system of
             sxioms for the type system with the type constructors
             $\rightarrow, \rightarroW, \cup, \cap, {\sf C}, \bottom, \top$.},
  crossref = {TLCA93},
  WKloc = {A-0187}
}

@InProceedings{Otto-1994,
  title = {Generalized Quantifiers for Simple Properties},
  author = {Martin Otto},
  pages = {30--39},
  crossref = {LICS9},
  abstract = {We consider extensions of fixed-point logic by means of
      generalized quantifiers in the context of descriptive complexity. By
      the well-known theorem of Immerman and Vardi, fixed-point logic
      captures PTime over linearly ordered structures. It fails, however,
      to express even most fundamental structural properties, like simple
      cardinality properties, in the absence of order. \par In the present
      investigation we concentrate on extensions by generalized quantifiers
      which serve to adjoin simple or basic structural properties. An
      abstract notion of simplicity is put forward which isolates those
      structural properties, that can be characterized in terms of a
      concise structural invariant. The key examples are provided by all
      monadic and cardinality properties in a very general sense. \par The
      main theorem establishes that no extension by any family of such
      simple quantifiers can cover all of PTime. These limitations are
      proved on the basis of the semantically motivated notion of
      simplicity; in particular there is no implicit bound on the arities
      of the generalized quantifiers involved. Quite to the contrary, the
      natural applications concern infinite families of quantifiers
      adjoining certain structural properties across all arities in a
      uniform way.}
}

@InProceedings{Ounalli-Jaoua-Belkhiter-1994,
  author = {H. Ounalli and Ali Jaoua and N. Belkhiter},
  title = {Rectangular Decomposition of {$n$}-ary Relations},
  booktitle = {{$7^{th}$} SIAM Conf.\null{} on Discrete Mathematics},
  year = 1994,
  month = jun,
  address = {Albuquerque, NM},
  pages = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Oury-Swierstra-2008,
  author =       {Nicolas Oury and Wouter Swierstra},
  title =        {The Power of {Pi}},
  crossref =  {ICFP2008},
  WKloc =        {A-1756},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTpages =     {},
  OPTmonth =     {},
  OPTaddress =   {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote =      {},
  OPTannote =    {}
}

@Book{Ousterhout-1994,
  author = {J. Ousterhout},
  title = {{Tcl} and the {Tk} Toolkit},
  publisher = {Addison-Wesley},
  year = 1994,
  ISBN = {0-201-63337-X}
}

@Article{Owre-Rushby-Shankar-vonHenke-1995,
  title = {Formal Verification for Fault-Tolerant Architectures:
	   Prolegomena to the Design of {PVS}},
  author = {Sam Owre and John Rushby and Natarajan Shankar and
		Friedrich von Henke},
	JOURNAL = {IEEE Transactions on Software Engineering},
	PAGES = {107--125},
	VOLUME = 21,
	NUMBER = 2,
	MONTH = feb,
	YEAR = 1995,
  bibliographies = {OPG, RelMiCS}
}

@InProceedings{Owre-Rushby-Shankar-1997,
  title = {Integration in {PVS}: Tables, Types, and Model Checking},
  author = {Sam Owre and John Rushby and N. Shankar},
  booktitle = {Tools and Algorithms for the Construction and Analysis
          of Systems {TACAS '97}},
  editor = {Ed Brinksma},
  address = {Enschede, The Netherlands},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  number = 1217,
  month = apr,
  year = 1997,
  pages = {366--383},
  WKloc = {doc/pap/BIB},
  bibliographies = {SpecTech, SeminarWT2000, RelMiCS}
}

@TechReport{Owre-Rushby-Shankar-1997a,
  title = {Analyzing Tabular and State-Transition Requirements Specification in {PVS}},
  author = {Sam Owre and John Rushby and N. Shankar},
  institution = {Computer Science Laboratory, SRI International},
  year = 1995,
  number = {CSL-95-12},
  address = {Menlo Park CA 94025 USA},
  month = JUN,
  note = {Revised April 1996},
  WKloc = {A-1149 abandoned in Tunis 2008},
  bibliographies = {SpecTech, RelMiCS}
}

@TechReport{Owre-Shankar-1999,
  author = 	 {Sam Owre and N. Shankar},
  title = 	 {The Formal Semantics of {PVS}},
  institution =  {SRI International, Computer Science Laboratory},
  year = 	 1999,
  number =	 {CSL-97-2R},
  month =	 MAR,
  WKloc = 	 {A-1716}
}

@Manual{PAKCS2007,
  title = 	 {PAKCS 1.8.0, The Portland Aachen Kiel Curry System},
  OPTkey = 	 {},
  OPTeditor = 	 {Michael Hanus},
  author = 	 {Michael Hanus and others},
  OPTorganization = {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  month = 	 MAR,
  year = 	 {2007},
  note = 	 {\url{http://www.informatik.uni-kiel.de/~pakcs/}},
  OPTannote = 	 {}
}

@Misc{POWER-guide-2004,
  author =	 {Brett Olsson and Anthony J. Marsala},
  title =	 {A Developer's Guide to the {POWER} Architecture}},
  OPThowpublished = {},
  OPTmonth = 	 MAR,
  OPTyear = 	 {2004},
  WKloc = 	 {A-1729},
  annote = 	 {11 pages PowerPC ISA overview}
}

@Manual{PowerPC-UserISA-2.01,
  title = 	 {{PowerPC} User Instruction Set Architecture},
  author =	 {Joe Wetzel and Ed Silha and Cathy May and Brad Frey},
  organization = {IBM},
  month =	 SEP,
  year =	 2003,
  WKloc = 	 {A-1730}
}

@TechManual{PVM,
  title = {The PVM System},
  year = {},
  key = {PVM},
  OPTmonth = {},
  institution = {},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  WKloc = {A-0783},
  keywords = {parallel virtual machine}
}

@Manual{PVS1999,
  title = {PVS Language Reference, Version 2.3},
  author = {S. Owre and N. Shankar and J. M. Rushby and D. W. J. Stringer-Calvert},
  organization = {SRI International},
  month = SEP,
  year = 1999,
  URL = {http://pvs.csl.sri.com/},
  WKloc = {B-0065},
  bibliographies = {HHOL}
}

@TechReport{PVS_Roadmap2003,
        Author= {{Formal Methods Program}},
        Title= {Formal Methods Roadmap: PVS, ICS, and SAL},
        Number= {SRI-CSL-03-05},
        Institution= {Computer Science Laboratory, SRI International},
        Address= {Menlo Park, CA},
        Month= oct,
        Year= 2003,
  WKloc = {doc/pap/BIB}
}

@Article{Paakki-1995,
  author = {Jukka Paakki},
  title = {Attribute Grammar Paradigms --- {A} High-Level
                 Methodology in Language Implementation},
  journal = ACMCS,
  volume = 27,
  number = 2,
  pages = {196--255},
  OPTmonth = jun,
  year = 1995,
  coden = {CMSVAN},
  ISSN = {0360-0300},
  WKloc = {B-0051, doc/pap/BIB},
  DOIURL = {http://doi.acm.org/10.1145/210376.197409},
  abstract = {Attribute grammars are a formalism for specifying
                 programming languages. They have been applied to a
                 great number of systems automatically producing
                 language implementations from their specifications. The
                 systems and their specification languages can be
                 evaluated and classified according to their level of
                 application support, linguistic characteristics, and
                 degree of automation.\par A survey of attribute
                 grammar-based specification languages is given. The
                 modern advanced specification languages extend the core
                 attribute grammar model with concepts and primitives
                 from established programming paradigms. The main ideas
                 behind the developed attribute grammar paradigms are
                 discussed, and representative specification languages
                 are presented with a common example grammar. The
                 presentation is founded on mapping elements of
                 attribute grammars to their counterparts in programming
                 languages. This methodology of integrating two
                 problem-solving disciplines together is explored with a
                 classification of the paradigms into structured,
                 modular, object-oriented, logic, and functional
                 attribute grammars. The taxonomy is complemented by
                 introducing approaches based on an implicit parallel or
                 incremental attribute evaluation paradigm.},
  acknowledgement = ack-nhfb,
  keywords = {design; languages; theory},
  subject = {{\bf D.3.1}: Software, PROGRAMMING LANGUAGES, Formal
                 Definitions and Theory. {\bf F.4.2}: Theory of
                 Computation, MATHEMATICAL LOGIC AND FORMAL LANGUAGES,
                 Grammars and Other Rewriting Systems. {\bf D.3.4}:
                 Software, PROGRAMMING LANGUAGES, Processors,
                 Compilers.}
}

@Book{Pacheco-1997,
  author =	 {Peter S. Pacheco},
  title = 	 {Parallel Programming with {MPI}},
  publisher = 	 {Morgan Kaufmann},
  year = 	 1997,
  McMaster = 	 {QA 76.642 .P3 1997}
}

@TechManual{Pacheco-1998,
  author = {Peter S. Pacheco},
  title = {A user's Guide to {MPI}},
  year = 1998,
  month = MAR,
  institution = {Department of Mathematics, University of San Francisco},
  WKloc = {A-0778}
}

@InProceedings{Padawitz-1978,
  WKloc = {A-0059},
  abstract = {Transformations of graphlike expressions are called correct
	if they preserve a given functional semantics of the expressions.
	Combining the algebraic theory of graph grammars (cf./Eh 78/) and
	the ADJ approach to semantics of programming languages it will be
	proved that the correctness of transformation rules carries over to
	the correctness of derivations via such rules. Applying this result
	to LISP we show that a LISP interpreter represented by a graph
	grammar is correct with respect to the functional semantics of
	graphlike LISP expressions.},
  title = {Graph Grammars and Operational Semantics},
  pages = {350--366},
  note = {preliminary version of: Padawitz82},
  crossref = {GG1978},
  author = {Peter Padawitz}
}

@Article{Padawitz-1982,
  WKloc = {A-0046},
  abstract = {Transformations of graphlike expressions are called correct
	if they preserve a given functional semantics of the expressions.
	Combining the algebraic theory of graph grammars (cf. [10]) and
	programming language semantics (cf. [1]) it will be
	proved that the correctness of transformation rules carries over to
	the correctness of derivations via such rules. Applying this result
	to LISP we show that a LISP interpreter represented by a graph
	grammar is correct with respect to the functional semantics of
	graphlike LISP expressions.},
  year = 1982,
  volume = 19,
  title = {Graph Grammars and Operational Semantics},
  pages = {117-141},
  number = 1,
  journal = TCS,
  author = {Peter Padawitz}
}

@Book{Padawitz-1992,
  ISBN = {0-521-41723-6},
  year = 1992,
  volume = 28,
  title = {Deduction and Declarative Programming},
  series = {Cambridge tracts in theoretical computer science},
  publisher = {Cambridge University Press},
  author = {Peter Padawitz},
  annote = {tubibmue}
}

@Article{Padawitz-2000,
  author = {Peter Padawitz},
  title = {Swinging Types = Functions + Relations + Transition Systems},
  pages = {61 pages},
  journal = TCS,
  year = 2000,
  volume = 243,
  pages = {93--165},
  URL = {http://ls5.cs.uni-dortmund.de/~peter/Swinging.html},
  WKloc = {A-0957},
  bibliographies = {RelMiCS, RelMiS}
}

@Unpublished{Padawitz-,
  author = {Peter Padawitz},
  title = {Swinging Types --- Syntax, Semantics, and Theory},
  pages = {24 pages},
  URL = {http://ls5.cs.uni-dortmund.de/~peter/Swinging.html},
  WKloc = {A-1669},
  bibliographies = {RelMiCS, RelMiS}
}

@Unpublished{Padawitz-2000a,
  author = {Peter Padawitz},
  title = {Sample Swinging Types},
  pages = {98 pages},
  note = {Last update: February 20, 2000},
  URL = {http://ls5.cs.uni-dortmund.de/~peter/Swinging.html},
  WKloc = {A-0958},
  bibliographies = {RelMiCS, RelMiS}
}

@Manual{Padawitz-2002a,
  title = {Expander2 --- a formal methods presenter and animator},
  author = {Peter Padawitz},
  month = OCT,
  year = 2002,
  WKloc = {A-1444},
  URL = {http://ls5-www.cs.uni-dortmund.de/~peter/Expander2/Expander2.html},
  bibliographies = {MathScheme}
}

@TechReport{Padberg-1993,
  author = {Julia Padberg},
  title = {Survey of High-Level Replacement Systems},
  year = 1993,
  abstract = {High-Level Replacement (HLR) Systems have been studied as a
             generalization of graph grammars and transformation systems to
             other high-level structures. This paper presents a survey of
             results in high-level replacement systems, various
             HLR-conditions and examples of HLR-categories.},
  number = {93-8},
  institution = {Technische Universit\"at Berlin},
  month = MAR,
  type = {Forschungsbericht des Fachbereichs Informatik},
  WKloc = {A-0225}
}

@InProceedings{Pagano-1998,
  author = {Bruno Pagano},
  title = {{X.R.X} : Explicit Reduction Systems --- A First-Order Calculus for Higher-Order Calculi},
  crossref = {CADE1998},
  pages = {72--87},
  OPTabstract = {},
  WKloc = {A-0603},
  keywords = {explicit substitutions}
}

@InProceedings{Paige-1994,
  author = {Robert Paige},
  title = {Viewing A Program Transformation System At Work},
  crossref = {PLILP1994},
  pages = {5--24},
  WKloc = {A-0306},
  abstract = {How to decrease labor and improve reliability in the
		  development of efficient implementations of
		  nonnumerical algorithms and labor intensive software
		  is an increasingly important problem as the demand
		  for computer technology shifts from easier
		  applications to more complex algorithmic ones; e.g.,
		  optimizing compilers for supercomputers, intricate
		  data structures to implement efficient solutions to
		  operations research problems, search and analysis
		  problems on genetic engineering, complex software
		  tools for workstations, design automation, etc. It
		  is also a difficult problem that is not solved by
		  current CASE tools and software management
		  disciplines, which are oriented towards data
		  processing and other applications, where the
		  implementation and a prediction of its resource
		  utilization follow more directly from the
		  specification.

                  Recently, Cai and Paige reported experiments
		  suggesting a way to implement nonnumerical
		  algorithms in C at a programming rate (i.e., source
		  lines per second) that is at least five times
		  greater than a conventional approach in which C
		  programs are written entirely by hand. The runtime
		  performance of the C programs produced by this new
		  approach was shown to be comparable to good hand
		  code. The proposed software development methodology
		  makes use of fully automatic, generic program
		  transformations that capture algorithm design
		  principles. This paper discusses some of the ideas
		  underlying the transformational methodology, and
		  illustrates these ideas through explanatory examples
		  of the APTS system at work.}
}

@Article{Paige-1997,
  author = {Paige, R.},
  title = {Future directions in program transformations},
  journal = {SIGPLAN Not.},
  issue_date = {Jan. 1997},
  volume = {32},
  number = {1},
  month = jan,
  year = {1997},
  issn = {0362-1340},
  pages = {94--98},
  numpages = {5},
  DOIURL = {http://doi.acm.org.libaccess.lib.mcmaster.ca/10.1145/251595.251609},
  DOI = {10.1145/251595.251609},
  acmid = {251609},
  OPTpublisher = {ACM},
  OPTaddress = {New York, NY, USA},
  abstract = {As the emphasis of Programming Languages has evolved
                  from computing on numbers to computing on programs,
                  the area of program transformations has naturally
                  gained in importance. This paper briefly surveys
                  what transformational programming is about, and how
                  to make progress in the field.}
}

@InProceedings{Paige-1998,
  author = {Richard F. Paige},
  title = {Comparing Extended {Z} with a Heterogeneous Notation for Reasoning about Time and Space},
  crossref = {ZUM1998},
  pages = {214--232},
  WKloc = {A-1332}
}

@InProceedings{Paindaveine-Milojicic-1996,
  author = {Yves Paindaveine and Dejan S. Milo{\'\j}i{\caron{c}}i{\'c}},
  title = {Process versus Task Migration},
  booktitle = {Proc. 29th Annual Hawaii International Conference on System Sciences},
  pages = {636--645},
  year = 1996,
  bibliographies = {ProcMig}
}

@Article{Palacz-2003,
  author =       {Wojciech Palacz},
  title =        {Algebraic Hierarchical Graph Transformation},
  journal =      {Journal of Computer and System Sciences},
  year =         2004,
  volume =    68,
  number =    3,
  pages =     {497-520 },
  DOIURL =     {http://dx.doi.org/10.1016/S0022-0000(03)00064-3},
  WKloc = {doc/pap/BIB},
  bibliographies =      {GraTraVis},
  abstract =    {This paper presents a framework
    for constructing hierarchical (hyper)graphs,
    using one of the well-known categories
    of traditional flat (hyper)graphs as a base.
    Hierarchical graphs are obtained from flat graphs
    by adding a parent assigning function to them.
    Any graph atom (vertex or edge)
    can be assigned as a child of any other atom.
    Hierarchical graphs are more expressive than flat graphs,
    yet similar enough that the
    double-pushout approach to graph transformation
    can be extended to them.}
}

@incollection{Palamidessi-1990,
  year={1990},
  isbn={978-3-540-52826-5},
  booktitle={Automata, Languages and Programming},
  volume={443},
  series=LNCS,
  editor={Paterson, Michael S.},
  DOI={10.1007/BFb0032046},
  title={Algebraic properties of idempotent substitutions},
  DOIURL={http://dx.doi.org/10.1007/BFb0032046},
  publisher={Springer Berlin Heidelberg},
  author={Palamidessi, Catuscia},
  pages={386--399},
  language={English},
  abstract = {This paper presents an algebra of idempotent substitutions whose operations have many properties. We provide an algorithm to compute these operations and we show how they are related to the standard composition. The theory of Logic Programming can be rewritten in terms of these new operations. The advantages are that both the operational and the declarative semantics of Horn Clause Logic can be formalized in a compositional way and the proofs of standard results, like the switching lemma, get easier and more intuitive. Moreover, this formalization can be naturally extended to a parallel computational model, and therefore it can be regarded as a basis for a theory of concurrent logic programming.}
}

@Misc{Palmgren-2005,
  author =    {Erik Palmgren},
  title =     {{Bishop}'s set theory},
  howpublished = {Slides presented at TYPES Summmer School in G\"oteborg},
  month =     AUG,
  year =      2005,
  WKloc =    {doc/pap/BIB},
  bibliographies = {AgdaTG}
}

@Article{Palmgren-2012,
  author =       {Erik Palmgren},
  title =        {Constructivist and Structuralist Foundations:
                  {Bishop}'s and {Lawvere}'s Theories of Sets},
  journal =      {Annals of Pure and Applied Logic},
  year =         {2012},
  volume =    {163},
  DOI =       {10.1016/j.apal.2012.01.011},
  DOIURL = {http://dx.doi.org/10.1016/j.apal.2012.01.011},
  number =    {10},
  pages =     {1384--1399},
  annote =     {Revised version, August 11, 2010: arXiv:1201.6272v1},
  WKloc =    {doc/pap/BIB},
  abstract = {Bishop's informal set theory is briefly discussed
    and compared to Lawvere's Elementary Theory of the Category of Sets (ETCS).
    We then present a constructive and predicative version of ETCS,
    whose standard model is based on the constructive type theory of Martin-Löf.
    The theory, CETCS, provides a
    structuralist foundation for constructive mathematics in the style of Bishop.},
  bibliographies = {AgdaTG}
}

@InProceedings{Palsberg-1994,
  title = {Efficient Inference of Object Types},
  author = {Jens Palsberg},
  pages = {186--195},
  crossref = {LICS9},
  WKloc = {A-0369},
  abstract = {Abadi and Cardelli have recently investigated a calculus
		  of objects.  The calculus supports a key feature of
		  object-oriented languages: an object can be emulated
		  by another object that has more refined methods.
		  Abadi and Cardelli presented four first-order type
		  systems for the calculus.  The simplest one is based
		  on finite types and no subtyping, and the most
		  powerful one has both recursive types and subtyping.
		  Open until now is the question of type inference,
		  and in the presence of subtyping ``the absence of
		  minimum typings poses practical problems for type
		  inference''.

                  In this paper we give an $O(n^3)$ algorithm for each
		  of the four type inference problems and we prove
		  that all the problems are P-complete.}
}

@InProceedings{Palsberg-1994a,
  author = {Jens Palsberg},
  title = {Global Program Analysis in Constraint Form},
  crossref = {CAAP94},
  pages = {276--290},
  keywords = {abstract interpretation, Sestoft, closure analysis}
}

@Article{Palsberg-OKeefe-1995,
  author = {Jens Palsberg and Patrick {O'Keefe}},
  title = {A Type System Equivalent to Flow Analysis},
  journal = ACM-TOPLAS,
  year = 1995,
  volume = 17,
  number = 4,
  month = JUL,
  pages = {576--599},
  OPTunibwm = {INF/Z},
  WKloc = {A-0666},
  abstract = {Flow-based safety analysis of higher-order languages has
               been studied by Shivers, and Palsberg and Schwartzbach.
               Open until now is the problem of finding a type system
               that accepts exactly the same programs as safety analysis.
               In this article we prove that Amadio and Cardelli's
               type system with subtyping and recursive types
               accepts the same programs as a certain safety analysis.
               The proof involves mappings from types to flow information
               and back. As a result, we obtain an inference algorithm
               for the type system, thereby solving an open problem.}
}

@Proceedings{Pape-1975,
  keywords = {graph, graph language, graph algorithm},
  year = 1976,
  volume = 1,
  title = {Graphen-{Sprachen} und {Algorithmen} auf {Graphen}},
  series = {Berichte zur praktischen {Informatik}},
  publisher = {Hanser},
  KEY = {GraSpra '75},
  editor = {Uwe Pape}
}

@Article{Pardo-1998,
  author =       {Alberto Pardo},
  title =        {Monadic Corecursion --- Definition, Fusion Laws and Applications},
  journal =      ENTCS,
  year =         1998,
  volume =    11,
  number =    {105--139},
  DOIURL = {http://dx.doi.org/10.1016/S1571-0661(04)00055-6},
  DOI = {10.1016/S1571-0661(04)00055-6},
  WKloc = {doc/pap/BIB},
  abstract = {This paper investigates corecursive definitions
     which are at the same time monadic.
     This corresponds to functions
     that generate a data structure following a corecursive process,
     while producing a computational effect modeled by a monad.
     We introduce a functional, called monadic anamorphism,
     that captures definitions of this kind.
     We also explore another class of monadic recursive functions,
     corresponding to the composition of a monadic anamorphism
     followed by (the lifting of) a function defined by structural recursion
     on the data structure that the monadic anamorphism generates.
     Such kind of functions are captured by so-called monadic hylomorphism.
     We present transformation laws for these monadic functionals.
     Two non-trivial applications are also described.}
}

@Article{Paredaens-1980,
  author = {J. Paredaens},
  title = {Transitive Dependencies in a Database Scheme},
  journal = RAIROICS,
  volume = 14,
  number = 2,
  year = 1980,
  pages = {149--163},
  bibliographies = {RelMiCS}
}

@Book{Pareigis-69,
  author = {Bodo Pareigis},
  title = {Kategorien und Funktoren},
  publisher = {B. G. Teubner},
  year = 1969,
  address = {Stuttgart},
  UniBwM = {MAG R6390 (HB Schmidt)},
  WKloc = {Q-010},
  keywords = {category, functor, universal algebra}
}

@InProceedings{Parigot-1992,
  author = {Michel Parigot},
  title = {$\lambda\mu$-Calculus: An Algorithmic Interpretation
		  of Classical natural Deduction},
  crossref = {LPAR92},
  pages = {194--201},
  WKloc = {A-0253},
  abstract = {This paper presents a way of extending the paradigm
		  ``proofs as programs'' to classical proofs. The
		  system we use is derived from the general Free
		  Deduction system presented in [3].},
  keywords = {Curry-Howard}
}

@InProceedings{Parigot-1994,
  author = {Michel Parigot},
  title = {On Extensions of the Paradigm ``Proofs-as-Programs''
				   to Classical Logic},
  crossref = {LPAR94},
  note = {invited talk},
  authorsAddress = {Universite Paris 7, parigot\@logique.jussieu.fr},
  abstract = {After two decades of intensive studies of the
		  computational aspects of intuitionistic systems
		  through the so-called Curry-Howard isomorphism,
		  relating intuitionistic proofs to functional
		  programs, a new research topic appeared in the last
		  few years: the extension of this isomorphism to
		  classical proofs. The extension appeared both from
		  logical and programming considerations:
		  \begin{itemize}
		  \item In the computational interpretations of
		    intuitionistic logic, programs are represented by
		    proofs and computation is represented by some kind
		    of cut-elimination (or normalisation) procedure. The
		    cut-elimination mechanism being also defined for
		    classical logic, it was natural to consider it as a
		    computation mechanism in this context too, and
		    several such computational interpretations of
		    classical logic have been proposed.
		  \item In functional programming languages, the
		    purely functional character of programs appeared
		    soon as an obstacle to practical
		    programming. Functional programming languages have
		    thus been enriched, in different ways, with control
		    operators, like call/cc in Scheme. The theory of
		    these control operators, lambda-C-calculus, has been
		    made by Felleisen, and Griffin has remarked that
		    they can be typed using some form of classical logic.
		  \end{itemize}
                  The new phenomenon which appears with this
		  extension is some kind of non-determinism:
		  computational interpretations of classical logic
		  seem to lead directly to non-confluent
		  calculi. There are at least two different ways to
		  tackle this phemomenon: either to try to avoid
		  non-confluence by chosing some appropriate confluent
		  sub-calculus, or to try to use non-confluence in
		  order to build a new non-deterministic computational model.

                  In this talk we intend to:
                  \begin{itemize}
                  \item analyse the sources of non-confluence of the
		    computational interpretations of classical logic;
                  \item compare different possible restrictions to
		    confluent calculi (are some of them better than the
		    others? do they significantly differ from those
		    obtained by translations into intuitionistic
		    logic?);
                  \item to discuss the meaning and the possible uses
		    of the non-determinism associated to classcal proofs.
                  \end{itemize}}
}

@TechReport{Parigot-Duris-Roussel-Jourdan-1995,
  author = {Didier Parigot and Etienne Duris and Gilles Roussel
                 and Martin Jourdan},
  title = {Attribute Grammars: a Declarative Functional Language},
  type = {rapport de recherche},
  WKloc = {B-0051},
  number = 2662,
  year = 1995,
  month = oct,
  institution = {INRIA},
  note = {{\tt
                 ftp://ftp.inria.fr/INRIA/publication/RR/RR-2662.ps.gz}},
  URL = {ftp://ftp.inria.fr/INRIA/publication/RR/RR-2662.ps.gz},
  abstract = {Although Attribute Grammars were introduced thirty
                 years ago, their lack of expressiveness has resulted in
                 limited use outside the domain of static language
                 processing. In this paper we show that it is possible
                 to extend this expressiveness. We claim that Attribute
                 Grammars can be used to describe computations on
                 structures that are not just trees, but also on
                 abstractions allowing for infinite structures. To gain
                 this expressiveness, we introduce two new notions: {\em
                 scheme productions\/} and {\em conditional
                 productions}. The result is a language that is
                 comparable in power to most first-order functional
                 languages, with a distinctive declarative character.
                 Our extensions deal with a different part of the
                 Attribute Grammar formalism than what is used in most
                 works on Attribute Grammars, including global analysis
                 and evaluator generation. Hence, most existing results
                 are directly applicable to our extended Attribute
                 Grammars, including efficient implementation (in our
                 case, using the FNC-2 system
                 http://www-rocq.inria.fr/oscar/FNC-2/ for more
                 information.)}
}

@TechReport{Parigot-Roussel-Jourdan-Duris-1996,
  author = {Didier Parigot and Gilles Roussel and Martin Jourdan
                 and Etienne Duris},
  title = {Dynamic {A}ttribute {G}rammars},
  type = {Rapport de recherche},
  number = 2881,
  year = 1996,
  month = may,
  WKloc = {B-0051},
  institution = {INRIA},
  note = {{\tt
                 ftp://ftp.inria.fr/INRIA/publication/RR/RR-2881.ps.gz}},
  URL = {ftp://ftp.inria.fr/INRIA/publication/RR/RR-2881.ps.gz},
  abstract = {Although Attribute Grammars were introduced long ago,
                 their lack of expressiveness has resulted in limited
                 use outside the domain of static language processing.
                 With the new notion of {\em Dynamic Attribute Grammars}
                 defined on top of {\em Grammar Couples}, informally
                 presented in a previous paper, we show that it is
                 possible to extend this expressiveness and to describe
                 computations on structures that are not just trees, but
                 also on abstractions allowing for infinite structures.
                 The result is a language that is comparable in power to
                 most first-order functional languages, with a
                 distinctive declarative character. In this paper, we
                 give a formal definition of Dynamic Attribute Grammars
                 and show how to construct efficient
                 visit-sequence-based evaluators for them, using
                 traditional, well-established AG techniques (in our
                 case, using the FNC-2 system). The major contribution
                 of this approach is to restore the intrinsic power of
                 Attribute Grammars and re-emphasize the effectiveness
                 of analysis and implementation techniques developed for
                 them.}
}

@InProceedings{Parigot-Roussel-Jourdan-Duris-1996a,
  author = {Didier Parigot and Gilles Roussel and Martin Jourdan
                 and Etienne Duris},
  title = {Dynamic {A}ttribute {G}rammars},
  crossref = {PLILP1996},
  pages = {122--136},
  OPTabstract = {},
  WKloc = {A-0444}
}

@InProceedings{Parikh-1998,
  author = {Rohit Parikh},
  title = {Knowledge Based Computation (Extended Abstract)},
  crossref = {AMAST1995},
  pages = {127--132},
  OPTabstract = {},
  WKloc = {A-0621}
}

@Article{Parisi-1984,
  author = {Francesco Parisi-Presicce},
  title = {Iterative Factor Algebras and Induced Metrics},
  journal = {Theoretical Computer Science},
  year = 1984,
  volume = 28,
  number = 3,
  pages = {286--298},
  abstract = {An algebra is said to be iterative if every nontrivial
               finite system of fixed-point equations has a unique
               solution. A necessary and sufficient condition for a
               congruence on the algebra of regular trees to induce an
               iterative factor is presented and possible induced
               distance functions are discussed.}
}

@Article{Parisi-1991,
  author = {Francesco Parisi-Presicce},
  title = {Foundations of Rule-Based Design of Modular Systems},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Theoretical Computer Science},
  year = 1991,
  volume = 83,
  number = 1,
  pages = {131--155},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  abstract = {By treating the interfaces of a module specification as
               a production, we combine notions from the well known
               theory of algebraic graph grammars with the theory of a
               large software system specifications to tackle the
               problem of designing modular systems. Given a goal
               specification, a library of module specifications as
               reusable software components and a collection of
               `primitive' realized data type specifications, the
               designing of a system consisting only of the library
               components to realize the goal is reduced to deriving
               the goal from the primitive specification using given
               productions. If a derivation sequence exists, direct
               derivations and operations on productions are converted
               into the design of a modular system.},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Parisi-1992,
  author = {Francesco Parisi-Presicce},
  title = {Single vs. double pushout derivation of graphs},
  pages = {248--262},
  crossref = {WG92},
  WKloc = {A-0117},
  abstract = {Graph grammars in which the generation of graphs
		  using productions and restricting derivation
		  sequences based on the double pushout, recently
		  introduced by H. Ehrig and the author and motivated
		  by modular software design, are shown to have
		  strictly more expressive power than those based on
		  the single pushout approach of \cite{Loewe-1990}.
		  Furthermore, not being based on the internal
		  structure of graphs, the approach can be immediately
		  generalized to other formalisms, such as algebraic
		  specifications, jungles, etc.}
}

@InProceedings{Parisi-1992-x,
  author = {Francesco Parisi-Presicce},
  title = {Single vs. double pushout derivation of graphs},
  pages = {248--262},
  WKloc = {A-0117},
  abstract = {Graph grammars in which the generation of graphs
		  using productions and restricting derivation
		  sequences based on the double pushout, recently
		  introduced by H. Ehrig and the author and motivated
		  by modular software design, are shown to have
		  strictly more expressive power than those based on
		  the single pushout approach of Loewe-1991.
		  Furthermore, not being based on the internal
		  structure of graphs, the approach can be immediately
		  generalized to other formalisms, such as algebraic
		  specifications, jungles, etc.},
  editor = {Ernst Mayr},
  booktitle = {Proc. 18th International Workshop on
		  Graph-Theoretic Concepts in Computer Science},
  year = 1993,
  volume = 657,
  series = {LNCS},
  publisher = {Springer-Verlag},
  month = JUN,
  UniBwM = {MAT880/Z-8055-18},
  address = {Wiesbaden}
}

@InProceedings{Parisi-Ehrig-Montanari-1986,
  author = {Francesco Parisi-Presicce and Hartmut Ehrig and Ugo
		  Montanari},
  title = {Graph Rewriting with Unification and Composition},
  pages = {496--515},
  crossref = {GG1986},
  WKloc = {A-0056},
  abstract = {The standard Algebraic Theory of Graph Grammars is based on
	the notion of ``color-preserving'' graph morphisms and on a ``double
	pushout'' construction to represent gluing of graphs. In this paper,
	we impose a simple structure on the sets of colors to allow variables
	in both graphs and productions. Instantiations are performed by graph
	morphisms. Using relative unification, we define the composition of
	rules and prove the Concurrency Theorem in this more general
	framework. By restricting our attention to rooted directed acyclic
	graphs, we can represent standard Term Rewriting with First order
	substitutions. One of the motivations for this study is the attempt
	to provide a description of the static behavior of Rule-Based Expert
	Systems.},
  bibliographies = {RelMiCS}
}

@InProceedings{Parisi-Pierantonio-1992,
  author = {Parisi Presicce, Francesco and A. Pierantonio},
  title = {Structured Inheritance for Algebraic Class Specifications},
  crossref = {SADT92},
  pages = {295--309},
  WKloc = {A-0333},
  abstract = {$\ldots$}
}

@InProceedings{Parisi-Piersanti-1994,
  author = {Francesco Parisi-Presicce and Gabriele Piersanti},
  title = {Multilevel Graph Grammars},
  abstract = {The classical double pushout approach to the algebraic theory
             of graph grammars is extended to multilevel graph
             representations, where parts of graphs are not visible and the
             information can be restored via the explicit application of
             productions. The notions of applicability and derivation are
             investigated and the compatibility of the representations with
             the derivations is shown. Production mechanisms for multilevel
             graphsare motivated by problems in Visual Languages and the
             representation of Iconic Languages in particular.},
  annote = {remarks to the authors:
           Abstract, line 5: multilevel graph -> multilevel graphs
           p.3, direct derivation, 2.: ``his subgraph'' -> ``its
		  subgraph''
           p.4, 1.7, line 4: limitation -> limitations
           p.4, line -2: ``BOUNDARY'' is undefined;
                 the definition of ``IDENTIFICATION'' does not match its use.
           p.6, Example: Where is the encoding for the bold nodes in
		  $G$? Why could the other $A$-node not be selected
		  for application of the restoring production? I could
		  not locate any discussion of this problem.
           p.7: 2.6 defines ``safe'' for sets; 2.7 employs ``safe''
		  for sequences.
           p.8, 2.8, line 3: ``sequence $\{p_1, \ldots,p_k\}\in
		  RP_G$'' ->
                  ``sequence $p_1,\ldots,p_k$ of elements of $RP_G$''
           p.9, Case 2, line 8: ``remains unchanged'' is an unlucky
		  procedural formulation.
           p.11, lines -5,-3,-2, p.12, lines 2,4,6: ``it's'' -> ``it is''.
           p.12, end of line 3: semicolon missing
           Bibliography: most items are never cited.
           [Lowe]: The name is ``L\"owe''.

        Summary:
                  This paper tackles multilevel graph grammars using the
		  algebraic approach top graph rewriting. Multilevel
		  graphs are motivated mainly by applications in the
		  field of visual programming languages, when
		  different levels of information presentation and
		  hiding are necessary.

                  Although the presentation in some places still needs
		  polishing, the paper presents an interesting and
		  potentially very useful mechanism for coping with
		  different levels of representation while rewriting graphs.},
  crossref = {WG1994},
  WKloc = {A-0230}
}

@InProceedings{Parisi-Piersanti-1994-x,
  author = {Francesco Parisi-Presicce and Gabriele Piersanti},
  title = {Multilevel Graph Grammars},
  abstract = {The classical double pushout approach to the algebraic theory
             of graph grammars is extended to multilevel graph
             representations, where parts of graphs are not visible and the
             information can be restored via the explicit application of
             productions. The notions of applicability and derivation are
             investigated and the compatibility of the representations with
             the derivations is shown. Production mechanisms for multilevel
             graphsare motivated by problems in Visual Languages and the
             representation of Iconic Languages in particular.},
  editor = {Ernst W. Mayr and Gunther Schmidt and Gottfried Tinhofer},
  booktitle = {Proc. 20th International Workshop on
		  Graph-Theoretic Concepts in Computer Science},
  year = 1995,
  volume = 903,
  series = {LNCS},
  publisher = {Springer-Verlag},
  WKloc = {A-0230}
}

@InProceedings{Park-1981,
  author = {David Park},
  address = {New York},
  booktitle = {Proc.\null{} {{$5^{th}$} GI Conf.}},
  pages = {167--183},
  publisher = Springer,
  title = {Concurrency and automata on infinite sequences},
  year = 1981,
  bibliographies = {RelMiCS}
}

@Book{Parkinson-1966,
  author = {G. H. R. Parkinson},
  year = 1966,
  title = {Leibniz: Logical Papers},
  publisher = CLARENDON,
  address = {Oxford},
  bibliographies = {RelMiCS}
}

@Book{Partsch-1990,
  author = {Helmuth A. Partsch},
  title = {Specification and Transformation of Programs},
  publisher = Springer,
  year = 1990,
  series = {Texts and Monographs in Computer Science},
  UniBwM = {Mag/T8766},
  bibliographies = {RelMiCS, SpecTech}
}

@InProceedings{Partsch-1994,
  author = {Helmuth Partsch},
  title = {{Transformationelle Entwicklung von SIMD-Algorithmen}},
  crossref = {Honnef94},
  pages = {94--98}
}

@Book{Partsch-1998,
  author = {Helmuth Partsch},
  title = {{Requirements-Engineering systematisch, Modellbildung f\"ur softwaregest\"utzte Systeme}},
  publisher = Springer,
  year = 1998,
  ISBN = {3-540-64391-5},
  UniBwM = {INF460/YE283}
}

@Misc{Partsch-Schulte-Vullinghs-,
  author = {Helmuth Partsch and Wolfram Schulte and Ton Vullinghs},
  title = {System Support for the Interactive Transformation of Functional Programs},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  abstract = {This paper describes the program transformation system \textsf{Ultra}. [$\ldots$]},
  WKloc = {A-1148},
  bibliographies = {HOPS}
}

@Article{Paschos-1997,
  author = {Vangelis T. Paschos},
  title = {A survey of approximately optimal solutions to some covering and packing problems},
  journal = {ACM Computing Surveys (CSUR)},
  volume = 29,
  number = 2,
  year = 1997,
  ISSN = {0360-0300},
  pages = {171--209},
  doi = {http://doi.acm.org/10.1145/254180.254190},
  publisher = {ACM Press}
}

@Book{Pasetti-2002,
  author = {Alessandro Pasetti},
  title = {Software Frameworks and Embedded Control Systems},
  publisher = Springer,
  year = 2002,
  volume = 2231,
  series = LNCS,
  WKloc = {owned}
}

@Article{Pasztor-1990,
  WKloc = {A-0054},
  year = 1990,
  volume = 70,
  title = {Recursive Programs and Denotational Semantics in Absolute Logics of Programs},
  pages = {127-150},
  journal = TCS,
  author = {Ana Pasztor},
  bibliographies = {RelMiCS}
}

@Article{Patcas-Lawford-Maibaum-2015,
  author = 	 {Lucian M. Patcas and Mark Lawford and Tom Maibaum},
  title = 	 {Implementability of Requirements in the Four-Variable Model},
  journal = 	 SCICOP,
  year = 	 2015,
  DOI = 	 {10.1016/j.scico.2015.05.007},
  OPTvolume = 	 {},
  number = 	 {05},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@PhDThesis{Paterson-1988,
  year = 1988,
  title = {Reasoning about {Functional} {Programs}},
  school = {University of Queensland},
  author = {Ross Paterson},
  address = {Brisbane}
}

@InProceedings{Paterson-1993,
  author = {M. Paterson},
  title = {Evolution of an Algorithm},
  crossref = {ESA93},
  pages = {306--308},
  OPTnote = {invited lecture},
  OPTauthorsaddress = {},
  OPTwkloc = {},
  OPTkeywords = {skew closure},
  OPTabstract = {In this talk I shall trace the origins and
		  subsequent development of a novel priority queue
		  algorithm. \cite{Fischer-Paterson-1992} $\ldots$ an
		  interesting algorithm of
		  unknown origin for computing the transitive closure
		  of symmetric Boolean matrices. The algorithm
		  involves two passes through the matrix, row by row,
		  in the course of which some rows are OR'ed into
		  later rows. $\ldots$ \cite{Fischer-Paterson-1982}},
  OPTannote = {}
}

@InProceedings{Paterson-1996,
  author = {Ross Paterson},
  title = {Compiling Laziness using Projections},
  crossref = {SAS-1996},
  pages = {255--269},
  OPTabstract = {},
  WKloc = {A-0449},
  UniBwM = {INF400/Z9623-3}
}

@InProceedings{Paterson-1997,
  title = {Transforming Lazy Functions using Comportment Properties},
  author = {Ross Paterson},
  crossref = {PLILP1997},
  pages = {111--125},
  URL = {http://www.soi.city.ac.uk/~ross/papers/embeddings.html},
  WKloc = {A-0977}
}

@InCollection{Paterson-200X,
  author = 	 {Ross Paterson},
  title = 	 {Arrows and Computation},
  crossref =	 {Gibbons-deMoor-2003},
  pages =	 {201--222},
  WKloc = 	 {A-1561},
  bibliographies  = {Coconut}
}

@Article{Paterson-Wegman-1978,
  keywords = {DAG unification},
  year = 1978,
  volume = 16,
  title = {Linear Unification},
  pages = {158--167},
  number = 2,
  journal = {Journal of Computer and System Sciences},
  author = {M.S. Paterson and M.N. Wegman}
}

@InProceedings{Paulin-Mohring-1993,
  author = {Christine Paulin-Mohring},
  title = {Inductive Definitions in the System {Coq}, Rules and Properties},
  pages = {328--345},
  abstract = {In the pure Calculus of Constructions, it is possible to
             represent data structures and predicates using higher-order
             quantification. However, this representation is not
             satisfatory, from the point of view of both the efficiency of
             the underlying programs and the power of the logical system.
             For these reasons, the calculus was extended with a primitive
             notion of inductive definitions [8]. This paper describes the
             rules for inductive definitions in the system Coq. They are
             general enough to be seen as one formulation of adding
             inductive definitions to a typed lambda-calculus. We prove
             strong normalization for a subsystem of Coq corresponding to
             the pure Calculus of Constructions plus Inductive Definitions
             with only weak eliminations.},
  crossref = {TLCA93},
  AuthorURL = {\url{http://www.lri.fr/~paulin/publis.html#Moh93}},
  annote = {see also LIP research report 92-49},
  WKloc = {A-0188}
}

@InProceedings{PaulinMohring-1994,
  author = {Christine Paulin-Mohring},
  title = {The Calculus of Inductive Definitions and its
		  Implementation: the Coq Proof Assistant},
  crossref = {LPAR94},
  note = {invited tutorial},
  authorsAddress = {LIP/ENS Lyon  cpaulin\@lip.ens-lyon.fr},
  abstract = {Type Theory serves as a basis for several environments
		  dedicated to the formalization of reasoning. We
		  shall present the theory and practice of one of
		  them: the Coq Proof Assistant.

		  This environment is based on a typed lambda-calculus
		  called the Calculus of Inductive Definitions. It is
		  a powerful language which extends the Calculus of
		  Constructions, introduced by Coquand and Huet, with
		  a mechanism for general inductive definitions in the
		  spirit of Martin-Lof's Intuitionistic Type Theory.

		  The Coq proof assistant can be decomposed into three parts.
		  \begin{itemize}
		  \item A specification language which combines
		  higher-order logic, functional programming and
		  inductive definitions of relations.
		  \item A tactic language which provides several tools
		  for the interactive development of proofs of formulas.
		  \item An environment for manipulating  proof-terms
		  built by the system, especially for extracting ML
		  programs out of constructive proofs of specifications.
		  \end{itemize}}
}

@Article{Paulson-1983,
  author = {Lawrence C. Paulson},
  title = {A Higher-Order Implementation of Rewriting},
  journal = SCICOP,
  volume = 3,
  number = 2,
  pages = {119--149},
  month = AUG,
  year = 1983,
  ISSN = {0167-6423},
  URL = {http://www.cl.cam.ac.uk/~lp15/papers/Reports/TR035-lcp-rewriting.pdf},
  affiliation = {Univ of Cambridge, Computer Lab, Cambridge, Engl},
  bibliographies = {HHOL}
}

@Article{Paulson-1986,
  author = {Lawrence C. Paulson},
  title = {Natural Deduction as Higher-Order Resolution},
  year = 1986,
  volume = 3,
  pages = {237--258},
  journal = JLOG,
  WKloc = {A-0062},
  keywords = {MartinLoef},
  bibliographies = {RelMiCS},
  abstract = {An interactive theorem prover, {\em Isabelle}, is under
		  development. In LCF, each inference rule is
		  representedby one function for forwards proof and
		  another (a {\em tactic}) for backwards proof. In
		  Isabelle, each inference rule is represented by a
		  Horn clause. Resolution gives both forwards and
		  backwards proofs, supporting a large class of
		  logics. Isabell has been used to prove theorems in
		  Martin-L\"of's constructive type theory. Quantifiers
		  pose several difficulties: substitution, bound
		  variables, Skloemization. Isabelle's representation
		  of logical syntax is the typed $\lambda$-calculus,
		  requiring higher-order unification. It may have
		  potential for logic programming. Depth-first
		  subgoaling along inference rules constitutes a
		  higher-order PROLOG.},
  bibliographies = {HHOL}
}

@Book{Paulson-1987,
  author = {L. C. Paulson},
  title = {Logic and Computation (Interactive Proof with
		  {Cambridge LCF})},
  year = 1987,
  volume = 2,
  series = Cambridge,
  publisher = CambridgeUP,
  UniBwM = {INF400/Q15128},
  bibliographies = {RelMiCS},
  contents = {I Preliminaries
	1 Survey and History of LCF
	2 Formal Proof in First Order Logic
	2.7 Understanding quantifiers
	2.8 The universal quantifier
	2.9 The existential quantifier
	2.10 Mathematical induction
	2.11 Equality
	2.12 A sequent calculus for natural deduction
	2.13 A sequent calculus for backwards proof
	2.14 Classical deduction in a sequent calculus
	2.15 How to find formal proofs
	3 A Logic of Computable Functions
	3.1 The lambda calculus
	3.2 Semantic questions
	3.5 Fixed point induction
	3.6 Admissibility of fixed point induction
	3.7 Further reading
	4 Structural Induction
	4.6 Recursive type definitions
	4.7 The inverse limit construction for recursive domains
	4.8 The type of lazy lists
	4.9 The type of strict lists
	4.10 Formal reasoning about types
	4.11 Structural induction over lazy lists
	4.12 Structural induction over strict lists
	4.13 Automating the derivation of induction
	4.14 Further reading
	II Cambridge LCF
	5 Syntactic Operations for PP$\lambda$
	5.1 The syntax of PP$\lambda$
	5.2 Quotations
	5.3 Primitive constructors and destructors
	5.4 Compound constructors and destructors
	5.5 Functions required for substitution
	5.6 Pattern matching primitives
	5.7 Terminal interaction and system functions
	6 Theory Structure
	7 Axioms and Inference Rules
	7.1 The representation of inference rules
	7.2 First order logic
	7.3 Domain Theory
	7.4 Forwards proof and derived rules
	7.5 Discussion and further reading
	8 Tactics and Tacticals
	9 Rewriting and Simplification
	9.1 The extraction of rewrite rules
	9.2 The standard rewriting strategy
	9.3 Top-level rewriting tools
	9.4 Conversions
	9.5 Implementing new rewriting strategies
	9.6 Further reading
	10 Sample proofs
	10.1 Addition of natural numbers
	10.2 Commutativity of addition
	10.3 Equality on the natural numbers
	10.4 A simple fixed point induction
	10.5 A mapping functional for infinite sequences
	10.6 Project suggestions
	Bibliography
	Index},
  annote = {--- PLGnotes --- fluid variables in 9.1 correspond to RHS variables
            not occurring in the LHS},
  bibliographies = {HHOL}
}

@Article{Paulson-1990,
  author = {Lawrence C. Paulson},
  title = {Isabelle: The Next 700 Theorem Provers},
  journal = {Logic and Computer Science},
  year = 1990,
  pages = {361--385},
  bibliographies = {HHOL}
}

@InCollection{Paulson-1992,
  author = {Lawrence C. Paulson},
  title = {Designing a Theorem Prover},
  pages = {415--175},
  crossref = {HBLCS-II},
  WKloc = {A-1359},
  bibliographies = {HHOL, FP, MathScheme}
}

@Manual{Paulson-2002a,
  title = {The {Isabelle} Reference Manual},
  author = {Lawrence C. Paulson},
  organization = {Computer Laboratory, University of Cambridge},
  note = {With contributions by Tobias Nipkow and Markus Wenzel},
  month = MAR,
  year = 2002,
  WKloc = {A-1385}
}

@Manual{Paulson-2002b,
  title = {Introduction to {Isabelle}},
  author = {Lawrence C. Paulson},
  organization = {Computer Laboratory, University of Cambridge},
  note = {With contributions by Tobias Nipkow and Markus Wenzel},
  month = MAR,
  year = 2002,
  WKloc = {A-1388}
}

@Manual{Paulson-2004c,
  title = {{Isabelle}'s Logics},
  author = {Lawrence C. Paulson},
  organization = {Computer Laboratory, University of Cambridge},
  note = {With contributions by Tobias Nipkow and Markus Wenzel},
  month = APR,
  year = 2004,
  WKloc = {A-1578}
}

@InProceedings{Paulson-2005,
  author = 	 {Lawrence C. Paulson},
  title = 	 {A Fixedpoint Approach to (Co)Inductive and (Co)Datatype Definitions},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2005},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1708},
  OPTannote = 	 {}
}

@Article{Paulson-2006,
  author = 	 {Lawrence C. Paulson},
  title = 	 {Defining Functions on Equivalence Classes},
  journal = 	 ACM-TOCL,
  year = 	 2006,
  volume = 	 7,
  number = 	 4,
  pages = 	 {658--675},
  month = 	 OCT,
  bibliographies = {RelMiCS},
  keywords = 	 {equivalence relation, Isabelle}
}

@Article{Paulson-2015,
  author={Paulson, Lawrence C.},
  title={A Mechanised Proof of {G{\"o}del}’s Incompleteness Theorems Using {Nominal Isabelle}},
  year={2015},
  month=APR,
  pages={1-37},
  journal={Journal of Automated Reasoning},
  DOI={10.1007/s10817-015-9322-8},
  DOIURL={http://dx.doi.org/10.1007/s10817-015-9322-8},
  publisher={Springer Netherlands},
  issn={0168-7433},
  keywords={Gödel’s incompleteness theorems; Isabelle/HOL; Nominal syntax; Formalisation of mathematics},
  language={English},
  abstract = {An Isabelle/HOL formalisation
    of Gödel’s two incompleteness theorems is presented.
    The work follows Świerczkowski’s detailed proof of the theorems
    using hereditarily finite (HF) set theory
    (Dissertationes Mathematicae 422, 1–58, 2003).
    Avoiding the usual arithmetical encodings of syntax
    eliminates the necessity to formalise elementary number theory
    within an embedded logical calculus.
    The Isabelle formalisation uses two separate treatments of variable binding:
    the nominal package (Logical Methods in Computer Science 8(2:14), 1–35, 2012)
    is shown to scale to a development of this complexity,
    while de Bruijn indices (Indagationes Mathematicae 34, 381–392, 1972)
    turn out to be ideal for coding syntax.
    Critical details of the Isabelle proof are described,
    in particular gaps and errors found in the literature.}
}

@inproceedings{Pavlovic-Smith-2001,
 author = {Pavlovic, Dusko and Smith, Douglas R.},
 title = {Composition and Refinement of Behavioral Specifications},
 booktitle = {Automated Software Engineering, {ASE 2001}},
 year = {2001},
 pages = {157--165},
 DOIURL = {http://dx.doi.org/10.1109/ASE.2001.989801},
 DOI = {10.1109/ASE.2001.989801},
 publisher = {IEEE Computer Society},
 abstract = {This paper presents a mechanizable framework for specifying, developing, and reasoning about complex systems. The framework combines features from algebraic specifications, abstract state machines, and refinement calculus, all couched in a categorical setting. In particular, we show how to extend algebraic specifications to evolving specifications (especs) in such a way that composition and refinement operations extend to capture the dynamics of evolving, adaptive, and self-adaptive software development, while remaining efficiently computable. The framework is partially implemented in the Epoxi system.}
}

@Book{Pawlak-1991,
  author = {Pawlak, Z.},
  title = {Rough sets},
  publisher = Kluwer,
  year = 1991,
  address = {Dordrecht},
  bibliographies = {RelMiCS}
}

@InProceedings{ Pawlak-1994,
  author = "Zdzislaw Pawlak",
  title = "Rough sets present state and further prospects",
  booktitle = {Third International Workshop on Rough Set and Soft Computing {(RSSC '94)}},
  pages = {72--76},
  year = "1994",
  url = "citeseer.ist.psu.edu/article/pawlak94rough.html"
}

@InProceedings{Paz-1994,
  author = {Azaria Paz},
  title = {An Elementary Algorithmic Problem from an Advanced
		  Standpoint},
  crossref = {Karhumaeki-Maurer-Rozenberg-1994},
  pages = {344--357},
  keywords = {three-dimensional Diophantine equations},
  abstract = {An optimal algorithm which finds all the solutions,
		  over the nonnegative integers of an equation of the
		  form $ax_1+bx_2+cx_3=m$, $a,b,c,m$ positive
		  integers, is given. The algorithm is polynomial in
		  the length of the input and is based on advanced
		  concepts and methods. The algorithm is not based on
		  Lenstra's integer programming algorithm.}
}

@TechReport{Pazel-1989,
  author = {Donald P. Pazel},
  title = {A Graphical Interface for Evaluating a Genetic Algorithm for Graph Layout},
  institution = {IBM Research Division, T.J. Watson Research Center},
  year = 1989,
  number = {RC14348}
}

@Misc{Pazel-2002,
  author = {Donald P. Pazel},
  title = {Resolution policy for direct manipulation on hierarchically structured visuals},
  howpublished = {United States Patent 6,441,835},
  month = {Aug.~27},
  year = 2002,
  WKloc = {A-1425}
}

@Book{Pedicchio-Tholen-2004,
  editor = 	 {Maria Cristina Pedicchio and Walter Tholen},
  title = {Categorical Foundations: Special Topics in Order, Topology, Algebra, and Sheaf Theory},
  publisher = CambridgeUP,
  pages = 440,
  ISBN = 0521834147,
  year = 	 2004,
  URL = {http://www.cambridge.org/0521834147},
  volume = 	 97,
  series = 	 {Encyclopedia of Mathematics and its Applications},
  contents = 	 {Introduction Walter Tholen;
    1. Ordered sets via adjunction R. J. Wood;
    2. Locales Jorge Picado, Ales Pultr and Anna Tozzi;
    3. A functional approach to general topology
         Maria Manuel Clementino, Eraldo Giuli and Walter Tholen;
    4. Regular, protomodular and abelian categories
         Dominique Bourn and Marino Gran;
    5. Aspects of monads John MacDonald and Manuela Sobral;
    6. Algebraic categories Maria Cristina Pedicchio and Fabrizio Rovatti;
    7. Sheaf theory Claudia Centazzo and Enrico M. Vitale;
    8. Beyond Barr exactness: effective descent morphisms
         George Janelidze, Manuela Sobral and Walter Tholen.}
}

@Article{Pedicini-Quaglia-2007,
 author = {Pedicini, Marco and Quaglia, Francesco},
 title = {PELCR: Parallel Environment for Optimal Lambda-calculus Reduction},
 journal = ACM-TOCL,
 issue_date = {July 2007},
 volume = {8},
 number = {3},
 month = JUL,
 year = {2007},
 issn = {1529-3785},
 articleno = {14},
 DOIURL = {http://doi.acm.org/10.1145/1243996.1243997},
 doi = {10.1145/1243996.1243997},
 acmid = {1243997},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Functional programming, geometry of interaction, linear logic, optimal reduction, parallel implementation, virtual reduction},
  abstract = {In this article we present the implementation of an environment supporting Lévy's optimal reduction for the $\lambda$-calculus on parallel (or distributed) computing systems. In a similar approach to Lamping's, we base our work on a graph reduction technique, known as \emph{directed virtual reduction}, which is actually a restriction of Danos-Regnier virtual reduction.

The environment, which we refer to as PELCR (parallel environment for optimal lambda-calculus reduction), relies on a strategy for directed virtual reduction, namely \emph{half combustion}. While developing PELCR we adopted both a message aggregation technique, allowing reduction of the communication overhead, and a fair policy for distributing dynamically originated load among processors.

We also present an experimental study demonstrating the ability of PELCR to definitely exploit the parallelism intrinsic to $\lambda$-terms while performing the reduction. We show how PELCR allows achieving up to 70--80\% of the ideal speedup on last generation multiprocessor computing systems. As a last note, the software modules have been developed with the C language and using a standard interface for message passing, that is, MPI, thus making PELCR itself a highly portable software package.}
}

@Article{Pedryzc-1991,
  author = 	 {W. Pedrycz},
  title = 	 {Processing in relational structures: Fuzzy relational equations},
  journal = 	 {Fuzzy Sets and Systems},
  year = 	 1991,
  volume =	 41,
  number =	 1,
  pages =	 {77--106},
  bibliographies = {RelMiCS},
  DOI = 	 {http://dx.doi.org/10.1016/0165-0114(91)90047-T},
  abstract = {The paper provides an overview of methodology for
     processing fuzzy information in relational structures completed
     in terms of fuzzy relational equations. The origin and the
     central role of relational calculus in general, and fuzzy sets in
     particular, is explained with special emphasis paid to its role
     and representation capabilities. It is pointed out that fuzzy
     relational equations play a significant role as a platform for a
     uniform development of techniques in fuzzy sets. Many of the
     problems and frameworks developed in fuzzy sets so far can be
     easily translated into the language of fuzzy relational equations
     which immediately takes a significant advantage of their solid,
     well developed formalisms, techniques, and clarity of
     exposition. We will study the role of approximate solutions to
     fuzzy relational equations as a convenient tool to handle
     probabilistic type of uncertainty. Moreover those solutions can
     easily explain the origin and generate a way in which fuzzy sets
     of higher generality (such as for instance type-2 sets,
     interval-valued sets) can be algorithmically determined. Finally,
     we study the role of equations in diverse fields of applications
     making use of an ample framework of general systems theory.}
}

@Article{Peirce-1870,
  author = {Charles Sanders Peirce},
  title = {Description of a Notation for the Logic of Relatives,
		Resulting from an Amplification of the Conceptions of
		{Boole's} Calculus of Logic},
  journal = MEM-ACAD,
  volume = 9,
  year = 1870,
  pages = {317--378},
  note = {Reprint by Welch, Bigelow and Co., Cambridge, MA, 1870, pp.
      1--62. Also reprinted in \cite{Peirce-1933} and \cite{Peirce-1984}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1875,
  author = {Charles Sanders Peirce},
  title = {On the Application of Logical Analysis to Multiple Algebra},
  journal = PRO-ACAD,
  volume = 10,
  year = 1875,
  pages = {392--394},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1877,
  author = {Charles Sanders Peirce},
  title = {A Note on Grassmann's Calculus of Extension},
  journal = PRO-ACAD,
  volume = 13,
  year = 1877,
  pages = {115--116},
  note = {reprinted in  \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1880,
  author = {Charles Sanders Peirce},
  title = {On the Algebra of Logic},
  journal = AJM,
  volume = 3,
  year = 1880,
  pages = {15--57},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1881,
  author = {Charles Sanders Peirce},
  title = {On the Logic of Number},
  journal = AJM,
  volume = 4,
  year = 1881,
  pages = {85--95},
  note = {reprinted in  \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1881a,
  author = {Charles Sanders Peirce},
  title = {On the Relative Forms of the Algebras},
  note = {addendum to B.\null{} Peirce \cite{Peirce-1881} reprinted in
      \cite{Peirce-1933}.},
  journal = AJM,
  volume = 4,
  year = 1881,
  pages = {221--229},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1881b,
  author = {Benjamin Peirce},
  title = {Linear Associative Algebras},
  note = {with footnotes and addenda by C.\null{} S.\null{} Peirce
      Originally published as a separate volume by D.\null{} Van Nostrand,
      New York, 1882, pp.\null{} 1--133; also reprinted in
      \cite{Cohen1980}. QA184.B44.},
  journal = AJM,
  year = 1881,
  volume = 4,
  pages = {97--229},
  bibliographies = {RelMiCS}
}

@Booklet{Peirce-1882,
  author = {Charles Sanders Peirce},
  title = {Brief Description of the Algebra of Relatives},
  note = {privately printed. Reprinted in
                \cite{Peirce-1933}, pp.\null{} 180--186},
  year = 1882,
  bibliographies = {RelMiCS}
}

@Article{Peirce-1882a,
  author = {Charles Sanders Peirce},
  title = {On the Relative Forms of Quaternions},
  journal = HOPKINS,
  volume = 13,
  year = 1882,
  pages = 179,
  note = {reprinted in  \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1882b,
  author = {Charles Sanders Peirce},
  title = {On a Class of Multiple Algebras},
  journal = HOPKINS,
  volume = 19,
  year = 1882,
  pages = {3--4},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Booklet{Peirce-1883a,
  author = {Charles Sanders Peirce},
  title = {Note B: The Logic of Relatives},
  note = {in \cite{Peirce-1883}, pp.\null{} 187--203 Reprinted in
      \cite{Peirce-1933} and in \cite{Peirce-1983}. See Proc.\null{} London
      Math.\null{} Soc., XII, p.\null{} 212, for reference to letter from
      Schl\"otel discussed by Peirce at the end of this paper.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1885,
  author = {Charles Sanders Peirce},
  title = {On the Algebra of Logic: A Contribution to the Philosophy of
      Notation},
  journal = AJM,
  volume = 7,
  year = 1885,
  pages = {180--202},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1892,
  author = {Charles Sanders Peirce},
  title = {The Critic of Arguments},
  journal = OPENC,
  volume = 6,
  year = 1892,
  pages = {3391--4, 3416--8},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Article{Peirce-1897,
  author = {Charles Sanders Peirce},
  title = {The Logic of Relatives},
  journal = MONIST,
  volume = 7,
  year = 1897,
  pages = {161--217},
  note = {reprinted in \cite{Peirce-1933}.},
  bibliographies = {RelMiCS}
}

@Booklet{Peirce-1903,
  author = {Charles Sanders Peirce},
  title = {Nomenclature and Divisions of Dyadic Relations},
  note = {1903. Reprinted in \cite{Peirce-1933}, 3.571--608},
  bibliographies = {RelMiCS}
}

@Book{Peirce-1933,
  year = 1933,
  title = {C.\null{} S.\null{} Peirce Collected Papers},
  publisher = HarvardUP,
  note = {edited by C.\null{} Hartshorne and P.\null{} Weiss},
  author = {Charles Sanders Peirce},
  address = {Cambridge},
  bibliographies = {RelMiCS}
}

@InCollection{Peirce-1933a,
  author = {Charles Sanders Peirce},
  title = {Description of a notation for the logic of relatives,
		  resulting from an amplification of the conceptions
		  of Boole's calculus of logic.},
  booktitle = {Collected Papers of Charles Sanders Peirce. III. Exact Logic.},
  publisher = HarvardUP,
  year = 1933,
  bibliographies = {RelMiCS}
}

@Book{Peirce-1983,
  author = {Charles Sanders Peirce},
  title = {Studies in Logic by Members of the Johns Hopkins University},
  year = 1983,
  publisher = Benjamins,
  address = {Amsterdam and Philadelphia},
  pages = {lviii+vi+203},
  note = {reprint of \cite{Peirce-1883}, with an Introduction by Max
      H.\null{} Fisch, and a Preface by Achim Eschbach},
  bibliographies = {RelMiCS}
}

@Book{Peirce-1984,
  author = {Charles Sanders Peirce},
  title = {Writings of Charles S.\null{} Peirce, A Chronological Edition},
  note = {edited by Edward C.\null{} Moore, Max H.\null{} Fisch, Christian J.\null{} W.\null{}
		Kloesel, Don D.\null{} Roberts, and Lynn A.\null{} Ziegler},
  annote = {B945.P4},
  publisher = IndianaUP,
  address = {Bloomington},
  year = 1984,
  bibliographies = {RelMiCS}
}

@Article{Pelavin-Allen-1986,
  author = {R. Pelavin and James F. Allen},
  title = {A Formal Logic of Plans in Temporally Rich Domains},
  journal = PRO-IEEE,
  volume = 74,
  number = 10,
  year = 1986,
  month = OCT,
  pages = {1364--1382},
  bibliographies = {RelMiCS}
}

@InProceedings{Pelavin-Allen-1987,
  author = {R. Pelavin and James F. Allen},
  title = {A Model for Concurrent Actions Having Temporal Extent},
  pages = {246--250},
  crossref = {AAAI1987},
  bibliographies = {RelMiCS}
}

@InProceedings{Peled-1994,
  author = {Doron Peled},
  title = {On Projective and Separable Properties},
  crossref = {CAAP94},
  pages = {291--307}
}

@Article{Pelletier-Rosicky-1997,
  author = 	 {Joan Wick Pelletier and J. Rosicky},
  title = 	 {Simple Involutive Quantales},
  journal = 	 {Journal of Algebra},
  year = 	 1997,
  volume =	 195,
  pages =	 {367--386},
  bibliography = {RelMiCS}
}

@InProceedings{Peltier-1998,
  author = {Nicolas Peltier},
  title = {System Description: An Equational Constraints Solver},
  crossref = {CADE1998},
  pages = {119--123},
  OPTabstract = {},
  WKloc = {A-0606}
}

@InProceedings{Pena-Ortega-Rubio-1999,
  author = {Pe{\~{n}}a, Ricardo and Yolanda Ortega and Fernando Rubio},
  title = {Teaching Monadic Algorithms to First-Year Students},
  crossref = {FDPE-1999},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0825}
}

@incollection{Pennemann-2008,
year={2008},
isbn={978-3-540-87404-1},
booktitle={Graph Transformations},
volume={5214},
series=LNCS,
editor={Ehrig, Hartmut and Heckel, Reiko and Rozenberg, Grzegorz and Taentzer, Gabriele},
doi={10.1007/978-3-540-87405-8_48},
title={Development of Correct Graph Transformation Systems},
DOIURL={http://dx.doi.org/10.1007/978-3-540-87405-8_48},
publisher=Springer,
author={Pennemann, Karl-Heinz},
pages={508--510}
}

@InProceedings{Pentus-1994,
  title = {Language Completeness of the {Lambek} Calculus},
  author = {M. Pentus},
  pages = {487--496},
  crossref = {LICS9},
  abstract = {We prove that the Lambek calculus (essentially a subsystem
     of non-commutative linear logic) is complete w.r.t.\ $L$-models, i.e.,
     free semigroup models.}
}

@Book{Pepper-2000,
  author = {Peter Pepper},
  title = {{Funktionale Programmierung in OPAL, ML, Haskell und Gofer}},
  publisher = Springer,
  year = 2000,
  series = {Springer-Lehrbuch},
  URL = {http://www.springer.de/cgi-bin/search_book.pl?isbn=3-540-64541-1},
  ISBN = {3-540-64541-1},
  bibliographies = {FP},
  abstract = {Dieses Lehrbuch gibt eine kompakte Einführung in die
      Konzepte, Methoden und Techniken der funktionalen oder applikativen
      Programmierung. Es setzt keine Programmierkenntnisse voraus und
      eignet sich damit insbesondere für Anfänger, aber auch für alle, die
      mit der imperativen Programmierung vertraut sind und sich in die
      Thematik einarbeiten möchten. Mathematisch fundiert werden die
      theoretischen Grundlagen der Programmierung und ihre praktische
      Umsetzung behandelt. Das Ziel dabei ist, auch große Systeme entwerfen
      und handhaben zu können. Am Beispiel der modernen Programmiersprachen
      OPAL, ML, HASKELL und GOFER werden sowohl elementare als auch
      weiterführende Aspekte vorgestellt. Im Vordergrund stehen dabei immer
      konzeptuelle Fragestellungen, und nicht vollständige
      Sprachbeschreibungen.},
  contents = {Bevor wir anfangen...
  Teil I. Elementare funktionale Programmierung: Was die Mathematik uns bietet;
  Funktionen als Programmiersprache
  Modularisierung
  Ausdrücke
  Rekursion
  Ein bisschen syntaktischer Zucker
  Drei Beispiele aus der Numerik

  Teil II. Weiterführende Aspekte funktionaler Programmierung: Funktionen höherer Ordnung
  Formalismen 1: Zur Semantik von Funktionen
  Formalismen 2: Namen und wo sie gelten
  Formalismen 3: Aufwand und Terminierung

  Teil III. Datenstrukturen: Konstruktion von Datenstrukturen
  Mehr syntaktischer Zucker
  Datenstrukturen und Modularisierung
  Listen (Sequenzen)
  Funktionale auf Listen
  Beispiel: Numerische Interpolation
  Bäume
  Formalismen 4: Parametrisierung und Polymorphie
  Suchen und Sortieren

  Teil IV. Wo, bitte, geht's zur realen Welt?
  Ein-/Ausgabe: Konzeptuelle Sicht
  Ein-/Ausgabe: Die Programmierung
  Compiler und Interpreter für OPAL, ML, HASKELL, GOFER

  Literaturverzeichnis
  Index}
}

@Article{Perfilieva-Gottwald-2003,
  author = 	 {Irina Perfilieva and Siegfried Gottwald},
  title = 	 {Solvability and Approximate Solvability of Fuzzy Relation Equations},
  journal = 	 {International Journal of General Systems},
  year = 	 2003,
  volume =	 32,
  number =	 4,
  pages =	 {361--372},
  WKloc = 	 {A-1622, doc/pap/BIB}
}

@TechReport{Persch-1987,
  author = {Guido Persch},
  title = {{Ein Transformationssystem zur Programmentwicklung}},
  institution = {Gesellschaft f\"ur Mathematik und Datenverarbeitung mbH},
  year = 1987,
  type = {GMD-Bericht},
  number = 167,
  note = {zugl. Dissertation},
  WKloc = {B-0034},
  keywords = {program transformation, program development},
  annote = { --- HOPSnotes ---}
}

@InProceedings{Petermann-1990,
  author = {U. Petermann},
  title = {Programmig Paradigms for Symbolic Computation
		  Systems --- Analysis of an Example},
  crossref = {DISCO90},
  pages = {71--80},
  keywords = {OO, LogLan}
}

@InProceedings{Peters-Lawford-Widemann-2007,
 author = {Peters, Dennis K. and Lawford, Mark and Widemann, Baltasar Tranc\'{o}n y},
 title = {An IDE for software development using tabular expressions},
 booktitle = {CASCON '07: Proceedings of the 2007 conference of the center for advanced studies on Collaborative research},
 year = {2007},
 pages = {248--251},
 location = {Richmond Hill, Ontario, Canada},
 doi = {http://doi.acm.org/10.1145/1321211.1321238},
 publisher = {ACM},
 address = {New York, NY, USA},
 WKloc = {doc/pap/BIB}
}

@Misc{Peterson-1995,
  author = {John C. Peterson},
  title = {Structures in Haskell},
  howpublished = {Available as {\tt ftp://haskell.cs.yale.edu/pub/haskell/yale/structs.dvi}},
  year = 1995,
  authorsAddress = {peterson-john\@CS.Yale.Edu},
  WKloc = {A-0388},
  abstract = {Algebraic data types as provided by the Haskell
		  language have a number of shortcomings. $\ldots$},
  keywords = {views, modules}
}

@Misc{PetriBib,
  title = {The Petri Nets Bibliography},
  key = {PetriBib},
  note = {http://www.informatik.uni-hamburg.de/TGI/pnbib/}
}

@InProceedings{Pettersson-1994,
  author = {Mikael Pettersson},
  title = {{RML} --- A New Language and Implementation for
		  Natural Semantics},
  crossref = {PLILP1994},
  pages = {117--131},
  WKloc = {A-0305, doc/pap/BIB},
  abstract = {RML is a programming language intended for the
		  implementation of Natural Semantics
		  specifications. The basic procedural elements are
		  {\em relations}: many-to-many mappings defined by a
		  number of {\em axioms} or {\em inference rules}. It
		  has control flow, logical variables and (explicit)
		  unification as in Prolog; from ML it borrows a
		  ploymorphic type system, data structures, and
		  pattern matching; a facility for
		  separately-compilable modules also exists. A simple
		  prototype compiler, based on translating RML to
		  Continuation-Passing Style and then to C, has been
		  implemented. Benchmarks indicate that this compiler
		  generates code that is {\em several orders of
		  magnitude} faster than Typol, and two times faster
		  than standard Prolog compilers.},
  bibliographies = {RelMiCS}
}

@Book{Pettersson-1999,
  author = {Mikael Pettersson},
  title = {Compiling Natural Semantics},
  year = 1999,
  publisher = Springer,
  series = LNCS,
  volume = 1549,
  UniBwM = {INF460/YE4285},
  keywords = {RML, Relational Meta Language},
  bibliographies = {RelMiCS}
}

@InProceedings{Pettersson-Fritzson-1992,
  authorsAddress = {Link\"oping, mpe/paf\@ida.liu.se},
  abstract = {DML, the Denotational Meta Language, is a
		  specification language and a compiler generation
		  tool for producing practical and efficient compilers
		  from Denotational Semantics specifications. This
		  means that code emitted from generated compilers
		  should be product quality, and that generated
		  compilers should have reasonable compilation speed,
		  and interface well with standard frontends and
		  back-ends.

		  To achieve this goal, the DML system contains two
		  main contributions compared to main contributions
		  compared to previous work in this area: (1) a
		  general algorithm for producing efficient quadruple
		  code from continuation semantics of Algol-like
		  languages, and (2) enhancements in the DML
		  specification language with BNF rules for abstract
		  syntax delarations and ``semantic brackets''
		  $[|\ldots|]$ with in-line concrete syntax and
		  pattern matching for readable and concise semantic
		  equations. Generated quadruple code is fed into a
		  standard optimizing back-end to obtain high quality
		  target code. The DML system generates efficient
		  compilers in C, and contains a foreign language
		  interface for communication e.g.\ with parsers or
		  optimizing back-ends. DML is a super set of Standard
		  ML and uses applicative order semantics, i.e.\ call
		  by value, for reasons of efficiency.},
  title = {{DML} --- A Meta-language and System for the
		  Generation of Practical and Efficient Compilers from
		  Denotational Specifications},
  pages = {127--136},
  crossref = {ICCL92},
  author = {Mikael Pettersson and Peter Fritzson},
  annote = {improving on MESS by Peter Lee}
}

@Article{Pettorossi-Proietti-1996,
  author = {Alberto Pettorossi and Maurizio Proietti},
  title = {Rules and strategies for transforming functional and logic programs},
  pages = {360--414},
  journal = {{ACM} Computing Surveys},
  year = 1996,
  volume = 28,
  number = 2,
  abstract = {We present an overview of the program transformation methodology,
       focusing our attention on the so-called ``rules + strategies'' approach in the
       case of functional and logic programs. The paper is intended to offer an
       introduction to the subject. The various techniques we present are illustrated
       via simple examples.},
  URL = {http://www.acm.org/pubs/citations/journals/surveys/1996-28-2/p360-pettorossi/},
  DOI = {10.1145/242224.242445},
  WKloc = {A-0985}
}

@Article{Pettorossi-Proietti-1996a,
  author = {Alberto Pettorossi and Maurizio Proietti},
  title = {Future Directions in Program Transformation},
  DOI = {10.1145/251595.251610},
  DOIURL = {http://dx.doi.org/10.1145/251595.251610},
 NEWjournal = {ACM Comput. Surv.},
 NEWissue_date = {Dec. 1996},
 NEWvolume = {28},
 NEWnumber = {4es},
 NEWmonth = dec,
 NEWyear = {1996},
 NEWissn = {0360-0300},
 NEWarticleno = {171},
  journal = {ACM Sigplan Notices},
  volume = 32,
  number = 1,
  month = JAN,
  year = 1997,
  pages = {99--102},
  note = {Position Statement at the Workshop on Strategic Directions in Computing
    Research, MIT, Cambridge, MA, USA, June 14-15, 1996.
    Also published in: ACM Computing Surveys, 28 (4es) (December 1996), pp. 171-es}
}

@Article{Pettorossi-Proietti-1998,
  author = {Alberto Pettorossi and Maurizio Proietti},
  title = {Program Specialization via Algorithmic Unfold/Fold Transformations},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 6},
  WKloc = {A-0902, 21--26},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{PeytonJones-1985,
  author = {Peyton Jones, Simon L.},
  title = {An Introduction to Fully-Lazy Supercombinators},
  pages = {176--208},
  booktitle = {Combinators and Functional Programming Languages},
  month = MAY,
  editor = {Guy Cousineau and Pierre-Louis Curien and Bernard Robinet},
  year = 1985,
  address = {Val d'Ajol},
  UniBwM = {INF460/P14528},
  WKloc = {A-0012},
  abstract = {Graph reduction has emerged as a powerful
		  implementaion model for lazy functional languages,
		  especially for parallel machines. Supercombinators
		  and full laziness are two of the key techniques
		  available for the efficient implementation of graph
		  reduction, and the purpose of this paper is to
		  provide an accessible introduction to these techniques.}
}

@Article{PeytonJones-1992,
  author = 	 {Peyton Jones, Simon L.},
  title = 	 {Implementing lazy functional languages on stock hardware:
	the {Spineless Tagless G-machine}},
  journal = 	 JFP,
  year = 	 1992,
  volume =	 2,
  number =	 2,
  pages =	 {127--202},
  month =	 APR
}

@TechReport{PeytonJones-1993,
  author = {Peyton Jones, Simon L.},
  title = {Implementing lazy functional languages on stack hardware:
	the {Spineless Tagless G-machine}},
  institution = {University of Glasgow},
  year = 1993,
  month = APR,
  number = {XX},
  note = {to appear in the Journal of Functional Programming},
  WKloc = {B-0014},
  abstract = {The Spineless Tagless G-machine is an abstract
		  machine designed to support non-strict higher-order
		  functional languages. This presentation of the
		  machine falls into three parts. Firstly, we give a
		  general discussion of the design issues involved in
		  implementing non-strict functional languages.

                  Next, we present the {\em STG language}, an austere
		  but recognisably-functional language, which as well
		  as a {\em denotational} meaning has a well-defined
		  {\em operational} semantics. The STG language is the
		  ``abstract machine code'' for the Spineless Tagless
		  G-machine.

                  Lastly, we discuss the mapping of the STG language
		  onto stock hardware. The success of an abstract
		  machine model depends largely on how efficient this
		  mapping can be made, though this topic is often
		  relegated to a short section. Instead, we give
		  detailed discussion of the design issues and the
		  choices we have made. Our principal target is the C
		  language, treating the C compiler as a portable
		  assembler.}
}

@InProceedings{PeytonJones-1996,
  author = {Peyton Jones, Simon L.},
  title = {Compiling Haskell by Program Transformation: A Report from the Trenches},
  crossref = {ESOP1996},
  pages = {18--44},
  OPTabstract = {},
  WKloc = {A-0462}
}

@InProceedings{PeytonJones-1996a,
  author = {Peyton Jones, Simon},
  title = {Bulk Types with Class},
  booktitle = {Proc.\null{} 1996 Glasgow Workshop on Functional Programming},
  year = 1996,
  note = {\\\url{http://www.dcs.gla.ac.uk/fp/workshops/fpw96/Proceedings96.html}},
  WKloc = {doc/pap/BIB},
  abstract = {Bulk types --- such as lists, bags, sets, finite maps,
    and priority queues --- are ubiquitous in programming.
    Yet many languages don't support them well,
    even though they have received a great deal of attention,
    especially from the database community.
    Haskell is currently among the culprits.

    This paper has two aims: to identify some of the technical difficulties,
    and to attempt to address them using Haskell's constructor classes.

    The paper can also be read as a concrete proposal
    for new Haskell bulk type libraries.}
}

@InProceedings{PeytonJones-1997,
  author = {Peyton Jones, Simon},
  title = {Bulk Types with Class},
  booktitle = {Proc. 1997 Haskell Workshop, Amsterdam, 7 June},
  year = 1997,
  OPTpages = {},
  OPTabstract = {},
  WKloc = {A-0784, doc/pap/BIB}
}

@Misc{PeytonJones-1997guards,
  author = {Peyton Jones, Simon L.},
  title = {A New View of Guards},
  year = 1997,
  month = APR,
  WKloc = {A-0860}
}

@InCollection{PeytonJones-2000,
  author = {Peyton Jones, Simon L.},
  title = {Tackling the awkward squad: monadic input/output, concurrency, exceptions, and foreign-language calls in Haskell },
  pages = {47--96},
  URL = {http://research.microsoft.com/~simonpj/papers/marktoberdorf.htm},
  WKloc = {A-1115, doc/pap/BIB}
}

@InProceedings{PeytonJones-2006,
  author = 	 {Peyton Jones, Simon L.},
  title = 	 {Beautiful Concurrency},
  booktitle =	 {\unfinished},
  year =	 2006,
  month =	 DEC,
  WKloc = 	 {A-1674},
  keywords = 	 {STM}
}

@InProceedings{PeytonJones-Gordon-Finne-1996,
  author = {Peyton Jones, Simon L. and Andrew Gordon and Sigbjorn Finne},
  title = {Concurrent {Haskell}},
  year = 1996,
  crossref = {POPL1996},
  DOI = {10.1145/237721.237794},
  pages = {295--308},
  WKloc = {A-0833}
}

@Misc{PeytonJones-Hall-Hammond-Partain-1993,
  author = {Peyton Jones, Simon L.},
  title = {The {Glasgow} {Haskell} Compiler: A Technical Overview},
  year = 1993,
  WKloc = {A-0484}
}

@Misc{PeytonJones-Jones-1996,
  author = {Peyton Jones, Simon L. and Mark Jones},
  title = {First-class Modules for Component-Based Programming --- Case for Support},
  year = 1996,
  WKloc = {A-0521}
}

@InProceedings{PeytonJones-Jones-Meijer-1997,
  author = {Peyton Jones, Simon L. and Mark Jones and Erik Meijer},
  title = {Type classes: an exploration of the design space},
  crossref = {Haskell1997},
  DOI = {10.1.1.48.5674},
  CiteSeer = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.5674},
  URL = {http://research.microsoft.com/~simonpj/papers/type-class-design-space},
  WKloc = {A-0433, doc/pap/BIB}
}

@Misc{PeytonJones-Launchbury-1991,
  author = {Peyton Jones, Simon L. and John Launchbury},
  title = {Unboxed Values as First Class Citizens in a Non-strict functional Language},
  year = 1991,
  WKloc = {A-0482}
}

@Article{PeytonJones-Lester-1991,
  author = {Peyton Jones, Simon L. and D. Lester},
  title = {A modular fully-lazy lambda lifter in {\sc Haskell}},
  OPTcrossref = {},
  OPTjournal = {Software --- Practice and Experience},
  OPTyear = 1991,
  OPTvolume = 21,
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {--- PLGnotes --- full laziness transformation
                  should IMHO be result of Wadsworth's graph reduction}
}

@Book{PeytonJones-Lester-1992,
  author = {Peyton Jones, Simon L. and D. Lester},
  title = {Implementing Functional Languages: A Tutorial},
  publisher = {Prentice-Hall},
  year = 1992,
  ISBN = {0-13-721952-0},
  URL = {http://research.microsoft.com/~simonpj/Papers/pj-lester-book},
  series = {Prentice Hall International Series in Computer Science},
  abstract = {Implementing Functional Languages: a Tutorial
		  presents a practical approach to understanding the
		  implementations of non-strict functional languages
		  using lazy graph reduction. The emphasis of this
		  book is on working prototypes of several functional
		  language implementations (template-instantiation,
		  the G-machine, The Three Instruction Machine (TIM)
		  and the parallel G-Machine). In each case the
		  authors provide a complete executable implementation
		  and lead the reader through a series of improvements
		  which broaden its scope and improve its
		  efficiency. This presentation is substantially
		  different to Peyton-Jones earlier book, and includes
		  new material on TIM and full laziness.}
}

@Misc{PeytonJones-Marlow-1999,
  author = {Peyton Jones, Simon and Simon Marlow},
  title = {Secrets of the Glasgow Haskell Compiler Inliner},
  year = 1999,
  month = SEP,
  annote = {``To be submitted to JFP''},
  URL = {http://www.haskell.org/~simonmar/bib.html},
  WKloc = {A-0989}
}

@Misc{PeytonJones-Meijer-Leijen-199X,
  author = {Peyton Jones, Simon L. and Erik Meijer and Daan Leijen},
  title = {Scripting {COM} Components in {Haskell}},
  year = {199X},
  WKloc = {A-0506}
}

@Misc{PeytonJones-Partain-199X,
  author = {Peyton Jones, Simon L. and Will Partain},
  title = {Measuring the Effectiveness of a Simple Strictness Analyser (Draft)},
  year = {199X},
  WKloc = {A-0483}
}

@InProceedings{PeytonJones-Ramsey-Reig-1999,
  author = {Peyton Jones, Simon and Norman Ramsey and Fermin Reig},
  title = {{\texttt{C{-}{-}}}: A Portable Assembly Language that Supports Garbage Collection},
  crossref = {PPDP1999},
  pages = {1--28},
  month = OCT,
  URL = {http://www.cminusminus.org/abstracts/ppdp.html},
  CiteSeer = {http://citeseer.ist.psu.edu/239439.html},
  note = {Invited talk},
  abstract = {For a compiler writer, generating good machine code for a
      variety of platforms is hard work. One might try to reuse a
      retargetable code generator, but code generators are complex and
      difficult to use, and they limit one's choice of implementation
      language. One might try to use C as a portable assembly language, but
      C limits the compiler writer's flexibility and the performance of the
      resulting code. The wide use of C, despite these drawbacks, argues
      for a portable assembly language. C-- is a new language designed
      expressly for this purpose. The use of a portable assembly language
      introduces new problems in the support of such high-level run-time
      services as garbage collection, exception handling, concurrency,
      profiling, and debugging. We address these problems by combining the
      C- - language with a C-- run-time interface. The combination is
      designed to allow the compiler writer a choice of source language
      semantics and implementation techniques, while still providing good
      performance.},
  WKloc = {doc/pap/BIB},
  bibliographies = {Coconut}
}

@InProceedings{PeytonJones-Reid-Hoare-Marlow-Henderson-1999,
  author = 	 {Peyton Jones, Simon L. and A. Reid and T. Hoare and S. Marlow and F. Henderson},
  title = 	 {A Semantics of Imprecise Exceptions},
  crossref =	 {PLDI1999},
  pages =	 {25--36}
}

@Article{PeytonJones-Santos-1998,
  author = {Peyton Jones, Simon L. and Andr{\'e} L. M. Santos},
  title = {A Transformation-based Optimiser for {Haskell}},
  journal = {Science of Computer Programming},
  year = 1998,
  volume = {??},
  OPTnumber = {},
  OPTmonth = {},
  pages = {??},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0849},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{PeytonJones-Shields-2003,
  author = {Peyton Jones, Simon and Mark Shields},
  title = {Practical type inference for arbitrary-rank types},
  journal = JFP,
  year = 2003,
  WKloc = {A-1462, doc/pap/BIB},
  URL = {http://research.microsoft.com/~simonpj/papers/putting/index.htm},
  note = {draft},
  abstract = {Haskell's popularity has driven the need for ever more
      expressive type system features, most of which threaten the
      decidability and practicality of Damas-Milner type inference. One
      such feature is the ability to write functions with higher-rank types
      --- that is, functions that take polymorphic functions as their
      arguments.

      Complete type inference is known to be undecidable for higher-rank
      type systems, but in practice programmers are more than willing to
      add type annotations to guide the type inference engine, and to
      document their code. However, the choice of just what annotations are
      required, and what changes are required in the type system and and
      its inference algorithm, has been an ongoing topic of research.

      We take as our starting point a lambda-calculus proposed by Odersky
      and Laufer. Their system supports arbitrary-ranked polymorphism
      through the exploitation of type annotations on lambda-bound
      arguments and arbitrary sub-terms. Though elegant, and more
      convenient than some other proposals, Odersky and Laufer's system is
      potentially very expensive at compile-time, and requires many
      annotations. We systematically address these two difficulties to
      yield a type inference system only mildly more complicated than
      existing implementations of Damas-Milner, and of comparable
      compile-time cost. To substantiate these claims, we describe a type
      inference algorithm in detail.}
}

@InBook{PeytonJones-Wadler-1987,
  author = {Peyton Jones, Simon L. and Philip Wadler},
  title = {Structured Types and the Semantics of Pattern-Matching},
  chapter = 4,
  pages = {51--77},
  crossref = {PeytonJones-1987},
  annote = {GFA}
}

@Misc{PeytonJones-Wadler-1992,
  author = {Peyton Jones, Simon L. and Philip Wadler},
  title = {A Static Semantics for {Haskell}},
  year = 1992,
  month = FEB,
  WKloc = {A-0862}
}

@InProceedings{PeytonJones-Wadler-1993,
  title = {Imperative Functional Programming},
  author = {Peyton Jones, Simon L. and Philip Wadler},
  pages = {71--84},
  crossref = {POPL1993},
  WKloc = {A-0105},
  keywords = {partial morphisms},
  FTP = {haskell sites},
  filename = {io-popl},
  DIRECTORY = {ghc/docs/papers},
  authorsAddress = {\{simonpj,wadler\}\@dcs.glasgow.ac.uk},
  abstract = {We present a new model, based on monads, for
		  performing input/output in a non-strict, purely
		  functional language. It is composable, extensible,
		  efficient, requires no extensions to the type
		  system, and extends smoothly to incorporate
		  mixed-language working and in-place array updates.}
}

@InProceedings{PeytonJones-Washburn-Weirich-2004,
  author = {Peyton Jones, Simon and Geoffrey Washburn and Stephanie Weirich},
  title = {Wobbly types: type inference for generalised algebraic data types},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month = 	 JUL,
  WKloc = {A-1556, doc/pap/BIB},
  URL = {http://research.microsoft.com/Users/simonpj/papers/gadt/index.htm},
  note = 	 {submitted to POPL 2005},
  abstract = {Generalised algebraic data types (GADTs), sometimes
     known as ``guarded recursive data types'' or ``first-class
     phantom types'', are a simple but powerful generalisation of the
     data types of Haskell and ML. Recent works have given compelling
     examples of the utility of GADTs, although type inference is
     known to be difficult.

     It is time to pluck the fruit. Can GADTs be added to Haskell,
     without losing type inference, or requiring unacceptably heavy
     type annotations? Can this be done without completely rewriting
     the already-complex Haskell type-inference engine, and without
     complex interactions with (say) type classes? We answer these
     questions in the affirmative, giving a type system that explains
     just what type annotations are required, and a prototype
     implementation that implements it. Our main technical innovation
     is \emph{wobbly types}, which express in a declarative way the
     uncertainty caused by the incremental nature of typical
     type-inference algorithms.}
}

@Article{Pfaltz-1996,
  title = {Closure Lattices},
  author = {John L. Pfaltz},
  pages = {217--236},
  journal = {Discrete Mathematics},
  year = 1996,
  volume = 154,
  WKloc = {A-0801}
}

@Misc{Pfaltz-1998,
  title = {Closure Spaces in the Plane},
  author = {John L. Pfaltz},
  year = 1998,
  month = DEC,
  WKloc = {A-0793}
}

@Misc{Pfaltz-1998a,
  author = {John L. Pfaltz},
  title = {Neighborhood Expansion Grammars},
  year = 1998,
  month = NOV,
  WKloc = {A-0800}
}

@InProceedings{Pfaltz-2000,
  author =       {John L. Pfaltz},
  title =        {Neighborhood Expansion Grammars},
  crossref =  {TAGT1998},
  pages =     {30--44},
  DOI =      {10.1007/b75045},
  URL =    {http://www.springerlink.com/content/1qqvqlkng0kbkn5u/},
  bibliographies =    {RelMiCS},
  abstract = {Phrase structure grammars,
    in which non-terminal symbols on the left side of a production
    can be rewritten by the string on the right side,
    together with their Chomsky hierarchy classification,
    are familiar to computer scientists.
    But, these grammars are most effective only to generate, and parse, strings.

    In this paper, we introduce a new kind of grammar
    in which the right side of the production
    is simply appended to the intermediate structure
    in such a way that the left side becomes
    its ``neighborhood'' in the new structure.

    This permits the grammatical definition of many different kinds
    of ``n-dimensional'' discrete structures.
    Several examples are given.

    Moreover, these grammars yield a formal theory
    grounded in antimatroid closure spaces.
    For example, we show that restricted neighborhood expansion grammars
    capture the essence
    of finite state and context free phrase structure grammars.}
}


@Misc{Pfaltz-Karro-1998,
  author = {John L. Pfaltz and John E. Karro},
  title = {Transformations of Antimatroid Closure Spaces},
  year = 1998,
  month = JUL,
  WKloc = {A-0802}
}

@InProceedings{Pfaltz-Rosenfeld-1969,
  author = {John L. Pfaltz and A. Rosenfeld},
  title = {Web Grammars},
  pages = {609--619},
  booktitle = {Proc. Int. Joint Conf. Art. Intelligence},
  year = 1969,
  address = {Washington}
}

@InCollection{ Pfeifer-Ruess-1998,
    author = "H. Pfeifer and H. Rue{\ss}",
    title = "Polytypic Abstraction in Type Theory",
    booktitle = "Informal Proceedings Workshop on Generic Programming, {WGP}'98, Marstrand, Sweden, 18 June 1998",
    publisher = "Dept.\ of Computing Science, Chalmers Univ.\ of Techn.\ and G{\"o}teborg Univ.",
    editor = "Roland Backhouse and Tim Sheard",
    year = "1998",
    url = "citeseer.ist.psu.edu/69922.html",
  WKloc = {doc/pap/BIB},
  abstract = {This paper is concerned with formalizations and verifications in type theory that are abstracted with respect to a large class of datatypes; i.e polytypic formalizations. The main advantage of these developments are that they can not only be used to polytypically define functions but also to formally state polytypic theorems and to...}
}

@InProceedings{Pfeiffer-Selke-1991,
  title = {On the Adequacy of Dependence-Based Representations for
		  Programs with Heaps},
  author = {Phil Pfeiffer and Rebecca Parsons Selke},
  pages = {365--386},
  crossref = {TACS1991},
  abstract = {Program dependence graphs ({\em pdgs\/}) are popular tools
		  for reasoning about a program's semantics.  This
		  report proves two fundamental theorems about the
		  representational soundness of {\em pdgs\/} for
		  languages with heap-allocated storage and reference
		  variables.  The first, the {\em Pointer-Language
		  Equivalence Theorem}, asserts that {\em pdgs\/}
		  adequately represent a program's threads of
		  computation.  These theorems are demonstrated with
		  two new lemmas about the semantics of {\em pdgs\/}
		  for languages that lack pointer variables.  These
		  lemmas, the {\em Dynamic Equivalence\/} and {\em
		  Dynamic Slicing Theorems}, state that an edge can
		  safely be removed from a program's {\em pdg\/} if
		  this edges represents a static dependence that does
		  not arise at run-time.}
}

@Misc{Pfenning-1991,
  author = {Frank Pfenning},
  title = {Logic Programming in the {LF} Logical Framework},
  year = 1991,
  WKloc = {A-0694}
}

@InProceedings{Pfenning-1996,
  author = {Frank Pfenning},
  title = {The Practice of Logical Frameworks},
  booktitle = {CAAP '96},
  year = 1996,
  pages = {119--134},
  OPTabstract = {},
  WKloc = {A-0552}
}

@InProceedings{Pfenning-1998,
  author = {Frank Pfenning},
  title = {Reasoning Abaout Deductions in Linear Logic},
  crossref = {CADE1998},
  pages = {1--2},
  OPTabstract = {},
  WKloc = {A-0599},
  note = {Invited Talk}
}

@InCollection{Pfenning-2001,
  author = {Frank Pfenning},
  title = {Logical Frameworks},
  crossref = {Robinson-Voronkov-2001},
  chapter = {Chapter XXI},
  WKloc = {A-1399, doc/pap/BIB}
}

@Article{Pfenning-Lee-1991,
 author = {Frank Pfenning and Peter Lee},
 title = {Metacircularity in the polymorphic $\lambda$-calculus},
 journal = TCS,
 volume = {89},
 number = {1},
 year = {1991},
 issn = {0304-3975},
 pages = {137--159},
 doi = {http://dx.doi.org/10.1016/0304-3975(90)90109-U},
 publisher = {Elsevier Science Publishers Ltd.},
}

@InProceedings{Phoa-1991,
  author = {Wesley K.-S. Phoa},
  title = {Two Results on Set-Theoretic Polymorphism},
  crossref = {CTCS1991},
  pages = {219--235},
  WKloc = {A-0101},
  abstract = {Moggi and Hyland showed how to model various
		  polymorphic $\lambda$-calculi inside the effective
		  topos and other realizability toposes; types are
		  modelled by the so-called {\em modest sets}, which
		  form an internal category {\bf Mod} in the topos
		  that is, in a certain sense, complete. Polymorphic
		  types are modelled as products indexed by the object
		  of modest sets. The same idea lets us model
		  polymorphism in reflective subcategories of the
		  category of modest sets---for example, the
		  categories of {\em synthetic domains} studied by
		  various authors.

                  This paper presents two alternative interpretations
		  of polymorphic types. The first is the {\bf groupoid
		  interpretation}. Unlike the Moggi-Hyland
		  interpretation, it is stable under equivalence; but
		  it is also very easy to define, and makes sense for
		  any small `complete' category in a topos.

                  The second is the {\em uniformized interpretation}
		  applicable to reflective subcategories of {\bf Mod}.
		  It clarifies the way in which they can be regarded
		  as PER models, and has applications to the
		  interpretation of subtyping and bounded quantification.}
}

@InProceedings{Phoa-1991a,
  title = {From Term Models to Domains},
  author = {Wesley Phoa},
  pages = {88--111},
  crossref = {TACS1991},
  abstract = {Let {\sf B} be the closed term model of the
		  $\lambda$-calculus in which terms with the same
		  B\"ohm tree are identified.  We investigate which
		  partial equivalence relations (PERs) on {\sf B} can
		  be regarded as predomains or domains.  Working
		  inside the realizability topos on {\sf B}'s, such
		  PERs can be regarded simply as sets in a particular
		  model of constructive set theory.

                  No well-behaved partial order has been identified
		  for any class of PERs; but it is still possible to
		  isolate those PERs which have `suprema of chains' in
		  a certain sense, and all maps between such PERs in
		  the model preserve such suprema of chains.  One can
		  also define what it means for such a PER to have a
		  `bottom'; partial function spaces provide an
		  example.  For these PERs, fixed points of arbitrary
		  endofunctions exist and are computed by the fixed
		  point combinator {\sf y}.  There is also a notion of
		  meet-closure for which all maps are stable.

                  The categories of predomains are closed under the
		  formation of total and partial function spaces,
		  polymorphic types and convex powerdomains.
		  (Subtyping and bounded quantification can also be
		  modelled.)  They in fact form reflective
		  subcategories of the realizability topos; and in
		  this set-theoretic context, these constructions are
		  very simple to describe.}
}

@TechReport{Phoa-1992,
  author = {Wesley Phoa},
  title = {An introduction to fibrations, topos theory, the effective
		  topos and modest sets},
  institution = {Laboratory for Foundations of Computer Science},
  year = 1992,
  number = {ECS-LFCS-92-208},
  address = {University of Edinburgh, Scotland, UK},
  ftpaddress = {ftp.dcs.ed.ac.uk},
  URL = {http://www.dcs.ed.ac.uk/lfcs/},
  WKloc = {B-0039},
  abstract = {A {\em topos} is a categorical model of constructive set
		  theory.   In particular, the {\em effective topos}
		  is the categorical `universe' of recursive
		  mathematics.   Among its objects are the {\em modest
		  sets}, which form a set-theoretic model for
		  polymorphism. More precisely, there is a {\em
		  fibration} of modest sets which satisfies suitable
		  categorical completeness properties, that make it a
		  model for various polymorphic type theories.

                  These lecture notes provide a reasonably thorough
		  introduction to this body of material, aimed at
		  theoretical computer scientists rather than topos
		  theorists.   Chapter 2 is an outline of the theory
		  of fibrations, and sketches how they can be used to
		  model various typed lambda-calculi.   Chapter 3 is
		  an exposition of some basic topos theory, and
		  explains why a topos can be regarded as a model of
		  set theory. Chapter 4 discusses the classical PER
		  model for polymorphism, and shows how it `lives
		  inside' a particular topos - the effective topos -
		  as the category of modest sets.   An appendix
		  contains a full presentation of the internal
		  language of a topos, and a map of the effective
		  topos.

                  Chapters 2 and 3 provide a sampler of categorical
		  type theory and categorical logic, and should be of
		  more general interest than Chapter 4.   They can be
		  read more or less independently of each other;  a
		  connection is made at the end of Chapter 3.

                  The main prerequisite for reading these notes is
		  some basic category theory:  limits and colimits,
		  functors and natural transformations, adjoints,
		  cartesian closed categories.   No  knowledge of
		  indexed categories or categorical logic is needed.
		  Some familiarity with `ordinary' logic and typed
		  lambda-calculus is assumed.}
}
a publication

@Article{Picard-Matthes-2011,
  author = {Picard, Celia and Matthes, Ralph},
  title = "{Coinductive Graph Representation : the Problem of Embedded Lists}",
  journal = {Electronic Communications of the EASST, Selected Revised Papers from the Third International Workshop on Graph Computation Models (GCM 2010)},
  publisher = {Electronic Communications of the EASST},
  address = {http://eceasst.cs.tu-berlin.de/},
  year = {2011},
  month = SEP,
  volume = {39},
  pages = {(on line)},
  WKloc = {doc/pap/BIB},
  URL = {http://journal.ub.tu-berlin.de/eceasst/article/view/649},
  keywords = {coinduction/corecursion, guardedness, theorem proving, dependent types, metamodels},
  abstract = {When trying to obtain formally certified model transformations, one may want to represent models as graphs and graphs as greatest fixed points. To do so, one is rather naturally led to define co-inductive types that use lists (to represent a finite but unbounded number of children of internal nodes). These concepts are rather well supported in the proof assistant Coq. However, their use in our intended applications may cause problems since the co-recursive call in the type definition occurs in the list parameter. When defining co-recursive functions on such structures, one will face guardedness issues, and in fact, the syntactic criterion applied by the Coq system is too rigid here.

    We offer a solution using dependent types to overcome the guardedness issues that arise in our graph transformations. We also give examples of further properties and results, among which finiteness of represented graphs. All of this has been fully formalized in Coq.}
}

@InProceedings{Pidgeon-1997,
  author = {Christophewr W. Pidgeon},
  title = {Organizing and Enabling Domain Engineering to Facilitate Software Maintenance},
  booktitle = {$8^{\mathrm{th}}$ Annual Workshop on Software Reuse},
  year = 1997,
  WKloc = {A-1129}
}

@InProceedings{Pientka-Pfenning-2003,
  author = {Brigitte Pientka and Frank Pfenning},
  title = {Optimizing Higher-Order Pattern Unification},
  OPTcrossref = {},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = {A-1415},
  abstract = {We present an abstract view of existential variables in a
      dependently typed lambda-calculus based on modal type theory. This
      allows us to justify optimizations to pattern unification such as
      linearization, which eliminates many unnecessary occurs-checks. The
      presented modal framework explains a number of features of the
      current implementation of higher-order unification in Twelf and
      provides insight into several optimizations. Experimental results
      demonstrate significant performance improvement in many example
      applications of Twelf, including those in the area of proof-carrying
      code.}
}

@TechReport{Pierce-1988,
  author = {Benjamin C. Pierce},
  title = {A Taste of Category Theory for Computer Scientists},
  institution = {Computer Science Department, Carnegie Mellon University},
  number = {CMU-CS-88-203},
  year = 1988
}

@Book{Pierce-1991,
  author = {Pierce, Benjamin C.},
  title = {Basic Category Theory for Computer Scientists},
  publisher = {{MIT} Press},
  series = {Foundations of Computing Series},
  year = 1991,
  ISBN = {0-262-66071-7},
  McMaster = {owned, QA 76.9 .M35 P54z 1991}
}

@InProceedings{Pierce-1992,
  author = {Benjamin C. Pierce},
  title = {Bounded Quantification is Undecidable},
  pages = {305--315},
  abstract = {$F_\leq$ is a typed $\lambda$-calculus with subtyping and
             bounded second-order polymorphism. First proposed by Cardelli
             and Wegner, it has been widely studied as a core calculus for
             type systems with subtyping.

             Curien and Ghelli proved the
             partial correctness of a recursive procedure for computing
             minimal types of $F_\leq$ terms and showed that the termination
             of this procedure is equivalent to the termination of its major
             component, a procedure for checking the subtype relation
             between $F_\leq$ types. This procedure was thought to terminate
             on all inputs, but the discovery of a subtle bug in a purported
             proof of this claim recently reopened the question of the
             decidability of subtyping, and hence of typechecking. This
             question is settled here in the negative, using a reduction
             from the halting problem for two-counter Turing machines to
             show that the subtype relation of $F_\leq$ is undecidable.},
  crossref = {POPL1992},
  authorsAddress = {bcp\@cs.cmu.edu},
  WKloc = {A-0164}
}

@InProceedings{Pierce-1993,
  author = {Benjamin C. Pierce},
  title = {Intersection Types and Bounded Polymorphism},
  pages = {346--360},
  abstract = {Intersection type and bounded quantification are complementary
             extensions of first-order a statically typed programming
             language with subtyping. We define a typed $\lambda$-calculus
             combining these extensions, illustrate its properties, and
             develop proof-theoretic results leading to algorithms for
             subtyping and typechecking.},
  crossref = {TLCA93},
  WKloc = {A-0189}
}

@InCollection{Pierce-1996,
  author = {Benjamin C. Pierce},
  title = {Foundational Calculi for Programming Languages},
  booktitle = {{CRC} Handbook of Computer Science and Engineering},
  year = 1996,
  WKloc = {A-0764}
}

@InProceedings{Pierce-Turner-1993,
  author = {Benjamin C. Pierce and David N. Turner},
  title = {Object-Oriented Programming Without Recursive Types},
  pages = {299--312},
  abstract = {It ia widely agreed that recursive types are inherent in the
             static typing of the essential mechanisms of objectoriented
             programming: encapsulation, message passing, subtyping, and
             inheritance. We demonstrate here that modeling object
             encapsulation in terms of existential types yields a
             substantially more straightforward explanation of these
             features in a simpler calculus without recursive types.},
  crossref = {POPL1993},
  WKloc = {A-0198}
}

@TechReport{Pietrzykowski-1971,
  author = {T. Pietrzykowski},
  title = {A Complete Mechanization of Second-Order Logic},
  institution = {Dept. of Appl. Anal. and Comp. Sci., University of
                 Waterloo},
  year = 1971,
  type = {Research report},
  number = {CSSR 2038}
}

@Article{Pietrzykowski-1973,
  title = {A Complete Mechanization of Second-Order Type Theory},
  author = {Tomasz Pietrzykowski},
  area = {Theorem Proving},
  pages = {333--365},
  journal = JACM,
  month = apr,
  year = 1973,
  volume = 20,
  number = 2,
  keywords = {theorem-proving, resolution, second-order logic, type
                 theory, unification, matching},
  cr64-categories = {3.61, 5.21},
  references = {\cite{JACM::Robinson1965}}
}

@InProceedings{Pinto-1993,
  author = {Lu\'{\i}s Pinto},
  title = {Cut Formulae and Logic Programming},
  crossref = {ELP93},
  pages = {282--300},
  authorsAddress = {St Andrews, Scotland},
  abstract = {In this this paper we present a mechanism to names
		  for proof-witnesses of formulae and thus to use
		  Gentzen's cut-rule in logic programming. We consider
		  a program to be a set of logical formulae together
		  with a list of such definitions. Occurrences of the
		  defined names guide the proof-search by indicating
		  when an instance of the cut-rule should be
		  attempted. By using the cut-rule there are proofs
		  that can be made dramatically shorter. We explain
		  how this idea of using the cut-rule can be applied
		  to the logic of hereditary Harrop formulae.}
}

@InCollection{Pinto-2000,
  author={Pinto, Jorge Sousa},
  title={Sequential and Concurrent Abstract Machines for Interaction Nets},
  year=2000,
  crossref = {FoSSaCS2000},
  DOI={10.1007/3-540-46432-8_18},
  DOIURL={http://dx.doi.org/10.1007/3-540-46432-8_18},
  pages={267--282},
  abstract = {This paper is a formal study of how to implement interaction nets, filling an important gap in the work on this graphical rewriting formalism, very promising for the implementation of languages based on the $\lambda$-calculus. We propose the first abstract machine for interaction net reduction, based on a decomposition of interaction rules into more atomic steps, which tackles all the implementation details hidden in the graphical presentation. As a natural extension of this, we then give a concurrent shared-memory abstract machine, and show how to implement it, resulting in the first parallel implementation of interaction nets}
}

@InCollection{Pinto-2001RTA,
  author={Pinto, Jorge Sousa},
  title={Parallel Evaluation of Interaction Nets with {MPINE}},
  isbn={978-3-540-42117-7},
  crossref = {RTA2001},
  DOI={10.1007/3-540-45127-7_26},
  DOIURL={http://dx.doi.org/10.1007/3-540-45127-7_26},
  pages={353--356},
  abstract = {We describe the MPINE tool, a multi-threaded evaluator for Interaction Nets. The evaluator is an implementation of the present author’s Abstract Machine for Interaction Nets [5] and uses POSIX threads to achieve concurrent execution. When running on a multi-processor machine (say an SMP architecture), parallel execution is achieved effortlessly, allowing for desktop parallelism on commonly available machines.}
}

@InProceedings{Piperno-RonchiDellaRocca-1994,
  title = {Type Inference and Extensionality},
  author = {Adolfo Piperno and Ronchi della Rocca, Simona},
  pages = {196--205},
  crossref = {LICS9},
  WKloc = {A-0370},
  abstract = {The polymorphic type assignment system~$F_2$ is the type
		  assignment counterpart of Girard's and Reynolds'
		  system~$F$.  Though introduced in the early
		  seventies, both the type inference and the type
		  checking problems for~$F_2$ remained open for a long
		  time. Recently, an undecidability result was
		  announced.  Consequently, it is considerably
		  interesting to find decidable restrictions of the
		  system.

                  We show a bounded type inference and a bounded type
		  checking algorithm, both based on the study of the
		  relationship between the typability of a term and
		  the typability of terms that ``properly''
		  $\eta$-reduce to it.}
}

@InProceedings{Pirzadeh-Dube-200X,
  author = 	 {Heidar Pirzadeh and Danny Dub{\'e}},
  title = 	 {Encoding the Program Correctness Proofs as Programs in {PCC} Technoogy},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1707},
  OPTannote = 	 {}
}

@Article{Pitts-1983,
  author = {Andrew M. Pitts},
  title = {Amalgamation and Interpolation in the Category of {Heyting} Algebras},
  journal = JPAA,
  year = 1983,
  volume = 29,
  pages = {155--165},
  bibliographies = {RelMiCS}
}

@InProceedings{Pitts-1993,
  author = {Andrew M. Pitts},
  title = {Relational Properties of Recursively Defined Domains},
  crossref = {LICS8},
  year = 1993,
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Pitts-1993a,
  author = {Andrew M. Pitts},
  title = {Relational Properties of Domains},
  institution = {Cambridge University Computer Laboratory},
  number = 321,
  year = 1993,
  month = DEC,
  WKloc = {A-0966},
  note = {long version of \cite{Pitts-1993}},
  bibliographies = {RelMiCS, LogRel}
}

@InCollection{Pitts-2001,
  author = {Andrew M. Pitts},
  title = {Categorical Logic},
  year = 2001,
  booktitle = {Handbook of Logic in Computer Science},
  subtitle = {Logic and algebraic methods},
  volume = {5},
  editor = {S. Abramsky and D. M. Gabbay and T. S. E. Maibaum},
  publisher = {Oxford University Press},
  WKloc = {A-0922},
  bibliographies = {DepObj},
  OPTISBN = {0198537816},
  pages = {39--128}
}

@InProceedings{Pitts-Gabbay-2000,
  author = {Andrew M. Pitts and Murdoch J. Gabbay},
  title = {A Metalanguage for Programming with Bound Names Modulo Renaming.},
  crossref = {MPC2000},
  pages = {230--255},
  year = 2000,
  WKloc = {A-1040},
  URL = {http://www.cl.cam.ac.uk/users/amp12/freshml/}
}

@TechManual{Pizzini-1998,
  author = {Ken Pizzini},
  title = {dc, an Arbitrary Precision Calculator},
  year = 1998,
  OPTmonth = {},
  institution = {Free Software Foundation},
  note = {original manual by Richard Stallman},
  OPTnumber = {},
  OPTaddress = {},
  WKloc = {A-0773}
}

@InCollection{Plaisted-1993,
  filename = {*},
  DIRECTORY = {~kahl/doc/pap/plaisted},
  title = {Equational Reasoning and Term-Rewriting Systems},
  crossref = {Handbook-LAILP},
  author = {David Plaisted},
  WKloc = {B-0001}
}

@InProceedings{Plandowski-1994,
  author = {W. Plandowski},
  title = {Testing Morphism Equivalence on Context-Free Languages},
  crossref = {ESA94},
  OPTpages = {},
  OPTnote = {},
  authorsAddress = {Uniwersytet Warszawski},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Plank-Thomason-2001,
  author = {James S. Plank and Michael G. Thomason},
  title = {Processor Allocation and Checkpoint Interval Selection in Cluster Computing Systems},
  journal = {Journal of Parallel and Distributed Computing},
  year = 2001,
  WKloc = {doc/pap/BIB},
  bibliographies = {ProcMig},
  abstract = {Performance prediction of checkpointing systems in the
      presence of failures is a well-studied research area. While the
      literature abounds with performance models of checkpointing systems,
      none address the issue of selecting runtime parameters other than the
      optimal checkpointing interval. In particular, the issue of processor
      allocation is typically ignored. In this paper, we present a
      performance model for long-running parallel computations that execute
      with checkpointing enabled. We then discuss how it is relevant to
      today's parallel computing environments and software, and present
      case studies of using the model to select runtime parameters.},
  keywords = {Checkpointing, performance prediction, parameter selection, parallel computation, Markov chain, exponential failure and repair distributions.}
}

@Book{Plasmeijer-vanEekelen-1993,
  author = {Rinus Plasmeijer and van Eekelen, Marko},
  title = {Functional Programming and Parallel Graph Rewriting},
  publisher = {Addison-Wesley},
  year = 1993,
  series = {International Computer Science Series},
  ISBN = {0-201-41663-8},
  WKloc = {Q-014},
  McMaster = {QA 76.62 .P55 1993}
}

@InProceedings{Pleban-Lee-1987,
  WKloc = {A-0068},
  abstract = {In the course of implementing a semantics directed
		  compiler generator we have developed {\em high-level
		  semantics}, a new style of semantic definition which
		  overcomes fundamental problems concerning the
		  specification techniques used in traditional
		  denotational semantics. In the past, these problems
		  have precluded the generation of realistic compilers
		  from traditional denotational specifications. By
		  contrast, high level semantic specifications are
		  suitable for both {\em defining} the functional {\em
		  semantics} of programming languages, and {\em
		  describing} realistic compiler {\em implementations}
		  which are automatically generated from the
		  semantics.

                  In an earlier paper we described the MESS system, a
		  prototype implementation of a compiler generator
		  incorporating the principles of high-level
		  semantics. Here, we summarize the salient
		  characteristics of our methodology. A comprehensive
		  overview of high-level semantics and the MESS
		  compiler generator can be found in the dissertation
		  of the second author.},
  title = {High-Level Semantics, An integrated approach to
		  programming language semantics and the specification
		  of implementations},
  pages = {550--571},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Uwe F. Pleban and Peter Lee}
}

@Article{Plotkin-1975,
  WKloc = {A-0045},
  title = {Call-By-Name, Call-By-Value and the $\lambda$-Calculus},
  year = 1975,
  volume = 1,
  pages = {125-159},
  journal = {Theoretical Computer Science},
  author = {Gordon Plotkin},
  abstract = {This paper examines the old question of the
		  relationship between ISWIM and the
		  $\lambda$-calculus, using the distinction between
		  call-by-value and call-by-name. It is held that the
		  relationship should be mediated by a standardisation
		  theorem. Since this leads to difficulties, a new
		  $\lambda$-calculus is introduced whose
		  standardisation theorem gives a good correspondence
		  with ISWIM as given by the SECD machine, but without
		  the {\em letrec} feature. Next a call-by-name
		  variant of ISWIM is introducedwhich is in an
		  analogous correspondence with the usual
		  $\lambda$-calculus. The relation between
		  call-by-value and call-by-name is then studied by
		  giving simulations of each language by the other and
		  interpretations of each calculus in the other. These
		  are obtaines as another application of the
		  continuation technique. Some emphasis is placed
		  throughout on the motion of operational equality (or
		  contextual equality). If terms can be proved equal
		  in a calculus they are operationally equal in the
		  corresponding language. Unfortunately, operational
		  equality is not preserved by either of the simulations.}
}

@Article{Plotkin-1976,
  author = {Gordon Plotkin},
  title = {A Powerdomain Construction},
  journal = SIAMCOMP,
  year = 1976,
  volume = 5,
  pages = {452--487},
  bibliographies = {RelMiCS}
}

@InCollection{Plotkin-1980,
  author = {Gordon D. Plotkin},
  title = {Lambda-Definability in the Full Type Hierarchy},
  crossref = {Seldin-Hindley-1980},
  pages = {363--373},
  bibliographies = {LogRel, RelMiCS}
}

@Misc{Plotkin-1983,
  WKloc = {B-0004},
  FTP = {theory.lcs.mit.edu:/pub/papers/Plotkin},
  filename = {Plotkin-*},
  DIRECTORY = {~kahl/doc/pap},
  year = 1983,
  title = {Domains},
  publisher = {Department of Computer Science, University of Edinburgh},
  howpublished = {course notes},
  author = {Gordon Plotkin}
}

@InProceedings{Plotkin-1991,
  title = {A Semantics for Type Checking},
  author = {Gordon Plotkin},
  pages = {1--17},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {Curry's system for F-deducibility is the basis for
		  implicit type checking for programming languages
		  such as ML\null. If a natural ``preservation of
		  types by conversion'' rule is added it becomes
		  undecidable, but complete relative to a variety of
		  model classes. We show completeness for
		  F-deducibility itself, relative to an extended
		  notion of model which validates reduction but not
		  conversion. Both term model and filter model proofs
		  are given, and the extension to polymorphic typing
		  is also considered.}
}

@Article{Plotkin-2004,
  author = 	 {Gordon Plotkin},
  title = 	 {The origins of structural operational semantics},
  journal = 	 JLAP,
  year = 	 2004,
  volume = 	 {60-61},
  pages = 	 {3--15},
  DOI = 	 {http://dx.doi.org/10.1016/j.jlap.2004.03.009},
  abstract = 	 {We review the origins of structural operational
                  semantics. The main publication `A Structural
                  Approach to Operational Semantics,' also known as
                  the `Aarhus Notes,' appeared in 1981 [G.D. Plotkin,
                  A structural approach to operational semantics,
                  DAIMI FN-19, Computer Science Department, Aarhus
                  University, 1981]. The development of the ideas
                  dates back to the early 1970s, involving many people
                  and building on previous work on programming
                  languages and logic. The former included abstract
                  syntax, the SECD machine, and the abstract
                  interpreting machines of the Vienna school; the
                  latter included the $\lambda$-calculus and formal systems.

                  The initial development of structural operational
                  semantics was for simple functional languages, more
                  or less variations of the $\lambda$-calculus; after
                  that the ideas were gradually extended to include
                  languages with parallel features, such as Milner's
                  CCS. This experience set the ground for a more
                  systematic exposition, the subject of an invited
                  course of lectures at Aarhus University; some of
                  these appeared in print as the 1981 Notes.

                  We discuss the content of these lectures and some
                  related considerations such as `small state' versus
                  `grand state,' structural versus compositional
                  semantics, the influence of the Scott-Strachey
                  approach to denotational semantics, the treatment of
                  recursion and jumps, and static semantics. We next
                  discuss relations with other work and some immediate
                  further development. We conclude with an account of
                  an old, previously unpublished, idea: an
                  alternative, perhaps more readable, graphical
                  presentation of systems of rules for operational
                  semantics. }
}

@InProceedings{Plotkin-Abadi-1993,
  author = {Gordon Plotkin and Martin Abadi},
  title = {A Logic for Parametric Polymorphism},
  pages = {361--375},
  abstract = {In this paper we introduce a logic for parametric
             polymorphism. Just as LCF is a logic for the simply-Typed
             $\lambda$-calculus with recursion and arithmetic, our logic is
             a logic for System F. The logic permits the formal presentation
             and use of use of relational parametricity. Parametricity
             yields--for example--encodings of initial algebras, final
             co-algebras and abstract datatypes, with corresponding proof
             principles of induction, co-induction and simulation.},
  crossref = {TLCA93},
  WKloc = {A-0190}
}

@InProceedings{Plotkin-Abadi-Cardelli-1994,
  title = {Subtyping and Parametricity},
  author = {Gordon Plotkin and Mart{\'i}n Abadi and Luca Cardelli},
  pages = {310--319},
  crossref = {LICS9},
  WKloc = {A-0371},
  abstract = {In this paper we study the interaction of subtyping and
		  parametricity.  We describe a logic for a
		  programming language with parametric polymorphism
		  and subtyping.  The logic supports the formal
		  definition and use of relational parametricity. We
		  give two models for it, and compare it with other
		  formal systems for the same language. In particular,
		  we examine the ``Penn interpretation'' of subtyping
		  as implicit coercion.

                  Without subtyping, parametricity yields, for
		  example, an encoding of abstract types and of
		  initial algebras, with the corresponding proof
		  principles of simulation and induction. With
		  subtyping, we obtain partially abstract types and
		  certain initial order-sorted algebras, and may
		  derive proof principles for them.}
}

@InProceedings{Plotkin-Winskel-1994,
  author = {Gordon Plotkin and G. Winskel},
  title = {Bistructures, Bidomains and Linear Logic},
  pages = {352--363},
  crossref = {ICALP1994},
  authorsAddress = {GP: Edinburgh; GW: Aarhus University},
  abstract = {Bistructures are a generalisation of event structures to
		  represent spaces of functions at higher types; the
		  partial order of causal dependency is replaced by
		  two orders, one associated with input
                  and the other output in the behaviour of
                  functions. Bistructures form a categorical model of
                  Girard's linear logic in which the involution of
		  linear logic is modelled, roughly speaking, by a
		  reversal of the roles of input and output. The
		  comonad of the modal has associated a
                  co-Kleisli category which is equivalent to a
                  cartesian-closed full subcategory of Berry's
                  bidomains.}
}

@InProceedings{Plump-1990,
  author = {Detlef Plump},
  title = {Graph-Reducible Term Rewriting Systems},
  pages = {622--636},
  crossref = {GG1990},
  keywords = {term rewriting, graph reduction, completeness of graph reduction},
  contents = {1 Introduction
	2 Jungle Evaluation
	3 Graph-Reducibility
	4 Reduction Strategies
	5 Two Classes of Parallelly Normalizing Systems
	6 Conclusion},
  abstract = {Term rewriting is commonly interpreted by graph reduction in
	order to improve efficiency. In general, however graph reduction is
	not complete: a term may be not normalizable through graph
	derivations although a normal form exists. Term rewriting systems
	which permit a complete implementation by graph reduction are called
	graph-reducible. We show that the following property is sufficient
	for graph-reducibility: every term having a normal form can be
	normalized by parallel term rewrite steps in which a rule is applied
	to all occurrences of some subterm. As a consequence, a broad class
	of term rewriting systems which includes all terminating and all
	orthogonal systems can be shown to be graph-reducible.},
  annote = {--- PLGnotes ---
		  term rewriting to jungle evaluation}
}

@PhDThesis{Plump-1993,
  author = {Detlef Plump},
  title = {Evaluation of Functional Expressions by Hypergraph Rewriting},
  school = {Universit\"at Bremen},
  year = 1993,
  type = {Dissertation}
}

@InProceedings{Plump-1994,
  author = {Detlef Plump},
  title = {Critical Pairs in Term Graph Rewriting},
  crossref = {MFCS94},
  pages = {556--566},
  WKloc = {A-0312},
  abstract = {Term graphs represent functional expressions such
		  that common subexpressions can be shared, making
		  expression evaluation more efficient than with
		  strings or trees. $\ldots$},
  annote = {``folding'' for identification}
}

@InProceedings{Plump-1994a,
  author = {Detlef Plump},
  title = {Dschungelersetzung},
  crossref = {Honnef94},
  pages = {104--107}
}

@InCollection{Plump-1999,
  author = {Detlef Plump},
  title = {Term Graph Rewriting},
  crossref = {HBGraTraII},
  pages = {3--61},
  chapter = 1,
  OPTabstract = {},
  WKloc = {B-0046},
  bibliographies = {Coconut}
}

@PhDThesis{Plump-1999a,
  author = {Detlef Plump},
  title = {Computing by Graph Rewriting},
  school = {Universit\"at Bremen, Fachbereich Mathematik und Informatik},
  year = 1999,
  type = {Habilitation thesis}
}

@InProceedings{Plump-2009,
  author =       {Detlef Plump},
  title =        {The Graph Programming Language {GP}},
  DOI = {0.1007/978-3-642-03564-7_6},
  pages =     {99--122},
  booktitle = {Algebraic Informatics, {CAI 2009}},
  lncsbooktitle = {{CAI 2009}},
  year =      2009,
  volume =    5725,
  series =    LNCS,
  abstract = {GP (for Graph Programs) is a rule-based,
    nondeterministic programming language
    for solving graph problems at a high level of abstraction,
    freeing programmers from handling low-level data structures.
    The core of GP consists of four constructs:
    single-step application of a set of conditional graph-transformation rules,
    sequential composition, branching and iteration.
    This paper gives an overview on the GP project.
    We introduce the language by discussing a sequence of
    small programming case studies,
    formally explain conditional rule schemata
    which are the building blocks of programs,
    and present a semantics for GP
    in the style of structural operational semantics.
    A special feature of the semantics is
    how it uses the notion of finitely failing programs
    to define powerful branching and iteration commands.
    We also describe GP’s prototype implementation.}
}

@InProceedings{Plump-Habel-1994,
  author = {Detlef Plump and Annegret Habel},
  title = {Graph Unification and Matching},
  crossref = {GG1994},
  pages = {75--89}
}

@InProceedings{Plump-Steinert-2004,
  title  =    {Towards Graph Programs for Graph Algorithms},
  author =    {Detlef Plump and Sandra Steinert},
  crossref =  {ICGT2004},
  pages =     {128--143},
  DOI =      {10.1007/978-3-540-30203-2_11},
  WKloc =    {doc/pap/BIB},
  abstract =    {Graph programs as introduced by Habel and Plump [8]
    provide a simple yet computationally complete language
    for computing functions and relations on graphs.
    We extend this language such that
    numerical computations on labels can be conveniently expressed.
    Rather than resorting to some kind of attributed graph transformation,
    we introduce conditional rule schemata which are instantiated to
    (conditional) double-pushout rules over ordinary graphs.
    A guiding principle in our language extension
    is syntactic and semantic simplicity.
    As a case study for the use of extended graph programs,
    we present and analyse two versions of Dijkstra's shortest path algorithm.
    The first program consists of just three rule schemata
    and is easily proved to be correct
    but can be exponential in the number of rule applications.
    The second program is a refinement of the first
    which is essentially deterministic
    and uses at most a quadratic number of rule applications.}
}

@InProceedings{Pnueli-Shalev-1991,
  title = {What is in a Step: On the Semantics of Statecharts},
  author = {A. Pnueli and M. Shalev},
  pages = {244--265},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {This paper presents a proposal for the definition of a
		  step in the execution of a statechart. The proposed
		  semantics maintains the {\em synchrony
		  hypothesis\/}, by which the system is infinitely
		  faster than its environment, and can always finish
		  computing its response before the next stimulus
		  arrives. However, it corrects some inconsistencies
		  present in previous definitions, by requiring global
		  consistency of the step.}
}

@Book{Poeschel-Kaluzhnin-1979,
  author = {R. P\"oschel and L. A. Kaluzhnin},
  title = {Funktionen- und Relationenalgebren. Ein Kapitel der
		  diskreten Mathematik},
  note = {Mathematische Monographien, Band 15},
  OPTnote = {MR 81f:03075, Zbl 418.03044.},
  publisher = Deuts,
  address = {Berlin},
  year = 1979,
  pages = 259,
  bibliographies = {RelMiCS}
}

@TechReport{PoetzschHeffter-1992a,
  year = 1992,
  title = {Implementing Higher Level Identification Specifications},
  number = {92-22 R15/22},
  institution = {Technische Universit\"at M\"unchen, Fakult\"at f\"ur
		  Informatik},
  author = {A. Poetzsch-Heffter}
}

@TechReport{PoetzschHeffter-1992b,
  year = 1992,
  title = {Identification as Programming Language Principle},
  number = {92-23 R16/23},
  institution = {Technische Universit\"at M\"unchen, Fakult\"at f\"ur
		  Informatik},
  author = {A. Poetzsch-Heffter}
}

@InProceedings{Poigne-1992,
  author = {A. Poign\'e},
  title = {Identity and Existence, and Types in Algebra --- A
		  Survey of Sorts},
  crossref = {SADT92},
  pages = {53--78},
  note = {invited paper},
  WKloc = {A-0339},
  abstract = {We survey partiality as found in algebra covering
		  the various approaches which have surfaced and are
		  being used in computer science, particularly in the
		  data type community.}
}

@Article{Poizat-1971,
  author = {B. Poizat},
  title = {Th\'eorie de {G}alois des Relations},
  journal = CRPARIS,
  volume = 272,
  year = 1971,
  pages = {645--648},
  bibliographies = {RelMiCS}
}

@TechReport{Poll-Thompson-1999,
  author = {Erik Poll and Simon Thompson},
  title = {The Type System of {Aldor}},
  month = JUL,
  year = 1999,
  pages = 51,
  URL = {http://www.cs.ukc.ac.uk/pubs/1999/874},
  address = {Kent CT2 7NF, UK},
  institution = {Computing Laboratory, University of Kent at Canterbury},
  number = {11-99},
  abstract = {This paper gives a formal description of (at least a part of)
     the type system of Aldor, the extension language of the
     computer algebra system AXIOM. In the process of doing this
     a critique of the design of the system emerges.},
  WKloc = {A-1208, doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@InProceedings{Poll-Thompson-2000,
  author = {Erik Poll and Simon Thompson},
  title = {Integrating Computer Algebra and Reasoning through the Type System of {Aldor}},
  pages = {136-150},
  crossref = {FroCos2000},
  keywords = {computer algebra, reasoning, Axiom, Aldor},
  WKloc = {A-1417},
  URL = {http://www.cs.ukc.ac.uk/pubs/2000/1039}
}

@Article{Poll-Zwanenburg-2001,
  author =       {Erik Poll and Jan Zwanenburg},
  title =        {From Algebras and Coalgebras to Dialgebras},
  journal =      {ENTCS},
  year =         {2001},
  booktitle =       {{CMCS 2001}, Coalgebraic Methods in Computer Science},
  volume =    {44},
  number =    {1},
  pages =     {289--307},
  DOI =     {10.1016/S1571-0661(04)80915-0},
  DOIURL =     {http://dx.doi.org/10.1016/S1571-0661(04)80915-0},
  OPTnote =      {},
  abstract =    {This paper investigates the notion of \emph{dialgebra}, which generalises the notions of algebra and coalgebra. We show that many (co)algebraic notions and results can be generalised to dialgebras, and investigate the essential differences between (co)algebras and arbitrary dialgebras.}
}

@phdthesis{ Pollack-1994,
  author = "Robert Pollack",
  title = "The Theory of {LEGO}: {A} Proof Checker for the Extended Calculus of Constructions",
  year = "1994",
  CiteSeer = {http://citeseer.ist.psu.edu/pollack94theory.html},
  WKloc = {doc/pap/BIB},
  abstract = {LEGO is a computer program for interactive
     typechecking in the Extended Calculus of Constructions and two of
     its subsystems. LEGO also supports the extension of these three
     systems with inductive types. These type systems can be viewed as
     logics, and as meta languages for expressing logics, and LEGO is
     intended to be used for interactively constructing proofs in
     mathematical theories presented in these logics. I have developed
     LEGO over six years, starting from an implementation of the
     Calculus of Constructions by Gerard Huet. LEGO has been used for
     problems at the limits of our abilities to do formal mathematics.

     In this thesis I explain some aspects of the meta-theory of
     LEGO's type systems leading to a machine-checked proof that
     typechecking is decidable for all three type theories supported
     by LEGO, and to a verified algorithm for deciding their typing
     judgements, assuming only that they are normalizing. In order to
     do this, the theory of Pure Type Systems (PTS) is extended and
     formalized in LEGO. This extended example of a formally developed
     body of mathematics is described, both for its main theorems, and
     as a case study in formal mathematics. In many examples, I
     compare formal definitions and theorems with their informal
     counterparts, and with various alternative approaches, to study
     the meaning and use of mathematical language, and suggest
     clarifications in the informal usage.

     Having outlined a formal development far too large to be surveyed
     in detail by a human reader, I close with some thoughts on how
     the human mathematician's state of understanding and belief might
     be affected by posessing such a thing.}
}

@InProceedings{Pollack-1998,
  AUTHOR = {Robert Pollack},
  TITLE = {How to Believe a Machine-Checked Proof},
  BOOKTITLE = {Twenty Five Years of Constructive Type Theory},
  EDITOR = {G. Sambin and J. Smith},
  YEAR = 1998,
  PUBLISHER = {Oxford Univ. Press},
  PSURL = {http://homepages.inf.ed.ac.uk/rpollack/export/believing.ps.gz}
}

@Article{Pomykala-1988,
  author = {Pomykala, J.A.},
  title = {On definability in the nondeterministic information system},
  journal = BUPOL,
  year = 1988,
  volume = 36,
  pages = {193--210},
  bibliographies = {RelMiCS}
}

@Booklet{Poncova,
  author = {Poncova},
  title = {Groupoids with Multioperators},
  note = {see Zbl 393.08001},
  bibliographies = {RelMiCS}
}

@Article{Ponder-McGeer-Ng-1988,
  author = {Carl Ponder and Patrick McGeer and Antony Ng},
  title = {Are Applicative Languages Inefficient?},
  journal = notices,
  year = 1988,
  volume = 23,
  number = 6,
  pages = {x}
}

@Proceedings{Ponse-deRijke-Venema-1995,
  editor = {Ponse, A. and Maarten de Rijke and Yde Venema},
  address = {Stanford},
  volume = 53,
  publisher = CSLI_P,
  series = {CSLI Lecture Notes},
  title = {Modal Logic and Process Algebra},
  year = 1995,
  bibliographies = {RelMiCS}
}

@Misc{Pope-2002,
  author = {Bernie Pope},
  title = {Hatchet: A Type Checking and Inference Tool for {Haskell 98}, Version 0.1},
  year = 2002,
  WKloc = {A-1434}
}

@Misc{Popplestone-19XX,
  author = {Robin Popplestone},
  title = {A Typed Operational Semantics Based on Grammatical Characterisation of an Abstract Machine},
  year = {19??},
  WKloc = {A-0861}
}

@Article{Posey-1977,
  author = {Alma E. Posey},
  title = {On Difunctional and Circular Relations},
  journal = EPSILON,
  volume = 6,
  number = 7,
  year = 1977,
  pages = {394--399},
  bibliographies = {RelMiCS}
}

@InProceedings{Poskitt-Plump-2010,
  author =       {Christopher M. Poskitt and Detlef Plump},
  title =        {A {Hoare} Calculus for Graph Programs},
  crossref =  {ICGT2010},
  pages =     {139--154},
  DOI =      {10.1007/978-3-642-15928-2_10},
  abstract =    {We present Hoare-style axiom schemata and inference rules for verifying the partial correctness of programs in the graph programming language GP. The pre- and postconditions of this calculus are the nested conditions of Habel, Pennemann and Rensink, extended with expressions for labels in order to deal with GP’s conditional rule schemata and infinite label alphabet. We show that the proof rules are sound with respect to GP’s operational semantics.}
}

@Book{PostScriptReference2,
  author = {{Adobe Systems Incorporated}},
  title = {PostScript Langange Reference Manual},
  publisher = {Addison-Wesley},
  year = 1990,
  key = {PostScript},
  edition = {2nd},
  note = {The ``red book''.},
  McMaster = {QA 76.73 .P67P67 1990}
}

@Book{Potter-Sinclair-Till-1996,
  author = {B. F. Potter and J. E. Sinclair and D. Till},
  title = {An Introduction to Formal Specification and {Z}},
  publisher = {Prentice Hall International Series in Computer Science},
  year = 1996,
  edition = {2nd},
  length = 434,
  ISBN = {0-13-242207-7},
  bibliographies = {RelMiCS},
  annote = {Contents: Formal specification in the context of software
                  engineering; An informal introduction to logic and set
                  theory; A first specification; The Z notation: the
                  mathematical language, relations and functions, schemas and
                  specification structure; A first specification; Formal
                  reasoning; From specification to program: data and
                  operation refinement, operation decomposition; From theory
                  to practice.},
  UniBwM = {INF560/X2512}
}

@InProceedings{Pottier-2000,
  author = {Fran{\c{c}}ois Pottier},
  title = {A 3-Part Type Inference Engine},
  crossref = {ESOP2000},
  pages = {320--335},
  authorsAddress = {Francois.Pottier@inria.fr},
  URL = {http://link.springer.de/link/service/series/0558/bibs/1782/17820320.htm},
  WKloc = {doc/pap/BIB},
  abstract = {Extending a subtyping-constraint-based type inference framework
              with conditional constraints and rows yields a powerful
              type inference engine. We illustrate this claim by proposing
              solutions to three delicate type inference problems:
              ``accurate'' pattern matchings, record concatenation, and
              ``dynamic'' messages. Until now, known solutions required
              significantly different techniques; our theoretical contribution
              is in using only a single (and simple) set of tools.
              On the practical side, this allows all three problems
              to benefit from a common set of
              constraint simplification techniques,
              leading to efficient solutions.}
}

@InProceedings{Pouillard-Pottier-2010,
 author = {Pouillard, Nicolas and Pottier, Fran\c{c}ois},
 title = {A fresh look at programming with names and binders},
 crossref = {ICFP2010},
 pages = {217--228},
 numpages = {12},
 WKloc = {A-1748 (extended), doc/pap/BIB},
 DOIURL = {http://doi.acm.org/10.1145/1863543.1863575},
 DOI = {10.1145/1863543.1863575},
 keywords = {binders, higher-order abstract syntax, meta-programming, name abstraction, names},
 URL = {http://nicolaspouillard.fr/publis/pouillard-pottier-fresh-look-agda-2010/},
 abstract = {A wide range of computer programs,
     including compilers and theorem provers,
     manipulate data structures that involve names and binding.
     However, the design of programming idioms
     which allow performing these manipulations in a safe and natural style
     has, to a large extent, remained elusive.

     In this paper, we present a novel approach to the problem.
     Our proposal can be viewed either as a programming language design
     or as a library: in fact, it is currently implemented within Agda.
     It provides a safe and expressive means
     of programming with names and binders.
     It is abstract enough to support multiple concrete implementations:
     we present one in nominal style and one in de Bruijn style.
     We use logical relations to prove that
     ``well-typed programs do not mix names with different scope''.
     We exhibit an adequate encoding
     of Pitts-style nominal terms into our system.}
}

@inproceedings{Pouillard-2011,
 author = {Pouillard, Nicolas},
 title = {Nameless, painless},
 booktitleLONG = {{Proceedings of the 16th ACM SIGPLAN international conference on Functional programming}},
 booktitle = {{Proc.\null{} 16th International Conf.\null{} Functional Programming, ICFP 2011}},
 series = {ICFP '11},
 year = {2011},
 isbn = {978-1-4503-0865-6},
 location = {Tokyo, Japan},
 pages = {320--332},
 numpages = {13},
 DOIURL = {http://doi.acm.org/10.1145/2034773.2034817},
 doi = {10.1145/2034773.2034817},
 acmid = {2034817},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {binders, de bruijn indices, meta-programming, name abstraction, names},
}

@InProceedings{Power-1989,
  author = {A. J. Power},
  title = {An Abstract Formulation for Rewrite Systems},
  pages = {300--312},
  ISBN = {3-540-51662-X},
  editor = {D. H. Pitt and D. E. Rydeheard and P. Dybjer and A. M.
                 Pitts and A. Poign{\'e}},
  booktitle = {Proceedings of the Conference on Category Theory and
                 Computer Science},
  month = sep,
  series = {LNCS},
  volume = 389,
  publisher = {Springer},
  address = {Berlin},
  year = 1989
}

@Article{Power-1991,
  author = {A. J. Power},
  title = {A General Coherence Result},
  journal = {Journal of Pure and Applied Algebra},
  volume = 57,
  pages = {165--173},
  year = 1991
}

@Article{Power-1995,
  title = {Why Tricategories?},
  author = {A. J. Power},
  pages = {251--262},
  journal = {Information and Computation},
  month = {1~} # aug,
  year = 1995,
  volume = 120,
  number = 2
}

@Article{Power-2001,
 author = {John Power},
  title = 	 {Models for the computational $\lambda$-calculus},
  journal = 	 ENTCS,
  year = 	 {2001},
  volume = 	 {40},
  pages = 	 {288-301},
  DOI = 	 {http://dx.doi.org/10.1016/S1571-0661(05)80056-8},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {We consider several different sound and complete
                  classes of models for the computational
                  $\lambda$-calculus, explain the definitions, and
                  outline why one might be interested in the various
                  classes. We first consider the class of closed
                  $?$-categories, a natural and direct generalisation
                  of the notion of cartesian closed category. We then
                  consider closed $\lambda$-categories, which are
                  based upon indexed categories and which are closely
                  related to modern compiling technology. Finally, we
                  consider the class of cartesian closed categories
                  $?$ together with a $\lambda$-enriched monad. The
                  latter class has the most developed abstract theory,
                  which one can adopt and by which one can dispense
                  with coherence details in the spirit of Mac Lane
                  involving strengths.}
}

@Article{Power-2002,
 author = {John Power},
 title = {Premonoidal Categories as Categories with Algebraic Structure},
 journal = TCS,
 volume = {278},
 number = {1--2},
 year = {2002},
 issn = {0304-3975},
 pages = {303--321},
 WKloc = {A-1565, doc/pap/BIB},
 doi = {http://dx.doi.org/10.1016/S0304-3975(00)00340-6}
}

@Article{Power-Robinson-1997,
  author = {A. J. Power and E. P. Robinson},
  title = {Premonoidal categories and notions of computation},
  journal = {Mathematical Structures in Computer Science},
  year = 1997,
  volume = 7,
  number = 5,
  pages = {453--468},
  unibwm =	 {INF/Z},
  WKloc = {A-0877, doc/pap/BIB}
}

@InProceedings{Power-Thielecke-1999,
  author = {John Power and Hayo Thielecke},
  title = {Closed Freyd- and $\kappa$-categories},
  crossref = {ICALP1999},
  pages = {625--634},
  abstract = {We give two classes of sound and complete models for the
      computational $\lambda$-calculus, or $\lambda_c$-calculus. For the
      first, we generalise the notion of cartesian closed category to that
      of closed Freyd-category. For the second, we generalise simply
      indexed categories. The former gives a direct semantics for the
      computational $\lambda$-calculus. The latter corresponds to an
      idealisation of stack-based intermediate languages used in some
      approaches to compiling.},
  WKloc = {A-0811}
}

@Article{Power-Watanabe-1998,
  author = {John Power and Hiroshi Watanabe},
  title = {An axiomatics for categories of coalgebras},
  journal = {Electric Notes in Theoretical Computer Science},
  year = 1999,
  volume = 11,
  note = {Proceedings of the First Workshop on Coalgebraic Methods in Computer Science '98,
     28-29 March 1998 in Lisbon},
  OPTunibwm = {INF/Z},
  URL = {http://www.etl.go.jp/~hirowata/publication/src/cmcs98.ps,
         http://www.elsevier.nl/locate/entcs/volume11.html},
  WKloc = {A-0932},
  bibliographies = {ETL},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Power-Watanabe-1999,
  author = {John Power and Hiroshi Watanabe},
  title = {Distributivity for a monad and a comonad},
  journal = {Electric Notes in Theoretical Computer Science},
  year = 1999,
  volume = 19,
  note = {Proceedings of the second Workshop on Coalgebraic Methods in Computer Science '99,
     20-21 March 1999 in Amsterdam},
  OPTunibwm = {INF/Z},
  URL = {http://www.etl.go.jp/~hirowata/publication/src/cmcs99.ps,
         http://www.elsevier.nl/locate/entcs/volume19.html},
  WKloc = {A-0931},
  bibliographies = {ETL},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@TechReport{Power-Watanabe-1999a,
  author = {John Power and Hiroshi Watanabe},
  title = {Combining a monad and a comonad},
  year = 1999,
  institution = {ETL},
  OPTtype = {ETL Technical Report},
  number = {TR-99-33},
  OPTaddress = {},
  URL = {http://www.etl.go.jp/~hirowata/publication/src/monadcomonad.ps},
  WKloc = {A-0929},
  bibliographies = {ETL}
}

@Article{Power-Wells-1992,
  title = {A formalism for the specification of essentially
                 algebraic structures in 2-categories},
  author = {A. J. Power and Charles Wells},
  pages = {1--28},
  journal = {Mathematical Structures in Computer Science},
  month = mar,
  year = 1992,
  volume = 2,
  number = 1
}

@InProceedings{Prasetya-1993,
  author = {ISWB Prasetya},
  title = {On the Style of Mechanical Proving},
  crossref = {HUG93},
  pages = {475--488},
  authorsAddress = {Utrecht: {\tt wishnu@cs.ruu.nl}},
  WKloc = {A-0320}
}

@Article{PrattTW-1971,
  author =       {Terrence W. Pratt},
  title =        {Pair Grammars, Graph Languages and String-to-Graph Translations},
  journal =      {Journal of Computer and System Sciences},
  McMaster = {Thode Periodicals Range 066C, QA 76.5 .J7},
  year =         1971,
  volume =    5,
  number =    6,
  pages =     {560--595},
  DOIURL =     {http://dx.doi.org/10.1016/S0022-0000(71)80016-8},
  abstract =    {Translation between string and graph representations of programs and data may be formally defined by means of pair grammars. A pair grammar is composed of a pair of grammars whose rules and nonterminals are paired. The pair grammar defines a correspondence between elements of the languages defined by the two grammars. This correspondence may be viewed as a definition of the translation of the elements of one language into the elements of the other. Of particular interest is the case in which the first language is a set of strings and the second is a set of directed graphs with labeled arcs and nodes.

Preliminary to the definition of pair grammars, a class of graph grammars are defined which are a generalization of ordinary context-free grammars. A graph grammar defines a language composed of a set of directed graphs. Pair grammars are constructed from pairs of graph grammars. Each unambiguous pair grammar defines a reversible function mapping one graph language onto another. Special cases of interest include string-to-graph, graph-to-string, and string-to-string mappings. In the general case a pair grammar defines a transformation on a set of graphs.

    Two extensions to the elementary pair grammars allow representation of hierarchies of graphs and constructs such as labels and go to statements. Examples are given of the translation of a major subset of Algol into flowchart graphs and the translation of Lisp S-expressions into list structure graphs with structured atoms.}
}

@InProceedings{PrattTW-1979,
  author = {Terrence W. Pratt},
  title = {Definition of Programming Language Semantics Using Grammars for Hierarchical Graphs},
  authorsaffiliation = {University of Virginia},
  crossref =  {GG1978},
  pages =     {389--400},
  DOI =      {10.1007/BFb0025735},
  URL =    {http://www.springerlink.com/content/d832506331820712/},
  abstract =    {Directed graphs are a useful formal structure
    in the modeling and definition of programming language semantics.
    Graph grammars are a valuable tool for defining sets of directed graphs
    in this setting.
    The goal of this paper is to describe this application of graph grammars
    and to provide the underlying formal definitions
    for the particular graph structures and graph grammars used.
    Translation of programming languages into graph structures
    may also be defined using
    graph grammars paired with ordinary BNF (context-free) grammars.
    This closely related application is also described.

    The graph grammar form used here
    and the theory underlying its application
    were developed about 1970 (see \cite{PrattTW-1971}).
    Graph grammars have been used regularly since then
    in the definition of a number of programming languages,
    including ALGOL 60, PASCAL, LISP, and HAL/S.},
  annote = {ALGOL 60 was developed ``since then''???},
  keywords = {precursor to multilevel graphs}
}

@InProceedings{Pratt-1976,
  author = {Pratt, V.R.},
  title = {Semantical considerations on {Floyd-Hoare} logic.},
  booktitle = {Proc.\null{} {{$17^{th}$} Annual IEEE Sympos.\null{} on
      Foundations of Computer Science}},
  year = 1976,
  month = OCT,
  pages = {109--121},
  bibliographies = {RelMiCS}
}

@InProceedings{Pratt-1979,
  author = {Pratt, Vaughan},
  booktitle = {Proc.\null{} of the {{$20^{th}$} IEEE Sympos.\null{} on
      Foundations of Computer Science}},
  pages = {115--122},
  title = {Models of program logics},
  year = 1979,
  bibliographies = {RelMiCS}
}

@Article{Pratt-1986,
  author = {Vaughan Pratt},
  title = {Modelling concurrency with partial orders},
  journal = IJPP,
  year = 1986,
  volume = 15,
  number = 1,
  pages = {33--71},
  month = FEB,
  URL = {http://boole.stanford.edu/pub/ijpp.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1986\_ijpp.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Concurrency has been expressed variously in terms of formal
      languages (typically via the shuffle operator), partial orders, and
      temporal logic, inter alia. In this paper we extract from these three
      approaches a single hybrid approach having a rich language that mixes
      algebra and logic and having a natural class of models of concurrent
      processes. The heart of the approach is a notion of partial string
      derived from the view of a string as a linearly ordered multiset by
      relaxing the linearity constraint, thereby permitting partially
      ordered multisets or pomsets. Just as sets of strings form languages,
      so do sets of pomsets form processes. We introduce a number of
      operations useful for specifying concurrent processes and demonstrate
      their utility on some basic examples. Although none of the operations
      is particularly oriented to nets it is nevertheless possible to use
      them to express processes constructed as a net of subprocesses, and
      more generally as a system consisting of components. The general
      benefits of the approach are that it is conceptually straightforward,
      involves fewer artificial constructs than many competing models of
      concurrency, yet is applicable to a considerably wider range of types
      of systems, including systems with buses and ethernets, analog
      systems, and real-time systems.}
}

@Misc{Pratt-1989,
  author = {Vaughan Pratt},
  title = {Enriched Categories and the {Floyd-Warshall} Connection},
  month = APR,
  year = 1989,
  WKloc = {A-1262},
  abstract = {We give a correspondence between enriched categories and the
      Gauss-Kleene-Floyd-Warshall connection familiar to computer
      scientists. This correspondence shows this generalization of
      categories to be a close cousin to the generalization of transitive
      closure algorithms. Via this connection we may bring categorical and
      2-categorical constructions into an active but algebraically
      impoverished arena presently served only by semiring constructions.
      We illustrate these techniques by applying then to Birkhoff's poset
      arithmetic, interpretable as an algebra of ``true concurrency''.}
}

@InProceedings{Pratt-1990,
  author = {Vaughan Pratt},
  title = {Dynamic algebras as a well-behaved fragment of
                 relation algebras},
  pages = {77--110},
  crossref = {ALUACS1988},
  WKloc = {A-0940},
  bibliographies = {RelMiCS}
}

@TechReport{Pratt-1990a,
  type = {Technical Report},
  number = {CS-TR-90-1309},
  title = {Dynamic Algebras as a well-behaved fragment of
                 Relational Algebras},
  month = mar,
  notes = {[Adminitrivia V1/RAM/19940914]},
  pages = 32,
  year = 1990,
  bibdate = {September 14, 1994},
  author = {Vaughan Pratt},
  abstract = {The varieties RA of relation algebras and DA of
                 dynamic algebras are similar with regard to
                 definitional capacity, admitting essentially the same
                 equational definitions of converse and star. They
                 differ with regard to completeness and decidability.
                 The RA definitions that are incomplete with respect to
                 representable relation algebras, when expressed in
                 their DA form are complete with respect to
                 representable dynamic algebras. Moreover, whereas the
                 theory of RA is undecidable, that of DA is decidable in
                 exponential time. These results follow from
                 representability of the free intensional dynamic
                 algebras.},
  institution = {Stanford University, Department of Computer Science},
  bibliographies = {RelMiCS}
}

@InProceedings{Pratt-1990b,
  author = {Vaughan Pratt},
  title = {Action Logic and Pure Induction},
  pages = {97--120},
  crossref = {JELIA1990},
  bibliographies = {RelMiCS}
}

@InProceedings{Pratt-1991,
  author = {Vaughan Pratt},
  title = {Modeling Concurrency with Geometry},
  pages = {311--322},
  abstract = {The phenomena of branching time and true or noninterleaving
             concurrency find their respective homes in automata and
             schedules. But these two models of computation are formally
             equivalent via Birkhoff duality, an equivalence we expound on
             here in tutorial detail. So why should these phenomena prefer
             one over the other? We identify dimension as the culprit:
             1-dimensional automata are skeletons permitting only
             interleaving concurrency, whereas {\em true $n$-fold
             concurrency resides in transitions of dimension $n$}. The truly
             concurrent automaton dual to a schedule is not a skeletal
             distributive lattice but a solid one! We introduce true
             nondeterminism and define it as monoidal homotopy; from this
             perspective nondeterminism in ordinary automata arises from
             forking and joining creating nontrivial homotopy. The automaton
             dual to a poset schedule is simply connected whereas that dual
             to an event structure schedule need not be, according to
             monoidal homotopy though not to group homotopy. We conclude
             with a formal definition of higher dimensional automaton as
             $n$-complex or $n$-category, whose two essential axioms are
             associativity of concatenation within dimension and an
             interchange principle between dimensions.},
  crossref = {POPL1991},
  WKloc = {A-0176},
  URL = {http://boole.stanford.edu/pub/cg.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1991\_cg.ps.gz},
  bibliographies = {RelMiCS}
}

@InProceedings{Pratt-1992,
  title = {Origins of the Calculus of Binary Relations},
  author = {Vaughan Pratt},
  pages = {248--254},
  booktitle = {Proceedings, Seventh Annual {IEEE} Symposium on Logic
                 in Computer Science},
  year = 1992,
  month = {22--25 } # jun,
  address = {Santa Cruz, California},
  organization = {IEEE Computer Society Press},
  bibliographies = {RelMiCS},
  URL = {http://boole.stanford.edu/pub/ocbr.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1992\_ocbr.ps.gz},
  abstract = {The calculus of binary relations was introduced by De Morgan
      in 1860, and was subsequently greatly developed by Peirce and
      Schroeder. Half a century later Tarski, J'onsson, Lyndon, and Monk
      further developed the calculus from the perspective of modern model
      theory.}
}

@InProceedings{Pratt-1992a,
  author = {Pratt, V.R.},
  title = {Arithmetic + Logic + Geometry = Concurrency},
  booktitle = {Proc. First Latin American Symposium on Theoretical
  	     Informatics, {Sao Paulo, Brazil}},
  series = LNCS,
  volume = 583,
  pages = {430--447},
  publisher = {Springer-Verlag},
  month = APR,
  year = 1992,
  URL = {http://boole.stanford.edu/pub/algecon.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1992\_algecon.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We relate the arithmetic of concurrent schedules to the
      higher-dimensional cellular geometry of concurrent automata using the
      logic of their Birkhoff-Stone duality. This collects and unifies
      ideas from several of the author's previous papers.}
}

@InProceedings{Pratt-1992e,
  author = {Pratt, V.R.},
  title = {The Duality of Time and Information},
  booktitle = {Proc. of CONCUR'92, Stonybrook, New York},
  pages = {237--253},
  publisher = Springer,
  month = AUG,
  year = 1992,
  URL = {http://boole.stanford.edu/pub/dti.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1992\_dti.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {The states of a computing system bear information and change
      time, while its events bear time and change information. We develop a
      primitive algebraic model of this duality of time and information for
      rigid local computation, or straightline code, in the absence of
      choice and concurrency, where time and information are linearly
      ordered. This shows the duality of computation to be more fundamental
      than the logic of computation for which choice is disjunction and
      concurrency conjunction.

      To accommodate flexible distributed computing systems we then bring
      in choice and concurrency and pass to partially ordered time and
      information, the formal basis for this extension being Birkhoff-Stone
      dualtiy. A degree of freedom in how this is done permits a perfectly
      symmetric logic of computation amounting to Girard's full linear
      logic, which we view as the natural logic of computation when equal
      importance is attached to choice and concurrency.

      We conclude with an assessment of the prospects for extending the
      duality to other organizations of time and information besides
      partial orders in order to accommodate real time, nonmonotonic logic,
      and automata that can forget, and speculate on the philosophical
      significance of the duality.}
}

@InProceedings{Pratt-1993,
  title = {The Second Calculus of Binary Relations},
  author = {Vaughan R. Pratt},
  editor = {Andrzej M. Borzyszkowski and Stefan Sokolowski},
  booktitle = {Mathematical Foundations of Computer Science 1993,
                 18th International Symposium},
  address = {Gda{\'{n}}sk, Poland},
  month = {30~} # aug # {-- 3~} # sep,
  year = 1993,
  series = LNCS,
  volume = 711,
  publisher = Springer,
  pages = {142--155},
  bibliographies = {RelMiCS},
  URL = {http://boole.stanford.edu/pub/scbr.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1993\_scbr.ps.gz},
  abstract = {We view the Chu space interpretation of linear logic
   as an alternative interpretation of the language of the Peirce calculus
   of binary relations. Chu spaces amount to $K$-valued binary relations,
   which for $K=2^n$ we show generalize $n$-ary relational structures.
   We also exhibit a four-stage unique factorization system for Chu transforms
   that illuminates their operation.}
}

@InProceedings{Pratt-1993a,
  author = {Pratt, V.R.},
  title = {Linear Logic for Generalized Quantum Mechanics},
  booktitle = {Proc. Workshop on Physics and Computation (PhysComp'92, Dallas)},
  publisher = {{IEEE}},
  pages = {166--180},
  year = 1993,
  URL = {http://boole.stanford.edu/pub/ql.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1993\_ql.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Quantum logic is static, describing automata having
      uncertain states but no state transitions and no Heisenberg
      uncertainty tradeoff. We cast Girard's linear logic in the role of a
      dynamic quantum logic, regarded as an extension of quantum logic with
      time nonstandardly interpreted over a domain of linear automata and
      their dual linear schedules. In this extension the uncertainty
      tradeoff emerges via the ``structure veil.'' When VLSI shrinks to
      where quantum effects are felt, their computer-aided design systems
      may benefit from such logics of computational behavior having a
      strong connection to quantum mechanics.}
}

@TechReport{Pratt-1994,
  author = {Pratt, V.R.},
  title = {Chu Spaces: Complementarity and Uncertainty in Rational Mechanics},
  note = {Course notes, TEMPUS summer school, 35pp},
  address = {Budapest},
  year = 1994,
  URL = {http://boole.stanford.edu/pub/bud.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1994\_bud.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Notes for five lectures given at the Tempus summer school,
      Budapest, July 1994. Topics covered: Introduction to Chu spaces.
      Behavior: from event structures to rational mechanics. Algebra: from
      linear logic to process algebra. Relational structures. Heisenberg
      uncertainty in Chu spaces.}
}

@InProceedings{Pratt1994a,
  author = {Pratt, V.R.},
  title = {Chu Spaces: Automata with Quantum Aspects},
  booktitle = {Proc. Workshop on Physics and Computation
  	    (PhysComp'94, Dallas)},
  pages = {186--195},
  publisher = {{IEEE}},
  year = 1994,
  URL = {http://boole.stanford.edu/pub/ph94.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1994\_ph.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Chu spaces are a recently developed model of concurrent
      computation extending automata theory to express branching time and
      true concurrency. They exhibit in a primitive form the quantum
      mechanical phenomena of complementarity and uncertainty. The
      complementarity arises as the duality of information and time,
      automata and schedules, and states and events. Uncertainty arises
      when we define a measurement to be a morphism and notice that
      increasing structure in the observed object reduces clarity of
      observation. For Chu spaces this uncertainty can be calculated in an
      attractively simple way directly from its dimensions.}
}

@Misc{Pratt-1994b,
  author = {Vaughan Pratt},
  title = {Shorter proof of universality of {Chu} spaces},
  note = {Stanford University},
  month = AUG,
  year = 1994,
  URL = {http://boole.stanford.edu/pub/uni.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1994\_uni.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We give a shorter proof of the result in section 5 of our
      MFPS'93 paper \cite{Pratt-1993}, that every $k$-ary relational
      structure is realizable as a Chu space.}
}

@InProceedings{Pratt-1994c,
  author = {Pratt, V.R.},
  title = {Time and Information in Sequential and Concurrent Computation},
  booktitle = {Proc. Theory and Practice of Parallel Programming
  	     {(TPPP'94) Sendai, Japan}},
  month = NOV,
  pages = {1--24},
  year = 1994,
  URL = {http://boole.stanford.edu/pub/tppp.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1994\_tppp.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Time can be understood as dual to information in extant
      models of both sequential and concurrent computation. The basis for
      this duality is phase space, coordinatized by time and information,
      whose axes are oriented respectively horizontally and vertically. We
      fit various basic phenomena of computation, and of behavior in
      general, to the phase space perspective. The extant two-dimensional
      logics of sequential behavior, the van Glabbeek map of branching time
      and true concurrency, event-state duality and schedule-automaton
      duality, and Chu spaces, all fit the phase space perspective well, in
      every case confirming our choice of orientation.}
}

@Misc{Pratt-1994d,
  author = {Vaughan Pratt},
  title = {Chu realizes all small concrete categories},
  note = {Stanford University},
  month = JUL,
  year = 1994,
  URL = {http://boole.stanford.edu/pub/embed.ps.gz},
  WKloc = {A-1017, doc/pap/BIB/Pratt-1994\_embed.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {The category Chu is concretely universal for much of
   concrete mathematics; in particular it concretely represents
   or realizes all categories of relational structures and their
   homomorphisms, as well as all topological such.
   This note extends these results to all small concrete categories,
   equivalently all small subcategories of $Set$. The category $C$ is
   realized in $Chu(Set,K)$ where $K$ is the disjoint union
   of the underlying sets of objects of $C$. Each object
   is realized as the normal Chu space $(A,X)$ where $X$ consists of all
   functions from $A$ in $C$ astricted to $K$. }
}

@InProceedings{Pratt-1995a,
  author = {Pratt, V.R.},
  title = {Rational Mechanics and Natural Mathematics},
  booktitle = {TAPSOFT'95},
  series = LNCS,
  volume = 915,
  pages = {108--122},
  publisher = Springer,
  year = 1995,
  URL = {http://boole.stanford.edu/pub/ratmech.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1995\_ratmech.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Chu spaces have found applications in computer science,
      mathematics, and physics. They enjoy a useful categorical duality
      analogous to that of lattice theory and projective geometry. As
      natural mathematics Chu spaces borrow ideas from the natural
      sciences, particularly physics, while as rational mechanics they cast
      Hamiltonian mechanics in terms of the interaction of body and mind.

      This paper addresses the chief stumbling block for Descartes'
      17th-century philosophy of mind-body dualism, how can the
      fundamentally dissimilar mental and physical planes causally interact
      with each other? We apply Cartesian logic to reject not only divine
      intervention, preordained synchronization, and the eventual mass
      retreat to monism, but also an assumption Descartes himself somehow
      neglected to reject, that causal interaction within these planes is
      an easier problem than between. We use Chu spaces and residuation to
      derive all causal interaction, both between and within the two
      planes, from a uniform and algebraically rich theory of between-plane
      interaction alone. Lifting the two-valued Boolean logic of binary
      relations to the complex-valued fuzzy logic of quantum mechanics
      transforms residuation into a natural generalization of the inner
      product operation of a Hilbert space and demonstrates that this
      account of causal interaction is of essentially the same form as the
      Heisenberg-Schr\"odinger quantum-mechanical solution to analogous
      problems of causal interaction in physics.}
}

@InProceedings{Pratt-1995c,
  author = {Pratt, V.R.},
  title = {The {Stone} Gamut: A Coordinatization of Mathematics},
  booktitle = {Logic in Computer Science},
  pages = {444--454},
  month = JUN,
  publisher = {IEEE Computer Society},
  year = 1995,
  URL = {http://boole.stanford.edu/pub/gamut.ps.gz},
  WKloc = {A-1015, doc/pap/BIB/Pratt-1995\_gamut.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We give a uniform representation of the objects of
      mathematical practice as Chu spaces, forming a concrete self-dual
      bicomplete closed category and hence a constructive model of linear
      logic. This representation distributes mathematics over a
      two-dimensional space we call the Stone gamut. The Stone gamut is
      coordinatized horizontally by coherence, ranging {from} -1 for sets
      to 1 for complete atomic Boolean algebras (CABA's), and vertically by
      complexity of language. Complexity 0 contains only sets, CABA's, and
      the inconsistent empty set. Complexity 1 admits noninteracting
      set-CABA pairs. The entire Stone duality menagerie of partial
      distributive lattices enters at complexity 2. Groups, rings, fields,
      graphs, and categories have all entered by level 16, and every
      category of relational structures and their homomorphisms eventually
      appears. The key is the identification of continuous functions and
      homomorphisms, which puts Stone-Pontrjagin duality on a uniform basis
      by merging algebra and topology into a simple common framework.}
}

@InCollection{Pratt-1995d,
  author = {Pratt, V.R.},
  title = {Chu Spaces and their Interpretation as Concurrent Objects},
  booktitle = {Computer Science Today: Recent Trends and
  	 Developments},
  series = LNCS,
  volume = 1000,
  pages = {392--405},
  editor = {van Leeuwen, J.},
  publisher = Springer,
  year = 1995,
  URL = {http://boole.stanford.edu/pub/chuconc.ps.gz},
  WKloc = {A-1020, doc/pap/BIB/Pratt-1995\_chuconc.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {A Chu space is a binary relation =| from a set A to an
      antiset X defined as a set which transforms via converse functions.
      Chu spaces admit a great many interpretations by virtue of realizing
      all small concrete categories and most large ones arising in
      mathematical and computational practice. Of particular interest for
      computer science is their interpretation as computational processes,
      which takes A to be a schedule of events distributed in time, X to be
      an automaton of states forming an information system in the sense of
      Scott, and the pairs (a,x) in the =| relation to be the individual
      transcriptions of the making of history. The traditional homogeneous
      binary relations of transition on X and precedence on A are recovered
      as respectively the right and left residuals of the heterogeneous
      binary relation =| with itself. The natural algebra of Chu spaces is
      that of linear logic, made a process algebra by the process
      interpretation.}
}

@InProceedings{Pratt-1996_llcocl,
  author = {Pratt, V.R.},
  title = {Linear Logic complements Classical Logic},
  booktitle = {Preliminary proceedings, Linear Logic '96, Tokyo},
  year = 1996,
  URL = {http://boole.stanford.edu/pub/llcocl.ps.gz},
  WKloc = {A-1021, doc/pap/BIB/Pratt-1996\_llcocl.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {Classical logic enforces the separation of individuals and
      predicates, linear logic draws them together via interaction; these
      are not right-or-wrong alternatives but dual or complementary logics.
      Linear logic is an incomplete realization of this duality. While its
      completion is not essential for the development and maintenance of
      logic, it is crucial for its application. We outline the
      ``four-square'' program for completing the connection, whose corners
      are set, function, number, and arithmetic, and define ordinal Set, a
      bicomplete \emph{equational} topos, meaning its canonical
      isomorphisms are identities, including associativity of product.}
}

@InProceedings{Pratt-1997a,
  author = {Pratt, V.R.},
  title = {Towards Full Completeness for the Linear Logic of Chu spaces},
  booktitle = {Proc. Math. Foundations of Programming Semantics (MFPS'97, Pittsburgh)},
  series = {ENTCS (Electronic Notes of Theoretical Computer Science)},
  year = 1997,
  URL = {http://boole.stanford.edu/pub/fcllchu.ps.gz},
  WKloc = {doc/pap/BIB/Pratt-1997\_fcllchu.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We prove full completeness for a fragment of the linear
      logic of the self-dual monoidal category of Chu spaces over 2, namely
      that the proofs between semisimple (conjunctive normal form) formulas
      of multiplicative linear logic without constants having two
      occurrences of each variable are in bijection with the dinatural
      transformations between the corresponding functors. The proof assigns
      to variables domains having at most four elements, demonstrating a
      uniform finite model property for this fragment. We define a notion
      of proof function analogous to the notion of truth function,
      determining a transformation between functors, and show that the
      transformation denoted by a proof net is dinatural if and only if the
      proof net is sound, namely acyclic and connected. Proof functions are
      of independent interest as a 2-valued model of MLL with MIX.}
}

@InCollection{Pratt-1997b,
  author = {Pratt, V.R.},
  title = {Chu spaces from the representational viewpoint},
  booktitle = {Parikh Festschrift},
  year = 1997,
  URL = {http://boole.stanford.edu/pub/parikh.ps.gz},
  WKloc = {A-1018, doc/pap/BIB/Pratt-1997\_parikh.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We give an elementary introduction to Chu spaces. The
      perspective taken views their elements as represented by words of a
      fixed length over some alphabet. This perspective dualizes the
      alternative view of Chu spaces as generalized topological spaces, and
      has the advantage of substituting the intuitions of formal language
      theory for those of topology.}
}

@InProceedings{Pratt-1997c,
  author = {Pratt, V.R.},
  title = {Types as Processes, via {Chu} spaces},
  booktitle = {EXPRESS'97 Proceedings},
  year = 1997,
  URL = {http://boole.stanford.edu/pub/tap.ps.gz},
  WKloc = {A-1016, doc/pap/BIB/Pratt-1997\_tap.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {We match up types and processes by putting values in
      correspondence with events, coproduct with (noninteracting) parallel
      composition, and tensor product with orthocurrence. We then bring
      types and processes into closer correspondence by broadening and
      unifying the semantics of both using Chu spaces and their
      transformational logic. Beyond this point the connection appears to
      break down; we pose the question of whether the failures of the
      corrrespondence are intrinsic or cultural.}
}

@Article{Pratt-1998b,
  author = {Pratt, V.R.},
  title = {Chu spaces as a semantic bridge between linear logic and
  		mathematics},
  journal = TCS,
  year = 1998,
  URL = {http://boole.stanford.edu/pub/bridge.ps.gz},
  WKloc = {A-1019, doc/pap/BIB/Pratt-1998\_bridge.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {(Note: this supersedes "Broadening the Denotational
      Semantics of Linear Logic", doubling its length and adding much new
      material.) The motivating role of linear logic is as a ``logic behind
      logic.'' We propose a sibling role for it as a logic of
      transformational mathematics via the self-dual category of Chu
      spaces, a generalization of topological spaces. These create a bridge
      between linear logic and mathematics by soundly interpreting linear
      logic while fully and concretely embedding a comprehensive range of
      concrete categories of mathematics. Our main goal is to treat each
      end of this bridge in expository detail. In addition we introduce the
      dialectic lambda-calculus, and show that dinaturality semantics is
      not fully complete for the Chu interpretation of linear logic.}
}

@Book{PrattTW-Zelkowitz-2001,
  author = {Terrence W. Pratt and Marvin V. Zelkowitz},
  title = {Programming Languages: Design and Implementation},
  publisher = {Prentice-Hall},
  year = 2001,
  edition = {4th},
  ISBN = {0-13-027678-2},
  WKloc = {owned; Instructor's Guide: A-1432},
  bibliographies = {SE3E}
}

@InProceedings{Prediger-1998,
  author = {Susanne Prediger},
  title = {Simple Concept Graphs: A Logic Approach},
  pages = {225--239},
  OPTabstract = {},
  URL = {ftp://ftp.mathematik.tu-darmstadt.de/pub/department/preprints/1977.ps.gz},
  WKloc = {A-0948},
  year = 1998,
  month = {},
  OPTcrossref = {},
  booktitle = {Conceptual Structures: Theory, Tools and Application},
  editor = {M.-L. Mugnier and M. Chein},
  publisher = Springer,
  series = LNAI,
  volume = 1453,
  annote = {(Susanne Prediger is now Silke Pollandt?)},
  bibliographies = {RelMiCS}
}

@TechReport{Prediger-1998a,
  author = {Susanne Prediger},
  title = {{Einfache Begriffsgraphen: Syntax und Semantik}},
  year = 1998,
  institution = {FB Mathematik, TU Darmstadt},
  type = {Preprint},
  URL = {ftp://ftp.mathematik.tu-darmstadt.de/pub/department/preprints/1962.ps.gz},
  WKloc = {A-0947},
  annote = {(Susanne Prediger is now Silke Pollandt.)},
  bibliographies = {RelMiCS}
}

@InProceedings{Prediger-Wille-1999,
  author = {Susanne Prediger and Wille, Rudolf},
  title = {The Lattice of Concept Graphs of a Relationally Scaled Context},
  pages = {401--414},
  OPTabstract = {},
  URL = {ftp://ftp.mathematik.tu-darmstadt.de/pub/department/preprints/2033.ps.gz},
  WKloc = {A-0949},
  year = 1999,
  booktitle = {Conceptual Structures: Standards and Practices},
  editor = {W. Tepfenhart and W. Cyre},
  publisher = Springer,
  series = LNAI,
  volume = 1640,
  annote = {(Susanne Prediger is now Silke Pollandt?)},
  bibliographies = {RelMiCS}
}

@InProceedings{Prehofer-1994,
  title = {Higher-Order Narrowing},
  author = {Christian Prehofer},
  pages = {507--516},
  crossref = {LICS9},
  abstract = {We introduce several approaches for solving higher-order
      equational problems by higher-order narrowing and give first
      completeness results. The results apply to higher-order
      functional-logic programming languages and to higher-order
      unification modulo a higher-order equational theory. \par We lift the
      general notion of first-order narrowing to so-called higher-order
      patterns and argue that the full higher-order case is problematic.
      Integrating narrowing into unification, called lazy narrowing, can
      avoid these problems and can be adapted to the full higher-order
      case. For the second-order case, we develop a version where the
      needed second-order unification remains decidable. Finally we discuss
      a method that combines both approaches by using narrowing on
      higher-order patterns with full higher-order constraints.}
}

@Article{Prenowitz-1943,
  author = {Walter Prenowitz},
  title = {Projective Geometries and Multigroups},
  journal = AJM,
  volume = 65,
  year = 1943,
  pages = {235--256},
  bibliographies = {RelMiCS}
}

@Article{Prenowitz-Jantosciak-1972,
  author = {Walter Prenowitz and J. Jantosciak},
  title = {Geometries and Join Spaces},
  journal = JMAT,
  volume = 257,
  year = 1972,
  pages = {100--128},
  bibliographies = {RelMiCS}
}

@Book{Prenowitz-Jantosciak-1979,
  author = {Walter Prenowitz and J. Jantosciak},
  title = {Join Geometry},
  publisher = Springer,
  year = 1979,
  bibliographies = {RelMiCS}
}

@Book{Prestowitz-1989,
  year = 1989,
  title = {Trading Places, How we are giving our future to the
		  Japanese and how to reclaim it},
  publisher = {Basic Books},
  author = {Clyde Prestowitz},
  address = {New York}
}

@Book{Preusche-1987,
  author = {Andrea Preusche},
  title = {{Geometrische Form als semiotisches Ph\"anomen}},
  publisher = {Gunter Narr Verlag},
  year = 1987,
  volume = 17,
  series = {KODIKAS/CODE Supplement},
  address = {T\"ubingen},
  UniBwM = {PSY170/R3833}
}

@InProceedings{Priami-Yankelevich-1994,
  author = {Corrado Priami and Daniel Yankelevich},
  title = {Read-Write Causality},
  crossref = {MFCS94},
  pages = {567--576}
}

@incollection{Priss-2009,
  author={Priss, Uta},
  title={Relation Algebra Operations on Formal Contexts},
  DOI={10.1007/978-3-642-03079-6_20},
  DOIURL={http://dx.doi.org/10.1007/978-3-642-03079-6_20},
  pages={257--269},
  bibliographies = {RelMiCS},
  year={2009},
  isbn={978-3-642-03078-9},
  booktitle={Conceptual Structures: Leveraging Semantic Technologies},
  volume={5662},
  series=LNCS,
  editor={Rudolph, Sebastian and Dau, Frithjof and Kuznetsov, SergeiO.},
  publisher=Springer,
  language={English},
  abstract = {This paper discusses the use
    of relation algebra operations on formal contexts.
    These operations are a generalisation of some of the context operations
    that are described in the standard FCA textbook (Ganter & Wille, 1999).
    This paper extends previous research in this area
    with respect to applications and implementations.
    It also describes a software tool (FcaFlint)
    which in combination with FcaStone facilitates the application
    of relation algebra operations to contexts stored in many formats.}
}

@Book{Prusinkiewicz-Lindenmayer-1990,
  author = {P. Prusinkiewicz and A. Lindenmayer},
  title = {The Algorithmic Beauty of Plants},
  year = 1990,
  publisher = Springer,
  address = {New York}
}

@PhDThesis{Puls-1990,
  WKloc = {B-0003},
  filename = {puls.tar.gz},
  DIRECTORY = {~kahl/doc/pap},
  abstract = {The basis for the work presented in this thesis is the
      construction of formally verified programs from functionality
      specifications of declarative character.

      Such a construction starts with a specification written in a
      specification language, and (hopefully) ends with an implementation
      in an implementation language. Furthermore, the fact that the
      implementation fulfils the specification is formally verified in a
      verification logic.

      In this paper we use an applicative subset of Standard ML (SML) as
      implementation language; a language of annotated SML types as
      specification language; and a constructive type theory (CTT) as
      verification logic.

      The logic presented in this paper --- called the annotation logic ---
      is constructed with its properties as a verification logic in mind.
      Therefore we have focused on things like: syntax that provides
      intuition; expressive specification and implementation languages,
      etc. We have also abandoned the types-as-propositions paradigm, since
      this paradigm seems more interesting in a mathematical context than
      in the context of programming, so in this logic types are types and
      propositions are propositions. This means that the logic of
      propositions is classical, while the logic of types is constructive
      (intuitionistic).

      The annotation logic has been shown to have strength closely
      comparable to strength of Martin-L\"of's CTT, but with constructions
      and syntax as found in contemporary functional programming languages,
      which means that the constructions are easier to interpret.

      The annotation logic has been prototype implemented in the generic
      theorem prover `Isabelle'.

      All examples in the thesis have been verified in this prototype
      implementation.},
  year = 1990,
  title = {{Annotation Logic} for {Standard ML}},
  school = {Department of Computer Science, The Thechnical University of
      Denmark},
  note = {Published as technical report: ID-TR:1990-74},
  author = {Thomas Puls},
  address = {DK-2800 Lyngby, Denmark}
}

@InProceedings{Qian-1990,
  author = {Zhenyu Qian},
  title = {Higher-Order Order-Sorted Algebras},
  pages = {86--100},
  abstract = {The aim of this paper is to present a new semantics of
             higher-order types for functional programming, data type
             specification and program transformation. Our type discipline
             unifies higher-order functions, overloading and subtype
             polymorphism in a very simple way. The new approach can be
             considered as an extension of order-sorted algebra with
             higher-order functions. We show the existence of initial
             algebras and give a sound and complete equational deduction
             system.},
  crossref = {ALP1990}
}

@InProceedings{Qian-1990a,
  abstract = {This paper presents an algorithm to compute unifiers
		  of simply typed $\lambda$-terms w.r.t.\ the union of
		  $\alpha,\beta$ and $\eta$ conversion and a set of
		  first-order equational theory $E$, where  a
		  $\lambda$-unification algorithm and an algorithm to
		  check the wird problem w.r.t.\ $E$ are assumed to be
		  given. If the above algorithms are terminating and
		  complete, then our algorithm is temrinating and
		  complete, provided that $\lambda$-terms are second
		  order and $E$ is consistent, linear and shallow. An
		  equational theory is called shallow if its axioms
		  are all of the form $f(x_1,\ldots,x_m) = g(y_1,\ldots,y_n)$
		  or $f(x_1,\ldots,x_m) = y_1$, where $f,g$ are
		  function symbols, $x_1,\ldots,x_m,y_1,\ldots,y_n$
		  are variables and $m,n \geq 0$. Equations defining
		  projections or commutativity of functions are
		  examples of such equational theories.},
  title = {Second-Order Unification in the Presence of Linear
		  Shallow Algebraic Equations},
  pages = {449--453},
  crossref = {CTRS1990},
  author = {Zhenyu Qian},
  bibliographies = {RelMiCS}
}

@InProceedings{Qian-1994,
  author = {Zhenyu Qian},
  title = {Higher-Order Equational Logic Programming},
  crossref = {POPL1994},
  pages = {254--267},
  authorsAddress = {Bremen},
  keywords = {$\ldots$ This paper extends several importnat
		  classes of first-order equational unification
		  algorithms to the higher-order setting; only
		  problems of the extensions are discussed and
		  first-order equational unifications are viewed as
		  black boxes whenever possible.

                  We first extend narrowing and show that the
		  completeness of many higher-order narrowing
		  strategies reduces to that of their underlying
		  first-order counterparts. Then we propose an
		  algorithm for higher-order equational unifications
		  of free higher-order patterns in an arbitrary
		  equational theory. Finally a general approach to
		  extend first-order unification combination
		  algorithms is sketched informally. The termination
		  property of the above higher-order extensions is
		  considered in a uniform way.}
}

@Article{Qian-Moulah-2000,
  author = {Zhenyu Qian and Besma Abd Moulah},
  title = {Combining Object-oriented and Functional Language Concepts},
  journal = {J. of Software},
  year = 2000,
  volume = 11,
  pages = {8--22},
  URL = {http://www.kestrel.edu/HTML/people/qian/abs-comcon.html},
  WKloc = {doc/pap/BIB/Qian-Moulah-2000.ps},
  abstract = {This paper considers the problem of combining the
       object-oriented and functional programming paradigms.
       Most existing combinations have at least one of the
       following drawbacks: First, they do not contain all
       important concepts in widespread mainstream languages
       in both paradigms; second, well-known ideas in the paradigms
       are often embodied as language concepts that are totally different
       from those in widespread mainstream languages;
       third, these totally different language concepts often
       influence the whole language so that
       ``you have to pay for them, no matter you use them or not''.
       We propose a core language for functional object-oriented programming
       together with a simple and straightforward operational semantics,
       which overcomes the drawbacks mentioned above. The core language
       combines the following key language concepts from the languages
       Eiffel, Java, ML and Haskell: objects, classes, multiple inheritance,
       method redefinition, dynamic binding, static type safety,
       binary methods, algebraic data types, higher-order functions,
       ML-polymorphism. Two implementations of the core language are well under the way.}
}

@InProceedings{Qian-Wang-1994,
  author = {Zhenyu Qian and Kang Wang},
  title = {Modular {AC}-Unification of Higher-Order Patterns},
  crossref = {CCL94},
  pages = {105--120},
  keywords = {pure patterns}
}

@InProceedings{QianZhenyu-1990,
  abstract = {This paper presents an algorithm to compute unifiers
		  of simply typed $\lambda$-terms w.r.t.\ the union of
		  $\alpha,\beta$ and $\eta$ conversion and a set of
		  first-order equational theory $E$, where  a
		  $\lambda$-unification algorithm and an algorithm to
		  check the wird problem w.r.t.\ $E$ are assumed to be
		  given. If the above algorithms are terminating and
		  complete, then our algorithm is temrinating and
		  complete, provided that $\lambda$-terms are second
		  order and $E$ is consistent, linear and shallow. An
		  equational theory is called shallow if its axioms
		  are all of the form $f(x_1,\ldots,x_m) = g(y_1,\ldots,y_n)$
		  or $f(x_1,\ldots,x_m) = y_1$, where $f,g$ are
		  function symbols, $x_1,\ldots,x_m,y_1,\ldots,y_n$
		  are variables and $m,n \geq 0$. Equations sdefining
		  projections or commutativity of functions are
		  examples of such equational theories.},
  title = {Second-Order Unification in the presence of Linear
		  Shallow Algebraic Equations},
  pages = {449--453},
  crossref = {CTRS1990},
  author = {Zhenyu Qian}
}

@article{QiuJing-XiangXiaoyan-ChenZhijian-MengJianyi-DingYong-2016,
  title={Ultralow Power Processor Employing Block Instruction for {ECG} Applications},
  author={Jing Qiu and Xiaoyan Xiang and Zhijian Chen and Jianyi Meng and Yong Ding},
  journal={IEICE Electronics Express},
  volume={advpub},
  number={\unfinished },
  pages={\unfinished},
  year={2016},
  DOI={10.1587/elex.13.20150493},
  WKloc = {doc/pap/BIB},
  DIRECTURL = {https://www.jstage.jst.go.jp/article/elex/advpub/0/advpub_13.20150493/_article},
  abstract = {Conventional processors that execute a single instruction at a time
    are easy to implement but lack the power efficiency.
    This paper presents a novel hardware-software co-designed method
    to save power consumption for ECG applications.
    The software generates block instruction which is comprised of several atomic operations,
    reduces the instruction memory space,
    and merges memory operations within a block.
    The hardware executes instructions block by block,
    eliminates redundant fetching and decoding operations.
    The experiments indicate that the proposed design methodology
    can reduce the active power consumption and code size
    by 40\%{} and 55\%{} relative to CK802 (a conventional processor).}
}

@InProceedings{Queinnec-2000,
  author = {Christian Queinnec},
  title = {The Influence of Browsers on Evaluators or, Continuations to Program Web Servers},
  crossref = {ICFP2000},
  pages = {23--33},
  URL = {http://youpou.lip6.fr/queinnec/Papers/webcont.ps.gz},
  WKloc = {A-1042},
  abstract = {While developping the software of a browser-operated educational CD-ROM, we had to face a number of problems.
         This paper presents these problems and the solutions we found. Amusingly, most of our solutions rely on
         continuations. Are browsers and multimedia the future of continuations?

      Through their ``Back'' button or ``Clone window'' menu item, browsers have powerful abilities that force servers to take
      care of multiply and simultaneously answered questions. A comprehensive tool to apprehend these problems as well as to
      solve them is to view these abilities as operators acting on the continuations of the computation performed by servers.

      Thematical trails are provided to walk through the CD-ROM but do not prevent students to wander elsewhere. A trail
      may contain choices or quizzes so the rest of the trail should adapt to the walked part. We consider the trail as a
      computation and the position of the student as a continuation within that computation.

      Moreover this paper advocates a computation-centric view of servers (in opposition to the usual page-centric view)
      where interactions with users suspend the computation into continuations that may be later resumed. This approach is
      superior because the continuation reifies automatically and without errors the whole state of the computation.}
}

@Article{Quine-1960,
  author = {Willard Van Orman Quine},
  title = {Variables Explained Away},
  journal = PRO-APA,
  volume = 140,
  pages = {343--347},
  year = 1960,
  bibliographies = {RelMiCS}
}

@InCollection{Quine-1972,
  author = {Willard Van Orman Quine},
  title = {Algebraic Logic and Predicate Functors},
  booktitle = {Logic \'a Art. Essays in Honor of Nelson Goodman},
  publisher = Bobbs,
  pages = {214--238},
  year = 1972,
  note = {Published separately by Bobbs-Merrill, 1971, pp.\null{} 25},
  bibliographies = {RelMiCS}
}

@TechReport{RFB-3.8,
  author = 	 {Tristan Richardson},
  title = 	 {The RFB Protocol, Version 3.8},
  institution =  {RealVNC Ltd},
  year = 	 {2009},
  WKloc = {A-1751}
}

@Misc{RDF-1999,
  editor = {Ora Lassila and Ralph R. Swick},
  title = {Desource Description Framework {(RDF)} Model and Syntax Specification},
  howpublished = {W3C Recommendation},
  month = {22 } # FEB,
  year = 1999,
  URL = {http://www.w3.org/TR/1999/REC-rdf-syntax-19990222},
  WKloc = {A-1140},
  bibliographies = {DigBib},
  note = {latest version: \textsf{URL: http://www.w3.org/TR/REC-rdf-syntax}}
}

@Manual{RFC2068,
  title = {RFC2068: Hypertext Transfer Protocol -- HTTP/1.1},
  key = {RFC2068},
  author = {R. Fielding and J. Gettys and J. Mogul and
		  H. Frystyk and T. Berners-Lee},
  organization = {Network Working Group},
  year = 1997,
  month = JAN
}

@Book{Rabhi-Lapalme-1999,
  author =	 {Fethi Rabhi and Guy Lapalme},
  title = 	 {Algorithms: A Functional Programming Approach},
  publisher = 	 {Addison-Wesley},
  year = 	 1999,
  ISBN = {0-201-59604-0},
  pages = 	 256,
  URL = 	 {http://www.iro.umontreal.ca/~lapalme/Algorithms-functional.html},
  McMaster = 	 {QA 76.9 .A43R34 1999},
  bibliographies = {FP},
  abstract = {This book challenges more traditional methods of
     teaching algorithms by using a functional programming context,
     with Haskell as the implementation language. This leads to
     smaller, clearer and more elegant programs which enable the
     programmer to understand the algorithm itself more quickly and to
     use that understanding to explore alternative solutions. Placing
     the emphasis on program development rather than the mathematical
     properties of algorithms, the book uses a succession of practical
     programming examples to develop in the reader problem-solving
     skills which can be easily transferred to other language
     paradigms.}
}

@InCollection{Rabin-Lehmann-1994,
  author = {Michael O. Rabin and Daniel Lehmann},
  title = {The Advantages of Free Choice: A symmetric and fully distributed solution for the dining philosophers problem},
  crossref = {Roscoe-1994},
  pages = {333--352},
  chapter = 20,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Rabinovich-Trakhtenbrot-1991,
  title = {On Nets, Algebras and Modularity},
  author = {Alexander Rabinovich and Boris A. Trakhtenbrot},
  pages = {176--203},
  crossref = {TACS1991},
  bibliographies = {RelMiCS},
  abstract = {We aim at a unified and coherent presentation of net
		  models for concurrency like Petri nets and dataflow
		  networks from the perspective of modularity and
		  substitutivity.  The major goal is to achieve a
		  better understanding of the links between modularity
		  issues for nets and laws (or anomalies) in algebras
		  of processes and algebras of relations. To this end
		  we develop Mazurkiewicz's compositional approach
		  which requires a careful analysis of homomorphisms
		  from algebras of nets into algebras of processes and
		  relations.}
}

@TechReport{Rackoff-1974,
  UniBwM = {KYB800/L13971},
  year = 1974,
  title = {On the Complexity of the Theories of Weak Direct Products},
  month = JAN,
  institution = MIT,
  author = {Charles Rackoff},
  bibliographies = {RelMiCS}
}

@InProceedings{Raffali-1993,
  author = {Christophe Raffali},
  title = {Data Types, Infinity and Equality in System $AF_2$},
  crossref = {CSL93},
  pages = {280--294},
  WKloc = {A-0343},
  abstract = {This work presents an extension of system $AF_2$ to
		  allow the use of infinite data types. We extend the
		  logic with inductive and coinductive types, and show
		  that the ``programming method'' is still correct. We
		  prove propositions about the normalization of typed
		  terms. Moreover, since we only use the pure
		  $\lambda$-calculus to represent data types, we prove
		  uniqueness of the representation of data up to
		  B\"ohm tree equivalence. We also study the problem
		  of equality on infinite structures and we introduce
		  a new approach, different from the coinduction
		  scheme, and which suits our system better.}
}

@Misc{Raffali-1995,
  author = {Christophe Raffali},
  title = {A Package for Abstract Syntax with Binder},
  year = 1995,
  month = OCT,
  institution = {Chalmers University of Technology},
  OPTtype = {},
  OPTnumber = {},
  OPTaddress = {},
  WKloc = {A-0572}
}

@TechManual{Raffali-1995a,
  author = {Christophe Raffali},
  title = {Algorithmes pour la logique},
  institution = {Laboratoire de Math{\'e}matiques {LAMA}, Universit{\'e} de Savoie},
  year = 1997,
  month = NOV,
  URL = {http://www.lama.univ-savoie.fr/sitelama/Membres/pages_web/RAFFALLI/logalgo.html},
  keywords = {logic algorithms},
  WKloc = {B-0102}
}

@TechManual{Raffali-1997,
  author = {Christophe Raffali},
  title = {A Normaliser for Pure and Typed $\lambda$-Calculus},
  institution = {Laboratoire de Math{\'e}matiques {LAMA}, Universit{\'e} de Savoie},
  year = 1997,
  month = NOV,
  WKloc = {B-0102}
}

@TechManual{Raffali-1998,
  author = {Christophe Raffali},
  title = {A Package for Abstract Syntax with Binder, version 2.1},
  year = 1998,
  month = JAN,
  WKloc = {B-0102}
}

@Manual{Raffali-2001a,
  author = {Christophe Raffali},
  title = {The {PhoX} Proof Checker Documentation},
  organization = {University Paris VII, Logic team},
  year = 2001,
  note = {Version 0.7, draft},
  WKloc = {A-1245}
}

@Manual{Raffali-2001b,
  author = {Christophe Raffali},
  title = {User's Manual of the {PhoX} library},
  organization = {University Paris VII, Logic team},
  year = 2001,
  note = {Version 0.7, draft},
  WKloc = {A-1246}
}

@InCollection {Raiser-2007,
   author = {Raiser, Frank},
   affiliation = {Faculty of Engineering and Computer Sciences, University of Ulm, Germany},
   title = {Graph Transformation Systems in {CHR}},
   booktitle = {Logic Programming},
   series = {Lecture Notes in Computer Science},
   editor = {Dahl, V{\'e}ronica and Niemel{\"a}, Ilkka},
   publisher = {Springer Berlin / Heidelberg},
  WKloc = {doc/pap/BIB},
   pages = {240--254},
   volume = {4670},
   DOIURL = {http://dx.doi.org/10.1007/978-3-540-74610-2_17},
   DOI = {10.1007/978-3-540-74610-2_17},
   abstract = {In this paper we show it is possible to embed graph transformation systems (GTS) soundly and completely in constraint handling rules (CHR). We suggest an encoding for the graph production rules and we investigate its soundness and completeness by ensuring equivalence of rule applicability and results. We furthermore compare the notion of confluence in both systems and show how to adjust a standard CHR confluence check to work for an embedded GTS.},
   year = {2007}
}
@InProceedings{Rajan-Shankar-Srivas-1995,
  author = {S. Rajan and N. Shankar and M. K. Srivas},
  title = {An Integration of Model Checking with Automated Proof Checking},
  booktitle = {CAV '95},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1995,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1138},
  bibliographies = {SpecTech}
}

@Article{Rajlich-2006,
  author = 	 {V{\'a}clav Rajlich},
  title = 	 {Changing the Paradigm of Software Engineering},
  journal = 	 CACM,
  year = 	 2006,
  volume =	 49,
  number =	 8,
  pages =	 {67--70},
  month =	 AUG
}

@Article{Raju-Majumdar-1988,
  author = {K.V.S.V.N. Raju and A.K. Majumdar},
  title = {Fuzzy Functional Dependencies and Lossless Join Decomposition
      of Fuzzy Relational Database Systems},
  journal = ACM-TDS,
  volume = 13,
  number = 2,
  year = 1988,
  pages = {129--166},
  bibliographies = {RelMiCS}
}

@TechReport{Ramalingam-Reps-1989,
  author = {G. Ramalingam and Thomas W. Reps},
  title = {Semantics of Program Representation Graphs},
  institution = {University of Wisconsin, Madison CS},
  year = 1989,
  number = 900,
  month = DEC,
  file = {~kahl/doc/pap/Ramalingam-Reps-1989},
  WKloc = {A-0246},
  abstract = {Program representation graphs are a recently
		  introduced intermediate representation form for
		  programs. In this paper, we develop a mathematical
		  semantics for these graphs by interpreting them as
		  data-flow graphs. We also study the relation between
		  this semantics and the standard operational
		  semantics of programs. We show that the semantics of
		  the program representation graphs is more defined
		  than the program semantics and that for states on
		  which a program terminates normally, the PRG
		  semantics is identical to the program semantics}
}

@InProceedings{Ramalingam-Reps-1994,
  author = {G. Ramalingam and Thomas Reps},
  title = {An Incremental Algorithm for Maintaining the
		  Dominator Tree of a Reducible Flowgraph},
  crossref = {POPL1994},
  pages = {287--296}
}

@InProceedings{Rambow-Satta-1994,
  author = {Owen Rambow and Giorgio Satta},
  title = {A Rank Hierarchy for Deterministic Tree-Walking Transducers},
  crossref = {CAAP94},
  pages = {308--321}
}

@InProceedings{Ramesh-Ramakrishnan-1988,
  author = {R. Ramesh and I. V. Ramakrishnan},
  title = {Nonlinear Pattern Matching in Trees},
  pages = {473--488},
  booktitle = {Automata, Languages and Programming},
  editor = {T. Lepist\"o and A. Salomaa},
  year = 1988,
  publisher = {Springer-Verlag},
  bibliographies = {RelMiCS},
  nutshell = {HO'D/AC like top down algorithm for patterns with
		  {\em repeating variables} (hence 'nonlinear')}
}

@Article{Ramesh-Ramakrishnan-1992,
  author = {R. Ramesh and I. V. Ramakrishnan},
  title = {Nonlinear Pattern Matching in Trees},
  pages = {295--316},
  journal = JACM,
  year = 1992,
  volume = 39,
  number = 2,
  month = APR,
  nutshell = {A journal version of \cite{Ramesh-Ramakrishnan-1988}},
  bibliographies = {RelMiCS}
}

@TechReport{Ramsey-1997,
  author = {Norman Ramsey},
  title = {Eliminating Spurious Error Messages Using Exceptions,
                 Polymorphism, and Higher-Order Functions},
  institution = {Department of Computer Science, University of
                 Virginia},
  number = {CS-97-06},
  month = APR # { 3},
  year = 1997,
  WKloc = {A-0522},
  URL = {ftp://ftp.cs.virginia.edu/pub/techreports/CS-97-06.ps.Z},
  abstract = {Many language processors make assumptions after
                 detecting an error. If the assumptions are invalid,
                 processors may issue a cascade of error messages in
                 which only the first represents a true error in the
                 input; later messages are side effects of the original
                 error. Eliminating such spurious error messages
                 requires keeping track of values within the compiler
                 that are not available because of a previously detected
                 error. Examples include symbol-table entries, types,
                 and intermediate code. This paper presents a discipline
                 for tracking unavailable values and avoiding cascading
                 error messages. The discipline itself is unsurprising,
                 but it is both formalized and implemented in terms of a
                 type constructor and combinators expressed in
                 Standard~ML, and the ML type rules enforce the
                 discipline. The type constructor distinguishes
                 intermediate results that are unavailable because of a
                 previously detected error. The combinators transform
                 ordinary functions, which assume all intermediate
                 results are available, into functions that silently
                 propagate the unavailability of intermediate results.
                 ML's type system guides the application of the
                 combinators; if the compiler writer does not account
                 for a potentially unavailable value, the source code of
                 the compiler does not type-check. The techniques
                 presented exploit several features of
                 Standard~ML, including exceptions, higher-order
                 functions, parametric polymorphism, and static type
                 checking. Using these features enables the ML type
                 system to ensure that the error-tracking discipline is
                 applied consistently, relieving the programmer of that
                 burden.},
  note = {Mon, 21 Apr 1997 18:05:19 GMT}
}

@InProceedings{Ramsey-2000,
  author = {Norman Ramsey},
  title = {Pragmatic Aspects of Reusable Program Generators
                  (Position Paper)},
  crossref = {SAIG2000},
  pages = {149--171},
  WKloc = {doc/pap/BIB},
  URL = {http://link.springer.de/link/service/series/0558/bibs/1924/19240149.htm},
  abstract = {When writing a program generator requires considerable
      intellectual effort, it is pleasant to amortize that effort by using
      the generator to build more than one application. When a program
      generator serves multiple clients, however, the implementor must
      address pragmatic questions that implementors of single-use program
      generators can ignore. In how many languages should generated code be
      written? How should code be packaged? What should the interfaces to
      the client code look like? How should a user control variations? This
      paper uses examples from SLED, $\lambda$-RTL, and ASDL to elaborate
      on these questions. It is hoped that the paper will stimulate
      discussion and the development of better techniques. Most urgently
      needed is a simple, clear way to control interfaces to generated
      code.},
  bibliographies = {Coconut}
}

@Misc{Ramsey-Davidson-1997,
  author = {Norman Ramsey and Jack W. Davidson},
  title = {Machine Descriptions to Build Tools for Embedded Systems},
  year = 1997,
  WKloc = {A-0471}
}

@Misc{Ramsey-Davidson-1997a,
  author = {Norman Ramsey and Jack W. Davidson},
  title = {Specifying Instructions' Semantics Using {CSDL} (Preliminary Report)},
  year = 1997,
  month = NOV,
  WKloc = {A-0472},
  bibliographies = {Coconut, OPG}
}

@UNPUBLISHED{Ramsey-Davidson-Fernandez-2000,
  AUTHOR = {Norman Ramsey and Jack W. Davidson and Mary F. Fern\'andez},
  TITLE = {Design Principles for Machine-Description Languages},
  URL = {http://www.eecs.harvard.edu/~nr/pubs/desprin.pdf },
  note = {\textsf{http://www.eecs.harvard.edu/\~{}nr/pubs/desprin.pdf}},
  bibliographies = {Coconut},
  year = 2000,
  abstract = {We have taken a federated approach to the design of
	  domain-specific languages that support automatic generation
	  of systems software. These languages are intended to help
	  generate parts of software tools that manipulate machine
	  instructions. This paper explains the oals and principles
	  that have governed the design and implementation of these
	  languages. The primary design goal for languages in the
	  confeederation is to ensure that the same machine
	  descriptions can be used to help build a variety of
	  different tools.  Other goals include producing useful
	  results from partial descriptions, providing mechanisms that
	  build users' trust in descriptions, and making descriptions
	  concise and readable. Our design and development of
	  machine-description languages has been guided by a dozen
	  language-design principles. Using examples both from our
	  work and from others' work, we describe and illustrate each
	  principle. Some of the principles are well known to the
	  programming-language community and have been previously
	  described in the literature, but many of the principles were
	  not obious from the beginning and have not been as well
	  described in the literature.  These principles have emerged
	  from our goals and our application area. They will interest
	  other researchers who design machine-description languages
	  as well as other designers of domain-specific languages.}
}

@Article{Ranta-2004,
  author = 	 {Aarne Ranta},
  title = 	 {Grammatical Framework, A Type-Theoretical Grammar Formalism},
  journal = 	 JFP,
  year = 	 2004,
  volume =	 14,
  number =	 2,
  pages =	 {145--189},
  URL = 	 {http://www.cs.chalmers.se/~aarne/GF/},
  URL_PS = {http://www.cs.chalmers.se/~aarne/articles/gf-jfp.ps.gz},
  WKloc = 	 {doc/pap/BIB}
}

@Article{Raoult-1984,
  author = {J. C. Raoult},
  title = {On Graph Rewritings},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Theoretical Computer Science},
  year = 1984,
  volume = 32,
  OPTnumber = {},
  pages = {1--24},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Raoult-Voisin-1993,
  author = {Jean-Claude Raoult and Fr\'ed\'erick Voisin},
  title = {Set-Theoretic Graph Rewriting},
  crossref = {GTCS93},
  pages = {312--325},
  abstract = {Considering graphs as multisets of arcs, we define
		  their rewritings as simple set-theoretic rewritings:
		  applying a rule consists of removing the left-hand
		  side and adding the right-hand side. This method can
		  simulate categorical graph rewritings, provided that
		  no two vertices are identified. On the other hand,
		  rewriting is possible in cases where the
		  double-pushout method does not apply, and where the
		  single-pushout method yields an unexpected
		  result. The simplicity of our approach is
		  illustrated by its stability under composition and
		  by a criterion of local confluence.},
  WKloc = {A-0296}
}

@InCollection{Rapp-1979,
  author = {Friedrich Rapp},
  title = {Technik als Mythos},
  booktitle = {Philosophie und Mythos},
  pages = {110--129},
  year = 1979,
  editor = {Hans Poser},
  address = {Berlin},
  WKloc = {A-1247},
  bibliographies = {Engineering}
}

@InProceedings{Rasiowa-Marek-1989,
  author = {Rasiowa, H. and Marek, W.},
  title = {On reaching consensus by groups of intelligent agents},
  booktitle = {Methodologies for Intelligent Systems {{$4^{th}$}
                  {Proc.\null{} of ISMIS'89}}},
  editor = {Ras, Z.},
  year = 1989,
  publisher = NoHo,
  pages = {134--243},
  bibliographies = {RelMiCS}
}

@Article{Rasiowa-Sikorski-1960,
  author = {Rasiowa, Helena and Sikorski, Roman},
  title = {On the {Gentzen} Theorem},
  journal = FUND,
  year = 1960,
  volume = 48,
  pages = {57--69},
  annote = {Tableaux system for first-order logic; rules transform finite sequences of FOL formulae},
  bibliographies = {RelMiCS}
}

@Book{Rasiowa-Sikorski-1963,
  author = {Rasiowa, H. and Sikorski, R.},
  title = {The Mathematics of Metamathematics},
  publisher = {Polish Science Publishers},
  year = 1963,
  address = {Warsaw},
  bibliographies = {RelMiCS}
}

@InProceedings{Rasiowa-Skowron-1985,
  author = {Rasiowa, H. and Skowron, A.},
  title = {Approximation logics},
  booktitle = {Mathematical Methods of Specification and Synthesis of
      Software Systems},
  editor = {Bibel, W. and Jantke, K.P.},
  year = 1985,
  publisher = AKADEMIE,
  address = {Berlin},
  pages = {123--139},
  bibliographies = {RelMiCS}
}

@InProceedings{Rathsack-Scholz-1992,
  author = {C. Rathsack and S.B. Scholz},
  title = {{${\cal L}${\it i}{\sc sa}} --- A Lazy interpreter
		  for a Full-Fledged $\lambda$-Calculus},
  crossref = {Kuchen-Loogen-1992},
  WKloc = {A-0269},
  abstract = {\def\LISA{${\cal L}${\it i}{\sc sa}}
                  \def\PiRed{$\pi$-${\sc Red}^*$}
                  \LISA{} is the lazy evaluation counterpart of the
		  applicative-order graph reducer \PiRed{} developed
		  at the University of Kiel. It truly realizes the
		  reduction semantics of an applied
		  $\lambda$-calculus, using full-fledged
		  $\beta$-reduction as the major reduction rule. Thus
		  we have full support for higher-order functions,
		  self-applications and for interactively controlled
		  stepwise program execution.

                  Internally, \LISA{} uses essentially the same
		  graph-structures as \PiRed{} except for the fact
		  that nested functions containing relatively free
		  variables are not converted into supercombinators.

                  A well known problem in terms of efficiency is the
		  instantiation of recursive functions. Systems like
		  the G-machine or SKI-combinator reducers accomplish
		  this either by creating a new graph or by copying a
		  graph-template and, while doing this, immediately
		  substituting actual for formal parameters from the
		  current context (runtime stack-frame). The approach
		  taken with \LISA{} is to delay these substitutions
		  until it is absolutely necessary to do so and to
		  avoid copying or creating new graph instances
		  altogether. In order to achieve this, it is
		  indispensable to use sophisticated environments with
		  argument framesa chained up in the order in which
		  the functions are statically nested in the original
		  program. Easy access (via offsets) to actual
		  parameters during the processing phase can be
		  accomplished by converting the program-graph's
		  variables into environment indices. Hence instances
		  of user-defined functions are created by pairing a
		  pointer to the function-graph with a pointer to the
		  actual environment, whereas the substitution is
		  postponed until the demand for the argument actually
		  arises in the course of performing
		  $\delta$-reductions.

                  A competitive performance test with \PiRed{} shows
		  that average program runtimes of \LISA{} for a
		  representative set of examples is less than a factor
		  of 2 higher than that of \PiRed{}. This figure is a
		  remarkable improvement over a factor of about three
		  by which lazy evaluation is ususlly slower than
		  eager evaluation under otherwise identical conditions.}
}

@Article{Rauszer-1974,
  author = {C. Rauszer},
  title = {{Semi-Boolean} Algebras and their Application to Intuitionistic Logic with Dual Operators},
  journal = FUND,
  year = 1974,
  volume = 83,
  pages = {219--249},
  bibliographies = {RelMiCS}
}

@InCollection{Rauszer-Skowron-1992,
  author = {Rauszer, C. and Skowron, A.},
  title = {The discernibility matrices and functions
                  in information systems},
  publisher = Kluwer,
  year = 1992,
  booktitle = {Intelligent Decision Support.
                  Handbook of Applications and Advances
                  in the Rough Set Theory},
  editor = {Slowinski, R.},
  address = {Dordrecht},
  pages = {331--362},
  bibliographies = {RelMiCS}
}

@InProceedings{Raz-1995,
  author = {D. Raz},
  title = {On Slender Context-free Languages},
  crossref = {STACS1995},
  pages = {445--454},
  authorsAddress = {Weizmann Institute of Science},
  abstract = {In this paper we study slender context-free
		  languages, i.e., those containing at most a constant
		  number of words of each length. Recently, Ilie
		  proved that every such language can be described by
		  a finite union of terms of the form $uv^iWx^iy$
		  [I]. We provide a completely different proof of
		  this,using constructive methods. This enables us to
		  prove that thinness and slenderness are
		  decidable. Our proofs are based upon a novel
		  characterization of languages in terms of the
		  structure of the infinite paths in their prefix
		  closure. This characterization seems to be
		  interesting in itself, and can be expanded to more
		  general families of languages.}
}

@PhDThesis{Rebout-2008,
  author = {Rebout, Maxime},
  title = {Une approche cat{\'e}gorique unifi{\'e}e pour la r{\'e}criture de graphes attribu{\'e}s},
  year = {2008},
  month = JUL,
  type = {Th{\'e}se de doctorat},
  school = {Universit{\'e} Paul Sabatier},
  address = {Toulouse, France},
  URL = {ftp://ftp.irit.fr/IRIT/MACAO/TheseMRebout.pdf},
  keywords = {grammaires de graphes attribués, récriture, catégories},
  abstract = {une approche unifiée utilisant la construction de double pushout-pullback pour la récriture de graphes est developpée}
}

@InProceedings{Rebout-Feraud-MarieMagdeleine-Soloviev-2009,
  author = {Rebout, Maxime and F{\'e}raud, Louis and Marie-Magdeleine, Lionel and Soloviev, Sergei},
  title = {Computations in Graph Rewriting: Inductive types and Pullbacks in {DPO} Approach},
  booktitle = {{IFIP TC2 Central and East European Conference on Software Engineering Techniques (CEE-SET), Krakow, Pologne, 12/10/2009-14/10/2009}},
  editor = {Szmuc, Tomasz and Szpyrka, Marcin and Zendulka, Jaroslav},
  year = {2009},
  publisher = Springer,
  address = {http://www.springerlink.com/},
  pages = {164--177},
  language = {anglais}
}

@incollection{Rebout-Feraud-Soloviev-2008,
   author = {Rebout, Maxime and F{\'e}raud, Louis and Soloviev, Sergei},
   title = {A Unified Categorical Approach for Attributed Graph Rewriting},
   booktitle = {Computer Science --- Theory and Applications, {CSR 2008}},
   lncsbooktitle = {CSR 2008},
   year = 2008,
   series = LNCS,
   editor = {Hirsch, Edward and Razborov, Alexander and Semenov, Alexei and Slissenko, Anatol},
   publisher = Springer,
   pages = {398--409},
   volume = {5010},
   DOIURL = {http://dx.doi.org/10.1007/978-3-540-79709-8_39},
   DOI = {10.1007/978-3-540-79709-8_39},
   abstract = {Attributed graphs are often used in software engineering. Mainly algorithms concerning programs and models transformations are based on rewriting techniques. We suggest a unified categorical approach for the description and the verification of such algorithms and programs. This contribution which is a generalization of the double pushout approach can be seen as a mix between pushout and pullback. This will facilitate the computations on attributes within a unified framework. It should be particularly helpful for model to model transformation in the domain of  Model Driven Architecture .}
}

@InProceedings{Reddy-1993,
  author = {Uday S. Reddy},
  title = {Higher-order Aspects of Logic Programming},
  crossref = {ELP93},
  pages = {301--321},
  bibliographies = {RelMiCS},
  keywords = {functional programming, lambda calculus, logic
		  variables, types, semantics, relations, linear logic},
  abstract = {Are higher-order extensions to logic programming
		  needed? We suggest a negative answer by showing that
		  higher-order features are already available in pure
		  logic programming. It is demonstrated that
		  higher-order lambda calculus-based languages can be
		  compositionally embedded in logic programming
		  languages preserving their semantics and abstraction
		  facilities. Further, we show that such higher-order
		  techniques correspond to programming techniques
		  often praticed in logic programming.}
}

@InProceedings{Reddy-1994,
  title = {Passivity and Independence},
  author = {Uday S. Reddy},
  pages = {342--352},
  crossref = {LICS9},
  abstract = {Most programming languages have certain phrases (like
      expressions) which only read information from the state and certain
      others (like commands) which write information to the state. These
      are called {\em passive\/} and {\em active\/} phrases respectively.
      Semantic models which make these distinctions have been hard to find.
      For instance, most semantic models have expression denotations that
      (temporarily) change the state. Common reasoning principles, such as
      the Hoare's assignment axiom, are not valid in such models. We define
      here a semantic model which captures the notions of ``change,''
      ``absence of change'' and ``independent change'' etc. This is done by
      extending the author's ``linear logic model of state'' with
      dependence/independence relations so that sequential traces give way
      to pomset traces.}
}

@Book{Reed-Roscoe-Watcher-,
  author = {Reed and Roscoe and Watcher},
  title = {Topology and Category Theory in Computer Science},
  publisher = {},
  year = {},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Reed92,
  WKloc = {A-0040},
  abstract = {This design method increases reusability by focusing
		  on the interactions between closely related
		  classes.},
  year = 1992,
  volume = 9,
  title = {Object-Oriented Design by Orthogonality},
  number = 1,
  month = JAN,
  journal = {Computer Language},
  author = {Harvey Reed}
}

@TechReport{Reekie-1994,
  author = {H. John Reekie},
  title = {Visual Haskell: A First Attempt},
  institution = {School of Electrical Engineering, University of
		  Technology, Sydney},
  year = 1994,
  type = {Research Report},
  number = {94.5},
  address = {Key Centre for Advanced Computing Sciences,
		  University of Technology, Sydney, PO Box 123,
		  Broadway NSW 2007, Australia},
  month = AUG,
  URL = {ftp://ftp.ee.uts.edu.au/pub/prose/visual-haskell.ps.gz},
  authorsAddress = {johnr\@ee.uts.edu.au},
  file = {~arne/papers/visual-haskell.ps},
  abstract = {This paper presents  the Visual Haskell
		  language. Visual Haskell is intended as i) a means
		  of visualising Haskell programs, and ii) as a
		  complementary programming notation to standard
		  Haskell. The syntax of Visual Haskell is specified
		  by first giving a translation from Haskell into an
		  intermediate, textual form; the visual syntax is
		  then specified as a direct translation from the
		  intermediate form into visual
		  representations. Visual Haskell is kept as close to
		  Haskell as possible, in the hope that it could be
		  used in a ``two-view'' programming system. Several
		  examples of function definitions illustrate how
		  Visual Haskell looks and is used.

                  Visual Haskell is incomplete, lacking syntax for
		  modules, type declarations, type classes and
		  instances, arrays, and user-defined operators. These
		  are seen as temporary omissions rather than
		  insurmountable drawbacks.}
}

@Misc{Reggio-2000,
  author = {Gianna Reggio},
  title = {[Formalising {UML} Statecharts]},
  year = 2000,
  URL = {ftp://ftp.disi.unige.it/person/ReggioG/UMLreport/},
  WKloc = {A-1049}
}

@TechReport{Reichel-1998,
  author = {Horst Reichel},
  title = {Nested Sketches},
  institution =  {School of Informatics at The University of Edinburgh},
  year =         {1998},
  number =    {ECS-LFCS-98-401},
  WKloc = {A-1276, doc/pap/BIB},
  abstract = {Since the fundamental work of Lawvere in 1963
      \cite{Lawvere-1963} it is common to understand a theory as a category
      with additional structure, to understand a model of the theory as a
      functor preserving the additional structure, and to represent
      homomorphisms by natural transformations. The resulting model cateory
      becomes a suitable subcategory of a functor category. Many different
      classes of mathematical structures have been described and
      investigated this way. The aim of this paper is, to find a functorial
      model theory for those classes of algebras that appear naturally as
      semantics of algebraic specifications of prameterized data types,
      using initial respectively more general free functor semantics, and
      to extend the model theory to specificcations that use well
      \emph{inductively defined data types} as \emph{coinductively defined
      patterns of behavior} and their systematic combinations.

      The final result is a categorical model theory of discrete
      mathematical structures whose \emph{basic operations may have
      arbitrarily structured domains and codomains}. Such kind of
      structures have been first systematically investigated by T. Habino
      on his PhD thesis \cite{Hagino-}. [$\ldots$]}
}

@Misc{Reid-Peterson-1995,
  author = {Alastair Reid and John Peterson},
  title = {Prelude and Library Issues in {Haskell 1.3}},
  year = 1995,
  month = FEB,
  WKloc = {A-0618}
}

@InProceedings{Reif-1994,
  author = {Wolfgang Reif},
  title = {An Approach to Parameterized First-Order Specifications: Semantics, Correctness, Parameter Passing},
  crossref = {Bjorner-Broy-Pottosin-1994},
  pages = {67--80},
  WKloc = {A-0997},
  bibliographies = {SpecTech}
}

@InCollection{Reif-1995,
  author = {W. Reif},
  title = {{The KIV-approach to Software Verification}},
  crossref = {KORSO-1995},
  bibliographies = {SpecTech},
  WKloc = {A-1062}
}

@Article{Reif-1997a,
  author = {W. Reif},
  title = {{Software-Verifikation und ihre Anwendungen}},
  journal = {{it+ti Themenheft Formale Entwurfsmethoden --
		  Software Correctness}},
  number = 3,
  year = 1997,
  publisher = {Oldenbourg Verlag},
  bibliographies = {SpecTech},
  WKloc = {A-1060}
}

@Article{Reif-1998,
  author = {W. Reif},
  title = {{Software-Sicherheit mit formalen Methoden}},
  journal = {Softwaretechnik-Trends},
  year = 1998,
  volume = 18,
  number = 3,
  month = AUG,
  OPTannote = {{GI-Zeitschrift, Sonderheft mit den Beitr\"{a}gen der
                  GI-Fachtagung Softwaretechnik 98, Paderborn, 7. --
                  9. September 1998}},
  bibliographies = {SpecTech},
  WKloc = {A-1057}
}

@Article{Reif-1999,
  author = {Wolfgang Reif},
  title = {{Formale Methoden f\"ur sicherheitskritische Software --- Der KIV-Ansatz}},
  journal = {Informatik Forschung und Entwicklung},
  volume = 14,
  number = 4,
  year = 1999,
  pages = {193--202},
  bibliographies = {SpecTech}
}

@InCollection{Reif-Schellhorn-1998,
  author = { W. Reif and G. Schellhorn},
  title = {{Theorem Proving in Large Theories}},
  booktitle = {{Automated Deduction---A Basis for Applications}},
  publisher = {Kluwer Academic Publishers},
  address = {Dordrecht},
  volume = {III, 2},
  year = 1998,
  editor = {W. Bibel and P. Schmitt},
  bibliographies = {SpecTech},
  WKloc = {A-1058}
}

@InCollection{Reif-Schellhorn-Stenzel-Balser-1998,
  author = {W. Reif and G. Schellhorn and K. Stenzel and M. Balser},
  title = {Structured Specifications and Interactive Proofs
		  with {KIV}},
  booktitle = {{Automated Deduction---A Basis for Applications}},
  publisher = {Kluwer Academic Publishers},
  address = {Dordrecht},
  year = 1998,
  editor = {W. Bibel and P. Schmitt},
  bibliographies = {SpecTech},
  WKloc = {A-1059}
}

@TechReport{Reif-Stenzel-1992,
  author = {Wolfgang Reif and Kurt Stenzel},
  title = {Reuse of Proofs in Software Verification},
  institution = {Universit\"at Karlsruhe, Fakult\"at f\"ur Informatik},
  year = 1992,
  number = {26/92},
  keywords = {KIV},
  WKloc = {C-0014}
}

@Misc{Reilly-1998,
  author = {David Reilly},
  title = {Mobile Agents - Process migration and its implications},
  howpublished = {http://www.davidreilly.com/topics/software_agents/mobile_agents/},
  month = NOV,
  year = 1998,
  WKloc = {doc/pap/BIB},
  abstract = {Mobile agents are agents that can physically travel across a
      network, and perform tasks on machines that provide agent hosting
      capability. This allows processes to migrate from computer to
      computer, for processes to split into multiple instances that execute
      on different machines, and to return to their point of origin. Unlike
      remote procedure calls, where a process invokes procedures of a
      remote host, process migration allows executable code to travel and
      interact with databases, file systems, information services and other
      agents. The technology behind mobile agents is examined, and an
      analysis of its uses and implications is offered.}
}

@Book{Reimer-1991,
  UniBwM = {KYB800/W10526},
  PRICE = {DM 46.-},
  keywords = {knowledge representation},
  ISBN = {3-519-02241-9},
  contents = {1 Einleitung
                  2 Nicht-objektzentrierte Repr\"asentationsformate
                  2.1 Nat\"urliche Sprache
                  2.2 Logik
                  2.3 Produktionsregeln
                  2.4 Analoge (direkte) Repr\"asentation
                  3 Semantische Netze
                  3.1 Einf\"uhrung
                  3.2 Modellierung mit semantischen Netzen
                  3.3 Operationen auf semantischen Netzen
                  3.4 Erweiterung der Ausdruckskraft durch
		      Partitionierung
                  3.5 Vor- und Nachteile semantischer Netze
                  3.6 Erg\"anzende Bemerkungen und weiterf\"uhrende
		      Literatur
                  3.7 \"Ubungsaufgaben
                  4 Frame-artige Repr\"asentationsformate
                  4.1 Einf\"uhrung
                  4.2 Modellierung mit frame-artigen Konstrukten
                  4.3 Operationen auf Frame-Repr\"asentationen
                  4.4 Vor- und Nachteile frame-artiger
		      Repr\"asentationsformate
                  4.5 Erg\"anzende Bemerkungen und weiterf\"uhrende
		      Literatur
                  4.6 \"Ubungsaufgaben
                  A L\"osungen zu den \"Ubungsaufgaben
                  B Literaturnachweise
                  Stichwortverzeichnis
                  Verzeichnis der Definitionen},
  year = 1991,
  title = {{Einf\"uhrung in die Wissensrepr\"asentation}},
  series = {Leitf\"aden der angewndten Informatik},
  publisher = {B.G.~Teubner},
  author = {Ulrich Reimer},
  address = {Stuttgart}
}

@Book{Reinartz-1999,
  author = {Thomas Reinartz},
  title = {Focusing Solutions for Data Mining, Analytical Studies and Experimental Results in  Real-World Domains},
  publisher = Springer,
  year = 1999,
  volume = 1623,
  series = LNCS,
  UniBwM = {INF300/YE6805}
}

@TechReport{Reinke-1998,
  author = {Claus Reinke},
  title = {Functions, Frames, and Interactions --- completing a
      $\lambda$-calculus-based purely functional language with respect to
      programming-in-the-large and interactions with runtime environments},
  institution = {Institut f\"ur Informatik und praktische Mathematik,
       Christian-Albrechts-Universit\"at Kiel},
  year = 1998,
  type = {Bericht},
  number = 9804,
  month = MAY,
  note = {zugl. Dissertation},
  WKloc = {B-0060, doc/pap/BIB}
}

@Misc{Rekers-1994,
  author = {J. Rekers},
  title = {On the use of Graph Grammars for defining the Syntax of Graphical Languages},
  year = 1994,
  WKloc = {A-0420}
}

@Misc{Rekers-1995,
  author = {Jan Rekers},
  title = {Visuele Talen --- Visual Languages},
  year = 1995,
  note = {274 slides},
  WKloc = {B-0118}
}

@Article{Rekers-Schuerr-1997,
  author = {J. Rekers, A. Schürr},
  title = {Defining and Parsing Visual Languages with Layered Graph Grammars},
  journal = {Journal of Visual Languages and Computing},
  year = 1997,
  volume = 8,
  number = 1,
  pages = {27--55},
  publisher = {Academic Press},
  address = {London},
  WKloc = {doc/pap/BIB}
}

@BibTeX{RelMiCS-bib,
  title = {Bibliography for Relational Methods in Computer Science - RelMiCS},
  author = {Wolfram Kahl},
  author-url = {http://ist.unibw-muenchen.de/kahl/},
  email = {kahl@ist.unibw-muenchen.de},
  address = {Institute for Software Technology
             Department of Computing Science
             Federal Armed Forces University Munich},
  address-url = {http://ist.unibw-muenchen.de/},
  URL = {http://ist.unibw-muenchen.de/relmics/bib/RelMiCS.bib.gz},
  format = {bibtex},
  date = {2000/03/08},
  supported = {yes},
  permission = {search},
  abstract = {Relational Methods in Computer Science:
              Foundations and applications of relation algebra and
              other relational methods, particularly in their uses
              towards analysing, modelling or resolving
              computer science problems such as
                  program specification,
                  heuristic approaches for program derivation,
                  automatic prover design,
                  database and software decomposition,
                  program fault tolerance,
                  testing,
                  data abstraction and information coding,
                  spatial reasoning.},
  keywords = {relation algebra, relational methods, allegory},
  otherurls = {http://ist.unibw-muenchen.de/cgi-bin/relmics/bibsearch
               http://ist.unibw-muenchen.de/relmics/},
  area = {Theory}
}

@InProceedings{Rem-1985,
  author = {M. Rem},
  title = {Concurrent Computation and {VLSI} Circuits},
  pages = {399--437},
  crossref = {Marktoberdorf-1985}
}

@InProceedings{Remy-1989,
  author = {Didier R{\'e}my},
  title = {Typechecking records and variants in a natural
		  extension of ML},
  crossref = {POPL1989},
  pages = {77--88},
  abstract = {Strongly typed languages with records may have
		  inclusion rules so that records with more fields can
		  be used instead of records with less fields. But
		  these rules lead to a global treatment of record
		  types as a special case. We solve this problem by
		  giving an ordinary status to records without any
		  {\em ad-hoc} assertions, replacing inclusion rules
		  by extra information in record types. With this
		  encoding ML naturally extends its polymorphism to
		  records but any other host language will also
		  transmit its power.}
}

@InProceedings{Remy-Yakobowski-2007,
  author = 	 {Didier R{\'e}my and Boris Yakobowski},
  title = 	 {A Graphical Presentation of $\mathsf{ML}^{\mathsf{F}}$ Types with a Linear-Time Unification Algorithm},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2007},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1677, doc/pap/BIB},
  OPTannote = 	 {}
}

@InProceedings{RenardelDeLavalette-1993,
  author = {Renardel de Lavalette, Gerard R.},
  title = {From Implicit vi Inductive to Explicit Definitions},
  crossref = {SoSL93},
  pages = {304--314},
  abstract = {This paper reports on a method to provide general
		  implicit descriptions with a sound logical
		  semantics. This ethod has been applied in the
		  specification languages COLD and VVSL.}
}

@Book{Renaud-1996,
  author = {Francis Renaud},
  title = {S\'emantique du temps et lambda-calcul},
  publisher = {Presses Universitaires de France},
  year = 1996,
  series = {Linguistique nouvelle},
  annote = {showed by Jacques Riguet during Relmics 2000}
}

@InProceedings{Renear-Mylonas-Durand-1996,
  author = {Allen Renear and Elli Mylonas and David Durand},
  title = {Refining our Notion of What Text Really Is: The Problem of Overlapping Hierarchies},
  booktitle = {Research in Humanities Computing},
  publisher = OUP,
  editor = {Nancy Ide and Susan Hockey},
  year = 1996,
  WKloc = {A-1125},
  bibliographies = {DigBib}
}

@InProceedings{Rensink-1994,
  author = {A. Rensink},
  title = {Methodological Aspects of Action Refinement},
  crossref = {PROCOMET94},
  pages = {219--241},
  keywords = {Programming Languages, Formal Definitions and
		  Theory; Specifying and Verifying and Reasoning about
		  Programs; Studies of Program Constructs; Refinement
		  of Concurrent Systems}
}

@Book{Reps-Teitelbaum-1989,
  author = {Reps, Thomas and Teitelbaum, Tim},
  title = {The {Synthesizer} Generator: A System for Constructing Language-based Editors},
  year = 1989,
  publisher = Springer,
  series = {Texts and Monographs in Computer Science},
  UniBwM = {INF460/T10701},
  bibliographies = {EdComb},
  WKloc = {Q-013}
}

@PhDThesis{Resek-1975,
  author = {Diane Resek},
  title = {Some Results on Relativized Cylindric Algebras},
  note = Doct,
  school = BERKELEY,
  address = {Berkeley},
  year = 1975,
  pages = {iii+290},
  bibliographies = {RelMiCS}
}

@InProceedings{Revesz-1987,
  WKloc = {A-0072},
  abstract = {many implementation techniques proposed for
		  functional languages in the literature are based on
		  lambda-calculus and the theory of combinators [5,
		  12, 14]. The main advantage of functional languages
		  over the more conventional programming languages is
		  their lack of side-effects which makes their
		  semantics much simpler [2].

                  This paper presents a $\lambda$-calculus dialect
		  with list handling extensions that can be used for
		  defining an operational semantics for functional
		  programs. This semantics is based on a set of
		  elementary $\alpha$-rules and $\beta$-rules which
		  collectively implement the substitution operation
		  without explicitely using it [10]. Two extra
		  reduction rules, called $\gamma$-rules, have been
		  added to the system for list manipulations [11].
		  Combining $\lambda$-calculus with list-handling
		  capabilities makes an efficient vectorization of
		  $\lambda$-calculus possible. This way we obtain a
		  very elegant treatment of mutual recursion in our
		  system.

                  The reduction rules described in this paper
		  represent, in fact, an operational semantics for our
		  extended $\lambda$-notation. An interpreter program
		  for this extended $\lambda$-calculus has been
		  developed by a direct implementation of the
		  reduction rules which makes the correctness proof of
		  the interpreter very easy. The notion of {\em
		  controlled reduction} is introduced here to
		  guarantee the existence of a normal form for
		  $\lambda$-expressions representing recursively
		  defined functions using the $Y$ combinator.},
  title = {Rule-Based Semantics for an Extended Lambda-Calculus},
  pages = {43--56},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Gy\"orgy E. R\'ev\'esz}
}

@Book{Revesz-1988,
  author = {Gy\"orgy E. Revesz},
  title = {Lambda-Calculus, Combinators, and Functional Programming},
  year = 1988,
  volume = 4,
  series = {Cambridge Tracts in Theoretical Computer Science},
  publisher = {Cambridge University Press},
  UniBwM = {INF700/S271},
  contents = {1 Introduction
	1.1 Variables and functions in mathematics and in programming languages
	1.2 Domains, types, and higher-order functions
	1.3 Polymorphic functions and Currying
	2 Type-free lambda-calculus
	2.1 Syntactic and semantic considerations
	2.2 Renaming, $\alpha$-congruence, and substitution
	2.3 Beta-reduction and equality
	2.4 The Church-Rosser theorem
	2.5 Beta-reduction revisited
	3 Combinators and constant symbols
	3.1 Lambda-expressions without free variables
	3.2 Arithmetic and other constants and combinators
	3.3 Recursive definitions and the {bf Y} combinator
	3.4 Elimination of bound variables: bracket abstraction
	4. List manipulation in lambda-calculus
	4.1 An extension of the syntax os $\lambda$-expressions
	4.2 Additional axioms for list manipulation
	4.3 List manipulating functions
	4.4 Dealing with mutual recursion
	4.5 Computing with infinite lists
	5 Rule-based semantics of $\lambda$-expressions
	5.1 Program transformation as a computation
	5.2 Controlled reduction
	5.3 The functional programming system FP
	5.4 Translation of functional programs to $\lambda$-calculus
	5.5 List comprehension in Miranda
	6 Outlines of a reduction machine
	6.1 Graph representation of $\lambda$-expressions
	6.2 The instructions of a reduction machine
	6.3 Implementation of primitive functions
	6.4 Demand-driven evaluation
	7 Towards a parallel graph-reduction
	7.1 Harnessing the implicit parallelism
	7.2 On-the-fly garbage collection
	7.3 Control of parallelism
	A A proof of the Church-Rosser theorem
	B Introduction to typed $\lambda$-calculus
	Bibliographical notes
	References},
  annote = { disabling the Y combinator to Y' after expansion;
                  reenabling after $\beta$-reduction outside.
	      5 $\beta$-rules for step-by-step $\beta$-reduction
		  without substitution.}
}

@InProceedings{Revesz-1997,
  author = {Peter Z. Revesz},
  title = {Problem Solving in the {DISCO} Constraint Database System},
  crossref = {Proc. Workshop on Constraint Databases and Applications},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0738}
}

@InProceedings{Revesz-1998,
  author = {Peter Z. Revesz},
  title = {Constraint Databases: A Survey},
  crossref = {Semantics in Databases},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0737}
}

@InProceedings{Reynaud-1990,
  author = {Jean Claude Reynaud},
  title = {Putting Algebraic Components Together: A Dependent
		  Type Approach},
  crossref = {DISCO90},
  pages = {141--150},
  WKloc = {A-0352},
  abstract = {We define a framework based on dependent types for
		  putting algebraic componenets together. It is
		  defined with freely generated categories. In order
		  to preserve initial, loose and constrained semantics
		  of components, we introduce the notion of
		  SPEC-categories which look like specific finitely
		  co-complete categories. A constructive approach
		  which includes parameterization techniques is used
		  to define new components from basic predefined
		  ones. The problem of the internal coding of external
		  signature symbols is introduced}
}

@Article{Reyner-1977,
  year = 1977,
  volume = 6,
  title = {An Analysis of a Good Algorithm for the Subtree Problem},
  pages = {730-732},
  number = 4,
  month = DEC,
  journal = {SIAM Journal on Computing},
  author = {Reyner, S. W.}
}

@Book{Reynolds-1981,
  author = {John C. Reynolds},
  title = {The Craft of Programming},
  publisher = Prentice,
  year = 1981,
  series = PrenticeCS,
  McMaster = {QA 76.6 .R47}
}

@InProceedings{Reynolds-1983,
  year = 1983,
  title = {Types Abstraction and Parametric Polymorphism},
  publisher = {North Holland},
  booktitle = {Information Processing '83},
  editor = {R.E.A. Mason},
  author = {John C. Reynolds},
  pages = {513--523}
}

@Unpublished{Reynolds-1989,
  WKloc = {A-0003, ~/doc/pap/reynolds/normhard/},
  keywords = {intersection types, polymorphic lambda calculus,
		  PSPACE-hard},
  authorsAddress = {CMU},
  year = 1989,
  title = {Even Normal Forms Can be Hard to Type},
  note = {unpublished extended abstract},
  month = DEC,
  author = {John C. Reynolds}
}

@TechReport{Reynolds-1990,
  UPDATED = {9 August 1990},
  WKloc = {A-0006, ~/doc/pap/reynolds/functexp/},
  abstract = {Given a model of the polymorphic typed lambda calculus
		  based upon a Cartesian closed category $\ctK$, there
		  will be functors from $\ctK$ to $\ctK$ whose action
		  on objects can be expressed by type expressions and
		  whose action on morphisms can be expressed by
		  ordinary expressions. We show that if $T$ is such a
		  functor then there is a weak initial $T$-algebra and
		  if, in addition, $\ctK$ possesses equalizers of all
		  subsets of its morphism sets, then there is an
		  initial $T$-algebra. These results are used to
		  establish the impossibility of certain models,
		  including those in which types denote sets and
		  $\morphs{S}{S'}$ denotes the set of all functions
		  from $S$ to $S'$.},
  year = 1990,
  type = {Report},
  title = {On Functors Expressible in the Polymorphic Typed Lambda Calculus},
  number = {CMU-CS-90-147},
  note = {Also to appear in Information and Computation},
  month = {July 6},
  institution = {Carnegie Mellon University, School of Computer Science},
  author = {Reynolds, John C. and Plotkin, Gordon D.},
  annote = {revision of REYNOLDS88 and REYNOLDS90}
}

@InProceedings{Reynolds-1991,
  title = {The Coherence of Languages with Intersection Types},
  author = {John C. Reynolds},
  pages = {657--700},
  crossref = {TACS1991},
  WKloc = {A-0005, ~/doc/pap/reynolds/coherence/},
  abstract = {When a programming language has a sufficiently rich type
		  structure, there can be more than one proof of the
		  same typing judgement; potentially this can lead to
		  semantic ambiguity since the semantics of a typed
		  language is a function of such proofs.  When no such
		  ambiguity arises, we say that the language is {\em
		  coherent}.  In this paper we prove the coherence of
		  a class of lambda-calculus-based languages that use
		  the intersection type discipline, including both a
		  purely functional programming language and the
		  Algol-like programming language Forsythe.}
}

@InProceedings{Reynolds-1991-x,
  UPDATED = {31 October 1991},
  WKloc = {A-0005, ~/doc/pap/reynolds/coherence/},
  PLACE = {Sendai, Japan},
  DATES = {September 24--27, 1991},
  abstract = {When a programming language has a sufficiently rich type
		  structure, there can be more than one proof of the
		  same typing judgement; potentially this can lead to
		  semantic ambiguity since the semantics of a typed
		  language is a function of such proofs. When no such
		  ambiguity arises, we say that the language is {\em
		  coherent}. In this paper we prove the coherence of a
		  class of lambda-calculus-based languages that use
		  the intersection type discipline, including both a
		  purely functional programming language and the
		  Algol-like programming language Forsythe.},
  year = 1991,
  volume = 526,
  title = {The Coherence of Languages with Intersection Types},
  series = {Lecture Notes in Computer Science},
  publisher = {Springer-Verlag},
  pages = {675--700},
  editor = {Takayasu Ito and Albert R. Meyer},
  booktitle = {Theoretical Aspects of Computer Software, International
		  Conference TACS '91, Proceedings},
  author = {Reynolds, John C.},
  address = {Berlin}
}

@Unpublished{Reynolds-1991a,
  WKloc = {A-0004, ~/doc/pap/reynolds/forsytheintro/},
  authorsAddress = {CMU},
  abstract = {By using concepts of modern programming-language
		  theory, such as higher-order procedures and
		  intersection types, it is possible to design a
		  programming language that is extremely general
		  without being complicated. This thesis is
		  illustrated by the Algol-like language Forsythe.},
  year = 1991,
  title = {Replacing Complexity with Generality: The Programming Language Forsythe},
  note = {unpublished introductory},
  month = APR,
  author = {John C. Reynolds}
}

@TechReport{Reynolds-Plotkin-1990,
  UPDATED = {9 August 1990},
  WKloc = {A-0006, ~/doc/pap/reynolds/functexp/},
  abstract = {Given a model of the polymorphic typed lambda calculus
		  based upon a Cartesian closed category $\ctK$, there
		  will be functors from $\ctK$ to $\ctK$ whose action
		  on objects can be expressed by type expressions and
		  whose action on morphisms can be expressed by
		  ordinary expressions. We show that if $T$ is such a
		  functor then there is a weak initial $T$-algebra and
		  if, in addition, $\ctK$ possesses equalizers of all
		  subsets of its morphism sets, then there is an
		  initial $T$-algebra. These results are used to
		  establish the impossibility of certain models,
		  including those in which types denote sets and
		  $\morphs{S}{S'}$ denotes the set of all functions
		  from $S$ to $S'$.},
  year = 1990,
  type = {Report},
  title = {On Functors Expressible in the Polymorphic Typed Lambda
      Calculus},
  number = {CMU-CS-90-147},
  note = {Also to appear in Information and Computation},
  month = JUL,
  institution = CARNEG_DCS,
  author = {Reynolds, John C. and Plotkin, Gordon D.},
  annote = {revision of REYNOLDS88 and REYNOLDS90},
  bibliographies = {RelMiCS, LogRel}
}

@TechReport{Rich-Waters-1989,
  author = {Charles Rich and Richard C. Waters},
  title = {Intelligent Assistance for Program Recognition, Design, Optimization, and Debugging},
  institution = {MIT Artificial Intelligence Lab.},
  year = 1989,
  OPTkey = {},
  OPTtype = {A.I.~Memo},
  OPTnumber = 1100,
  OPTaddress = {},
  OPTmonth = JAN,
  WKloc = {A-1366, doc/pap/BIB},
  keywords = {Plan Calculus, CAKE},
  abstract = {We describe research in four related areas, base on the
      following theoretical principles: the assistant approach (incremental
      automation) and the exploitation of clich\'es (using knowledge of
      common engineering practice).

      Each investigation involves the construction of a prototype system to
      provide intelligent assistance for a person performing the task: A
      recognition assistant will help reconstruct the design of a program,
      given only its source code. A design assistant will assist a
      programmer by detecting errors and inconsistencies in his design
      choices and by automatically making many straightforward
      implementation decisions. An optimization assistant will help improve
      the performance of programs by identifying intermediate results that
      can be reused. A debugging assistant will aid in the detection,
      localization, and repair of errors in designs as well as completed
      programs.

      These prototypes will be constructed using two shared technologies: a
      programming language independent formal representation for programs
      and programming language knowledge (the Plan Calculus) and an
      automated reasoning system (\textsf{cake}, which supports both
      general logical reasoning and special-purpose decision procedures.

      \emph{Revised version of proposal submitted to the NSF}.}
}

@Article{Richardson-1953,
  author = {M. Richardson},
  title = {Solutions of Irreflexive Relations},
  journal = ANMA,
  year = 1953,
  volume = 58,
  pages = {573--590},
  bibliographies = {RelMiCS}
}

@InProceedings{Richardson-Smaill-Green-1998,
  author = {Julian Richardson and Alan Smaill and Ian Green},
  title = {System Description: Proof Planning in Higher-Order Logic with $\lambda${\it Clam}},
  crossref = {CADE1998},
  pages = {129--133},
  OPTabstract = {},
  WKloc = {A-0607}
}

@Article{Richardson-StaffordFraser-Wood-Hooper-1998.pdf,
  author = {Tristan Richardson and Quentin Stafford-Fraser and Kenneth R. Wood and Andy Hopper},
  title = {Virtual Network Computing},
  journal = {IEEE Internet Computing},
  year = 1998,
  WKloc = {A-1350},
  volume = 2,
  number = 1,
  pages = {33--38},
  month = JAN # {/} # FEB,
  keywords = {VNC},
  URL = {http://www.uk.research.att.com/vnc/docs.html}
}

@InProceedings{Riecke-Subrahmanyam-1993,
  author = {Jon G. Riecke and Ramesh Subrahmanyam},
  title = {Algebraic Reasoning and Completeness in Typed Languages},
  pages = {185--195},
  abstract = {We consider the following problem in proving observational
             congruences in functional languages: given a call-by-name
             language based on the simplytyped $\lambda$-calculus with
             algebraic operations axiomatized by algebraic equations E, is
             the set of observational congruences between terms exactly
             those provable from ($\beta$), ($\eta$), and E? We find
             conditions determining whether $\beta \eta$E-equational
             reasoning is complete for proving the observational congruences
             between such terms. We demonstrate the power and generality of
             the theorems by presenting a number of easy corollaries for
             particular algebras.},
  crossref = {POPL1993},
  WKloc = {A-0195}
}

@InProceedings{Rieckhoff-1992,
  author = {Catharina Rieckhoff},
  title = {Towards a Theory for the Animation of Algebraic
		  Specifications},
  crossref = {SADT92},
  pages = {310--320},
  authorsAddress = {TU Berlin},
  WKloc = {A-0332}
}

@Article{Rieffel-Polak-2000,
  author = {Eleanor Rieffel and Wolfgang Polak},
  title = {An introduction to quantum computing for non-physicists},
  journal = ACMCS,
  year = 2000,
  volume = 32,
  number = 3,
  pages = {300--335},
  month = SEP,
  WKloc = {A-1239, doc/pap/BIB},
  abstract = {Richard Feynman's observation that certain quantum
      mechanical effects cannot be simulated efficiently on a computer led
      to speculation that computation in general could be done more
      efficiently if it used these quantum effects. This speculation proved
      justified when Peter Shor described a polynomial time quantum
      algorithm for factoring intergers.In quantum systems, the
      computational space increases exponentially with the size of the
      system, which enables exponential parallelism. This parallelism could
      lead to exponentially faster quantum algorithms than possible
      classically. The catch is that accessing the results, which requires
      measurement, proves tricky and requires new nontraditional
      programming techniques.The aim of this paper is to guide computer
      scientists through the barriers that separate quantum computing from
      conventional computing. We introduce basic principles of quantum
      mechanics to explain where the power of quantum computers comes from
      and why it is difficult to harness. We describe quantum cryptography,
      teleportation, and dense coding. Various approaches to exploiting the
      power of quantum parallelism are explained. We conclude with a
      discussion of quantum error correction.}
}

@PhdThesis{Riehle-2000,
  author =       {Dirk Riehle},
  title =        {Framework Design: A Role Modeling Approach},
  school =       {ETH Z\"urich, Institute of Computer Systems},
  year =         {2000},
  OPTkey =       {},
  type =      {Dissertation No.~13509},
  OPTaddress =   {},
  OPTmonth =     FEB,
  note =      {(Chapter 8 discusses the JHotDraw design.)},
  URL = {http://dirkriehle.com/computer-science/research/dissertation/},
  abstract =    {This dissertation presents role modeling for framework design.
    Role modeling makes designing, learning, and using object-oriented frameworks
    easier than possible with traditional class-based approaches,
    because role modeling for framework design
    reduces class complexity and object collaboration complexity,
    and improves clarity of requirements put on use-clients.
    The dissertation extends class-based modeling with role modeling concepts
    and introduces frameworks as explicit design and implementation artifacts
    with well-defined boundaries.
    The dissertation's claims are validated with the help of three case studies
    that show how role modeling for framework design works in practice.}
}

@MastersThesis{Riehn-1994,
  author = {Thorsten Riehn},
  title = {{Relationale Betrachtung Paralleler Prozesse}},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 1994,
  type = {Diplomarbeit},
  note = {UniBwM-ID 33/94, betreut von Gunther Schmidt und Michael Winter},
  WKloc = {B-0084},
  bibliographies = {RelMiCS}
}

@PhDThesis{Rietman-1995,
  author = {Rietman, Frans Johan},
  title = {A Relational Calculus for the Design of Distributed Algorithms},
  school = {Dept.\null{} of Computing Science, Utrecht Univ.\null{}},
  year = 1995,
  bibliographies = {RelMiCS}
}

@Article{Riguet-1948,
  author = {Jacques Riguet},
  title = {Relations Binaires, Fermetures, Correspondances de {Galois}},
  journal = BUFRANCE,
  volume = 76,
  year = 1948,
  pages = {114--155},
  URL = {https://eudml.org/doc/86820},
  bibliographies = {RelMiCS},
  annote = {source for ``noyau'', ``noy''}
}

@Article{Riguet-1950,
  author = {Jacques Riguet},
  title = {Quelques propri\'et\'es des relations difonctionnelles},
  journal = CRPARIS,
  year = 1950,
  volume = 230,
  pages = {1999--2000},
  bibliographies = {RelMiCS}
}

@Article{Riguet-1954,
  author = {Jacques Riguet},
  title = {Sur l'Extension de Calcul des Relations Binaires au
		Calcul des Matrices a Elements dans une Algebre de Boole Complete},
  journal = CRPARIS,
  volume = 238,
  year = 1954,
  pages = {2382--2385},
  bibliographies = {RelMiCS}
}

@Article{Riguet-1998,
  title = {Deducibility and Exactness},
  author = {Jacques Riguet},
  pages = {341--347},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {What I intend to show in this short paper is how one can
      translate in relational terms the concepts of deducibility and
      exactness which are the result of a sequence of works on homology
      theory and algebraic topology. As we shall see, we shall obtain as a
      final product the possibility to associate to an arbitrary binary
      relation $R$ a difunctional relation $\difun{R}$ contained in $R$, in
      contrast with the difunctional closure of $R$ which is larger that
      $R$. In \cite{Rig49} we have built from a given Ferrers relation $R$
      the relation (I am using the notation $S R$ for the result of the
      composition of the relations defined as
      $\setof{\pair{x}{z}}{\pair{x}{y} \in R \wedge \pair{y}{z} \in S}$,
      whereas in \cite{Schmidt-Stroehlein-1993} are using the reverse
      notation $R S$. I also use the symbol $\dif$ for denoting the
      operation of Boolean difference. Give an arbitrary relation $R$, I
      denote by $\rect{R}$ the rectangular closure of $R$: $\rect{R} =
      \dom{R} \times \cod{R}$. By $\subst{R}$ I denote the substratction of
      $R$ from its rectangular closure: $\subst{R} = \rect{R} \dif R$.
      Obviously, $\conv{\(\subst{R}\)} = \subst{\(\conv{R}\)}$.) $R\ \dif\
      R\subst{\(\conv{R}\)}R$ and proved its difunctionality, but in fact,
      as already noticed by Schmidt and Str\"ohlein (\cite{SS93} p.~78
      Prop.~4.4.14) $R\ \dif\ R\subst{\(\conv{R}\)}R$ is difunctional even
      when $R$ is arbitrary. I shall show that, in fact, $R\ \dif\
      R\subst{\(\conv{R}\)}R$ and $\difun{R}$ are identical. It is
      important to notice that the construction used here for the
      definition of $\difun{R}$ is made without using the Boolean
      difference operation.},
  bibliographies = {RelMiCS}
}

@TechReport{Rijke-1992,
  author = {Maarten de Rijke},
  note = {To appear in  J.\null{} Philos.\null{} Logic},
  type = {CSLI Research Report},
  institution = {Stanford Univ.},
  number = {92-170},
  title = {A System of Dynamic Modal Logic},
  year = 1992,
  bibliographies = {RelMiCS}
}

@PhDThesis{Rijke-1993,
  author = {Maarten de Rijke},
  school = {ILLC Dissertation series 1993--4},
  title = {Extending Modal Logic},
  year = 1993,
  bibliographies = {RelMiCS}
}

@InCollection{Rijke-1994,
  author = {Maarten de Rijke},
  address = {Cambridge, MA},
  booktitle = {Logic and Information Flow},
  editor = {Eijck, J. van and Visser, A.},
  pages = {170--195},
  publisher = MIT_P,
  title = {Meeting some neighbours},
  year = 1994,
  bibliographies = {RelMiCS}
}

@TechReport{Rijke-1994a,
  author = {Maarten de Rijke},
  note = {To appear in J.\null{}  Logic Lang.\null{} Inform.},
  title = {The logic of {Peirce} algebras},
  institution = {CWI, Amsterdam},
  number = {CS-R9467},
  year = 1994,
  bibliographies = {RelMiCS}
}

@Misc{Rijmen-Preneel-199X,
  author = {Vincent Rijmen and Bart Preneel},
  title = {A Family of Trapdoor Ciphers},
  year = {199?},
  WKloc = {A-0775}
}

@InProceedings{Rinard-2001,
  author = {Martin Rinard},
  title = {Analysis of Multithreaded Programs},
  crossref = {SAS2001},
  pages = {1--19}
}

@InProceedings{Ringeissen-1994,
  author = {C. Ringeissen},
  title = {Combination of Matching Algorithms},
  crossref = {STACS1994},
  pages = {187--200},
  abstract = {This paper addresses the problem of systematically
		  building a matching algorithm for the union of two
		  equational theories. $\ldots$}
}

@InProceedings{Rissanen-1978,
  author = {J. Rissanen},
  title = {Theory of Relations for Databases -- A Tutorial Survey},
  booktitle = {Mathematical Foundations of Computer Science 1978,
      Proc.\null{} of {$7^{th}$} Sympos.\null{} on Mathematical Foundations
      of Computer Science},
  series = LNCS,
  volume = 64,
  publisher = Springer,
  year = 1978,
  OPTaddress = {Poland},
  pages = {537--551},
  bibliographies = {RelMiCS}
}

@InProceedings{Ritter-1993,
  author = {E. Ritter},
  title = {Normalisation for Typed Lambda Calculi with Explicit
		  Substitutions},
  crossref = {CSL93},
  pages = {295--304},
  WKloc = {A-0342},
  abstract = {$\ldots$}
}

@InProceedings{Ritter-dePaiva-1997,
  author = {Eike Ritter and de paiva, Valeria},
  title = {On Explicit Substitutions and Names},
  crossref = {ICALP1997},
  UniBwM = {INF700/Z5792-24},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0797}
}

@Misc{Robbins-Robbins-1998,
  author = {Kay Robbins and Steve Robbins},
  title = {Empirical Exploration in undergraduate Operating Systmes},
  howpublished = {Draft},
  month = AUG,
  year = 1998,
  WKloc = {A-1397},
  bibliographies = {SE3B}
}

@Book{Robbins-Robbins-2001,
  author = {Kay Robbins and Steve Robbins},
  title = {Practical UNIX Programming:
                  A Guide to Concurrency, Communication, and Multithreading},
  publisher = {Prentice Hall},
  year = 2001,
  ISBN = {0-13-443706-3},
  WKloc = {owned},
  bibliographies = {SE3B},
  URL = {http://vip.cs.utsa.edu/pup/}
}

@Article{Robinson-1965,
  author = {J. Alan Robinson},
  title = {A Machine-Oriented Logic Based on the Resolution Principle},
  year = 1965,
  volume = 12,
  pages = {23--41},
  number = 1,
  month = JAN,
  journal = JACM,
  WKloc = {A-0094},
  bibliographies = {RelMiCS},
  abstract = {Theorem-proving on the computer, using procedures
		  based on the fundamental theorem of Herbrand
		  concerning the first-order predicate calculus, is
		  examined with a view towards improving the
		  efficiency and widening the range of practical
		  applicability of these procedures. A close analysis
		  of the process of substitution (of terms for
		  variables), and the process of truth-functional
		  analysis of the results of such substitutions,
		  reveals that both processes can be combined into a
		  single new process (called {\em resolution}),
		  iterating which is vastly more efficient than the
		  older cyclic procedures consisting of substitution
		  stages alternating with truth-functional analysis
		  stages.

                  The theory of the resolution process is presented in
		  the form of a system of first-order logic with just
		  one inference principle (the resolution principle).
		  The completeness of the system is proved; the
		  simplest proof-procedure based on the system is then
		  the direct implementation of the proof of
		  completeness. However, this procedure is quite
		  inefficient, and the paper concludes with a
		  discussion of several principles (called search
		  principles) which are applicable tho the design of
		  efficient proof-procedures employing resolution as
		  the basic logical process.}
}

@Article{Robinson-Rosolini-1988,
  author = {E. Robinson and G. Rosolini},
  title = {Categories of Partial Maps},
  pages = {95--130},
  journal = {Information and Computation},
  month = nov,
  year = 1988,
  volume = 79,
  number = 2,
  DOIURL = "http://dx.doi.org/10.1016/0890-5401(88)90034-X",
  DOI = "10.1016/0890-5401(88)90034-X",
  DirectURL = "http://www.sciencedirect.com/science/article/pii/089054018890034X",
  WKloc = {doc/pap/BIB},
  abstract = {This paper attempts to reconcile the various abstract
                  notions of ``category of partial maps'' which appear
                  in the literature. First a particular algebraic
                  theory ($p$-categories) is introduced and a
                  representation theorem proved. This gives the
                  authors a coherent framework in which to place the
                  various other definitions. Both algebraic theories
                  and theories which make essential use of the
                  poset-enriched structure of partial maps are
                  discussed. Proofs of equivalence are given where
                  possible and counterexamples where known. The paper
                  concludes with brief sections on the representation
                  of partial maps and on partial algebras.}
}

@InProceedings{Robinson-Rosolini-1990,
  author = {Robinson, Edmund and Rosolini, Giuseppe},
  title = {Polymorphism, Set Theory, and Call-by-Value},
  crossref = {LICS5},
  pages = {12--18},
  abstract = {We discuss set-theoretic (or rather the more general
		  topos-theoretic) models of polymorphic
		  lambda-calculi under the assumption that the
		  datatypes of the language are to be interpreted as
		  sets, and the operations as partial functions.  We
		  show that it is not possible to obtain a model in
		  which function spaces are interpreted by the full
		  partial function space, but that it is nevertheless
		  possible to have models which incorporate a usefully
		  large class of partial functions.}
}

@InProceedings{Robinson-Rosolini-1994,
  title = {Reflexive Graphs and Parametric Polymorphism},
  author = {E. P. Robinson and G. Rosolini},
  pages = {364--371},
  crossref = {LICS9},
  WKloc = {A-0375},
  bibliographies = {LogRel},
  abstract = {The pioneering work on relational parametricity for the
		  second order lambda calculus was done by John
		  Reynolds under the assumption of the existence of
		  set-based models, and subsequently reformulated by
		  him, in conjunction with his student Ma, using the
		  technology of PL-categories.  The aim of this paper
		  is to use the different technology of internal
		  category theory to re-examine Ma and Reynolds'
		  definitions.  Apart from clarifying some of their
		  constructions, this view enables us to prove that if
		  we start with a non-parametric model which is left
		  exact and which satisfies a completeness condition
		  corresponding to Ma and Reynolds' ``suitability for
		  polymorphism,'' then we can recover a parametric
		  model with the same category of closed types.  This
		  implies, for example, that any suitably complete
		  model (such as the PER model) has a parametric counterpart.}
}

@InProceedings{Rock-Stephan-Wolpers-Balser-Reif-Scheer-1999,
  author = {G. Rock and W. Stephan and A. Wolpers and M. Balser and W. Reif and S. Scheer},
  title = {Structured Formal Development in {VSE II}: The {Robertino} Case Study},
  booktitle = {{Sicherheit und Zuverl\"{a}ssigkeit software-basierter Systeme}},
  year = 1999,
  editor = {F. Saglietti and W. Goerigk},
  number = {ISBN 3-00-004872-3},
  publisher = {ISTec},
  bibliographies = {SpecTech},
  WKloc = {A-1056}
}

@Article{Roehrich-1987,
  author = {Johannes R{\"o}hrich},
  title = {Graph Attribution with Multiple Attribute Grammars},
  journal = {ACM SIGPLAN Notices},
  volume = 22,
  number = 11,
  pages = {55--70},
  month = nov,
  year = 1987,
  abstract = {Attributed graphs are well suited to represent
                 structured sets of data objects. A multiple attribute
                 grammar, MTAG, consists of several attribute grammars
                 that describe an attributed graph as a union of
                 attributed trees. Thus, from an MTAG one can
                 automatically generate attribute evaluators for graphs.
                 Dependency analysis at generation time of the attribute
                 evaluator leads to fast attribute evaluation, and to
                 significant space savings due to attribute overlay.},
  keywords = {vari.GG graph attribution}
}

@InCollection{Roever-1972,
  author = {de Roever, Willem Paul},
  publisher = IRIA,
  title = {A Formalization of Various Parameter Mechanisms as
		Products of Relations within a Calculus of Recursive Program Schemes},
  booktitle = {Th\'eorie des Algorithmes, des Languages et de la
		Programmation, S\'eminaires IRIA},
  year = 1972,
  pages = {55--88},
  bibliographies = {RelMiCS}
}

@Book{Roever-1976,
  author = {de Roever, Willem Paul},
  title = {Recursive Program Schemes: Semantics and Proof Theory},
  series = MCTracts,
  volume = 70,
  note = {JSL XL 658},
  publisher = MC,
  address = {Amsterdam},
  year = 1976,
  pages = {vi+112},
  bibliographies = {RelMiCS},
  WKloc = {B-0103}
}

@TechReport{Rohou-Bodin-Seznec-LeFol-Charot-Raimbault-1996,
    author = "Erven Rohou and Francois Bodin and Andre Seznec and Le Fol, Gwendal and Francois Charot and Frederic Raimbault",
    title = "{SALTO}: System for Assembly-Language Transformation and Optimization",
    number = "RR-2980",
    pages = "27 p.",
  year = {1996},
  institution = {INRIA},
  CiteSeer = "citeseer.ist.psu.edu/rohou96salto.html",
  bibliographies = {Coconut}
}

@InProceedings{Romanenko-1990,
  author = {Sergei A. Romanenko},
  title = {Arity Raiser and its Use in Program Specialization},
  crossref = {ESOP1990},
  pages = {341--360},
  abstract = {Experiments on generating compilers by specializing
		  specializers with respect to interpreters have shown
		  that the compilers thus obtained have a natural
		  structure only if the specializer does {\em variable
		  splitting}. Variable splitting can result in a
		  residual program using several variables to
		  represent the values of a single variable of the
		  original program. In the case of functional
		  programming variable splitting is done by raising
		  the arities of functions. The paper describes the
		  structure and principles of operation of an arity
		  raiser dealing with programs in a subset of pure Lisp.},
  keywords = {arity raiser, compiler generator, partial
		  evaluation, retyping, specializer, variable splitting},
  annote = {variable splitting roughly corresponds to currying}
}

@InProceedings{Romanescu=Lebeck-Sorin-2010,
  author = 	 {Bogdan F. Romanescu and Alvin R. Lebeck and Daniel J. Sorin},
  title = 	 {Specifying and Dynamically Verifying Address Translation-Aware Memory Consistency},
  crossref =	 {ASPLOS2010},
  pages =	 {323--334}
}

@Article{RonchiDellaRocca-1988,
  WKloc = {A-0050},
  abstract = {The intersection type discipline for the $\lambda$-calculus
	(ITD) is an extension of the classical functionality theory of Curry.
	In the ITD a term satisfying a given property has a principal type
	scheme in an extended meaning, i.e., there is a type scheme deducible
	for it from which all and only the type schemes deducible for it are
	reachable, byu means of suitable operations. The problem of finding
	the principle type scheme for a term, if it exists, is semidecidable.
	In the paper a procedure is shown, building the principle type scheme
	of a term through the construction of the most general unifier for
	intersection type schemes.},
  year = 1988,
  volume = 59,
  title = {Principal Type Scheme and Unification for Intersection Type Discipline},
  pages = {181-209},
  journal = {Theoretical Computer Science},
  author = {Ronchi della Rocca, Simona}
}

@Book{Roos-Terlaky-Vial-1997,
  author = {C. Roos and T. Terlaky and J.-Ph. Vial},
  title = {Interior Point Approach to Linear Optimization: Theory and Algorithms},
  publisher = {John Wiley & Sons},
  year = 1997,
  address = {New York},
  note = {(second print 1998)},
  WKloc = {A-1092: Chapter 1}
}

@Article{Roper-1980,
  author = {P. Roper},
  title = {Intervals and Tenses},
  journal = JPHIL,
  volume = 9,
  year = 1980,
  bibliographies = {RelMiCS}
}

@InCollection{Roscoe-1994a,
  author = {A. W. Roscoe},
  title = {Model-Checking {CSP}},
  crossref = {Roscoe-1994},
  pages = {353--378},
  chapter = 21,
  OPTnote = {},
  OPTannote = {}
}

@InCollection{Rose-1993,
  author = {K.H. Rose},
  title = {Graph-based Operational Semantics of a Lazy
		  Functional Language},
  pages = {303--316},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  chapter = 22,
  contents = {1 Introduction
                  2 Term graph rewriting
                  3 Graph representation of BAWL programs
                  4 GOS---graph operational semantics
                  5 Discussion}
}

@Book{Rosen-2007,
  author =    {Kenneth H. Rosen},
  title =     {Discrete Mathematics and Its Applications},
  publisher = {McGraw-Hill},
  year =      2007,
  edition =   {6th},
  ISBN =      {978-0-07-288008-3},
  LoC =       {QA39.3.R67 2007}
}

@Article{Rosenblatt-1963,
  author = {D. Rosenblatt},
  title = {On the Graphs of Finite Idempotent Boolean Relation Matrices},
  journal = JRES,
  volume = {67B},
  year = 1963,
  pages = {249--256},
  bibliographies = {RelMiCS}
}

@Unpublished{Rossberg-Sulzmann-2002,
  author =       {Andreas Rossberg and Martin Sulzmann},
  title =        {Beyond Type Classes},
  year =      2002,
  WKloc = {doc/pap/BIB},
  note =      {unfinished (incomplete BibTeX run); edcomms on page 13; \fbox{apparently unpublished}},
  annote =    {Chameleon}
}

@Book{Rossing-2007,
  editor =	 {Rossing, Thomas D.},
  title = 	 {Springer Handbook of Acoustics},
  publisher = 	 {Springer},
  year = 	 2007,
  URL = 	 {http://springer.r.delivery.net/r/r?2.1.Ee.2Tp.1W6Vup.BvS5oi..M.DaS2.2p%2a6.DEEae000},
  series =	 {Springer Reference},
  ISBN = 	 {978-0-387-30446-5},
  bibliographies = {Vati}
}

@InProceedings{Rosu-2001,
  author = 	 {Grigore Ro{\c{s}}u},
  title = 	 {Complete Categorical Equational Deduction},
  crossref =  {CSL2001},
  pages = 	 {528--538},
  OPTnote = 	 {},
  WKloc = {doc/pap/BIB},
  abstract = {A categorical four-rule deduction system for
      equational logics is presented. We show that under reasonable
      finiteness requirements this system is complete with respect to
      equational satisfaction ed as injectivity. The generality of the
      presented framework allows one to derive conditional equations
      as well at no extra cost. In fact, our deduction system is also
      complete for conditional equations, a new result at the author's
      knowledge.}
}

@Article{Roth-1975,
  author = {R. L. Roth},
  title = {Character and Conjugacy Class Hypergroups of a Finite Group},
  journal = ANDIMAT,
  volume = 105,
  year = 1975,
  pages = {295--311},
  bibliographies = {RelMiCS}
}

@InProceedings{Rothamel-LiuYanhong-Heitmeyer-Leonard-2006,
 author = {Tom Rothamel and Yanhong A. Liu and Constance L. Heitmeyer and Elizabeth I. Leonard},
 title = {Generating optimized code from SCR specifications},
 booktitle = {LCTES '06: Proceedings of the 2006 ACM SIGPLAN/SIGBED conference on Language, compilers and tool support for embedded systems},
 year = {2006},
 isbn = {1-59593-362-X},
 pages = {135--144},
 location = {Ottawa, Ontario, Canada},
  URL = {http://portal.acm.org/citation.cfm?id=1134670},
 doi = {http://doi.acm.org/10.1145/1134650.1134670},
 publisher = {ACM Press},
 address = {New York, NY, USA},
 bibliographies = {Tables},
  abstract = {A promising trend in software development is the
       increasing adoption of model-driven design. In this approach, a
       developer first constructs an abstract model of the required
       program behavior in a language, such as Statecharts or
       Stateflow, and then uses a code generator to automatically
       transform the model into an executable program. This approach
       has many advantages---typically, a model is not only more
       concise than code and hence more understandable, it is also
       more amenable to mechanized analysis. Moreover, automatic
       generation of code from a model usually produces code with
       fewer errors than hand-crafted code.One serious problem,
       however, is that a code generator may produce inefficient
       code. To address this problem, this paper describes a method
       for generating efficient code from SCR (Software Cost
       Reduction) specifications. While the SCR tabular notation and
       tools have been used successfully to specify, simulate, and
       verify numerous embedded systems, until now SCR has lacked an
       automated method for generating optimized code. This paper
       describes an efficient method for automatic code generation
       from SCR specifications, together with an implementation and an
       experimental evaluation. The method first synthesizes an
       execution-flow graph from the specification, then applies three
       optimizations to the graph, namely, input slicing,
       simplification, and output slicing, and then automatically
       generates code from the optimized graph. Experiments on seven
       benchmarks demonstrate that the method produces significant
       performance improvements in code generated from large
       specifications. Moreover, code generation is relatively fast,
       and the code produced is relatively compact.}
}

@Manual{Roundy-2004a,
  title = 	 {{Darcs 0.9.15}, {David}'s Advanced Revision Control System},
  author = 	 {David Roundy},
  URL = 	 {http://abridgegame.org/repos/darcs},
  month = 	 JAN # {~9},
  year = 	 2004,
  WKloc = 	 {A-1525}
}

@Manual{Roundy-2006a,
  title = 	 {{Darcs 1.0.6pre1}, {David}'s Advanced Revision Control System},
  author = 	 {David Roundy},
  URL = 	 {http://abridgegame.org/repos/darcs},
  month = 	 JAN # {~8},
  year = 	 2006,
  WKloc = 	 {A-1648}
}

@TechReport{Roussel-Parigot-Jourdan-1995,
  author = {Gilles Roussel and Didier Parigot and Martin Jourdan},
  title = {Static and Dynamic Coupling Attribute Evaluators},
  year = 1995,
  number = 2670,
  month = oct,
  type = {Rapport de recherche},
  institution = {INRIA},
  WKloc = {B-0051},
  note = {{\tt
                 ftp://ftp.inria.fr/INRIA/publication/RR/RR-2670.ps.gz}},
  URL = {ftp://ftp.inria.fr/INRIA/publication/RR/RR-2670.ps.gz},
  abstract = {Several years ago, the notion of attribute coupled
                 grammars was introduced by Ganzinger and Giegerich,
                 together with their descriptional composition. The
                 latter works essentially at the specification level,
                 i.e.,\ it produces an attribute grammar which specifies
                 the composition of two attribute coupled grammars. We
                 introduce a new approach to this composition of
                 attribute coupled grammars. It no longer works at the
                 specification level but rather at the evaluator level.
                 It produces a special kind of attribute evaluator,
                 called {\em coupling evaluator}. We present both a
                 static version and a dynamic version of coupling
                 evaluators. Both versions retain the good property of
                 descriptional composition that intermediate trees are
                 not physically constructed. In addition---and this is
                 the main advantage of our approach, compared with
                 descriptional composition---, it is possible to build
                 separately the dynamic coupling evaluator of each
                 attribute coupled grammar; in other words we achieve
                 real {\em separate compilation\/} of AG modules.}
}

@Article{Rovatti-Fantuzzi-1998,
  author = 	 {Riccardo Rovatti and Cesare Fantuzzi},
  title = 	 {Quantized norms and generalized relational
     composition on dense universes},
  journal = 	 {International Journal of Approximate Reasoning},
  year = 	 1998,
  volume =	 19,
  pages =	 {299--314},
  keywords = {Norm-based connectives; Repetitive information;
              Relational composition},
  WKloc = 	 {doc/pap/BIB},
  bibliography = {RelMiCS},
  abstract = { Decision-making procedure based on fuzzy relational
     composition may require that the involved connectives are not
     idempotent to handle significant information properly. Yet,
     non-idempotence may result in meaningless inference when the
     universes of discourse are dense. Here we propose a novel kind of
     connective and a formal definition of relational composition for
     dense universes of discourse which allow the aggregation of
     different path of reasoning confirming each other without
     incurring in total loss of information.}
}

@Article{Royce-1905,
  author = {Josiah Royce},
  title = {The Relation of the Principles of Logic to the
		Foundations of Geometry},
  journal = TRAMS,
  volume = 6,
  year = 1905,
  pages = {353--415},
  bibliographies = {RelMiCS}
}

@InProceedings{Ruess-Pfeifer-vonHenke-1995,
  author =    "Harald Rue{\ss}, Holger Pfeifer, Friedrich W. von Henke",
  title =     "Formalization and Reasoning in a Reflective Architecture",
  editor =    "M. Ibrahim, P.  Cointe, F. Cummins, F. Giunchiglia, and
               J. Malenfant",
  year =      1995,
  booktitle = "IJCAI 1995 Workshop on Reflection and Meta Level Architecture
               and their Application in AI",
  address =   "Montreal, Canada",
  month =     "August",
  URL = {http://www.informatik.uni-ulm.de/ki/Papers/ijcai95-ws-reflection.html},
  WKloc = {A-1571, doc/pap/BIB},
  abstract = {This paper is concerned with developing a reflective
      architecture for formalizing and reasoning about entities that
      occur in the process of software development, such as
      specifications, theorems, programs, and proofs. The starting
      point is a syntactic extension of the type theory ECC. An
      encoding of this object calculus within itself comprises the
      meta-level, and reflection principles are provided for switching
      between different levels. These reflection principles are used
      to mix object- and meta-level reasoning, to generate "standard"
      units by executing meta-operators, and to apply formal tactics
      that allow for abstraction from the base logic.}
}

@Unpublished{Ruksenas-2000,
  author = {Rimvydas Ruk{\hacek{s}}{\.{e}}nas},
  title = {Compositional Refinement in the Synthesis of Asynchronous Circuits},
  note = {unpublished draft},
  month = AUG,
  year = 2000,
  annote = {submitted to Proc. FMTOOLS 2000},
  WKloc = {A-1031, doc/pap/BIB/Ruksenas-2000.ps.gz}
}

@Misc{Runciman-199X,
  author = {Colin Runciman},
  title = {{TIP} in Haskell --- Another Exercise in Functional Programming},
  year = {199?},
  WKloc = {A-0760}
}

@TechReport{Runciman-Wakeling-1992,
  author = {Colin Runciman and David Wakeling},
  title = {Heap Profiling of Lazy Functional Programs},
  year = 1992,
  number = 172,
  note = {held at Dagstuhl 1992 Seminar},
  month = APR,
  institution = {University of York},
  WKloc = {B-0011},
  abstract = {We describe the design, implementation and use of a new kind
      of profiling tool that yields valuable information about the memory
      use of lazy functional programs. The tool has two parts: a modified
      functional language implementation which generates profiling
      information during the execution of programs, and a separate program
      which converts this information to graphical form. With the aid of
      profile graphs, one can make alterations to a functional program
      which dramatically reduce its space consumption. We demonstrate this
      in the case of a genuine example --- the first to which the tool has
      been applied --- for which the results are strikingly successful.},
  bibliographies = {RelMiCS}
}

@Book{Runciman-Wakeling-1994,
  editor = {Colin Runciman and David Wakeling},
  title = {Applications of Functional Programming},
  year = 1994,
  publisher = {UCL Press},
  ISBN = {1-85728-377-5},
  UniBwM = {Mag YA10322 --- HB Kahl},
  McMaster = {QA 76.62 .A67 1995},
  contributors = {Iain Checkland, Sandra Foubister, Phil Grant,
                  Kevin Hammond, Keith Hanna, Gareth Howells,
                  Simon Peyton Jones, Colin Runciman, Paul Sanders,
                  John Sharp, David Wakeling, Mike Webster, Xiaoming Zhang},
  bibliographies = {FP}
}

@Article{Ruparelia-2010,
  author = {Ruparelia, Nayan B.},
  title = {Software Development Lifecycle Models},
  journal = {SIGSOFT Softw.\null{} Eng.\null{} Notes},
  volume = {35},
  number = {3},
  year = {2010},
  issn = {0163-5948},
  pages = {8--13},
  DOIURL = {http://doi.acm.org/10.1145/1764810.1764814},
  DOI = {10.1145/1764810.1764814},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {waterfall model, V-model. spiral model},
  bibliographies = {SE},
  WKloc = {doc/pap/BIB}
}

@Article{Rupert-1997,
  author = 	 {C. P. Rupert},
  title = 	 {Slightly Commutative Kleene Semigroups},
  journal = 	 {Journal of Algebra},
  year = 	 1997,
  volume =	 192,
  pages =	 {235--260},
  WKloc = 	 {A-1583, doc/pap/BIB}
}

@Misc{Rushby-1995,
  author = {John Rushby},
  title = {Model Checking and Other Ways of Automating Formal Methods},
  year = 1995,
  note = {Position paper for panel on Meodel Checking for Concurrent Programs, Software Quality Week, San Francisco, May/June 1995},
  WKloc = {A-0974}
}

@InProceedings{Rushby-1999,
  author = {John Rushby},
  title = {Integrated Formal Verification: Using Model
      Checking With Automated Abstraction, Invariant Generation, and
      Theorem Proving},
  booktitle = {Theoretical and Practical Aspects of SPIN Model Checking:
          5th and 6th International SPIN Workshops},
  editor = {D. Dams and R. Gerth and S. Leue and M. Massink},
  address = {Trento, Italy, and Toulouse, France},
  month = jul # { and } # sep,
  year = 1999,
  pages = {1--11},
  publisher = {Springer-Verlag},
  series = {Lecture Notes in Computer Science},
  volume = 1680,
  bibliographies = {SpecTech, SeminarWT2000}
}

@Article{Russell-1901,
  author = {Bertrand Russell},
  title = {Sur la logique des relations avec des applications \`a la the\'eorie des s\'eries},
  journal = Rivista_di_Matematica,
  year = 1901,
  volume = {VII},
  pages = {115--148},
  bibliographies = {RelMiCS},
  note = {{English} as Paper \textbf{8}:
   \emph{The Logic of Relations with Some Applications to the Theory of Series}
   in \cite{Russell_CP3_1993}},
  WKloc = {A-1338, A-1339},
  McMaster = {Russel's Library No. 66 QA 1 .R5 1900/01},
  abstract = {La logique des relatives,
              tell qu'on la trouve chez MM Peirce et Schr\"oder,
              est difficile et compliqu\'ee \'a un si haut degr\'e
              qu'il est possible de do\^uter de son utilit\'e. [$\ldots$]}
}

@Article{Russell-1902,
  author = {Bertrand Russell},
  title = {Th\'eorie g\'en\'erale des s\'eries bien-ordonn\'ees},
  journal = Rivista_di_Matematica,
  year = 1902,
  volume = {VIII},
  number = {1--2},
  pages = {12--43},
  bibliographies = {RelMiCS},
  WKloc = {A-1340, A-1341},
  note = {{English} as Paper \textbf{12}:
   \emph{General Theory of Well-Ordered Series}
   in \cite{Russell_CP3_1993}}
}

@Article{Russell-1908,
  author = 	 {Bertrand Russell},
  title = 	 {Mathematical Logic as Based on the Theory of Types},
  journal = 	 {American Journal of Mathematics},
  year = 	 1908,
  volume = 	 30,
  number = 	 3,
  pages = 	 {222--262}
}

@Misc{Russell-1902a,
  author = {Bertrand Russell},
  title = {On Likeness},
  howpublished = {Russell Archives Manuscript RA 230.030830},
  year = 1902,
  bibliographies = {RelMiCS},
  WKloc = {A-1342},
  note = {Published as Paper \textbf{15} in \cite{Russell_CP3_1993},
          pp.~437--451}
}

@Book{Russell-1919,
  author = {Bertrand Russell},
  title = {Introduction to Mathematical Philosophy},
  publisher = {Routledge},
  year = 1919,
  note = {Reprinted 1998},
  annote = {Ridha Khedri}
}

@Book{Russell-Whitehead-1935,
  author = {Russell, B. and Whitehead, A.N.},
  title = {Principia Mathematica.},
  publisher = CambridgeUP,
  year = 1935,
  edition = {third edition},
  bibliographies = {RelMiCS}
}

@PhDThesis{RussellD-2001,
  author = {Dan Russell},
  title = {{FAD: A Functional Analysis and Design Methodology}},
  month = JAN,
  year = 2001,
  pages = 307,
  keywords = {functional programming, software engineering, design, analysis, methodology, graphical, diagram},
  URL = {http://www.cs.ukc.ac.uk/pubs/2001/1152},
  school = {Computing Laboratory, University of Kent at Canterbury},
  type = {PhD thesis},
  WKloc = {doc/pap/BIB},
  bibliographies = {MathScheme},
  abstract = {This thesis presents the functional analysis and design
         methodology FAD. By functional we mean that it naturally
         supports software development within the
         functional programming paradigm (FP).

         Every popular methodology has a graphical modelling language
         which presents various pictorial representations of a system.
         FAD's modelling language provides the typical elements of
         functional programming, types and functions,
         plus elements to support modular development such as modules,
         subsystems and two forms of signature which specify and
         interface or a behavioural requirement.
         The language also includes relationships and associations
         between these elements, and provides simple representations
         of functional designs. The methodology has an
         integrated set of techniques which guide the development
         of an implementable solution from the
         deliverables of requirements engineering.
         FAD's data dictionary provides an organised repository
         for entities during and after development.

         The thesis thus provides a development medium which has hitherto
         been absent from the functional programming paradigm. }
}

@Article{RussellDM-Striz-Winograd-2005,
  author = 	 {Daniel M. Russell and Norbert A. Streitz and Terry Winograd},
  title = 	 {Building disappearing computers},
  journal = 	 CACM,
  year = 	 2005,
  URL = 	 {http://doi.acm.org/10.1145/1047671.1047702},
  volume =	 48,
  number =	 3,
  pages =	 {42--48},
  month =	 MAR,
  WKloc = 	 {doc/pap/BIB}
}

@InProceedings{Russi-Zompi-1990,
  author = {V. Russi and R. Zompi},
  title = {Graphical Object Oriented Executable Specification
		  for an Automation Oriented Paradigm of Software Development},
  crossref = {DISCO90},
  pages = {225--234},
  keywords = {Visual Programming},
  abstract = {This paper presents PROTOB, an object-oriented CASE
		  system based on high-level Petri nets called PROT
		  nets. $\ldots$}
}

@PhDThesis{Russo-1998,
  author = {Claudio V. Russo},
  title = {Types For Modules},
  school = {University of Edinburgh},
  year = 1998,
  type = {{LFCS thesis ECS-LFCS-98-389}},
  WKloc = {B-0058},
  URL = {http://www.dcs.ed.ac.uk/home/cvr/ECS-LFCS-98-389.html},
  abstract = {The programming language Standard ML is an amalgam of two,
      largely orthogonal, languages. The Core language expresses details of
      algorithms and data structures. The Modules language expresses the
      modular architecture of a software system. Both languages are
      statically typed, with their static and dynamic semantics specified
      by a formal definition.

      Over the past decade, Standard ML Modules has been the source of
      inspiration for much research into the type-theoretic foundations of
      modules languages. Despite these efforts, a proper type-theoretic
      understanding of its static semantics has remained elusive. In this
      thesis, we use Type Theory as a guideline to reformulate the
      unconventional static semantics of Modules, providing a basis for
      useful extensions to the Modules language.

      Our starting point is a stylised presentation of the existing static
      semantics of Modules, parameterised by an arbitrary Core language. We
      claim that the type-theoretic concepts underlying Modules are type
      parameterisation, type quantification and subtyping. We substantiate
      this claim by giving a provably equivalent semantics with an
      alternative, more type-theoretic presentation. In particular, we show
      that the notion of type generativity corresponds to existential
      quantification over types. In contrast to previous accounts, our
      analysis does not involve first-order dependent types.

      Our first extension generalises Modules to higher-order, allowing
      modules to take parameterised modules as arguments, and return them
      as results. We go beyond previous proposals for higher-order Modules
      by supporting a notion of type generativity. We give a sound and
      complete algorithm for type-checking higher-order Modules. Our second
      extension permits modules to be treated as first-class citizens of an
      ML-like Core language, greatly extending the range of computations on
      modules. Each extension arises from a natural generalisation of our
      type-theoretic semantics.

      This thesis also addresses two pragmatic concerns. First, we propose
      a simple approach to the separate compilation of Modules, which is
      adequate in practice but has theoretical limitations. We suggest a
      modified syntax and semantics that alleviates these limitations.
      Second, we study the type inference problem posed by uniting our
      extensions to higher-order and first-class modules with an
      implicitly-typed, ML-like Core language. We present a hybrid type
      inference algorithm that integrates the classical algorithm for ML
      with the type-checking algorithm for Modules.}
}

@InProceedings{Russo-2000,
  author = {Claudio V. Russo},
  title = {First-Class Structures for Standard ML},
  crossref = {ESOP2000},
  pages = {336--350},
  authorsAddress = {cvr21@cl.cam.ac.uk},
  URL = {http://link.springer.de/link/service/series/0558/bibs/1782/17820336.htm},
  WKloc = {doc/pap/BIB},
  abstract = {Standard ML is a statically typed programming language that
              is suited for the construction of both small and large programs.
              ``Programming in the small'' is captured by Standard ML's
              Core language. ``Programming in the large'' is captured by
              Standard ML's Modules language that provides constructs for
              organising related Core language definitions into
              self-contained modules with descriptive interfaces.
              While the Core is used to express details of algorithms and
              data structures, Modules is used to express the
              overall architecture of a software system.
              The Modules and Core languages are stratified in the sense
              that modules may not be manipulated as ordinary values of the
              Core. This is a limitation, since it means that the
              architecture of a program cannot be reconfigured according to
              run-time demands. We propose a novel extension of the language
              that allows modules to be manipulated as first-class values
              of the Core language. The extension greatly extends the
              expressive power of the language and has been shown to be
              compatible with both Core type inference and a
              separate extension to higher-order modules.}
}

@InProceedings{Rutten-Turi-1992,
  author = {Jan J.M.M. Rutten and Daniele Turi},
  title = {On the Foundations of Final Semantics: Non-Standard
		  Sets, Metric Spaces, Partial Orders},
  crossref = {REX92},
  pages = {477--530},
  abstract = {Canonical solutions of domain equations are shown to
		  be {\em final coalgebras}, not only in a category of
		  non-standard sets (as already known), but also in
		  categories of metric spaces and partial
		  orders. Coalgebras are simple categorical structures
		  generalizing the notion of post-fixed point. They
		  are also used here for giving a new comprehensive
		  presentation of the (still) non-standard theory of
		  {\em non-well-founded sets} (as non-standard sets
		  are usually called).

                  This paper is meant to provide a basis to a more
		  general project aiming at a full exploitation of the
		  finality of the domains in the semantics of
		  programming languages --- concurrent ones among
		  them. Such a {\em final semantics} enjoys uniformity
		  and generality. For instance, semantic observational
		  equivalences like bisimulation can be derived as
		  instances of a single 'coalgebraic' definition
		  (introduced elsewhere), which is parametric of the
		  functor appearing in the domain equation. Some
		  properties of this general form of equivalence are
		  also studied in this paper.},
  keywords = {final semantics, category, functor, coalgebra,
		  domain equation, fixed point, non-well-founded sets,
		  non-standard set theory,metric spaces, partial
		  orders, concurrency, ($F$-)bisimulation, ordered
		  ($F$-)bisimulation}
}

@Article{Rutten-2000,
  author =       {Jan J.M.M. Rutten},
  title =        {Universal coalgebra: a theory of systems },
  journal =      TCS,
  year =         2000,
  volume =    249,
  number =    1,
  pages =     {3--80},
  DOI =          {10.1016/S0304-3975(00)00056-6},
  DOIURL =       {http://dx.doi.org/10.1016/S0304-3975(00)00056-6},
  WKloc =      {doc/pap/BIB},
  abstract =    {In the semantics of programming, finite data types such as finite lists, have traditionally been modelled by initial algebras. Later final \emph{coalgebras} were used in order to deal with \emph{infinite} data types. Coalgebras, which are the dual of algebras, turned out to be suited, moreover, as models for certain types of automata and more generally, for (transition and dynamical) \emph{systems}. An important property of initial algebras is that they satisfy the familiar principle of induction. Such a principle was missing for coalgebras until the work of Aczel (Non-Well-Founded sets, CSLI Leethre Notes, Vol. 14, center for the study of Languages and information, Stanford, 1988) on a theory of non-wellfounded sets, in which he introduced a proof principle nowadays called \emph{coinduction}. It was formulated in terms of \emph{bisimulation}, a notion originally stemming from the world of concurrent programming languages. Using the notion of \emph{coalgebra homomorphism}, the definition of bisimulation on coalgebras can be shown to be formally dual to that of congruence on algebras. Thus, the three basic notions of universal algebra: algebra, homomorphism of algebras, and congruence, turn out to correspond to coalgebra, homomorphism of coalgebras, and bisimulation, respectively. In this paper, the latter are taken as the basic ingredients of a theory called \emph{universal coalgebra}. Some standard results from universal algebra are reformulated (using the aforementioned correspondence) and proved for a large class of coalgebras, leading to a series of results on, e.g., the lattices of subcoalgebras and bisimulations, simple coalgebras and coinduction, and a covariety theorem for coalgebras similar to Birkhoff's variety theorem.}
}

@InProceedings{Ryan-Fiadeiro-Maibaum-1991,
  title = {Sharing Actions and Attributes in Modal Action Logic},
  author = {Mark Ryan and Jos\'e Fiadeiro and Tom Maibaum},
  pages = {569--593},
  crossref = {TACS1991},
  abstract = {Distributed systems may be specified in Structured Modal
		  Action Logic by decomposing them into {\em agents\/}
		  which interact by sharing {\em attributes\/}
		  (memory) as well as {\em actions}.

                  In the formalism we describe, specification texts
		  denote theories, and theories denote the set of
		  semantic structures which satisfy them. The semantic
		  structures are Kripke models, as is usual for modal
		  logic. The ``possible worlds'' in a Kripke model are
		  the states of the agent, and there is a separate
		  relation on the set of states for each action term.

                  Agents potentially share actions as well as
		  attributes in a way controlled by locality
		  annotations in the specification texts.  These
		  become locality axioms in the logical theories the
		  texts denote. These locality axioms provide a
		  refined way of circumscribing the effects of actions.

                  Safety and liveness conditions are expressed
		  (implicitly) by deontic axioms, which impose
		  obligations and deny permissions on actions. We show
		  that ``deontic defaults'' exist so that the
		  specifier need not explicitly grant permissions or
		  avoid obligations in situations where normative
		  behaviour is not an issue.}
}

@InProceedings{Rydeheard-Burstall-1986,
  author = {D.E. Rydeheard and R.M. Burstall},
  title = 	 {A categorical unification algorithm},
  booktitle = {Proc.\null{} Summer Workshop on Category Theory and Computer Programming 1985},
  pages = 	 {493--505},
  year = 	 1986,
  volume = 	 240,
  series = 	 LNCS,
  URL = {http://www.springerlink.com/content/p3x81n36q2777571/},
  DOI = {10.1007/3-540-17162-2_139},
  WKloc = {doc/pap/BIB},
  publisher = Springer,
  bibliographies = {RelMiCS}
}

@Book{Rydeheard-Burstall-1988,
  author = {D.E. Rydeheard and R.M. Burstall},
  title = {Computational Category Theory},
  publisher = {Prentice Hall},
  year = 1988,
  McMaster = {QA 169 .R93 1988},
  annote = {Used in \cite{Schied-1992,Barthelmann-Schied-1993},
                  treats graphs via comma categories.}
}

@InProceedings{Rydeheard-Stell-1987,
  author = 	 {D. E. Rydeheard and J. G. Stell},
  title = 	 { Foundations of equational deduction: A categorical treatment of equational proofs and unification algorithms},
  DOI = {10.1007/3-540-18508-9_23},
  URL = {http://www.springerlink.com/content/fgr53835423111nj/},
  crossref =  {CTCS1987},
  pages = 	 {114--139},
  WKloc = 	 {doc/pap/BIB},
  abstract = {We provide a framework for equational deduction based on
                  category theory. Firstly, drawing upon categorical
                  logic, we show how the compositional structure of
                  equational deduction is captured by a
                  2-category. Using this formulation, algorithms for
                  solving equations are derived from general
                  constructions in category theory. The basic
                  unification algorithm arises from constructions of
                  colimits. We also consider solving equations in the
                  presence of term rewriting systems and the
                  combination of unification algorithms.}
}

@Article{Rytter-1980,
  author = {Wojciech Rytter},
  title = {A Correct Preprocessing Algorithm for {Boyer-Moore} string searching},
  journal = SIAMCOMP,
  year = 1980,
  volume = 9,
  number = 3,
  pages = {509--512}
}

@TechReport{SRI_CSL-2004,
  organization = {Formal Methods and Dependable Systems Program},
  title = 	 {Formal Methods Roadmap: PVS, ICS, and SAL},
  institution =  {Computer Science Laboratory, SRI International},
  year = 	 2003,
  number =	 {SRI-CSL-03-05},
  month =	 NOV,
  note =	 {update dated } # OCT # { 2004},
  WKloc = 	 {A-1577}
}

@Article{Saabas-Uustalu-2005,
  author = 	 {Ando Saabas and Tarmo Uustalu},
  title = 	 {A compositional natural semantics and Hoare logic for low-level languages},
  journal = 	 ENTCS,
  year = 	 {2005},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {http://cs.ioc.ee/~tarmo/papers/sos05.pdf},
  WKloc = {doc/pap/BIB},
  OPTannote = 	 {SOS2005},
  abstract = {This is a new approach to semantic and logic
     descriptions of low-level languages (where a piece of code is a flat
     set of labelled instructions). The key idea is to use finite
     non-overlapping unions of pieces of code as a phrase structure.
     Although this may at first look like a very uninteresting phrase
     structure, it is actually perfectly good both metatheoretically and
     from the practical software technogy point of view. A salient feature
     is compositional compilability of high-level program proofs.}
}

@TechReport{Saaltink-1999,
  author = {Mark Saaltink},
  title = {The {Z/EVES 2.0 User's Guide}},
  institution = {ORA Canada},
  year = 1999,
  OPTkey = {},
  OPTtype = {},
  OPTnumber = {TR-99-5493-06a},
  OPTaddress = {One Nicholas Street, Suite 1208,
                  Ottawa, Ontario K1N 7B7, Canada},
  OPTmonth = OCT,
  WKloc = {B-0056}
}

@InProceedings{Sabadini-Vigna-Walters-1993,
  author = {N. Sabadini and S. Vigna and R.F.C. Walters},
  title = {A notion of refinement for automata},
  crossref = {AMAST1993},
  pages = {325--332},
  WKloc = {A-0971 is 3-page draft from January 1993}
}

@TechReport{Sabadini-Vigna-Walters-1993a,
  author = {N. Sabadini and S. Vigna and R.F.C. Walters},
  title = {An automata-theoretic approach to concurrency through
      distributive categories: on morphisms},
  institution = {University of Sydney, School of Mathematics and Statistics},
  year = 1993,
  type = {Research Report},
  number = {93-8},
  WKloc = {A-0972}
}

@Unpublished{Sabadini-Vigna-Walters-1994,
  author = {N. Sabadini and S. Vigna and R. F. C. Walters},
  title = {A Note On Recursive Functions},
  note = {Preprint},
  month = NOV,
  WKloc = {A-0935}
}

@TechReport{Sabadini-Walters-1993,
  author = {N. Sabadini and R.F.C. Walters},
  title = {On functions and processors: an automata-theoretic approach to
      concurrency through distributive categories},
  institution = {University of Sydney, School of Mathematics and Statistics},
  year = 1993,
  type = {Research Report},
  number = {93-7},
  month = NOV
}

@InProceedings{Sabadini-Walters-Weld-1993,
  author = {N. Sabadini and R. F. C. Walters and Henry Weld},
  title = {Distributive automata and Asynchronous Circuits},
  booktitle = {CTCS '93},
  year = 1993,
  month = MAY,
  booktitle = {CTCS93},
  annote = {{\tt ftp://ghost.sm.dsi.unimi.it/pub2/papers/sabadini}
          and at {\tt ftp://maths.usyd.edu.au/sydcat/papers/walters}}
}

@InProceedings{Sabadini-Walters-Weld-1994,
  author = {N. Sabadini and R. F. C. Walters and Henry Weld},
  title = {On Categories of Asynchronous Circuits},
  booktitle = {???},
  year = 1994,
  month = OCT,
  booktitle = {CTCS93},
  annote = {{\tt ftp://ghost.sm.dsi.unimi.it/pub2/papers/sabadini}
          and at {\tt ftp://maths.usyd.edu.au/sydcat/papers/walters}},
  WKloc = {A-1159}
}

@Unpublished{Sabetzadeh-2002,
  author = {Mehrdad Sabetzadeh},
  title = {Specification of Reactive Systems Using Statecharts: a Case Study},
  note = {Course work at University of Toronto},
  year = 2002,
  WKloc = {A-1395}
}

@Unpublished{Sabetzadeh-2006a,
  author = {Mehrdad Sabetzadeh},
  title = {Management of Incomplete and Inconsistent Views},
  note = {Ph.D.~thesis proposal at University of Toronto},
  year = 2006,
  WKloc = {A-1662, doc/pap/BIB},
  abstract = {Views have long been used as means to structure and
      manage conceptual models. Model management aims to keep track of
      the relationships between a collection of views as they evolve,
      and to describe the manipulations performed over them in terms
      of a set of predefined operators. Model management is
      complicated by the incompleteness and inconsistency of views:
      Views are typically partial and offer varying degrees of
      confidence about their statements. They often capture
      overlapping perspectives, and their development is distributed
      across time and over different groups of people. Hence,
      different views may have conflicting purposes, or disagreements
      over the choice of terminology and structures used for
      describing shared concepts.

      In this article, we describe a general framework for the
      management of incomplete and inconsistent views. We propose a
      technique, based on multiple-valued logics, that allows for
      manipulation of views without having to make them complete and
      consistent first. We use our framework as a basis for exploring
      the systematic application of important design principles, such
      as traceability, reusability and separation of concerns, in
      model management.}
}

@Unpublished{Sabetzadeh-2006b,
  author = {Mehrdad Sabetzadeh},
  title = {A Survey of Approaches to View Merging},
  note = {Ph.D.~depth paper at University of Toronto},
  year = 2006,
  WKloc = {A-1663, doc/pap/BIB},
  abstract = {View merging is an important problem in conceptual
      modelling. It is often desirable to consolidate a set of views
      into a seamless view to gain a unified perspective, to
      understand interactions between views, or to perform various
      kinds of end-to-end analysis. In this report, we present a
      survey of several existing approaches to view merging. Our
      survey spans three areas: database schema modelling, software
      requirements and design, and ontology modelling. We identify a
      number of important criteria, considerations, and qualities that
      are essential to the understanding of view merging in these
      areas.}
}

@InProceedings{Sabri-Khedri-2006,
   author =      {Khair Eddin M. Sabri and Ridha Khedri},
   title  =      {A Multi-view Approach for the Analysis of
   Cryptographic Protocols},
   booktitle =   {{Practice and Theory of IT Security, Satellite workshop
  of the Montreal Conference on e-Technologies MCETECH'06}},
   pages =       {21--27},
   year =        {2006},
   address =     {Montreal, Quebec, Canada},
   month =       {May 17--19},
   bibliographies = {RelMiCS}
}

@Article{Sabry-Felleisen-1994,
  author = {Amr Sabry and Matthias Felleisen},
  title = {Is Continuation-Passing Useful for Data Flow
                 Analysis?},
  journal = SIGPLAN,
  volume = 29,
  number = 6,
  pages = {1--12},
  month = jun,
  year = 1994,
  CODEN = {SINODQ},
  ISBN = {0-89791-598-4},
  ISSN = {0362-1340},
  bibdate = {Thu May 13 12:37:27 MDT 1999},
  bibsource = {http://www.acm.org/pubs/contents/proceedings/pldi/178243/index.html},
  URL = {http://www.acm.org:80/pubs/citations/proceedings/pldi/178243/p1-sabry/},
  abstract = {The widespread use of the continuation-passing style
                 (CPS) transformation in compilers, optimizers, abstract
                 interpreters, and partial evaluators reflects a common
                 belief that the transformation has a positive effect on
                 the analysis of programs. Investigations by Nielson
                 [13] and Burn/Filho [5,6] support, to some degree, this
                 belief with theoretical results. However, they do not
                 pinpoint the source of increased abstract information
                 and do not explain the observation of many people that
                 continuation-passing confuses some conventional data
                 flow analyses. To study the impact of the CPS
                 transformation on program analysis, we derive three
                 canonical data flow analyzers for the core of an
                 applicative higher-order programming language. The
                 first analyzer is based on a direct semantics of the
                 language, the second on a continuation-semantics of the
                 language, and the last on the direct semantics of CPS
                 terms. All analyzers compute the control flow graph of
                 the source program and hence our results apply to a
                 large class of data flow analyses. A comparison of the
                 information gathered by our analyzers establishes the
                 following points: 1. The results of a direct analysis
                 of a source program are {\em incomparable\/} to the
                 results of an analysis of the equivalent CPS program.
                 In other words, the translation of the source program
                 to a CPS version may increase or decrease static
                 information. The gain of information occurs in
                 non-distributive analyses and is solely due to the {\em
                 duplication\/} of the analysis of the continuation. The
                 loss of information is due to the confusion of distinct
                 procedure returns. 2. The analyzer based on the
                 continuation semantics produces more accurate results
                 than both direct analyzers, but again only in
                 non-distributive analyses due to the {\em
                 duplication\/} of continuations along every execution
                 path. However, when the analyzer explicitly accounts
                 for looping constructs, the results of the semantic-CPS
                 analysis are no longer computable. In view of these
                 results, we argue that, in practice, a direct data flow
                 analysis that relies on some amount of duplication
                 would be as satisfactory as a CPS analysis.},
  WKloc = {A-0954},
  acknowledgement = ack-nhfb,
  annote = {Published as part of the Proceedings of PLDI'94.},
  classification = {C6110 (Systems analysis and programming); C6150G
                 (Diagnostic, testing, debugging and evaluating
                 systems)},
  conflocation = {Orlando, FL, USA; 20-24 June 1994},
  conftitle = {ACM SIGPLAN '94 Conference on Programming Language
                 Design and Implementation (PLDI)},
  corpsource = {Dept. of Comput. Sci., Rice Univ., Houston, TX, USA},
  keywords = {algorithms; design; languages; applicative
                 higher-order programming language; canonical data flow
                 analyzers; continuation-passing style;
                 continuation-semantics; data flow analysis; direct
                 semantics; program compilers; program testing; systems
                 analysis},
  sponsororg = {ACM},
  subject = {{\bf D.3.4} Software, PROGRAMMING LANGUAGES,
                 Processors, Optimization. {\bf D.3.4} Software,
                 PROGRAMMING LANGUAGES, Processors, Compilers. {\bf
                 D.3.4} Software, PROGRAMMING LANGUAGES, Processors,
                 Interpreters. {\bf D.3.2} Software, PROGRAMMING
                 LANGUAGES, Language Classifications, Applicative
                 (functional) languages. {\bf F.4.1} Theory of
                 Computation, MATHEMATICAL LOGIC AND FORMAL LANGUAGES,
                 Mathematical Logic, Lambda calculus and related
                 systems.},
  treatment = {T Theoretical or Mathematical}
}

@InProceedings{SacerdotiCoen-2004,
  author = 	 {Sacerdoti Coen, Claudio},
  title = 	 {Mathematical Libraries as Proof Assistant Envorinments},
  crossref =     {MKM2004},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = {A-1540},
  URL = {http://mowgli.cs.unibo.it/},
  bibliographies = {HHOL},
  abstract = {In this paper we analyse the modifications on
       logical operations --- as proof checking, type inference,
       reduction and convertibility --- that are required for the
       identification of a proof assistant environment with a
       distributed mathematical library, focusing on proof assistants
       based on the Curry-Howard isomorphism.

       This identification is aimed at the integration of Mathematical
       Knowledge Management tools with interactive theorem provers:
       once the distinction between the proof assistant environment
       and a mathematical library is blurred, it is possible to
       exploit Mathematical Knowledge Management rendering, indexing
       and searching services inside an interactive theorem prover, a
       first step towards effective loosely-coupled collaborative
       mathematical environments.}
}

@Article{SacerdotiCoen-Tassi-2010,
  author =       {Sacerdoti Coen, Claudio and Enrico Tassi},
  title =        {Formalizing Overlap Algebras in {Matita}},
  journal =      MSCS,
  year =         2011,
  volume =    21,
  number =    3,
  pages =     {1--31},
  DOI =     {10.1017/S0960129511000107},
  WKloc =      {doc/pap/BIB},
  bibliographies = {AgdaTG},
  abstract =    {We describe some formal topological results,
    formalized in Matita 1/2,
    presented in predicative intuitionistic logic and in terms of Overlap Algebras.
    Overlap Algebras are new algebraic structures designed to ease reasoning
    about subsets in an algebraic way within intuitionistic logic.
    We find that they also ease the formalization of formal topological results
    in an interactive theorem prover.
    Our main result is the existence of a functor
    between two categories of `generalized topological spaces',
    one with points (Basic Pairs) and the other point-free (Basic Topologies).
    The reported formalization is part as a wider scientific collaboration
    with the inventor of the theory, Giovanni Sambin.
    His goal is to verify in what sense, and with what difficulties,
    his theory is `implementable'.
    We check that all intermediate constructions
    respect the stringent size requirements imposed by predicative logic.
    The formalization is quite unusual,
    since it has to make explicit size information that is often hidden.

    We found that the version of Matita used for the formalization
    was largely inappropriate.
    The formalization drove several major improvements of Matita
    that will be integrated in the next major release (Matita 1.0).
    We show some motivating examples for these improvements,
    taken directly from the formalization.
    We also describe a possibly sub-optimal solution in Matita 1/2,
    exploitable in other similar systems.
    We briefly discuss a better solution available in Matita 1.0.}
}

@Misc{Sage-1999,
  author = {Meurig Sage},
  title = {{TclHaskell} -- User Manual},
  year = 1999,
  note = {Baased on first version by Chris Dornan.
    URL:\hfil\strut\penalty-2000 \url{http://haskell.cs.yale.edu/FranTk/}},
  WKloc = {A-1215}
}

@Misc{Sage-2000,
  author = {Meurig Sage},
  title = {{FranTk} -- A Declarative {GUI} System for {Haskell}},
  year = 1999,
  note = {URL:\hfil\strut\penalty-2000 \url{http://haskell.cs.yale.edu/FranTk/}},
  WKloc = {A-1007}
}

@InProceedings{Saidi-Shankar-1999,
  author = {Hassen Sa{\"{\i}}di and Natarajan Shankar},
  title = {Abstract and Model Check while you Prove},
  booktitle = {CAV '99},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  year = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1137},
  bibliographies = {SpecTech}
}

@Booklet{Sain-1987,
  author = {Ildik{\'o} Sain},
  title = {On the Search for a Finitizable (w.\null{} r.\null{} t.\null{} the
		Representables) Algebraization of First Order Logic},
  note = {Preprint, Math.\null{} Inst.\null{} Hungar.\null{} Acad.\null{} Sci., pp.\null{} 58},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Booklet{Sain-1987a,
  author = {Ildik{\'o} Sain},
  title = {Positive Results Related to the J\'onsson, Tarski-Givant
		Representation Problem for RA's},
  note = {Preprint, pp.\null{} 8},
  year = 1987,
  bibliographies = {RelMiCS}
}

@TechReport{Sain-Nemeti-1994,
  author = {Ildik{\'o} Sain and Istv\'an  N{\'e}meti},
  title = {Fork Algebras in Usual as well as in
		  Non-well-founded Set Theories},
  institution = {Mathematical Inst.\null{} of the Hungarian Academy of Sciences},
  year = 1994,
  OPTkey = {},
  type = {Preprint},
  OPTnumber = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@Unpublished{Sain-Simon-199X,
  author = {Ildik{\'o} Sain and Andr\'as Simon},
  title = {The Complexity of the Equational Theory of
                  Relational Algebras with Standard Projection Elements},
  note = {?},
  OPTkey = {},
  year = {199?},
  OPTmonth = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{SaintDizier-1990,
  author = {Patrick Saint-Dizier},
  title = {On Logic Programming Interpretations of {Dislog}:
		  Programming Long-Distance Dependencies in Logic},
  crossref = {ALP1990},
  pages = {232--246},
  abstract = {In this document, we present Dislog, an extension to
		  Prolog designed to deal with long-distance relations
		  and constaints in a transparent and declarative way.
		  We then give a meta-interpreter for Dislog and
		  present two interpretations for Dislog: a
		  well-formedness constraint on proof trees
		  interpretation and a constraint logic programming
		  interpretation.

                  Dislog mainly emerged from the specification of a
		  computational model (Saint-Dizier 87) for Government
		  and Binding theory (noted hereafter as GB) (Chomsky
		  81, 86), in particular to model Movement theory and
		  Quantifier raising (May 86) and theri related
		  constraints and filters. Our goal is not to model GB
		  theory, but we feel that the transfer of some of its
		  principles to natural and also to formal language
		  processing is woth investigating and very promising
		  (Saint-Dizier 89a). The GB description of linguistic
		  phenomena turns out to be concise, modular and
		  parameterized; more interestingly, it is a
		  constraint-based linguistic description of language.
		  All those reasons make GB attractive to logic programmers.}
}

@Book{Salomaa-1969,
  author =	 {Arto Salomaa},
  title = 	 {Theory of Automata},
  publisher = 	 {Pergamon Press},
  year = 	 1969,
  volume =	 100,
  series =	 {Monographs in Pure and Applied Mathematics},
  McMaster = 	 {QA 267.5 .S4S3 1969}
}

@TechReport{Sampaio-1990,
  author = {Augusto Sampaio},
  title = {A comparative study of theorem provers: proving correctness
		  of compiling specifications},
  year = 1990,
  number = {PRG-TR-20-90},
  note = {A dissertation submitted for transfer from M.Sc. to
		  D.Phil status},
  institution = {Programming Research Group, Oxford University
		  Computing Laboratory},
  WKloc = {A-0091},
  abstract = {We briefly describe an algebraic approach to prove
		  correctness of compilation and exemplify how proof
		  obligations are discharged through algebraic
		  transformations. The example is then used as a case
		  study to explore the suitability of some formal
		  systems to support the approach. The systems are
		  compared and evaluated based on some stated criteria.},
  bibliographies = {RelMiCS, RelMiS}
}

@TechReport{Sander-1995,
  type = {Technical Report},
  number = {Sep26-34},
  institution = {Technical University of Munich},
  title = {Graph Layout through the {VCG} Tool},
  month = sep # { 26},
  pages = 22,
  year = 1995,
  bibdate = {July 11, 1996},
  author = {Georg Sander},
  abstract = {The VCG tool allows the visualization of graphs that
                 occur typically as data structures in programs. We
                 describe the functionality of the VCG tool, its layout
                 algorithm and its heuristics. Our main emphasis in the
                 selection of methods is to achieve a very good
                 performance for the layout of large graphs. The tool
                 supports the partitioning of edges and nodes into edge
                 classes and nested subgraphs, the folding of regions,
                 and the management of priorities of edges. The algorithm
                 produces good drawings and runs reasonably fast even
                 onvery large graphs.},
  WKloc = {B-0109}
}

@TechReport{Sander-1996,
  author = {Georg Sander},
  title = {Graph Layout for Applications in Compiler Construction},
  institution = {Universit\"at des Saarlandes},
  year = 1996,
  number = {A/01/96},
  month = FEB,
  WKloc = {B-0109},
  keywords = {VCG}
}

@Article{Sander-1999,
  author = {Georg Sander},
  title = {Graph layout for applications in compiler
                 construction},
  journal = TCS,
  volume = 217,
  number = 2,
  pages = {175--214},
  day = 06,
  month = apr,
  year = 1999,
  ISSN = {0304-3975},
  URL = {http://www.elsevier.com/cas/tree/store/tcs/sub/1999/217/2/3078.pdf},
  keywords = {VCG},
  annote = {see \cite{Sander-1996}}
}

@Book{Sanderson-1980,
  author = {J.G. Sanderson},
  title = {A Relational Theory of Computing},
  series = LNCS,
  volume = 82,
  year = 1980,
  publisher = Springer,
  URL = {http://www.springerlink.com/content/l35t0j456u5r/},
  DOI = {10.1007/3-540-09987-5},
  ISBN = {978-3-540-09987-1},
  WKloc = {doc/pap/BIB (partial)},
  bibliographies = {RelMiCS}
}

@InProceedings{Sands-1990,
  author = {David Sands},
  title = {Complexity Analysis for a lazy Higher-Order Language},
  crossref = {ESOP1990},
  pages = {361--376},
  OPTabstract = {This paper is concerned with the time-analysis of
		  functional programs. Techniques which enable us to
		  reason formally about a program's execution costs
		  have had relatively little attention in the study of
		  functional programming. We concentrate here on the
		  construction of equations which compute the
		  time-complexity of expressins ina lazy higher-order
		  language.

                  The problem with higher-order functions is that
		  complexity is dependent on the cost of applying
		  functional parameters. Structures called {\em
		  cost-closures} are introduced to allow us to model
		  both functional parameters {\em and} the cost of
		  their application.

                  The problem with lazyness is that complexity is
		  dependent on {\em context}. Projections are used to
		  characterise the context in which an expression is
		  evaluated, and cost-equations are parameterised by
		  this context-description to give a compositional
		  time-analysis. Using this form of context
		  information we introduce two types of time-equation:
		  {\em sufficient-time} equations and {\em
		  necessary-time} equations, which together provide
		  bounds on the exact time-complexity.}
}

@InProceedings{Sands-1995,
  author = {David Sands},
  title = {Total correctness by local improvement in program transformation},
  pages = {221--},
  crossref = {POPL1995},
  abstract = {The goal of program transformation is to improve efficiency while preserving meaning. One of the best known
        transformation techniques is Burstall and Darlington's unfold-fold method. Unfortunately the unfold-fold method
        itself guarantees neither improvement in efficiency nor total-correctness. The correctness problem for unfold-fold is
        an instance of a strictly more general problem: transformation by locally equivalence-preserving steps does not
        necessarily preserve (global) equivalence.

        This paper presents a condition for the total correctness of transformations on recursive programs, which, for the
        first time, deals with higher-order functional languages (both strict and non-strict) including lazy data structures. The
        main technical result is an {\em improvement theorem} which says that if the local transformation steps are guided by certain
        optimisation concerns (a fairly natural condition for a transformation, then correctness of the transformation follows.

        The improvement theorem makes essential use of a formalised improvement-theory; as a rather pleasing corollary it
        also guarantees that the transformed program is a formal improvement over the original. The theorem has immediate
        practical consequences:

        It is a powerful tool for proving the correctness of existing transformation methods for higher-order functional
        programs, without having to ignore crucial factors such as {\em memoization} or {\em folding}. We have applied the theorem to
        obtain a particularly simple proof of correctness for a higher-order variant of {\em deforestation}.

        It yields a simple syntactic method for guiding and constraining the unfold/fold method in the general case so that
        total correctness (and improvement) is always guaranteed.},
  ACMcats = {Theory of Computation - Logics and Meanings of Programs -
      Studies of Program Constructs (F.3.3): Program and recursion schemes;
      Theory of Computation -Mathematical Logic and Formal Languages -
      Mathematical Logic (F.4.1): Logic and constraint programming;
      Software -Programming Languages - Language Constructs and Features
      (D.3.3): Procedures, functions, and subroutines; Theory of
      Computation -Logics and Meanings of Programs - Semantics of
      Programming Languages (F.3.2): Operational semantics; Software -
      Software Engineering - Software/Program Verification (D.2.4):
      Correctness proof}
}

@InProceedings{Sands-1995b,
  author = {D. Sands},
  title = {Higher-Order Expression Procedures},
  crossref = {PEPM1995},
  booktitle = { Proceeding of the ACM SIGPLAN Syposium on Partial
                  Evaluation
                  and Semantics-Based Program Manipulation, PEPM'95},
  year = 1995,
  pages = {190--201},
  abstract = {Summary: We investigate the soundness of a specialisation technique due to Scherlis, expression procedures, in the context of a higher-order
     non-strict language. An expression procedure is a generalised procedure construct which provides a means of expressing contextual properties of
     functions, and thereby facilitates the manipulation and contextual specialisation of programs. Generalised programs provide a route to the correct
     specialisation of recursive functions, are transformed by means of three basic transformation rules: composition, application and abstraction. In this
     paper we show that the expression procedure approach is correct for call-by-name evaluation, including lazy data structures and higher order
     functions. },
  URL = {http://www.diku.dk/research-groups/topps/bibliography/1995.html#D-223},
  ps = {http://www.cs.chalmers.se/~dave/papers/ep2.ps.gz}
}

@Article{Sands-1996,
  author = {David Sands},
  title = {Total Correctness by Local Improvement in the Transformation of
      Functional Programs},
  journal = {ACM Transactions on Programming Languages and Systems
                  (TOPLAS)},
  year = 1996,
  volume = 18,
  number = 2,
  month = {March},
  pages = {175--234},
  note = {Extended version of \cite{Sands-1995}},
  WKloc = {B-0097},
  summary = {The goal of program transformation is to
                  improve efficiency while preserving meaning. One of the
                  best-known transformation techniques is Burstall and
                  Darlington's unfold-fold method.  Unfortunately the
                  unfold-fold method itself guarantees neither improvement
                  in efficiency nor total correctness.  The correctness
                  problem for unfold-fold is an instance of a strictly more
                  general problem: transformation by locally
                  equivalence-preserving steps does not necessarily
                  preserve (global) equivalence.  This article presents a
                  condition for the total correctness of transformations on
                  recursive programs, which, for the first time, deals with
                  higher-order functional languages (both strict and
                  nonstrict) including lazy data structures.  The main
                  technical result is an improvement theorem which
                  says that if the local transformation steps are guided by
                  certain optimization concerns (a fairly natural condition
                  for a transformation), then correctness of the
                  transformation follows.  The improvement theorem makes
                  essential use of a formalized improvement theory; as a
                  rather pleasing corollary it also guarantees that the
                  transformed program is a formal improvement over the
                  original.  The theorem has immediate practical
                  consequences: it is a powerful tool for proving the
                  correctness of existing transformation methods for
                  higher-order functional programs, without having to
                  ignore crucial factors such as memoization or
                  folding, and it yields a simple syntactic method
                  for guiding and constraining the unfold-fold method in
                  the general case so that total correctness (and
                  improvement) is always guaranteed.  },
  ps = {http://www.cs.chalmers.se/~dave/papers/sands-TOPLAS96.ps}
}

@Article{Sands-1996b,
  author = {David Sands},
  title = {Proving the
                  Correctness of Recursion-Based Automatic Program
                  Transformations},
  journal = TCS,
  volume = 167,
  number = 10,
  month = OCT,
  year = 1996,
  note = {Preliminary version in TAPSOFT'95, LNCS 915},
  abstract = {This paper shows how the {\em Improvement Theorem}---a
      semantic condition for establishing the total correctness of program
      transformation on higher-order functional programs---has practical
      value in proving the correctness of automatic techniques. To this end
      we develop and study a family of automatic program transformations.
      The root of this family is a well-known and widely studied
      transformation called {\em deforestation}; descendants include
      generalisations to richer input languages (e.g.\ higher-order
      functions), and more powerful transformations, including a
      source-level representation of some of the techniques known from
      Turchin's {\em supercompiler}.},
  ps = {http://www.cs.chalmers.se/~dave/papers/sands-TCS96.ps}
}

@InProceedings{Sangiorgi-1994,
  author = {D. Sangiorgi},
  title = {Bisimulation in Higher-Order Process Calculi},
  crossref = {PROCOMET94},
  pages = {201--218},
  keywords = {Higher-Order Calculi; Bisimulation; Concurrent
		  Programming; Process Algebra}
}

@TechReport{Sankaran-1994,
  author = {Nandakumar Sankaran},
  title = {A Bibliography on Garbage Collection},
  institution = {Clemson University},
  address = {$4^{th}$ Floor, R. C. Edwards Hall, Computer Science
             Department, Clemson, SC 29631},
  month = feb,
  year = 1994,
  type = {Technical Report},
  number = {94-102},
  ftp = {ftp.cs.clemson.edu:/pub/papers/nandu/94-102},
  annote = {This paper references literature related to garbage
                  collection in uniprocessor, parallel, distributed,
                  real-time, object-oriented, functional and logic
                  programming systems.}
}

@InProceedings{Santen-1998,
  author = {Thomas Santen},
  title = {On the Semantic Relation of {Z} and {HOL}},
  crossref = {ZUM1998},
  pages = {98--115},
  WKloc = {A-1331},
  bibliographies = {RelMiS},
  URL = {http://swt.cs.tu-berlin.de/~santen/pub/zum98.html},
  abstract = {We investigate the relation between the semantic models
     of Z, as proposed by the Z draft standard, and of the polymorphic
     version of higher-order logic that is the basis for proof systems
     such as HOL and Isabelle/HOL. Disregarding the names in schema
     types, the type models of the two systems can be identified up to
     isomorphism. That isomorphism determines to a large extent how terms
     of Z can be represented in higher-order logic. This justifies the
     soundness of proof support for Z based on higher-order logic, such
     as the encoding HOL-Z of Z in Isabelle/HOL. The comparison of the
     two semantic models also motivates a discussion of open issues in
     the development of a complete semantics of Z, in particular
     concerning the type system, generic constructs, and approaches to
     base the semantics of Z on a small kernel language.},
  bibliographies = {RelMiCS}
}

@PhDThesis{Santen-1999,
  author = {Thomas Santen},
  title = {A Mechanized Logical Model of {Z} and Object-Oriented
                 Specification},
  school = {Technical University Berlin},
  year = 1999,
  month = jun,
  note = {published in 2000 by Shaker Verlag, Aachen},
  URL = {http://www.shaker.de/Online-Gesamtkatalog/Details.idc?ISBN=3-8265-7650-0}
}

@InProceedings{Saraiva-2002,
  author = 	 {Jo{\~a}o Saraiva},
  title = 	 {{HaLeX}: A {Haskell} Library to Model, Manipulate and Animate Regular Languages},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {{Proceedings of the ACM Workshop on Functional and Declarative Programming in Education (FDPE/PLI'02), Pittsburgh, USA, October 2002}},
  OPTpages = 	 {},
  OPTyear = 	 {2002},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  WKloc = {doc/pap/BIB},
  URL = {http://www.di.uminho.pt/~jas/Research/HaLeX/HaLeX.html},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Saraswat-Jagadeesan-Gupta-1994,
  title = {Foundations of Timed Concurrent Constraint Programming},
  author = {Vijay Saraswat and Radha Jagadeesan and Vineet Gupta},
  pages = {71--80},
  crossref = {LICS9},
  abstract = {We develop a model for timed, reactive computation by
      extending the asynchronous, untimed concurrent constraint programming
      model in a simple and uniform way. In the spirit of process algebras,
      we develop some combinators expressible in this model, and reconcile
      their operational, logical and denotational character. We show how
      programs may be compiled into finite-state machines with loop-free
      computations at each state, thus guaranteeing bounded response time.}
}

@Article{Sargent-1997,
  author = {Sargent, R. G.},
  title = {Modeling queueing systems using hierarchical control flow graph models },
  journal = {Mathematics and Computers in Simulation},
  year = 1997,
  volume = 44,
  number = 3,
  pages = {233--},
  ISSN = {0378-4754},
  UniBwM = {Z1413-44x-1}
}

@InProceedings{Sargent-Daum-1997,
  author = {Sargent, R. G. and Daum, T.},
  title = {A Java Based System for Specifying Hierarchical Control Flow Graph Models},
  pages = {150--},
  year = 1997,
  booktitle = {Winter Simulation Conference},
  series = {ISSN 0743-1902},
  UniBwM = {E-INF001/Z4352}
}

@InProceedings{Sargent-Fritz-1995,
  author = {Sargent, R. G. and Fritz, D. G.},
  title = {An Overview of Hierarchical Control Flow Graph Models},
  pages = {1347--},
  year = 1995,
  booktitle = {Winter Simulation Conference},
  series = {ISSN 0743-1902},
  UniBwM = {E-INF001/Z4352}
}

@InProceedings{Sartipi-2001b,
  author = 	 {Kamran Sartipi},
  title = 	 {{Alborz}: A Query-based Tool for Software Architecture
Recovery},
  booktitle = {Proceedings of the Ninth International Workshop on Program Comprehension (IWPC'01)},
  pages =	 {115--116},
  year =	 2001,
  publisher = {IEEE Computer Society},
  month =	 {May},
  bibliographies = {OPG}
}

@InProceedings{Sartipi-Kontogiannis-200X,
  author = {Kamran Sartipi and Kostas Kontogiannis},
  title = {On Modeling Software Architecture Recovery as Graph Matching},
  WKloc = {A-1453},
  annote = {ksartipi@swen.uwaterloo.ca}
}

@Article{Sassone-1998,
  author = 	 {Sassone, Vladimiro},
  title = 	 {An Axiomatization of the Cateogry of {Petri} Net Computations},
  journal = 	 MSCS,
  year = 	 1998,
  volume = 	 8,
  number =       2,
  pages = 	 {117--151},
  abstract = {We introduce the notion of strongly concatenable process
    as a refinement of concatenable processes (Degano et al. 1996),
    which can be expressed axiomatically
    via a functor $\CalQ(\_)$ from the category of Petri nets
    to an appropriate category of symmetric strict monoidal categories,
    in the precise sense that, for each net $N$,
    the strongly concatenable processes of $N$
    are isomorphic to the arrows of $\CalQ(N)$.
    In addition, we identify a coreflection right adjoint to $\CalQ(\_)$
    and characterize its replete image,
    thus yielding an axiomatization of the category of net computations.}
}

@Article{Sassone-2000,
  author = 	 {Sassone, Vladimiro},
  title = 	 {On the Algebraic Structure of {Petri} Nets},
  journal = 	 {Bulletin of the European Association for Theoretical Computer Science},
  year = 	 2000,
  volume = 	 72,
  pages = 	 {133--148}
}

@InProceedings{Sato-1991,
  title = {Adding Proof Objects and Inductive Definition Mechanisms to
		  {Frege} Structures},
  author = {Masahiko Sato},
  pages = {53--87},
  crossref = {TACS1991},
  note = {invited paper},
  abstract = {A constructive theory RPT (Reflective Proof Theory) of
		  proofs which has the following three features is
		  introduced. (1)~Proofs as objects.  (2)~Hierarchies
		  of propositions and truths. (3)~The mechanisms of
		  inductive definitions of predicates.  Three kinds of
		  structures called Frege structures with inductively
		  defined predicates, Frege structures with proof
		  objects and proof structures are also introduced.
		  These structures are obtained by generalizing
		  certain aspects of RPT and they are all closely
		  related to Frege structures.}
}

@InProceedings{SattlerKlein-1994,
  author = {A. Sattler-Klein},
  title = {About Changing the Ordering During Knuth-Bendix Completion},
  crossref = {STACS1994},
  pages = {175--186},
  authorsAddress = {Kaiserslautern},
  keywords = {TRS}
}

@PhDThesis{Sauer,
  title = {Algorithmustransformationen beim Entwurf ,
      anwendungsspezifischer integrierter Schaltungen Workshop},
  author = {Matthias Sauer},
  year = {???},
  school = {TU M\"unchen, Lehrstuhl f\"ur Netzwerktheorie und
      Schaltungstechnik},
  bibliographies = {RelMiCS}
}

@Book{Schach-1997,
  author = {Stephen R. Schach},
  title = {Software Engineering with {Java}},
  publisher = {McGraw-Hill International Editions},
  year = 1997,
  series = {Computer Science Series},
  URL = {http://www.mhcollege.com/},
  ISBN = {0-07-115552-X},
  annote = {Chapter 8: Specification Phase; Section 8.8: Z},
  WKloc = {Sect 8.8: A-1078}
}

@PhDThesis{Schalk-1993,
  author = {Andrea Schalk},
  title = {Algebras for Generalized Power Constructions},
  school = {TH Darmstadt, Fachbereich Mathematik},
  URL = {http://www.ftp.cl.cam.ac.uk/ftp/papers/as213/diss.dvi.gz},
  year = 1993,
  WKloc = {A-0959},
  bibliographies = {RelMiCS}
}

@Misc{Schantz-Sollins-199X,
  editor = {Richard Schantz and Karen Sollins},
  title = {{NGI} Middleware Group Summary},
  year = {199?},
  WKloc = {A-0722}
}

@Article{Scheffczyk-Stutz-Borghoff-Siedersleben-2003,
  author = {Jan Scheffczyk and Christiane Stutz and Uwe M. Borghoff and Johannes Siedersleben},
  title = {Konsistente Software-Spezifikationen},
  journal = {Informatik: Forschung und Entwicklung},
  year = 2003,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  WKloc = {A-1481, doc/pap/BIB},
  abstract = {Abstract Specification is the base for the success of a
     software project. It is most challenging to completely and correctly
     capture all requirements to the specified system. Another problem yet
     failed to gain attention: Specifications for large systems consist of
     many documents following a different structure and containing
     different content. Until now huge manual effort must be paid to
     achieve concistency between all these results.  In this article we
     introduce analysis modules developed at the sd&m AG. These are a
     modular guide to build software specifications using best practice
     methods. For a specification using our analysis modules we precisely
     define what consistency actually means through formal consistency
     rules, i. e., predicate logic formulas. A tool we have developed
     precisely pinpoints inconsistencies and allows for flexible handling
     strategies.  Our tool-based approach helps software designers to
     concentrate on their actual work: the techincal correctness of a
     specification.}
}

@InProceedings{Scheffczyk-Borghoff-Roedig-Schmitz-2003a,
  author = 	 {Jan Scheffczyk and Uwe M. Borghoff and Peter R{\"o}dig and Lothar Schmitz},
  title = 	 {Repairing Inconsistent Repositories},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2003},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month = 	 NOV # {~13},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1512}
}

@InProceedings{Scheffczyk-Borghoff-Roedig-Schmitz-2003b,
  author = 	 {Jan Scheffczyk and Uwe M. Borghoff and Peter R{\"o}dig and Lothar Schmitz},
  title = 	 {{S-DAGs}: Towards Efficient Repair Generation},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2003},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month = 	 DEC,
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1513}
}

@InProceedings{Scheffczyk-etal-2003c,
  author = 	 {Jan Scheffczyk and others},
  title = 	 {Managing Inconsistent Repositories via Prioritized Repair Actions},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  year = 	 {2003},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  month = 	 DEC,
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1514}
}

@InProceedings{Scheffczyk-Borghoff-Roedig-Schmitz-2003_consistency,
  author = {J. Scheffczyk and U. Borghoff and P. R{\"o}dig and L. Schmitz},
  title = "Consistent Document Engineering",
  year = 2003,
  url = {http://www2-data.informatik.unibw-muenchen.de/cde.html},
  booktitle = {Proc.\null{} of the 2003 ACM Symp.\null{} on Document Engineering},
  address = {Grenoble, France},
  publisher = {ACM Press},
  pages = "140--149"
}

@InProceedings{Scheffczyk-Borghoff-Roedig-Schmitz-2003_incremental,
  author = {J. Scheffczyk and U. Borghoff and P. R{\"o}dig and L. Schmitz},
  title = "Efficient (In-)Consistency Management for Heterogeneous
Repositories",
  booktitle = {Proc.\ of the 4th Int.\ Conf.\ on Software Engineering,
Artificial Intelligence, Networking, and Parallel/Distributed
Computing (SNPD'03)},
  address = {L\"ubeck, Germany},
  year = "2003",
  url = {http://www2-data.informatik.unibw-muenchen.de/index_cde.html},
  publisher = {ACIS},
  pages = {370--377}
}


@TechReport{ Scheffczyk-Borghoff-Roedig-Schmitz-2003_consistencyreport,
  author = {J. Scheffczyk and U. Borghoff and P. R{\"o}dig and L. Schmitz},
  title = "A Comprehensive Description of Consistent Document Engineering",
  number = {03},
  year = "2003",
  address = "Univ.\ of the Federal Armed Forces, Munich"
}

@Article{Schein-1965,
  author = {Boris M. Schein},
  title = {Relation Algebras},
  journal = BUPOLON,
  volume = 13,
  year = 1965,
  pages = {1--5},
  bibliographies = {RelMiCS}
}

@Article{Schein-1965a,
  author = {Boris M. Schein},
  title = {Semigroups of Rectangular Binary Relations},
  journal = DOCL,
  volume = 165,
  year = 1965,
  pages = {1563--1566},
  bibliographies = {RelMiCS}
}

@Article{Schein-1970,
  author = {Boris M. Schein},
  title = {Relation Algebras and Function Semigroups},
  journal = SEMIGROUP,
  volume = 1,
  number = 1,
  year = 1970,
  pages = {1--61},
  bibliographies = {RelMiCS}
}

@Article{Schein-1974,
  author = {Boris M. Schein},
  title = {Representation of Involuted Semigroups by Binary Relations},
  journal = FUND,
  volume = 82,
  year = 1974,
  pages = {121--141},
  bibliographies = {RelMiCS}
}

@Article{Schein-1988,
  author = {Boris M. Schein},
  title = {Multigroups},
  journal = JALG,
  volume = 111,
  year = 1988,
  pages = {114--132},
  bibliographies = {RelMiCS}
}

@Misc{Schellhorn-1998,
  author = {Gerhard Schellhorn},
  title = {Proving Properties of Directed Graphs: A Problem Set for Automatic Theorem Provers},
  year = 1998,
  WKloc = {A-0517}
}

@PhDThesis{Scherlis-1980,
  author = {William L. Scherlis},
  title = {Expression Procedures and Program Derivation},
  institution = {Stanford University, Department of Computer Science},
  address = {Stanford, California},
  type = {Report},
  number = {STAN-CS-80-818 and AIM-341},
  month = AUG,
  year = 1980
}

@InProceedings{Scherlis-1981,
  author = {William L. Scherlis},
  title = {Program Improvement by Internal Specialization},
  pages = {41--49},
  crossref = {POPL1981},
  annote = {Program transformation with a generalized procedure
                 construct. 13 references.}
}

@InProceedings{Schewe-1994,
  author = {Klaus-Dieter Schewe},
  title = {{Zur Kombination algebraischer und modell-basierter
		  Spezifikationen und ihrer kategoriell-logischen Semantik}},
  crossref = {Honnef94},
  pages = {108--112},
  keywords = {Combination of algebraic and model based
		  specifications and their categorical-logical semantics}
}

@PhDThesis{Schied-1992,
  author = {Georg Schied},
  title = {{\"Uber Graphgrammatiken, eine Spezifikationsmethode
		  f\"ur Programmiersprachen und verteilte Regelsysteme}},
  school = {Universit\"at Erlangen-N\"urnberg},
  year = 1992,
  type = {Dissertation},
  note = {Erschienen in Arbeitsberichte des Inst.\ f.\ Math.\ Masch.\
         u.\ Datenverarb., Bd.~25, Nr.~2, Erlangen, Mai 1992}
}

@InProceedings{Schied-1993,
  author = {G. Schied},
  title = {On Relating Rewriting Systems and Graph Grammars to
		  Event Structures},
  crossref = {GTCS93},
  pages = {326--340},
  abstract = {In this paper, we investigate how rewriting systems
		  and especially graph grammars as operational models
		  of parallel and distributed systems can be related
		  to event structures as more abstract models. First,
		  {\em distributed rewriting systems} that are based
		  on the notion of contexts are introduced as a common
		  framework for different kinds of rewriting systems
		  and their parallelism properties are
		  investigated. Then we introduce {\em concrete graph
		  grammars} and show how they can be integrated into
		  this framework for rewriting systems. A construction
		  for the Mazurkiewicz trace language related to the
		  derivation sequences of a distributed rewriting
		  system is presented. Since there is a well-known
		  relation between trace languages and event
		  structures, this provides the link between (graph)
		  rewriting and event structures.}
}

@InProceedings{Schiffer-Froehlich-1994,
  author = {Stefan Schiffer and Joachim Hans Fr{\"o}hlich},
  title = {Concepts and Architecture of Vista --- a multiparadigm Origramming Environment},
  year = 1994,
  WKloc = {A-0435},
  booktitle = {Proceedings 1994 {IEEE} Symposium on Visual Languages,
      {Oct. 4-7, 1994, St. Louis, MO, USA}, pages = 40--47}
}

@Article{Schindler-Kempf-1999,
  author = {Claudia Schindler and Peter Kempf},
  title = {Towards a Formal Framework for Heterogeneous Relation Algebra},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {193--203},
  abstract = {We define a language for polymorphic typed relations and
      introduce a typing discipline for terms of that language. A modular
      type inference system for the derivation of the most general type of
      a term is presented and correctness and well as weak completeness of
      the type inference system w.r.t. the typing discipline is proven.
      Finally, we give an interpretation of our language based on the
      classical model of relation algebra.},
  keywords = {Heterogeneous relation algebra; Typing discipline; Type inference system},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/29/abstract.html},
  bibliographies = {RelMiCS}
}

@Misc{Schirmer-Schmidt-Schwesinger-Stroehlein-Wimmer-1973,
  year = 1973,
  title = {{METHUSALEM --- Ein Programmsystem f\"ur die Zuweisung von Lehrkr\"aften}},
  pages = 299,
  howpublished = {Forschungsauftrag Lehrerzuweisung des Bayer.
      Staatsministeriums f\"ur Unterricht und Kultus am Math. Institut der
      Technischen Universit\"at M\"unchen, Abschlu\ss{}dokumentation},
  author = {J. Schirmer and G. Schmidt and B. Schwesinger and T.
		  Str\"ohlein and K. Wimmer}
}

@InProceedings{Schlingloff-1992,
  author = {Bernd-Holger Schlingloff},
  title = {On the Expressive Power of Modal Logics on Trees},
  editor = {A. Nerode and M. Taitslin},
  volume = 620,
  series = {LNCS},
  pages = {441--451},
  booktitle = {Second International Symposiumon Logical Foundations
		  of Computer Science {(2 nd LFCS '92 Tver, Russia,
		  July 1992)}},
  year = 1992,
  publisher = {Springer-Verlag},
  WKloc = {A-0252},
  abstract = {Various logical languages are compared regarding
		  their expressive power with respect to models
		  consisting of finitely bounded branching infinite
		  trees. The basic multimodal logic with backward- and
		  forward necessity operators is equivalent to
		  Restricted first order logic; by adding the binary
		  temporal operators ``since'' and ``until'' we get
		  the expressive power of first order logic on
		  trees. Hence (restricted) propositional quantifiers
		  in temporal logic correspond to (restricted) set
		  quantifiers in predicate logic. Adding the ${\rm
		  CTL}^*$ path modality ``E'' to temporal logic gives
		  the expressive power of path logic. Tree grammar
		  operators give a logic as expressive as weak second
		  order logic, whereas adding fixed point quantifiers
		  (in the so-called propositional mu-calculus) results
		  in a logic expressivly equivalent to monadic second
		  order logic on trees.},
  keywords = {Modal logic, temporal logic, branching time
		  logic,computation tree logic, ${\rm CTL}^*$ ,
		  propositional $\mu$-calculus, L$\mu$, definability,
		  expressiveness, expressive completeness,
		  $\omega$-tree automata, $\omega$-trees,
		  $\omega$-tree languages, specification languages}
}

@InProceedings{Schlingloff-1998,
  author = {Holger Schlingloff},
  title = {Modelling Message Buffers with Binary Decision Diagrams},
  pages = {59--70},
  abstract = {Binary decision diagrams (BDDs, [Bry-1992]) have been
                   recognized as an extremely efficient data structure for
                   the representation of transition relations in the
                   verification of finite-state reactive systems. With BDDs,
                   it is possible to represent relations over domains with
                   more than $2^{100}$ elements [BCDM-1991], provided the
                   represented relation is well-structured. Asynchronous
                   parallel systems such as communication protocols often
                   use implicit or explicit buffering of messages which are
                   sent between the processes. In these notes, we analyze
                   the complexity of various possibilities to model the
                   transition relation of a bounded buffer with BDDs, and
                   discuss alternative approaches to this problem.},
  editor = {Ali Jaoua and Peter Kempf and Gunther Schmidt},
  booktitle = {Using Relational Methods in Computer Science},
  year = 1998,
  month = JUL,
  series = {Technical Report Nr.\null{} 1998-03},
  publisher = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  bibliographies = {RelMiCS}
}

@InCollection{Schlingloff-Heinle-1997,
  author = {Holger Schlingloff and Wolfgang Heinle},
  title = {Relation Algebra and Modal Logics},
  chapter = 5,
  pages = {70--89},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@Article{Schmeck-1983,
  author = {Hartmut Schmeck},
  title = {Algebraic Characterization of Reducible Flowcharts},
  journal = JCOMSYS,
  volume = 27,
  pages = {165--199},
  year = 1983
}

@Article{Schmeck-1983a,
  title = {Algebraic Semantics of Recursive Flowchart Schemes},
  author = {Hartmut Schmeck},
  pages = {108--126},
  journal = iandc,
  month = oct # {/} # nov # {/} # dec,
  year = 1983,
  volume = 59,
  number = {1--3}
}

@InProceedings{Schmeck-1983b,
  author = {Hartmut Schmeck},
  title = {Flow Graph Grammars and Flow Graph Languages},
  crossref = {WG83},
  pages = {319--329}
}

@PhDThesis{Schmerl-1978,
  authorsAddress = {inf2},
  year = 1978,
  title = {{Eine von Reflexionsformeln erzeugte Feinstruktur \"uber
      Erweiterungen der primitiv rekursiven Arithmetik}},
  school = {{Universit\"at}},
  author = {Schmerl, U.R.},
  address = {Heidelberg}
}

@InProceedings{Schmerl-1978a,
  authorsAddress = {inf2},
  year = 1979,
  title = {A fine structure generated by reflection formulas
		  over primitive recursive arithmetic},
  pages = {335--350},
  booktitle = {Logic Colloquium 1978},
  author = {Schmerl, U.R.},
  address = {Amsterdam}
}

@Article{Schmerl-1982,
  authorsAddress = {inf2},
  year = 1982,
  volume = 47,
  title = {Iterated reflection principles and the w-rule},
  pages = {721-733},
  journal = {Journal of Symbolic Logic},
  author = {Schmerl, U.R.}
}

@Article{Schmerl-1982a,
  authorsAddress = {inf2},
  year = 1982,
  volume = 22,
  title = {A proof-theoretical fine structure in systems of
		  ramified analysis},
  pages = {167-182},
  journal = {{Archiv f. math. Logik u. Grundlagenf.}},
  author = {Schmerl, U.R.}
}

@Article{Schmerl-1982b,
  authorsAddress = {inf2},
  year = 1982,
  title = {{\"Uber die schwach und die stark wachsende
		  Hierarchie zahlentheoretischer Funktionen}},
  pages = {1-18},
  journal = {Sitzungsbericht d. Bayer. Akad. d. Wissensch.},
  author = {Schmerl, U.R.}
}

@InProceedings{Schmerl-1982c,
  authorsAddress = {inf2},
  year = 1982,
  title = {Number theory and the {Bachmann/Howard} ordinal},
  series = {Logic Coll.},
  pages = {287-298},
  booktitle = {Herbrand Symposium 1981},
  author = {Schmerl, U.R.},
  address = {Amsterdam}
}

@InProceedings{Schmerl-1983,
  authorsAddress = {inf2},
  year = 1983,
  volume = 1104,
  title = {Diophantine equations in a fragment of number
		  theory, Computation and Proof Theory},
  series = {Lecture Notes in Math.},
  publisher = {Springer},
  pages = {389--398},
  booktitle = {Logic Coll.1983},
  author = {Schmerl, U.R.}
}

@PhDThesis{Schmerl-1984,
  authorsAddress = {inf2},
  year = 1984,
  type = {Habilitationsschrift},
  title = {Diophantische Gleichungen in Fragmenten der Arithmetik},
  school = {{Ludwig-Maximilians-Universit\"at M\"unchen}},
  author = {Schmerl, U.R.}
}

@InProceedings{Schmerl-1987,
  authorsAddress = {inf2},
  year = 1987,
  title = {Crit\`eres de l'ind\'ependance d'\'equations diophantiennes de fragments d'arithm\'etiques},
  pages = {303-308},
  booktitle = {Proc. Logic Coll. 1985},
  author = {Schmerl, U.R.},
  address = {Amsterdam}
}

@Article{Schmerl-1987a,
  authorsAddress = {inf2},
  year = 1988,
  volume = 25,
  title = {Resolution on formula-trees},
  pages = {425--438},
  journal = {Acta Informatica},
  author = {Schmerl, U.R.}
}

@InProceedings{Schmerl-1987b,
  authorsAddress = {inf2},
  year = 1987,
  volume = 152,
  title = {Resolution on formula-trees},
  series = {Informatik Fachberichte},
  pages = {211--220},
  booktitle = {{11th German Workshop on Artificial Intelligence GWAI 87}},
  author = {Schmerl, U.R.},
  annote = {Konferenz-Version von 87a}
}

@Article{Schmerl-1988,
  authorsAddress = {inf2},
  year = 1988,
  volume = 38,
  title = {Diophantine equations in fragments of arithmetic},
  pages = {135-170},
  journal = {Annals of Pure and Applied Logic},
  author = {Schmerl, U.R.}
}

@TechReport{Schmerl-1989,
  authorsAddress = {inf2},
  year = 1989,
  title = {Logic programming by compilation and interpretation of formal derivations},
  number = 8904,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Schmerl, U.R.}
}

@InCollection{Schmerl-1990,
  authorsAddress = {inf2},
  year = 1990,
  title = {{Herleitungen als Programme: Ihre Kompilation und
		  Interpretation}},
  publisher = {Springer Verlag},
  pages = {284-294},
  editor = {Manfred Broy},
  booktitle = {{Mathematik und Informatik}},
  author = {Schmerl, U.R.}
}

@TechReport{Schmerl-1990a,
  authorsAddress = {inf2},
  year = 1990,
  title = {{Beweise als Programme: Ihre Auswertung durch Schnittelimination}},
  number = 9007,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Schmerl, U.R.}
}

@InProceedings{Schmerl-1991-x,
  authorsAddress = {inf2},
  year = 1991,
  volume = 626,
  title = {A cut-elimination procedure designed for evaluating proofs as programs},
  series = {LNCS},
  publisher = {Springer Verlag},
  pages = {316--325},
  booktitle = {CSL 91},
  author = {Schmerl, U.R.},
  address = {Bern}
}

@Article{Schmerl-1991a,
  authorsAddress = {inf2},
  year = 1991,
  title = {Clause reduction in resolution calculi by {Herbrand}
                 substitutions of depth one},
  note = {submitted paper},
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Schmerl, U.R.}
}

@TechReport{Schmerl-1991b,
  authorsAddress = {inf2},
  year = 1991,
  title = {A resolution calculus giving definite answers},
  number = 9108,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Schmerl, U.R.}
}

@InProceedings{Schmerl-Berghammer-Elbl-1991,
  authorsAddress = {inf2},
  title = {Proving total correctness of programs in weak
		  second-order logic},
  author = {Schmerl, U. R. and Berghammer, R. and Elbl, B.},
  crossref = {REX92},
  pages = {51--72}
}

@InProceedings{Schmerl-Berghammer-Elbl-1991-x,
  author = {Schmerl, U. R. and Berghammer, R. and Elbl, B.},
  title = {Proving total correctness of programs in weak
		  second-order logic},
  pages = {51--72},
  booktitle = {Rex Workshop: Semantics: Foundations and Applications},
  authorsAddress = {inf2},
  year = 1991,
  series = {LNCS},
  publisher = {Springer Verlag},
  address = {Beekbergen}
}

@InProceedings{Schmidt-1976,
  author = {Gunther Schmidt},
  title = {{Eine relationenalgebraische Auffassung der Graphentheorie}},
  crossref = {WG76},
  pages = {315--325},
  OPTwkloc = {},
  OPTkeywords = {},
  OPTabstract = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-1976a,
  author = {Gunther Schmidt},
  title = {{Eine \"Uberlagerungstheorie f\"ur Wurzelgraphen}},
  crossref = {WG76},
  pages = {65--76},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTwkloc = {},
  OPTkeywords = {},
  OPTabstract = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-1976b,
  year = 1976,
  title = {{Eine \"Uberlagerungstheorie f\"ur Wurzelgraphen}},
  pages = {13--24},
  number = 7619,
  institution = {Fachbereich Mathematik der Technischen Univ.\null{} M\"unchen},
  author = {Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@Misc{Schmidt-1977,
  keywords = {programs as partial graphs},
  year = 1977,
  title = {{Programme als partielle Graphen}},
  howpublished = {Habil.\null{} Thesis, Fachbereich Mathematik der Technischen
		   Univ.\null{} M\"unchen, Bericht 7813},
  author = {Gunther Schmidt},
  note = {{English} as \cite{Schmidt-1981a,Schmidt-1981b}},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-1979,
  author = {Gunther Schmidt},
  title = {Investigating Programs in Terms of Partial Graphs},
  year = 1979,
  volume = 71,
  series = LNCS,
  publisher = Springer,
  pages = {505--519},
  editor = {Maurer, H.A.},
  booktitle = {Proceedings of the {$6^{th}$} International Colloquium on
      Automata, Languages and Programming, {Graz}},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-1980,
  title = {Investigating Programs in Terms of Partial Graphs (Extended
      Abstract)},
  crossref = {WG80},
  pages = {268--269},
  author = {Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@Article{Schmidt-1981a,
  WKloc = {doc/pap/BIB},
  abstract = {?},
  year = 1981,
  volume = 15,
  number = 1,
  title = {Programs as Partial Graphs {I}: Flow Equivalence and Correctness},
  pages = {1--25},
  DOI = {10.1016/0304-3975(81)90060-8},
  DOIURL = {http://dx.doi.org/10.1016/0304-3975(81)90060-8},
  journal = {Theoretical Computer Science},
  author = {Gunther Schmidt},
  bibliographies = {RelMiCS}
}

@Article{Schmidt-1981b,
  author = {Gunther Schmidt},
  title = {Programs as Partial Graphs {II}: Recursion},
  journal = {Theoretical Computer Science},
  year = 1981,
  volume = 15,
  number = 2,
  pages = {159--179},
  WKloc = {doc/pap/BIB},
  DOI = {10.1016/0304-3975(81)90068-2},
  DOIURL = {http://dx.doi.org/10.1016/0304-3975(81)90068-2},
  abstract = {In part I of the paper, we have proposed a unified
               relational algebra approach using partial graphs for
               theoretical investigations on semantics, correctness and
               termination. This approach is extended here to systems
               of recursive programs, allowing not only sequencing and
               conditional branching as a control structure but also
               flow diagrams. An equivalence proof of operational and
               denotational semantics is obtained which is strictly
               based on axioms of relational algebra. A short new proof
               of an important completeness result is given in the
               generalized setting of systems of recursive flow diagram
               prog rams. Finally Hitchock Park's theorem on
               derivatives is formulated in the general case of
               nondeterministic recursive flow diagram programs.},
  bibliographies = {RelMiCS}
}

@Misc{Schmidt-1990s,
  author = {Gunther Schmidt},
  title = {Towards Unsharp Relation Algebraic Products as Models for Parallelism},
  howpublished = {Talks at the RelMiCS conferences},
  year = {1994--1998}
}

@InCollection{Schmidt-1991,
  author = {Gunther Schmidt},
  title = {{Relationen und Programme}},
  pages = {98--114},
  crossref = {Broy-1991},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-1992,
  author = {Gunther Schmidt},
  title = {Ordering Isomorphism Classes of Semantic Domains},
  year = 1992,
  pages = 10,
  number = 9207,
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  bibliographies = {RelMiCS},
  WKloc = {C-0015}
}

@Misc{Schmidt-2001a,
  author = {Gunther Schmidt},
  title = {Basics on Relational Methods},
  howpublished = {Tutorial Lecture at RelMiS 2001, Genova, April 7--8},
  year = 2001,
  WKloc = {A-1205}
}

@TechReport{Schmidt-2002,
  author = 	 {Gunther Schmidt},
  title = 	 {Decomposing Relations},
  institution =  {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  year = 	 {2002},
  OPTkey = 	 {},
  OPTtype = 	 {},
  number = 	 {2002-09},
  URL = 	 {http://ist.unibw-muenchen.de/People/schmidt/DecompoHomePage.html},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@TechReport{Schmidt-Bayer-Kempf-1990,
  keywords = {HOPS2, code generation},
  year = 1990,
  title = {Hinweise zur interaktiven {Konstruktion} von {Bereichen},
		  {Objekten} und {Programmtexten} im {HOPS-System}},
  number = 9004,
  type = {Tech\-ni\-scher Bericht},
  month = MAY,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  author = {Gunther Schmidt and Ludwig J. Bayer and Peter
		  Kempf},
  refsfrom = { <(ifip94:4)> }
}

@Proceedings{Schmidt-Berghammer-1992,
  UniBwM = {?},
  year = 1992,
  title = {Proc.\null{} {$17^{th}$} Internat.\null{}  Workshop on Graph-Theoretic
		  Concepts in Computer Science},
  series = LNCS,
  volume = 570,
  publisher = Springer,
  month = JUN,
  editor = {Gunther Schmidt and Rudolf Berghammer},
  address = {Fischbachau},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Berghammer-Zierer-1986,
  year = 1986,
  title = {Describing Semantic Domains with Sprouts},
  number = {TUM-I8611},
  institution = {Institut f\"ur Informatik, Technische Universit\"at
		  M\"unchen},
  author = {Gunther Schmidt and Rudolf Berghammer and Hans Zierer},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-Berghammer-Zierer-1986a,
  year = 1986,
  title = {{Beschreibung semantischer Bereiche mit Keimen}},
  publisher = {Universit\"at Passau},
  pages = {199--216},
  booktitle = {{Tagungsband zur 9. Tagung Berichte aus den Informatik-Instituten}},
  author = {Gunther Schmidt and Rudolf Berghammer and Hans Zierer},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-Berghammer-Zierer-1987,
  author = {Gunther Schmidt and Rudolf Berghammer and Hans Zierer},
  title = {Describing Semantic Domains with Sprouts},
  pages = {299--310},
  crossref = {STACS1987},
  bibliographies = {RelMiCS},
  note = {gek\"urzte Version von \cite{Schmidt-Berghammer-Zierer-1986}}
}

@Article{Schmidt-Berghammer-Zierer-1989,
  year = 1989,
  volume = 27,
  title = {Describing Semantic Domains with Sprouts},
  pages = {217--245},
  journal = ACTIN,
  author = {Gunther Schmidt and Rudolf Berghammer and Hans Zierer},
  annote = {gek\"urzte Version von \cite{Schmidt-Berghammer-Zierer-1986}},
  bibliographies = {RelMiCS}
}

@InCollection{Schmidt-Hattensperger-Winter-1997,
  author = {Gunther Schmidt and Claudia Hattensperger and Michael Winter},
  title = {Heterogeneous Relation Algebra},
  chapter = 3,
  pages = {39--53},
  crossref = {Brink-Kahl-Schmidt-1997},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Kempf-1992,
  year = 1992,
  title = {Semantic Domains with Congruences},
  pages = 13,
  number = 9201,
  institution = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  author = {Gunther Schmidt and Peter Kempf},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Stroehlein-1973,
  year = 1973,
  title = {{Einige operative Ans\"atze zur L\"osung von Stundenplanproblemen}},
  pages = 57,
  number = 7312,
  institution = {Abteilung Mathematik der Technischen Universit\"at M\"unchen},
  author = {Gunther Schmidt and Thomas Str\"ohlein}
}

@InProceedings{Schmidt-Stroehlein-1974,
  year = 1974,
  title = {Some Aspects in the Construction of Timetables},
  publisher = {North-Holland},
  pages = {516--520},
  booktitle = {{Proc. IFIP Congress 74 Stockholm}},
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  address = {Amsterdam}
}

@TechReport{Schmidt-Stroehlein-1974a,
  year = 1974,
  title = {A Boolean Matrix Iteration in Timetable Construction},
  pages = 28,
  number = 7406,
  institution = {Abteilung Mathematik der Technischen Univ.\null{} M\"unchen},
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Stroehlein-1975,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {{Relationen, Graphen und Programme}},
  institution = {Inst.\null{} f\"ur Informatik der Technischen
      Univ.\null{} M\"unchen},
  note = {Internal Report},
  pages = 137,
  year = 1975,
  bibliographies = {RelMiCS}
}

@Article{Schmidt-Stroehlein-1976,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {A Boolean Matrix Iteration in Timetable Construction},
  year = 1976,
  volume = 15,
  pages = {27--51},
  journal = LINEAR,
  bibliographies = {RelMiCS}
}

@Article{Schmidt-Stroehlein-1980,
  year = 1980,
  volume = 23,
  title = {Timetable Construction --- An Annotated Bibliography},
  pages = {307--316},
  journal = {Computer Journal},
  author = {Gunther Schmidt and Thomas Str\"ohlein}
}

@InProceedings{Schmidt-Stroehlein-1982a,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {Kernels in Bipartite Graphs},
  pages = {251--256},
  crossref = {WG82},
  bibliographies = {RelMiCS}
}

@Article{Schmidt-Stroehlein-1985,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {Relation Algebras --- Concept of Points and Representability},
  year = 1985,
  volume = 54,
  pages = {83--92},
  journal = DISCR,
  DOI = {10.1016/0012-365X(85)90064-0},
  DOIURL = {http://dx.doi.org/10.1016/0012-365X(85)90064-0},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  abstract = {In the axiomatization of relation algebras by Chin and Tarski certain elements are called right ideals. Aiming at applications in the relational theory of graphs and programs, we call such ideals `points' and investigate an additional point axiom. First we prove a point insertion theorem. Then a representation theorem for such relation algebras is deduced by inherently relational methods, simplifying the proof of a similar result from Jónsson, Maddux and Tarski. Some historical remarks are inserted and an extended bibliography is added.}
}

@Article{Schmidt-Stroehlein-1985a,
  year = 1985,
  volume = 6,
  title = {On Kernels of Graphs and Solutions of Games --- A Synopsis
      Based on Relations and Fixpoints},
  pages = {54--65},
  journal = SIAMALG,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Stroehlein-1985b,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {{Diskrete Mathematik --- Relationen, Graphen und Programme I}},
  institution = {Inst.\null{} f\"ur Informatik der Technischen
      Univ.\null{} M\"unchen},
  note = {Internal Report},
  pages = 316,
  year = 1985,
  bibliographies = {RelMiCS}
}

@TechReport{Schmidt-Stroehlein-1986,
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  title = {{Diskrete Mathematik --- Relationen, Graphen und Programme II}},
  institution = {Inst.\null{} f\"ur Informatik der Technischen
      Univ.\null{} M\"unchen},
  note = {Internal Report.},
  pages = 180,
  year = 1986,
  bibliographies = {RelMiCS}
}

@Book{Schmidt-Stroehlein-1989,
  year = 1989,
  title = {{Relationen und Graphen}},
  series = {Mathematik f\"ur {Informatiker}},
  publisher = Springer,
  address = {Berlin},
  author = {Gunther Schmidt and Thomas Str\"ohlein},
  note = {English as \cite{Schmidt-Stroehlein-1993}},
  bibliographies = {RelMiCS}
}

@Book{Schmidt-Stroehlein-1989_nonote,
  year = 1989,
  title = {{Relationen und Graphen}},
  series = {Mathematik f\"ur {Informatiker}},
  publisher = Springer,
  address = {Berlin},
  author = {Gunther Schmidt and Thomas Str\"ohlein}
}

@Book{Schmidt-Stroehlein-1993,
  author = {Gunther Schmidt and Thomas Str{\"o}hlein},
  title = {Relations and Graphs, Discrete Mathematics for
		  Computer Scientists},
  year = 1993,
  OPTseries = {EATCS-Mono\-graphs on Theoretical Computer Science},
  series = {EATCS-Mono\-graphs on Theoret.\null{} Comput.\null{} Sci.},
  publisher = Springer,
  bibliographies = {RelMiCS},
  WKloc = {owned},
  abstract = {Relational methods can be found at various places in
               computer science, notably in data base theory,
               relational semantics of concurrency, relational type
               theory, analysis of rewriting systems, and modern
               programming language design. In addition, they appear in
               algorithms analysis and in the bulk of discrete
               mathematics taught to computer scientists. This book
               devoted to the background of these methods. It is the
               first to explain how to use relational and graph-
               theoretic methods systematically in computer science.
               The powerful calculus of relation algebra is developed
               with respect to applications to a diverse range of
               problem areas. Results are first motivated by practical
               examples, often visualized by both Boolean 0-1-matrices
               and graphs, and then derived algebraically.},
  contents = {Sets
                   Homogeneous Relations
                   Transitivity
                   Heterogeneous Relations
                   Graphs: Associated Relation, Incidence, Adjacency
                   Reachability
                   The Category of Graphs
                   Kernels and Games
                   Correspondences and Coverings
                   Programs: Correctness and Verification
                   Appendices:
                      Boolean Algebra
                      Abstract Relation Algebra
                      Fixedpoint Theorems and Antimorphisms
                   Indices},
  ISBN = {3-540-56254-0, 0-387-56254-0}
}

@Book{Schmidt-Stroehlein-1993_note,
  author = {Gunther Schmidt and Thomas Str{\"o}hlein},
  title = {Relations and Graphs, Discrete Mathematics for
		  Computer Scientists},
  year = 1993,
  series = {EATCS-Mono\-graphs on Theoretical Computer Science},
  publisher = Springer,
  ISBN = {3-540-56254-0, 0-387-56254-0},
  note = {German version: \cite{Schmidt-Stroehlein-1989}}
}

@Misc{Schmidt-Winter-1994,
  year = 1994,
  title = {Is every tabular relation function dense? {A} note on relation
      algebras},
  note = {2 p.},
  howpublished = {Internal note},
  institution = {Fakult\"at f\"ur Informatik, Univ.\null{} der Bundeswehr
      M\"unchen},
  author = {Gunther Schmidt and Michael Winter},
  bibliographies = {RelMiCS}
}

@InProceedings{Schmidt-Zimmermann-1994,
  author = {H.W. Schmidt and W. Zimmermann},
  title = {Reasoning about Complexity of Object-Oriented Programs},
  crossref = {PROCOMET94},
  pages = {541--560},
  keywords = {Object-oriented Programming; Complexity; Amortized Complexity}
}

@Article{SchmidtA-1951,
  author = {A. Schmidt},
  title = {{Die Zul\"assigkeit der Behandlung mehrsortiger
		  Theorien mittels der \"ublichen einsortigen
		  Pr\"adikatenlogik}},
  journal = {Mat. Ann.},
  year = 1951,
  volume = 123,
  pages = {187--200},
  annote = {\cite{Schobbens-1993} cites this as source for the
		  ``relativization technique'': we introduce a unary
		  predicate for each sort, and replace quantification
		  $\forall x:s.\phi$ by $\forall x. s(x) \Rightarrow
		  R(\phi)$, where $R(\phi)$ is (recursively) the
		  relativization of $phi$. The well-sortedness of
		  operators as to be expressed by an axiom $OPS(O)$:
		  $$\mathop{\bigwedge}_{f:S\rightarrow r \in O}
		  \forall x.s(x) \Rightarrow r(f(x))$$ $\ldots$

                  Relativization also allows a simple treatment of subsorts.},
  bibliographies = {RelMiCS}
}

@Book{SchmidtDA-1986,
  UniBwM = {INF400/Q8560},
  year = 1986,
  title = {Denotational Semantics. A Methodology for Language Development},
  publisher = {Allyn and Bacon, Inc.},
  author = {David A. Schmidt},
  address = {Boston}
}

@TechReport{SchmidtR-1991,
  author = {Renate A. Schmidt},
  title = {Algebraic Terminological Representation},
  institution = {Max-Planck-Institut f\"ur Informatik},
  year = 1991,
  number = {MPI-I-91-216},
  address = {Im Stadtwald, Saarbr\"ucken},
  month = NOV,
  authorsAddress = {schmidt@mpi-sb.mpg.de},
  WKloc = {C-0003},
  bibliographies = {RelMiCS},
  abstract = {This thesis investigates terminological
		  representation languages, as used in {\sc
		  kl-one}-type knowledge representation systems, from
		  an algebraic point of view. Terminological
		  representation languages are based on two primitive
		  syntax types, called concepts and roles, which are
		  usually interpreted model-theoretically as sets and
		  relations, respectively. I propose an algebraic
		  rather than a model-theoretic approach. I show that
		  terminological representations can be naturally
		  accomodated in equational algebras of sets
		  interacting with relations, and I use equational
		  logic as a vehicle for reasoning about concepts
		  interacting with roles.}
}

@TechReport{SchmidtR-1992,
  author = {Renate A. Schmidt},
  title = {Terminological Representation, Natural Language \&
		  Relation Algebra},
  institution = {Max-Planck-Institut f\"ur Informatik},
  year = 1992,
  number = {MPI-I-92-246},
  address = {Im Stadtwald, Saarbr\"ucken},
  month = OCT,
  authorsAddress = {Renate.Schmidt@mpi-sb.mpg.de},
  WKloc = {C-0006},
  bibliographies = {RelMiCS},
  abstract = {In this paper I establish a link between {\sc
		  kl-one}-based knowledge representation concerned
		  with {\em terminological representation} and the
		  work of P. Suppes (1976, 1979, 1981) and
		  M. B\"ottner (1985, 1989) in computational
		  linguistics. I show how this link can be utilised
		  for the problem of finding adequate terminological
		  representations for given information formulated in
		  ordinary English.},
  keywords = {Terminological representation, natural language,
		  relation algebra, Boolean modules},
  note = {To appear in {\em Proceedings of the German Workshop
		  on Artificial Intelligence (GWAI-92)},
		  Springer-Verlag, Berlin}
}

@Misc{SchmidtSchauss-1996,
  author = {Manfred Schmidt-Schau\3},
  title = {A Partial Rehabilitation of Side-Effecting I/O: Non-Determinism in Non-Strict Functional Languages},
  year = 1996,
  WKloc = {A-0516}
}

@Misc{SchmidtSchauss-1996a,
  author = {Manfred Schmidt-Schau\3},
  title = {{CPE}: A Claculus for Proving Equivalence of Expressions in a Non-Strict Functional Language},
  year = 1996,
  WKloc = {A-0595}
}

@Article{Schmitz-1982,
  year = 1982,
  title = {An exercise in program synthesis: algorithms for computing the
      transitive closure of a relation},
  number = 1,
  journal = {Science of Computer Programming},
  author = {Lothar Schmitz},
  bibliographies = {RelMiCS}
}

@Article{Schmitz-1982a,
  year = 1982,
  volume = 30,
  title = {An Improved Transitive Closure Algorithm},
  journal = COMPUTING,
  author = {Lothar Schmitz},
  bibliographies = {RelMiCS}
}

@Article{Schmitz-1984,
  year = 1984,
  volume = 15,
  title = {On the correct elimination of chain productions from {LR} parsers},
  number = 2,
  journal = {International Journal of Computer Mathematics},
  author = {Lothar Schmitz}
}

@TechReport{Schmitz-1986,
  author = {Lothar Schmitz},
  title = {Adapting the {Earley-Harrison} Algorithm to a Class
		  of Non-Context-Free Grammars},
  year = 1986,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  number = 8608,
  month = JUN
}

@TechReport{Schmitz-1989,
  year = 1989,
  title = {{Wiederverwendbarkeit in Ada und Smalltalk --- Vergleich an
      einem Beispiel aus der B\"urowelt}},
  number = 8903,
  month = JUL,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {Lothar Schmitz}
}

@TechReport{Schmitz-1990,
  year = 1990,
  title = {{Class Inheritance and Program Transformations}},
  number = 9001,
  month = MAR,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {Lothar Schmitz}
}

@TechReport{Schmitz-1990a,
  year = 1990,
  title = {{Zur Gestaltung einiger rechnergest\"utzter Lernwerkzeuge}},
  number = 9008,
  month = AUG,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {Lothar Schmitz}
}

@Article{Schmitz-1990b,
  year = 1990,
  volume = 13,
  title = {{Wiederverwendbarkeit von Software --- eine Fallstudie anhand
      von Ada und Smalltalk}},
  journal = {Informatik-Spektrum},
  author = {Lothar Schmitz}
}

@TechReport{Schmitz-1991,
  year = 1991,
  title = {Watching the Objects within a Compiler},
  number = 9112,
  month = NOV,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur
      Informatik},
  author = {Lothar Schmitz}
}

@Article{Schmitz-1992,
  year = 1992,
  title = {{Gl\"aserne Compiler generieren}},
  number = 6,
  journal = {unix/mail},
  author = {Lothar Schmitz}
}

@Article{Schmitz-1992a,
  year = 1992,
  title = {{Zur Spezifikation objekt-orientierter Programme}},
  journal = {GI Software-Technik Trends},
  author = {Lothar Schmitz}
}

@Article{Schmitz-1992b,
  year = 1992,
  volume = 13,
  title = {Using Inheritance to Explore a Family of Algorithms},
  journal = {Structured Programming},
  author = {Lothar Schmitz}
}

@Book{Schmitz-1995,
  author = {Lothar Schmitz},
  title = {{Syntaxbasierte Programmierwerkzeuge}},
  year = 1995,
  publisher = {B.~G.~Teubner},
  address = {Stuttgart}
}

@TechReport{Schmitz-Wiehle-1985,
  year = 1985,
  title = {{Systemdarstellung mit Hilfe eines programmbeschrifteten
      Netzmodells: Anmerkungen zur Beschreibungsmethodik}},
  number = 8507,
  month = SEP,
  institution = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at f\"ur Informatik},
  author = {Lothar Schmitz and H.R. Wiehle}
}

@TechReport{Schneider-1970,
  author = {H. J. Schneider},
  title = {{Chomsky-Systeme f\"ur partielle Ordnungen}},
  type = {Arbeitsber.\null{} d.\null{} Inst.\null{} f.\null{} Math.\null{} Masch.\null{} u.\null{} Datenver.},
  number = {3, 3},
  institution = {Friedrich-Alexander-Universit{\"a}t Erlangen-N{\"u}rnberg},
  year = 1970
}

@InProceedings{Schneider-1990,
  author = {Hans J{\"u}rgen Schneider},
  title = {Describing Distributed Systems by Categorical Graph
		  Grammars},
  crossref = {WG89},
  pages = {121--135}
}

@InProceedings{Schneider-1991,
  author = {H.-J. Schneider},
  title = {On Categorical Graph Grammars Integrating Structural
		  Transformations and Operations on Labels},
  crossref = {GraTra91},
  pages = {257--274},
  WKloc = {A-0152},
  abstract = {Graph-theoretic structures are an obvious means to
		  reason about systems of asynchrounous processes.
		  Their dynamic behaviour can be simulated by applying
		  productions of a graph grammar. The present paper is
		  motivated by looking for a formal method that is
		  able to describe the behaviour of systems of
		  processes that share data structures. We generalizze
		  the categorical graph-grammar approach by labelling
		  the graphs with elements of a suitable category
		  rather than with those of an alphabet. Thus,
		  operations can be performed on the labels while the
		  graphs are rewritten. After presenting the
		  fundamental definitions and some properties, we
		  demonstrate the usefulness of the approach by
		  modelling some well-known Petri nets as well as a
		  generalized net the places of which are labelled
		  with graphs. Finally, we show that known theoretical
		  techniques are applicable to the generalized
		  framework by exemplary discussing parallel
		  independence of derivation steps.}
}

@InProceedings{SchneiderF-2001,
  author = {Fred Schneider},
  title = {Language-Based Security: What's Neded and Why},
  crossref = {SAS2001},
  pages = 374
}

@InProceedings{Schobbens-1992,
  author = {Pierre-Yves Schobbens},
  title = {Second-Order Proof Systems for Algebraic
		  Specification Languages},
  crossref = {SADT92},
  pages = {321--337},
  annote = {see \cite{Schobbens-1993}}
}

@InProceedings{Schobbens-1993,
  author = {Pierre-Yves Schobbens},
  title = {Extensions of Initial Models and their Second-Order
		  Proof Systems},
  crossref = {HOA1993},
  pages = {326--344},
  bibliographies = {RelMiCS},
  abstract = {Besides explicit axioms, an algebraic specification
		  language contains model-theoretic constraints such
		  as initiality. For proving proerties of
		  specifications and refining them to programs, an
		  axiomatization of these constraints is needed;
		  unfortunaltely, no effective, sound and complete
		  proof system can be constructed for initial models,
		  and {\em a fortiori} for their extensions.

                  In this paper, we construct non-effective
		  second-order axiomatizations for the initiallity
		  constraint, and its recently proposed extensions
		  (minimal, quasi-free and surjective models) designed
		  to deal with disjunction and existential quantification.}
}

@Article{Schoenfeld-1979a,
  author = {Wolfgang Sch\"onfeld},
  title = {An Undecidability Result for Relation Algebras},
  journal = JSYLO,
  volume = 44,
  year = 1979,
  pages = {111--115},
  bibliographies = {RelMiCS}
}

@Book{Schoenfeld-1981,
  title = {Gleichungen in der Algebra der bin\"aren Relationen},
  author = {Wolfgang Sch\"onfeld},
  year = 1981,
  series = {Fachserie Naturwissenschaften},
  volume = {},
  note = {Habilitation},
  publisher = Minerva,
  bibliographies = {RelMiCS}
}

@Article{Schoenfeld-1982,
  author = {Wolfgang Sch\"onfeld},
  title = {Upper Bounds for a Proof-search in a Sequent Calculus
		for Relational Equations},
  journal = ZMALOG,
  volume = 28,
  year = 1982,
  pages = {239--246},
  bibliographies = {RelMiCS}
}

@Article{Schoenfinkel-1924,
  author = {M. Sch\"onfinkel},
  title = {{\"Uber die Bausteine der mathematischen Logik}},
  journal = {Mathematische Annalen},
  year = 1924,
  volume = 92,
  pages = {305--316},
  keywords = {combinators, curried, functional, schonfinkel},
  refsfrom = { <(wkd-intro)> }
}

@Book{Schoening-Pruim-1998,
  author = {Uwe Sch{\"o}ning and Randall Pruim},
  title = {Gems of Theoretical Computer Science},
  publisher = Springer,
  year = 1998,
  ISBN = {3-540-64425-3},
  UniBwM = {INF006/YE286}
}

@InProceedings{Schoenmakers-1992,
  author = {Berry Schoenmakers},
  title = {Inorder Traversal of a Binary Heap and its Inversion
		  in Optimal Time and Space},
  crossref = {MPC1992},
  pages = {291--301},
  WKloc = {A-0241},
  abstract = {In this paper we derive a linear-time,
		  constant-space algorithm to construct a binary heap
		  whose inorder traversal equals a given sequence. We
		  do so in two steps. First, we invert a program that
		  computes the inorder traversal of a binary heap,
		  using the proof rules for program inversion by W.
		  Chen and J.T.~Udding. This results in a linear-time
		  solution in terms of binary trees. Subsequently, we
		  data-refine this program to a constant-space
		  solution in terms of linked structures.}
}

@InProceedings{Schoesser-Geiss-2007a,
  author = 	 {Andreas Sch{\"o}sser and Rubino Geiss},
  title = 	 {Graph Rewriting for Hardware Dependent Program Optimisations},
  crossref =  {AGTIVE2007PP},
  pages = 	 {229--244}
}

@Book{SchrampferAzar-1999,
  author = {Betty Schrampfer Azar},
  title = {Understanding and Using English Grammar},
  publisher = {Longman},
  year = 1999,
  edition = {Third Edition},
  ISBN = {0-13-958661-X},
  note = {with answer book},
  bibliographies = {Liping}
}

@Misc{Schreiner-1993,
  author = {Wolfgang Schreiner},
  title = {Parallel Functional Programming --- An Annotated Bibliography},
  month = DEC,
  year = 1993,
  WKloc = {B-0121}
}

@InProceedings{Schrijvers-PeytonJones-Chakravarty-Sulzmann-2008,
  author =       {Tom Schrijvers and Peyton Jones, Simon and Manuel Chakravarty and Martin Sulzmann},
  title =        {Type Checking with Open Type Functions},
  crossref =  {ICFP2008},
  pages =     {51--62},
  WKloc =      {doc/pap/BIB},
  DOIURL =    {http://doi.acm.org/10.1145/1411204.1411215},
  abstract = {We report on an extension of Haskell with open
                  type-level functions and equality constraints that
                  unifies earlier work on GADTs, functional
                  dependencies, and associated types. The contribution
                  of the paper is that we identify and characterise
                  the key technical challenge of entailment checking;
                  and we give a novel, decidable, sound, and complete
                  algorithm to solve it, together with some
                  practically-important variants. Our system is
                  implemented in GHC, and is already in active use.}
}

@Book{Schroeder-1890,
  year = {1890 -- 1905},
  title = {{Vorlesungen \"uber die Algebra der Logik}, Volumes 1 to 3},
  publisher = Teubner,
  note = {Reprinted by Chelsea, New York, 1966},
  author = {E. Schr{\"o}der},
  address = {Leipzig},
  bibliographies = {RelMiCS},
  McMaster = {BC 135 .S38 bd.1, bd.2, bd.3}
}

@Book{Schroeder-1895,
  author = {Ernst Schr\"oder},
  title = {{Vorlesungen \"uber die Algebra der Logik (exacte Logik)}},
  note = {Vol.\null{} 3, Algebra und Logik der Relative, part I,
      {{$2^{nd}$} edition published by Chelsea, 1966.}},
  address = {Leipzig},
  publisher = Teubner,
  year = 1895,
  bibliographies = {RelMiCS},
  McMaster = {BC 135 .S38 bd.3}
}

@Article{Schroeder-1895a,
  author = {F. W. K. Ernst Schr\"oder},
  title = {{Note \"uber die Algebra der binaren Relative}},
  journal = MANN,
  volume = 46,
  year = 1895,
  pages = {144--158},
  bibliographies = {RelMiCS}
}

@Article{SchroederL-2000a,
  author = {Lutz Schr{\"o}der},
  title = {Isomorphisms and splitting of idempotents in semicategories},
  year = {2000},
  journal = {Cahiers de Topologie et G{\'e}om{\'e}trie Diff{\'e}rentielle cat{\'e}goriques},
  volume = {41},
  pages = {143--153},
  keywords = {isomorphism, split idempotent, semicategory},
  psurl = {http://www.informatik.uni-bremen.de/~lschrode/CTGD2.ps},
  abstract = {It is shown that for certain systems of generators
     and relations for categories, called semicategories, the
     generated category does not contain isomorphisms other than those
     already specified in the generating system. Furthermore, the
     splitting of idempotents in the generated category can be reduced
     to a splitting property in the generating semicategory.},
  bibliographies = {RelMiCS}
}

@Article{SchroederL-2006,
  author = {Lutz Schr{\"o}der},
  title = {The {HasCASL} Prologue ---
    Categorical Syntax and Semantics of the Partial {$\lambda$}-calculus},
  year = {2006},
  journal = TCS,
  volume = {353},
  pages = {1--25},
  WKloc = {A-1684},
  keywords = {partial lambda-calculus, partial cartesian closed category, Henkin model, HasCASL, CASL},
  pdfurl = {http://www.informatik.uni-bremen.de/~lschrode/hascasl/prologue.pdf},
  psurl = {http://www.informatik.uni-bremen.de/~lschrode/hascasl/prologue.ps},

  abstract = {We develop the semantic foundations of the
     specification language HasCASL, which combines algebraic
     specification and functional programming on the basis of Moggi's
     partial {$\lambda$}-calculus. Generalizing Lambek's classical
     equivalence between the simply typed {$\lambda$}-calculus and
     cartesian closed categories, we establish an equivalence between
     partial cartesian closed categories (pccc's) and partial
     {$\lambda$}-theories. Building on these results, we define
     (set-theoretic) notions of intensional Henkin model and syntactic
     {$\lambda$}-algebra for Moggi's partial
     {$\lambda$}-calculus. These models are shown to be equivalent to
     the originally described categorical models in pccc's via the
     global element construction. The semantics of HasCASL is defined
     in terms of syntactic {$\lambda$}-algebras. Correlations between
     logics and classes of categories facilitate reasoning both on the
     logical and on the categorical side; as an application, we
     pinpoint unique choice as the distinctive feature of topos logic
     (in comparison to intuitionistic higher-order logic of partial
     functions, which by our results is the logic of pccc's with
     equality). Finally, we give some applications of the
     model-theoretic equivalence result to the semantics of HasCASL
     and its relation to first-order CASL. }
}

@Article{SchroederL-Mateus-2002,
  author = {Lutz Schr{\"o}der and Paulo Mateus},
  title = {Universal aspects of probabilistic automata},
  year = {2002},
  journal = MSCS,
  volume = {12},
  pages = {481--512},
  keywords = {precategory probabilistic automaton adjunction decision tree},
  psurl = {http://www.informatik.uni-bremen.de/~lschrode/MSCS1.ps},
  abstract = {For lack of composability of their morphisms,
     probability spaces, and hence probabilistic automata, fail to
     form categories; however, they fit into the more general
     framework of precategories, which are introduced and studied
     here. In particular, the notion of adjunction and weak adjunction
     for precategories is presented and justified in detail. As an
     immediate benefit, a concept of (weak) product for precategories
     is obtained. Thus, universal properties can be used for
     characterizing well-known basic constructions in the theory of
     probabilistic automata: The aggregation of two automata is shown
     to be a weak product, whereas restriction and interconnection of
     automata are recognized as Cartesian lifts. Finally, we establish
     that the precategory of decision trees is coreflexive in the
     precategory of probabilistic automata.}
}

@Misc{Schubert-1997,
  author = {Aleksy Schubert},
  title = {Second-order Unification and Type Inference for {Church}-style polymorphism},
  year = 1997,
  WKloc = {A-0477}
}

@InProceedings{Schuerr-1990,
  author = {Andy Sch{\"u}rr},
  title = {Introduction to {PROGRES}, an Attribute Graph Grammar
		  Based Specification Language},
  pages = {151--165},
  crossref = {WG89}
}

@Book{Schuerr-1991,
  keywords = {PROGRESS, graph rewriting, IPSEN},
  year = 1991,
  title = {{Operationales Spezifizieren mit programmierten
		  Graphersetzungssystemen}},
  publisher = {Deutscher Universit\"ats-Verlag},
  author = {Andreas Sch\"urr}
}

@InProceedings{Schuerr-1993,
  author = {Andi Sch\"urr},
  title = {Logic Based Structure Rewriting Systems},
  crossref = {GTCS93},
  pages = {341--357},
  abstract = {This paper presents a new logic based framework for
		  the formal treatment of graph rewriting systems and
		  graph languages as special cases of rewriting
		  systems and languages for arbitrary relational data
		  structures. Considering its expressive power, the
		  new formalism is intended to contain almost all
		  forms of nonparallel algebraic as well as
		  algorithmic graph grammar approaches as special
		  cases. Furthermore, our formalism tries to close the
		  gap between the operation oriented manipulation of
		  data structures by means of logic based languages.

                  Nevertheless, the main motivation for the
		  development of yet another structure rewriting
		  approach was not to combine the advantages of graph
		  grammar based and logic based languages but to
		  provide us with a solid fundament for a formal
		  definition of our own graph grammar based language
		  PROGRES, which is a kind of visual data definition
		  and data manipulation language. This language has
		  been designed for specifying and rapid prototyping
		  of complex data structures and contains many
		  constructs which enhance its expressiveness
		  considerably (like definition of derived properties,
		  consistency constraints, rewrite rules with complex
		  application conditions and embedding rules) but
		  which were not definable within the framework of a
		  single already existing graph grammar approach.},
  keywords = {IPSEN, PROGRES},
  WKloc = {A-0297}
}

@InProceedings{Schuerr-1994,
  author = {Andy Sch\"urr},
  title = {Specification of Graph Translators with Triple Graph Grammars},
  abstract = {Data integration is a key issue for any integrated set of
             software tools where each tool has its own data structures (at
             least on the conceptual level), but where we have many
             interdependencies between these private data structures. A
             typical CASE environment, for instance, offers tools for the
             manipulation of requirements and software design documents and
             provides more or less sophisticated assistance for keeping
             these documents in a consistent state. Up to now almost all of
             these data consistency observing or preserving integration
             tools are hand-crafted due to the lack of generic
             implementation frameworks and the absence of adequate
             specification formalisms. Triple graph grammars, a proper
             superset of pair grammars, are intended to fill this gap and to
             support the specification of interdependencies between
             graph-like data structures on a very high level. Furthermore,
             they form a solid fundament of a new machinery for the
             production of batch-oriented as well as incrementally working
             data integration tools.},
  crossref = {WG1994},
  annote = {remarks to the author:
           The center graph of a graph tripel
           appears to be needed only for coding a relation between the left
           and right-hand sides of the triple graph. It is not clear what is
           to be gained by this coding, since the example only presents
           discrete center graphs. Perhaps relation algebraic considerations
           [Schmidt, Str\"ohlein: Relations and Graphs, Springer, 1993]
           might shed more light on this issue than the perhaps rather
           artificial coding into two fuctions. Could production triples not
           be presented as morphisms in a category of graph triples? Would
           this not turn the application of a production triple into a
           simple pushout in that category? Maybe this would even simplify
           some of the proofs; certainly it would give the comfort of
           staying within an established theory (of High Level Replacement
           Systems, see LNCS 532, pp. 269--287).

        Summary
           The problem dealt with in this paper is that of pairs of closely
           interrelated graphs, where changes can be applied selectively to
           one of the graphs, and some sort of consistency between the two
           graphs has to be controlled or even regained by updating the
           other one. This is motivated by applications in the field of
           software engineering. This paper refines the old idea of pair
           grammars to triple grammars, thus achieving the possibility to
           carry arbitrary relations (not only one-to-one) between the pairs
           of graphs in question over context-sensitive productions ---
           these are restricted to monotonic ones for practical reasons. A
           formalism is built for handling simple triple grammars, and
           properties are proven that allow the construction of tools for
           controlling or regaining consistency between two graphs.

           The paper starts out from the application oriented problem
           description and the presentation is clear throughout. It is
           probably the first paper from the PROGRES/IPSEN projects to
           introduce the algebraic approach of graph rewriting into their
           setting, and a few obvious questions in this respect remain
           unanswered (see remarks to the author).

           On the whole, this
           paper presents an effort to tackle an intricate probblem
           motivated by applications in a declarative, formal manner by
           introducing an interesting variant of graph grammars.},
  WKloc = {A-0231}
}

@InProceedings{Schuerr-1994b,
  author = {Andy Sch\"urr},
  title = {Programmed Graph Transformations and Graph Transformation Units in {GRACE}},
  pages = {122--136},
  crossref = {GG1994}
}

@InCollection{Schuerr-1997,
  author = {Andy Sch{\"u}rr},
  title = {Programmed Graph Replacement Systems},
  crossref = {HBGraTraI},
  pages = {479--546},
  OPTabstract = {},
  WKloc = {B-0045}
}

@Misc{Schuerr-2000v,
  author = {Andy Sch{\"u}rr},
  title = {Software-Engineering},
  howpublished = {Folien zur Vorlesung im FT 2000 an der uniBw M\"unchen},
  OPTyear = 2000,
  WKloc = {A-1033: 5. Objektorientierte Anforderungsanalyse}
}

@InCollection{Schuerr-Winter-Zuendorf-1999,
  author = {Andy Sch{\"u}rr and Andreas J. Winter and A. Z{\"u}ndorf},
  title = {The {PROGRES} Approach: Language and Environment},
  crossref = {HBGraTraII},
  pages = {487--550},
  chapter = 13,
  keywords = {DIANE Graphs}
}

@Book{Schuette-1987,
  author = {A. Sch{\"u}tte},
  title = {Spezifikation und Generierung von \"Ubersetzern f\"ur Graphsprachen durch attributierte Graphgrammatiken},
  publisher = {EXpress Edition},
  year = 1987,
  note = { Dissertation, EWH Koblenz}
}

@Misc{Schuetz-SchmidtSchauss-Panitz-199X,
  author = {Marko Sch{\"u}tz and Manfred Schmidt-Schau\3 and Sven Eric Panitz},
  title = {Efficient Strictness Analysis of Haskell in Haskell Using Abstract Reduction},
  year = {199?},
  WKloc = {A-0596}
}

@Book{SchukatTalamazzini-1995,
  author = {E.G. Schukat-Talamazzini},
  title = {Automatische Spracherkennung},
  publisher = {vieweg},
  year = 1995,
  UniBwM = {KYB800/YA6031},
  keywords = {ISADORA}
}

@PhDThesis{Schulte-2000,
  author = {Wolfram Schulte},
  title = {High-integrity Compilation and Secure Execution of Java},
  year = 2000,
  affiliation = {Abteilung Programmiermethodik und Compilerbau},
  school = {Fakult\"at f\"ur Informatik, Universit\"at Ulm},
  type = {Habilitationsschrift}
}

@Article{SchulteE-DavisonD-Dye-Dominik-2012,
  author =	{Eric Schulte and Dan Davison and Thomas Dye and Carsten Dominik},
  title =	{A Multi-Language Computing Environment for Literate Programming and Reproducible Research},
  journal =	"Journal of Statistical Software",
  volume =	"46",
  number =	"3",
  pages =	"1--24",
  day =  	"25",
  month =	"1",
  year = 	"2012",
  CODEN =	"JSSOBK",
  ISSN = 	"1548-7660",
  URL =  	"http://www.jstatsoft.org/v46/i03",
  WKloc = {doc/pap/BIB},
  keywords =	{org-mode},
}

@Book{Schulz-1986,
  author = {Klaus Schulz},
  title = {An Exact Algorithm for Interval-based Temporal Information},
  note = {FNS-Bericht-86-9},
  publisher = Tuebingen,
  address = {Univ.\null{}  T\"ubingen},
  year = 1986,
  bibliographies = {RelMiCS}
}

@Book{Schulz-1986a,
  author = {Klaus Schulz},
  title = {On the Categoricity of Countable Interval Structures},
  note = {SNS-Bericht 88-34},
  publisher = TuebingenS,
  address = {Univ.\null{}  T\"ubingen},
  year = 1986,
  bibliographies = {RelMiCS}
}

@Book{Schulz-1987,
  author = {Klaus Schulz},
  title = {Event and Interval Structures: {A} Mathematical Comparison},
  note = {FNS-Bericht-87-18},
  publisher = Tuebingen,
  address = {Univ.\null{}  T\"ubingen},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Book{Schumann-2001,
  author =	 {Schumann, Johann M.},
  title = 	 {Automated Theorem Proving in Software Engineering},
  publisher = 	 Springer,
  year = 	 2001,
  ISBN = 	 {3-540-67989-8},
  WKloc = 	 {owned, \lent{Ji Xiaoheng}}
}

@Misc{Schwabe-Rossi-Barbosa-199X,
  author = {Daniel Schwabe and Gustavo Rossi and Simone D. J. Barbosa},
  title = {Systematic Hypermedia Application Design with {OOHDM}},
  year = {199X},
  WKloc = {A-0495}
}

@Misc{Schwartzbach-1995,
  author = {Michael I. Schwartzbach},
  title = {Polymorphic Type Inference},
  year = 1995,
  WKloc = {A-0499}
}

@Manual{Schwarzkopf-1994,
  title = {A Manual of {Ipe}},
  author = {Otfried Schwarzkopf},
  organization = {Vakgroep Informatica, Universiteit Utrecht},
  edition = {version 5.0},
  month = AUG,
  year = 1994,
  WKloc = {B-0112}
}

@InProceedings{Schwarzkopf-1995,
  author = {Otfried Schwarzkopf},
  title = {The Extensible Drawing Editor {Ipe}},
  pages = {C10--C11},
  ISBN = {0-89791-724-3},
  booktitle = {Proceedings of the 11th Annual Symposium on
                 Computational Geometry},
  month = jun,
  publisher = {ACM Press},
  address = {New York, NY, USA},
  year = 1995
}

@InProceedings{Schweimeier-Jeffrey-1999,
  author = {Ralf Schweimeier and Alan Jeffrey},
  title = {A Categorical and Graphical Treatment of Closure Conversion},
  booktitle = {Proc.\null{} Mathematical Foundations of Programming Semantics},
  year = 1999,
  series = {Electronic Notes in Comp. Sci.},
  publisher = {Elsevier},
  WKloc = {A-0751},
  URL = {ftp://klee.cs.depaul.edu/ajeffrey/mfps99.ps.gz},
  bibliographies = {GraphCalc}
}

@Article{Schweitzer-1909,
  author = {A. R. Schweitzer},
  title = {A Theory of Geometrical Relations},
  journal = AJM,
  volume = 31,
  year = 1909,
  pages = {365--410},
  bibliographies = {RelMiCS}
}

@Book{Schweizer-Sklar-1983,
  author =	 {B. Schweizer and A. Sklar},
  title = 	 {Probabilistic Metric Spaces},
  publisher = 	 {North-Holland},
  year = 	 1983,
  address =	 {Amsterdam},
  annote = {cited by \cite{DiNola-Pedrycz-Sessa-1995} as textbook for triangular norms (t-norms)}
}

@Misc{Schwichtenberg-1993,
  author = {H. Schwichtenberg},
  title = {{Einf\"uhrung in den Lambda-Kalk\"ul}},
  howpublished = {Handout zum Seminar WS 1993/94},
  WKloc = {A-0258}
}

@Misc{Schwichtenberg-1995a,
  OPTkey = {},
  OPTauthor = {Helmut Schwichtenberg},
  OPTtitle = {Computational Content of Proofs},
  OPThowpublished = {International Summer School Marktoberdorf,
		  Working Material},
  OPTyear = 1995,
  OPTmonth = JUL,
  OPTnote = {},
  OPTwkloc = {A-0412},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Scott-1991,
  title = {Will Logicians be Replaced by Machines? ({Abstract})},
  author = {Dana S. Scott},
  pages = 771,
  crossref = {TACS1991},
  note = {invited lecture},
  abstract = {Many workers have been and will be soon displaced by
		  machines, and we have to ask whether it is only a
		  matter of time before teachers are as well. The
		  lecture will review the march of technology and how
		  developments are apt to affect teaching and research
		  in many subjects.  To be able to understand the
		  emerging situation, we also have to reflect on the
		  nature of studies in the Foundations of Mathematics.}
}

@Article{Scott-1993_PCF,
  author =    {Dana S. Scott},
  title =     {A type-theoretical alternative to {ISWIM}, {CUCH}, {OWHY}},
  journal =   TCS,
  year =      1993,
  volume =    121,
  pages =     {411--440},
  annote =    {Original definition of PCF},
  WKloc = {doc/pap/BIB}
}

@Book{ScottML-2005,
  author = 	 {Michael L. Scott},
  title = 	 {Programming Language Pragmatics},
  publisher = 	 {Morgan Kaufmann},
  year = 	 2005,
  edition = 	 {2nd edition},
  ISBN = {978-0126339512},
  AMAZON = {2008-05-11 \$72.62 http://www.amazon.ca/dp/0126339511/ref=pe_ar_x7},
  bibliographies = {SE3E}
}

@Book{Sebesta-,
  author = {Robert W. Sebesta},
  title = {Concepts of Programming Languages},
  publisher = {},
  year = {},
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  OPTnote = {},
  bibliographies = {SE3E}
}

@InProceedings{Secher-Soerensen-2002,
  author = {Jens Peter Secher and Morten Heine S\orensen},
  title = {From checking to inference via driving and dag grammars},
  crossref = {PEPM2002},
  journal = {SIGPLAN Not.},
  volume = {37},
  number = {3},
  year = {2002},
  issn = {0362-1340},
  pages = {41--51},
  doi = {http://doi.acm.org/10.1145/509799.503036},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {PMC, FLP},
  abstract = {Abramov and Gl\"uck have recently introduced a
      technique called URA for inverting first order functional
      programs. Given some desired output value, URA computes a
      potentially infinite sequence of substitutions/restrictions
      corresponding to the relevant input values. In some cases this
      process does not terminate. In the present paper, we propose a
      new program analysis for inverting programs. The technique works
      by computing a finite grammar describing the set of all input
      that relate to a given output. During the production of the
      grammar, the original program is implicitly transformed using
      so-called driving steps. Whereas URA is sound and complete, but
      sometimes fails to terminate, our technique always terminates
      and is complete, but not sound. As an example, we demonstrate
      how to derive type inference from type checking.The idea of
      approximating functional programs by grammars is not new. For
      instance, the second author has developed a technique using tree
      grammars to approximate termination behaviour of
      deforestation. However, for the present purposes it has been
      necessary to invent a new type of grammar that extends tree
      grammars by permitting a notion of sharing in the
      productions. These dag grammars seem to be of independent
      interest. }

}

@PhDThesis{Seidl-1992,
  author = {Helmut Seidl},
  title = {Ambiguity, Valuedness and Costs},
  school = {Fachbereich Informatik, Universit\"at des Saarlandes},
  year = 1992,
  type = {Habilitationsschrift},
  month = NOV,
  URL = {http://www.informatik.uni-trier.de/~seidl/},
  WKloc = {doc/pap/BIB/Seidl-1992.ps.gz},
  abstract = {This survey reports results on the ambiguity of
              finite tree automata, the valuedness problem of
              bottom-up finite state tree transducers and
              boundedness of cost automata.}
}

@TechReport{Sekar-Ramesh-Ramakrishnan-1992,
  WKloc = {B-0010},
  abstract = {Pattern matching is an important operation used in many
      applications such as functional programming, rewriting and rule-based
      expert systems. By preprocessing the patterns into a DFA-like
      automaton, wecan rapidly select the matching pattern(s) in a single
      scan of the relevant portions of the input term. This automaton is
      typically based on left-to-right traversal (of the patterns) or its
      variants. By {\em adapting} the traversal order to suit the set of
      input patterns, it is possible to considerably reduce the space and
      matching time requirements of the automaton. The design of such
      adaptive automata is the focus of this paper. In this context we
      study several important problems that have remained open even for
      automata based on left-to-right traversals. Such problems include
      upper and lower bounds on space complexity, construction of optimal
      DAG automata and impact of typing in pattern matching. An interesting
      consequence of our results is that lazy pattern matching in typed
      systems (such as ML) is computationally hard whereas it can be done
      efficiently in untyped systems.},
  year = 1992,
  title = {Adaptive Pattern Matching},
  note = {to appear at ICALP '92},
  institution = {SUNY at Stony Brook},
  author = {R.C. Sekar and R. Ramesh and I.V. Ramakrishnan}
}

@InProceedings{Sekerinski-1992,
  author = {Emil Sekerinski},
  title = {A Calculus for Predicative Programming},
  crossref = {MPC1992},
  pages = {302--322},
  abstract = {A calculus for developing programs from
		  specifications written as predicates that describe
		  the relationship between the initial and final state
		  is proposed. Such specifications are well known from
		  the specification language Z. All elements of a
		  simple sequential programming notation are defined
		  in terms of predicates. Hence programs form a subset
		  of specifications. In particular, sequential
		  composition is defined by `demonic composition',
		  nondeterministic choice by `demonic disjunction', and
		  iteration by fixed points. Laws are derived which
		  allow proving equivalence and refinement of
		  specifications and programs by a series of steps.
		  The weakest precondition calculus is also included.
		  The approach is compared to the predicative
		  programming approach of E. Hehner and to other
		  refinement calculi.},
  WKloc = {A-1285, doc/pap/BIB},
  URL = {http://www.cas.mcmaster.ca/~emil/publications/predicative/index.html},
  bibliographies = {RelMiCS, RelMiS}
}

@Article{Sekerinski-2003,
  author = 	 {Emil Sekerinski},
  title = 	 {Exploring Tabular Verification and Refinement},
  journal = 	 {Formal Aspects of Computing},
  year = 	 {2003},
  DOI = 	 {10.007/s00165-003-0010-9},
  volume = 	 {15},
  pages = 	 {215--236},
  WKloc = {A-1762, submitted: A-1539},
  URL = {http://www.cas.mcmaster.ca/~emil/publications/exploringtabular/},
  abstract = {Tabular representations have been proposed for
      structuring complex mathematical expressions as they appear in
      the specification of programs. We argue that tables not only
      help in writing and checking complex expressions, but also in
      their formal manipulation. More specifically, we explore the use
      of tabular predicates and tabular relations in program
      verification and refinement.}
}

@PhDThesis{Selinger-1997,
  author = {Peter Selinger},
  title = {Functionality, Polymorphism, and Concurrency: A Mathematical
      Investigation of Programming Paradigms},
  school = {University of Pennsylvania},
  year = 1997,
  month = JUN,
  note = {Appeared as IRCS Technical Report 97-17:
    \textsf{http://www.cis.upenn.edu/~ircs/reports/trs/abstracts97.html}},
  URL = {http://quasar.mathstat.uottawa.ca/~selinger/papers/index.html#diss},
  WKloc = {A-1487, doc/pap/BIB},
  abstract = {The search for mathematical models of computational
     phenomena often leads to problems that are of independent
     mathematical interest. Selected problems of this kind are
     investigated in this thesis. First, we study models of the untyped
     lambda calculus. Although many familiar models are constructed by
     order-theoretic methods, it is also known that there are some models
     of the lambda calculus that cannot be non-trivially ordered. We show
     that the standard open and closed term algebras are unorderable. We
     characterize the absolutely unorderable T-algebras in any algebraic
     variety T. Here an algebra is called absolutely unorderable if it
     cannot be embedded in an orderable algebra. We then introduce a
     notion of finite models for the lambda calculus, contrasting the
     known fact that models of the lambda calculus, in the traditional
     sense, are always non-recursive. Our finite models are based on
     Plotkin's syntactical models of reduction. We give a method for
     constructing such models, and some examples that show how finite
     models can yield useful information about terms. Next, we study
     models of typed lambda calculi. Models of the polymorphic lambda
     calculus can be divided into environment-style models, such as Bruce
     and Meyer's non-strict set-theoretic models, and categorical models,
     such as Seely's interpretation in PL-categories. Reynolds has shown
     that there are no set-theoretic strict models. Following a different
     approach, we investigate a notion of non-strict categorical
     models. These provide a uniform framework in which one can describe
     various classes of non-strict models, including set-theoretic models
     with or without empty types, and Kripke-style models. We show that
     completeness theorems correspond to categorical representation
     theorems, and we reprove a completeness result by Meyer et al. on
     set-theoretic models of the simply-typed lambda calculus with
     possibly empty types. Finally, we study properties of asynchronous
     communication in networks of communicating processes. We formalize
     several notions of asynchrony independently of any particular
     concurrent process paradigm. A process is asynchronous if its input
     and/or output is filtered through a communication medium, such as a
     buffer or a queue, possibly with feedback. We prove that the
     behavior of asynchronous processes can be equivalently characterized
     by first-order axioms.}
}

@Misc{Selinger-1998,
  author = {Peter Selinger},
  title = {A Note on Bainbridge's Powerset Construction},
  year = 1998,
  WKloc = {A-0809},
  bibliographies = {RelMiCS},
  abstract = {The category Rel of sets and relations has two natural
      traced monoidal structures: in $(Rel,+,Tr)$, the tensor is given by
      disjoint union, and in $(Rel,\times,Tr')$ by products of sets.
      Already in 1976, predating the definition of traced monoidal
      categories by 20 years, Bainbridge has shown how to model flowcharts
      and networks in these two respective settings. Bainbridge has also
      pointed out that one can move from one setting to the other via the
      powerset operation. However, Bainbridge's power operation is not
      functorial, and in this paper we show that there is no traced
      monoidal embedding of $(Rel,+,Tr)$ into $(Rel,x,Tr')$ whose object
      part is given by the powerset operation. On the other hand, we show
      that there is such an embedding whose object part is given by the
      power-multiset operation.}
}

@InProceedings{Selinger-1999,
  author = {Peter Selinger},
  title = {Categorical Structure of Asynchrony},
  WKloc = {A-0810},
  booktitle = {Proceedings of MFPS 15},
  year = 1999,
  month = JUL,
  publisher = {Elsevier Science B.V.},
  series = {Electronic Notes in Theoretical Computer Science},
  volume = 20,
  OPTpages = {?.1--?.20},
  abstract = {We investigate a categorical framework for the semantics of
      asynchronous communication in networks of parallel processes.
      Abstracting from a category of asynchronous labeled transition
      systems, we formulate the notion of a categorical model of asynchrony
      as a uniformly traced monoidal category with diagonals, such that
      every morphism is total and the focus is equivalent to a category of
      complete partial orders. We present a simple, non-deterministic,
      cpo-based model that satisfies these requirements, and we discuss how
      to refine this model by an observational congruence. We also present
      a general construction of passing from deterministic to
      non-deterministic models, and more generally, from non-linear to
      linear structure on a category.},
  annote = {``linear'' might be a generalisation of pre-monoidal.}
}

@InProceedings{Selinger-2001,
  author = {Peter Selinger},
  title = {Categorical Semantics of Control},
  crossref = {TLCA2001},
  pages = {6--7},
  WKloc = {doc/pap/BIB}
}

@Article{Selinger-2002,
  author = {Peter Selinger},
  title = {The Lambda Calculus is Algebraic},
  journal = JFP,
  year = 2002,
  volume = 12,
  number = 6,
  pages = {549-566},
  WKloc = {A-1488, doc/pap/BIB},
  URL = {http://quasar.mathstat.uottawa.ca/~selinger/papers/#combinatory},
  abstract = {This paper serves as a self-contained, tutorial
     introduction to combinatory models of the untyped lambda
     calculus. We focus particularly on the interpretation of free
     variables. We argue that free variables should not be interpreted as
     elements in a model, as is usually done, but as indeterminates. We
     claim that the resulting interpretation is more natural and leads to
     a closer correspondence between models and theories. In particular,
     it solves the problem of the notorious xi-rule, which asserts that
     equations should be preserved under binders, and which fails to be
     sound for the usual interpretation.}
}

@InProceedings{Sellink-1993,
  author = {M. P. A. Sellink},
  title = {Verifying Process Algebra Proofs in Type Theory},
  crossref = {SoSL93},
  pages = {315--339},
  keywords = {Calculus of Inductive Constructions, Coq, Curry-Howard}
}

@Misc{SemanticWeb-TR2001,
  author = {Mark Frauenfelder},
  title = {A Smarter Web},
  howpublished = {Technology Review, http://www.technologyreview.com/magazine/nov01/frauenfelderall.asp},
  month = NOV,
  year = 2001,
  WKloc = {doc/pap/BIB}
}

@InProceedings{Senizergues-1997,
  author = {G{\'{e}}raud S{\'{e}}nizergues},
  title = {Decidability of the equivalence problem for
		  deterministic pushdown automata},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTpages = {},
  booktitle = {Proceedings of INFINITY'97},
  year = 1997,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTannote = {}
}

@MastersThesis{Sepehr-2010,
  author = 	 {Sepandar Sepehr},
  title = 	 {Adding Nested Headers and a Proper
                  {GTK}-Based {GUI} to the {Haskell} Table Tools},
  school = 	 {Department of Computing and Software, McMaster University},
  year = 	 2010,
  month = 	 MAY,
  abstract = {Specification of Large Scale Systems like Safety Critical Softwares
    is an important yet frequently tedious process.
    The requirement analysts often face a large number of conditions and cases
    during the process of requirement elicitation.
    One of the tools used to deal with this
    is employing multi-dimensional mathematical expressions,
    called Tabular Expressions by Parnas,
    to show and to study the specifications
    and to also check the properties of the system.

    The compositional syntax that was introduced by Kahl
    is a simple and comprehensible basis
    for implementation of tabular expressions.
    Kahl's implementation of his formalization
    in the functional programming language Haskell
    started a new approach to syntax and semantics of tabular expressions.

    In this thesis, we expand the project that Kahl initiated
    using his formalization to establish tool support of regular tables.
    A tool like this one has always been missing
    for people who use tabular expressions in the requirement analysis.
    We have designed a platform-independent, graphically interactive,
    and expansible tool using GTK in Haskell.
    In this thesis, we have also added the support of nested headers
    and implemented some features needed to work with nested headers
    for a better practical tool.}
}

@Booklet{Sereny-1985,
  author = {G. Ser\'eny},
  title = {Lower Level Connections Between Representations of
		Relation Algebras},
  note = {Preprint, pp.\null{} 3},
  year = 1985,
  bibliographies = {RelMiCS}
}

@PhdThesis{Seres-2001,
  author = 	 {Silvija Seres},
  title = 	 {The Algebra of Logic Programming},
  school = 	 {Oxford University Computing Laboratory},
  year = 	 2001,
  WKloc = 	 {A-1697, doc/pap/BIB}
}

@Misc{Serot-199X,
  author = {Jocelyn Serot},
  title = {Embodying Parallel Functional Skeletons: An Experimental Implementation on Top of {MPI}},
  year = {199?},
  WKloc = {A-0700}
}

@InProceedings{Serrano-1994,
  author = {M. Serrano},
  title = {Using Higher-Order Control Flow Analysis When
		  Compiling Functional Languages},
  crossref = {PLILP1994},
  pages = {447--448},
  URL = {ftp://ftp.inria.fr/},
  keywords = {Bigloo, Scheme, ML}
}

@InCollection{Sestoft-Zamulin-1988,
  author = {Peter Sestoft and A. V. Zamulin},
  title = {Annotated Bibliography on Partial Evaluation and
                 Mixed Computation},
  pages = {589--622},
  booktitle = {Partial Evaluation and Mixed Computation},
  editor = {Dines Bj{\o}rner and Andrei P. Ershov and Neil D. Jones},
  publisher = {North-Holland},
  year = 1988
}

@InProceedings{Sewell-1994,
  title = {Bisimulation is Not Finitely (First Order) Equationally Axiomatisable},
  author = {Peter Sewell},
  pages = {62--70},
  crossref = {LICS9},
  WKloc = {A-0365},
  abstract = {This paper considers the existence of finite equational
		  axiomatisations of bisimulation over a calculus of
		  finite state processes.  To express even simple
		  properties such as $\mu X E=\mu X E[E/X]$
		  equationally it is necessary to use some notation
		  for substitutions.  Accordingly the calculus is
		  embedded in a simply typed lambda calculus, allowing
		  axioms such as the above to be written as equations
		  of higher type rather than as equation schemes.
		  Notions of higher order transition system and
		  bisimulation are then defined and using them the
		  nonexistence of finite axiomatisations containing at
		  most first order variables is shown.

                  The same technique is then applied to calculi of
		  star expressions containing a zero process --- in
		  contrast to the positive result given in [FZ93] for
		  BPA$^*$, which differs only in that it does not
		  contain a zero.}
}

@Misc{ShanChungchie-,
  author = 	 {Shan, Chung-chie},
  title = 	 {Sexy Types in Action},
  OPThowpublished = {},
  OPTmonth = 	 {},
  OPTyear = 	 {},
  OPTnote = 	 {},
  WKloc = 	 {A-1544}
}

@Misc{ShanChungchie-b,
  author = 	 {Shan, Chung-chie},
  title = 	 {{Haskell} has a Type System for Higher-Order Modules},
  OPThowpublished = {},
  OPTmonth = 	 {},
  OPTyear = 	 {},
  OPTnote = 	 {},
  WKloc = 	 {A-1546}
}

@InProceedings{Shankar-1995,
  author = {Natarajan Shankar},
  title = {Computer-Aided Computing},
  crossref = {MPC1995},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0709},
  annote = {PVS}
}

@InProceedings{ShaoZhong-1999,
  author = {Zhong Shao},
  title = {Transparent Modules with Fully Syntactic Signatures},
  booktitle = {{Proc. 1999 ACM SIGPLAN International Conference on Functional Programming (ICFP'99), Paris, France}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {220--232},
  OPTyear = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = SEP,
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1088, doc/pap/BIB}
}

@InProceedings{ShaoZhong-Appel-1993,
  author = {Zhong Shao and Andrew W. Appel},
  title = {Smartest Recompilation},
  pages = {439--450},
  abstract = {To separately compile a program module in traditional
             atatically-typed languages, one has to manually write down an
             import interface which expecifies all spelicitly all the
             external symbols referenced in the module. Whenever the
             definitions of these external symbols are changed, the module
             has to be recompiled. In this paper, we present an algorithm
             which can automatically infer the ``minimum''import interface
             for any module in languages based on the Damas-Milner type
             discipline (e.g., ML). By ``minimum'', we mean that the
             interface specifies a set of assumptions (for external symbols)
             that are just enough to make the module type-check and compile.
             By compiling each module using its ``minimum'' import
             interface, we get a separate compilation method that can
             achieve the following optimal property: $A$ compilation unit
             never needs to be recompiled unless its own implementation
             changes.},
  crossref = {POPL1993},
  WKloc = {A-0202}
}

@InProceedings{ShaoZhong-League-Monnier-1998,
 author = {Zhong Shao and Christopher League and Stefan Monnier},
 title = {Implementing Typed Intermediate Languages},
 crossref = {ICFP1998},
 pages = {313--323},
 doi = {http://doi.acm.org/10.1145/291251.289460},
 WKloc = {doc/pap/BIB}
}

@Article{Shapiro-Hardy-2002,
  author = {Jonathan S. Shapiro and Norm Hardy},
  title = {{EROS}: A Principle-Driven Operating System from the Ground Up},
  journal = {IEEE Software},
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = JAN,
  WKloc = {A-1370, doc/pap/BIB},
  bibliographies = {SE3B}
}

@InProceedings{Sharygina-Browne-Kurshan-2001,
  author = {Natasha Sharygina and James C. Browne and Robert P. Kurshan},
  title = {A Formal Object-Oriented Analysis for Software Reliability: Design for Verification},
  crossref = {FASE 2001},
  pages = {318--332},
  WKloc = {A-1279, doc/pap/BIB},
  abstract = {This paper presents the OOA design step in a
                  methodology which integrates automata-based model
                  checking into a commercially supported OO software
                  development process. We define and illustrate a set
                  of design rules for OOA models with executable
                  semantics, which lead to automata models with
                  tractable state spaces. The design rules yield OOA
                  models with functionally structured designs similar
                  to those of hardware systems. These structures
                  support model-checking through techniques known to
                  be feasible for hardware. The formal OOA
                  methodology, including the design rules, was applied
                  to the design of NASA robot control
                  software. Serious logical design errors that had
                  eluded prior testing, were discovered in the course
                  of model-checking.}
}

@Book{Shaw-Garlan-1996,
  author = {M. Shaw and D. Garlan},
  title = {Software Architecture: Perspectives on an Emerging Discipline},
  publisher = {Prentice-Hall},
  year = 1996
}

@InProceedings{Shaw-Wulf-1992,
  author = {Mary Shaw and Wm. A. Wulf},
  title = {{Tyrannical Languages {\em Still} Preempt System Design}},
  pages = {200--211},
  crossref = {ICCL92},
  authorsAddress = {MS: Carnegie Mellon University; WAW: University
		  of Virginia},
  abstract = {It is a prime tenet of most programming language
		  design that ``higher-level'' languages are a good
		  thing --- indeed the higher the level, the better.
		  The assumption is that the higher the level of the
		  language --- the more abstract the abstractions ---
		  the greater the leverage provided to the programmer.
		  The language designer usually ensures that the
		  higher-level constructs capture his intention by
		  completely specifying the associated semantics.

		  A decade ago, we challenged the ``higher-level is
		  better'' assumption. The paper in which we did this
		  has largely been ignored. Perhaps it should have
		  been, but we don't think so. In fact we see this
		  apparently benign assumption as aggressively
		  interfering with good application design.
		  Unfortunlately, the consequences of blind adherence
		  to this tenet are spreading in both current language
		  proposals and larger system designs.

		  ...

		  Language and systems designers continue to preempt
		  details that should be controllable by the
		  application programmer.},
  bibliographies = {RelMiCS}
}

@InProceedings{Sheard-2001,
  author = {Tim Sheard},
  title = {Generic Unification via Two-Level Types and Parameterized Modules},
  booktitle = {ICFP 2001},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  publisher = {acm press},
  note = {to appear},
  WKloc = {A-1083, doc/pap/BIB}
}

@Unpublished{Sheard-Harrison-Hook-2001,
  author = {Tim Sheard and William Harrison and James Hook},
  title = {Modeling the Fine Control of Demand in Haskell.},
  note = {(submitted to Haskell workshop 2001)},
  OPTkey = {},
  OPTmonth = {},
  year = 2001,
  WKloc = {A-1087, doc/pap/BIB}
}

@InProceedings{Sheard-PeytonJones-2002,
  author = {Tim Sheard and Peyton Jones, Simon},
  title = {Template meta-programming for {Haskell}},
  crossref = {Haskell2002},
  pages = {1--16},
  URL = {http://doi.acm.org/10.1145/581690.581691},
  WKloc = {doc/pap/BIB},
  abstract = {We propose a new extension to the purely functional
      programming language Haskell that supports compile-time
      meta-programming. The purpose of the system is to support the
      algorithmic construction of programs at compile-time.The ability to
      generate code at compile time allows the programmer to implement such
      features as polytypic programs, macro-like expansion, user directed
      optimization (such as inlining), and the generation of supporting
      data structures and functions from existing data structures and
      functions.Our design is being implemented in the Glasgow Haskell
      Compiler, ghc.}
}

@Misc{Sheard-PeytonJones-2002,
  author = {Tim Sheard and Peyton Jones, Simon},
  title = {Notes on {Template Haskell} Version 2},
  OPThowpublished = {},
  month = 	 NOV,
  year = 	 2003,
  WKloc = 	 {A-1587}
}

@MastersThesis{ShenH-1995,
  author = {Hong Shen},
  title = {Implementation of Table Inversion Algorithms},
  school = {McMaster Univ., Communications Research Laboratory},
  year = 1995,
  OPTaddress = {},
  type = {{M.~Eng.\null{} thesis}},
  month = DEC,
  OPTnote = {},
  WKloc = {A-1450},
  bibliographies = {RelMiCS, RelMiS}
}

@InProceedings{ShenH-Zucker-Parnas-1996,
  author = {Hong Shen and Jeffery I. Zucker and David L. Parnas},
  title = {Table Transformation Tools: Why and How},
  crossref = {COMPASS1996},
  pages = {3--11},
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {doc/pap/BIB}
}

@Book{Sheppard-1995,
  author = {D. Sheppard},
  title = {An Introduction to Formal Specification with {Z} and
                  {VDM}},
  publisher = {McGraw Hill},
  series = {International Series in Software Engineering},
  ISBN = {0-07-707907-8},
  URL = {http://www.blackwell.co.uk/cgi-bin/bb_item?0077079078},
  price = {\pounds19.95, paperback},
  length = 398,
  other = {Department of Computer Studies, University of Glamorgan,
                  UK},
  year = 1995
}

@PhDThesis{Shi-1994,
  author = {Hui Shi},
  title = {Extended Matching with Applications to Program Transformation},
  school = {Fachbereich Mathematik und Informatik, Universit\"at
		  Bremen},
  year = 1994,
  month = DEC,
  UniBwM = {INF700/YA2521}
}

@Article{ShihWK-HsuWL-1999,
  author = 	 {W. K. Shih and W. L. Hsu},
  title = 	 {A new planarity test},
  journal = 	 TCS,
  year = 	 1999,
  volume = 	 223,
  pages = 	 {179--191},
  annote = 	 {Wikipedia: In 1999, Shih and Hsu developed a
                  planarity testing algorithm that was significantly
                  simpler than classical methods based on a new type
                  of data structure called the PC tree and a postorder
                  traversal of the depth-first search tree of the
                  vertices.}
}

@InProceedings{Shields-PeytonJones-2001,
  author = {Mark Shields and Peyton Jones, Simon},
  title = {Object-Oriented Style Overloading for Haskell},
  booktitle = {Proc.\null{} Workshop on Multi-Language Infrastructure and Interoperability, {BABEL 2001, Firenze, 8 April 2001}},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  note = {30 pages},
  abstract = {Haskell has a sophisticated mechanism for overloading
      identifiers with multiple definitions at distinct types.
      Object-oriented programming has a similar notion of overloading for
      methods names. Unfortunately, it is not possible to encode
      object-oriented overloading directly using Haskell overloading. This
      deficiency becomes particularly tiresome when Haskell programs wish
      to call methods imported from an object-oriented library.

      We explore various encodings of object-oriented classes into Haskell,
      demonstrate precisely where Haskell's existing type class system is
      unsatisfactory, and propose two refinements. We proceed in three
      stages. Firstly, we discuss various ways of accommodating sub-typing;
      we conclude that a simple encoding using Haskell classes is better
      for our purpose than a more substantial language extension. Second,
      we introduce a new notion of closed class, and show how this enables
      improvement of constraints beyond what is possible in Haskell. Closed
      classes make it easy to encode the truely ad hoc overloading of
      object-oriented methods without the need for name mangling or
      gratuitous type annotations. Thirdly, we allow overlapping instances,
      and define what it means for one instance to be better than another.
      This mechanism will turn out to subsume the rather complex
      overloading resolution rules used by object-oriented languages to
      select the most-specific method at a call site.

      In the Appendix, we present type checking and inference rules, as
      well as details of constraint entailment and simplification. However,
      this workshop paper is somewhat exploratory: the design may shift
      once we gain experience with an implementation, and we have not
      devoted any time to showing any formal properties of our system.},
  WKloc = {A-1186, doc/pap/BIB}
}

@Unpublished{Shields-PeytonJones-2001a,
  author = {Mark Shields and Peyton Jones, Simon},
  title = {First-class Modules for Haskell},
  note = {Submitted to the Ninth International Conference on Foundations of Object-Oriented Languages {(FOOL 9), Portland, Oregon}},
  pages = 20,
  month = OCT,
  year = 2001,
  URL = {http://research.microsoft.com/~simonpj/papers/first-class-modules/index.htm},
  WKloc = {A-1237, doc/pap/BIB},
  abstract = {Though Haskell's module language is quite weak, its core
      language is highly expressive. Indeed, it is tantalisingly close to
      being able to express much of the structure traditionally delegated
      to a seperate module language. However, the encodings are awkward,
      and some situations can't be encoded at all.

      In this paper we refine Haskell's core language to support
      first-class modules with many of the features of ML-style modules.
      Our proposal cleanly encodes signatures, structures and functors with
      the appropriate type abstraction and type sharing, and supports
      recursive modules. All of these features work across compilation
      units, and interact harmoniously with Haskell's class system. Coupled
      with support for staged computation, we believe our proposal would be
      an elegant approach to run-time dynamic linking of structured code.

      Our work builds directly upon Jones' work on parameterised
      signatures, Odersky and Laufer's system of higher-ranked type
      annotations, Russo's semantics of ML modules using ordinary
      existential and universal quantification, and Odersky and Zenger's
      work on nested types. We motivate the system by examples, and include
      a more formal presentation in the appendix.}
}

@InProceedings{Shikida-Yamamoto-Kimura-Tokuda-1994,
  author = {M. Shikida and Y. Yamamoto and Y. Kimura and T. Tokuda},
  title = {A {TAHG} Model Based Software Generator System},
  booktitle = {Proceedings of the 1994 First {Asia-Pacific Software Engineering Conference}},
  year = 1994,
  publisher = {IEEE Computer Society Press},
  pages = {50--57},
  OPTabstract = {},
  WKloc = {A-0852}
}

@InCollection{Shilliday-Taylor-Clark-Bringsjord-2010,
  author =       {Andrew Shilliday and Joshua Taylor and Micah Clark and Selmere Bringsjord},
  title =        {Provability-Based Semantic Interoperability for Information Sharing and Joint Reasoning},
  pages =     {109--128},
  booktitle =    {Ontologies and Semantic Technologies for Intelligence},
  OPTcrossref =  {},
  publisher = {IOS Press},
  year =      {2010},
  editor =    {Leo Obrst and Terry Janssen and Werner Ceusters},
  DOI =    {10.3233/978-1-60750-581-5-109},
  PrePDFURL =    {http://www.cs.rpi.edu/~tayloj/DOCS/PUBS/PBSIISJR.pdf},
  abstract = {We describe provability-based semantic interoperability (PBSI),
              a framework transcending syntactic translation that enables
              robust, meaningful, knowledge exchange across diverse information systems.
              PBSI is achieved through translation graphs
              that capture complex ontological relationships,
              and through provability-based queries.
              We work through an example of automating an unmanned aerial vehicle
              by reasoning over information from a number of sources.},
  bibliographies = {ontology}
}

@Manual{Shipman-2005,
  title = 	 {{Relax NG} Compact Syntax {(RNC)}},
  author =	 {John W. Shipman},
  organization = {New Mexico Tech Computer Center},
  address =	 {www.nmt.edu/tcc},
  month =	 APR,
  year =	 2005,
  keywords  = 	 {XML, XML Schema},
  WKloc = 	 {A-1626}
}

@Misc{Shirahata-,
  author = {Masaru Shirahata},
  title = {A Sequent Calculus for Compact Closed Categories},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  WKloc = {A-1265},
  abstract = {In this paper, we introduce the system CMLL of sequent
      calculus and establish its correpondence with compact closed
      categories. CMLL is equivalent in provability to the system MLL of
      classical linear logic with the tensor and par connectives
      identified. We show that the system allows a fairly simple
      cut-elimination, and the proofs in the system have a natural
      interpretation in compact closed categories. However, the soundness
      of the cut-elimination procedure in terms of the categorical
      interpretation is by no means evident. We anwer to this question
      affirmatively and establish the soundness by using the coherence
      result on compact closed categories by Kelly and Laplaza.}
}

@Misc{Shivers-Feeley-1998,
  author = {Olin Shivers and Marc Feeley},
  title = {ICFP 1998 Programming Contest},
  year = 1998,
  WKloc = {A-0734}
}

@Book{Shoham-1988,
  author = {Yoav Shoham},
  title = {Reasoning About Change},
  publisher = MIT_P,
  address = {Cambridge, MA},
  year = 1988,
  bibliographies = {RelMiCS}
}

@Book{Shu-1988,
  author = {Nan C. Shu},
  title = {Visual Programming},
  publisher = {Van Nostrand Reinhold},
  year = 1988,
  address = {New York},
  authorsAddress = {IBM Los Angeles Scientific Center},
  UniBwM = {INF400/S3341}
}

@TechReport{ Shulman-1997,
    author = "Sherri Shulman",
    title = "A Meta-theory for Structured Presentations in the {COC}",
    number = "CSE-97-TH-001",
    month = "1,",
    year = "1997",
    WKloc = {doc/pap/BIB},
    url = "http://citeseer.ist.psu.edu/shulman97metatheory.html"
}

@Manual{Sidebotham-1996,
  title = {Cons: A Software Construction System, A guide and reference for version 1.1},
  author = {Bib Sidebotham},
  address = {rns@fore.com},
  month = NOV,
  year = 1996,
  WKloc = {A-1139}
}

@InProceedings{Sieber-1994,
  author = {Kurt Sieber},
  title = {Full Abstraction for the Second Order Subset of an
		  {\sc Algol}-like Language},
  crossref = {MFCS94},
  pages = {608--617}
}

@Book{Siefkes-1992,
  year = 1992,
  title = {{Formale Methoden und kleine Systeme: lernen, leben und
      arbeiten in formalen Umgebungen}},
  series = {Theorie der Informatik},
  publisher = {Vieweg},
  author = {Dirk Siefkes},
  address = {Braunschweig}
}

@InProceedings{Sieg-Wainer-1993,
  author = {Wilfried Sieg and Stanley S. Wainer},
  title = {Program Transformation and Proof Transformation},
  crossref = {CSL93},
  pages = {305--317},
  WKloc = {A-0341},
  abstract = {$\ldots$}
}

@InProceedings{Siff-Reps-1997,
  author = 	 {Michael Siff and Thomas Reps},
  title = 	 {Identifying Modules via Concept Analysis},
  booktitle = 	 {Proceedings of the International Conference on Software
Maintenance},
  pages =	 {170--179},
  year =	 1997,
  bibliographies = {OPG}
}

@Article{Siff-Reps-1999,
 author = {Michael Siff and Thomas Reps},
 title = {Identifying Modules via Concept Analysis},
 journal = {IEEE Trans. Softw. Eng.},
 volume = {25},
 number = {6},
 year = {1999},
 issn = {0098-5589},
 pages = {749--768},
 doi = {http://dx.doi.org/10.1109/32.824377},
 publisher = {IEEE Press},
}

@Book{Sikorski-1969,
  author = {Roman Sikorski},
  title = {Boolean Algebras},
  note = {Third edition, Second edition published by Chelsea, Bronx, New
      York, 1966.},
  publisher = Springer,
  address = {Berlin},
  year = 1969,
  bibliographies = {RelMiCS}
}

@Book{Silberschatz-Galvin-Gagne-2002,
  author = {A. Silberschatz and P. B. Galvin and G. Gagne},
  title = {Operating Systems Concepts},
  publisher = {Wiley},
  year = 2002,
  edition = {Sixth Edition},
  URL = {http://www.wiley.com/college/tlp/0,9842,_0471417432_BKS,00.html},
  ISBN = {0-471-41743-2},
  WKloc = {owned},
  bibliographies = {SE3B}
}

@Article{Simis-Ulrich-Vasconcelos-1993,
  author = {Simis, Aron and Ulrich, Bernd and Vasconcelos, Wolmer},
  title = {Jacobian dual fibrations},
  OPTcrossref = {},
  OPTkey = {},
  journal = {American journal of mathematics},
  year = 1993,
  volume = 115,
  number = 1,
  pages = {47--},
  month = FEB,
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Simmons-1983,
  author = {R G. Simmons},
  title = {The Use of Quantitative and Qualitative Simulations},
  booktitle = {Proc.\null{} of {$3^{rd}$} National Conf.\null{} on
		Artificial Intelligence (AAAI-83), 	Washington, D.C., August 1983},
  year = 1983,
  bibliographies = {RelMiCS}
}

@TechReport{Simmons-1993,
  author = {Harold Simmons},
  title = {The Glueing Construction and Lax Limits (with
		  applications to categories of structured posets)},
  institution = {University of Manchester},
  year = 1993,
  number = {UMCS-93-6-2},
  WKloc = {A-0226},
  abstract = {Starting life as a way of reconstructing a
		  topological space from a pair of complementary
		  subspaces, the glueing construction has found
		  employment in a wide range of different roles from
		  the construction of free distributive lattices to a
		  supporting part in the 2-categorical analysis of
		  types theories. In this latter role the construction
		  appears to be a fundamental factor in the behaviour
		  of higher order proof theory. What is going on here
		  ? Before that can be answered we need at least a
		  less ad hoc description of the contruction. In this
		  paper I set down what is, I believe, the beginnings
		  of a coherent account of the algebraic version of
		  glueing. As well as the abstract theory I give a
		  good selection of different examples to illutrate
		  the diverse nature of the uses of the construction.},
  file = {doc/pap/UCSTRI/UMCS-93-6-2.ps},
  URL = {ftp://m1.cs.man.ac.uk/pub/TR/UMCS-93-6-2.ps.Z}
}

@Article{Simon-Lee-1971,
  author = {Simon and Lee},
  title = {On the Optimal Solutions to And/Or Series-Parallel Graphs},
  journal = JACM,
  volume = 18,
  year = 1971
}

@InProceedings{Simons-Biersack-Raschke-1994,
  author = {M. Simons and M. Biersack and R. Raschke},
  title = {Literate and Structured Presentation of Formal Proofs},
  crossref = {PROCOMET94},
  pages = {58--79},
  keywords = {Software Engineering; Specifications, Tools and
		  Techniques; Mathematical Logic; Formal Proofs;
		  Support of Program Derivation}
}

@TechReport{Simonyi-1995,
  author = {Charles Simonyi},
  title = {The Death of Computer Languages, The Birth of Intentional Programming},
  institution = {Microsoft Research},
  year = 1995,
  number = {MSR-TR-95-52},
  month = {September}
}

@InProceedings{Simpson-1998,
  author = {Alex K. Simpson},
  title = {Lazy Functional Algorithms for Exact Real Functionals},
  booktitle = {Mathematical Foundations of Computer Science 1998},
  pages = {456--464},
  year = 1998,
  volume = 1450,
  series = LNCS,
  publisher = Springer,
  URL = {http://www.dcs.ed.ac.uk/home/als/Research/lazy.ps.gz},
  bibliographies = {MathScheme},
  WKloc = {doc/pap/BIB}
}

@Misc{Simpson-Martin-Gibbons-Davies-McKeever-2002,
  author = {Andrew Simpson and Andrew Martin and Jeremy Gibbons and Jim Davies and Steve McKeever},
  title = {On The Supervision and Assessment Of Part-Time Postgraduate Software Engineering Projects},
  howpublished = {Submitted for publication},
  month = OCT,
  year = 2002,
  URL = {http://web.comlab.ox.ac.uk/oucl/work/jeremy.gibbons/publications/index.html#softeng-projects},
  WKloc = {A-1456, doc/pap/BIB},
  abstract = {This paper describes existing practices in the
    supervision and assessment of projects undertaken by part-time,
    post-graduate students in Software Engineering.
    It considers this aspect of the learning experience,
    and the educational issues raised, in the context of
    existing literature much of which is focussed upon the
    experience of full-time, undergraduate students.
    The importance of these issues will increase with the
    popularity of part-time study at a postgraduate level;
    the paper presents a set of guidelines for
    project supervision and assessment.}
}

@Article{Sims-1967,
  author = {C. C. Sims},
  title = {Graphs and Finite Permutation Groups},
  journal = MATHZ,
  volume = 95,
  year = 1967,
  pages = {76--86},
  bibliographies = {RelMiCS}
}

@Misc{Sinha-Suri-,
  author = {Purendhu Sinha and Neeraj Suri},
  title = {On the Use of Formal Techniques for Analyzing Dependable Real-Time-Protocols},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  OPTnote = {},
  WKloc = {A-1147},
  keywords = {PVS}
}

@Misc{Skibinski-2000,
  author = {Jan Skibinski},
  title = {Quantum Vector},
  howpublished = {Literate Haskell module WWW page},
  month = MAY,
  year = 2000,
  WKloc = {A-1126}
}

@Article{Skillicorn-1990,
  year = 1990,
  volume = {{}},
  title = {Architecture-Independant Parallel Computation},
  pages = {38--50},
  number = {{}},
  month = DEC,
  journal = COMPUTER,
  author = {David B. Skillikorn},
  bibliographies = {RelMiCS}
}

@InProceedings{Skillicorn-1994,
  author = {David A. Skillicorn},
  title = {The Categorical Data Type Approach to
		  General-Purpose Parallel Computation},
  crossref = {IFIP94A},
  pages = {565--570},
  authorsAddress = {Queen's University, Kingston, Canada},
  abstract = {Categorical data types are an object and
		  skeleton-based model with a rich transformation
		  system and attractive implementation properties. We
		  provide a brief overview of the methodology, and
		  develop some example computations, including a novel
		  parallel algorithm for searching structured text.},
  acmcodes = {D.3.1; D.1.5; F.3.2},
  ACMcats = {Formal Definitions and Theory; Object-oriented
		  Programming; Semantics of Programming Languages}
}

@Article{Skillicorn-Talia-1998,
  author = {David B. Skillicorn and Domenico Talia},
  title = {Models and Languages for Parallel Computation},
  journal = {ACM Computing Surveys},
  year = 1998,
  volume = 30,
  number = 2,
  month = JUN,
  pages = {123--169},
  WKloc = {A-0901, doc/pap/BIB},
  URL = {http://doi.acm.org/10.1145/280277.280278},
  abstract = {We survey parallel programming models and languages using
      six criteria to assess their suitability for realistic portable
      parallel programming. We argue that an ideal model should by easy to
      program, should have a software development methodology, should be
      architecture-independent, should be easy to understand, should
      guarantee performance, and should provide accurate information about
      the cost of programs. These criteria reflect our belief that
      developments in parallelism must be driven by a parallel software
      industry based on portability and efficiency. We consider programming
      models in six categories, depending on the level of abstraction they
      provide. Those that are very abstract conceal even the presence of
      parallelism at the software level. Such models make software easy to
      build and port, but efficient and predictable performance is usually
      hard to achieve. At the other end of the spectrum, low-level models
      make all of the messy issues of parallel programming explicit (how
      many threads, how to place them, how to express communication, and
      how to schedule communication), so that software is hard to build and
      not very portable, but is usually efficient. Most recent models are
      near the center of this spectrum, exploring the best tradeoffs
      between expressiveness and performance. A few models have achieved
      both abstractness and efficiency. Both kinds of models raise the
      possibility of parallelism as part of the mainstream of computing.}
}

@InProceedings{Skodinis-Wanke-1994,
  author = {K. Skodinis and E. Wanke},
  title = {Exponential time analysis of confluent and boundary eNCE graph
          languages},
  abstract = {eNCE (edge label neighborhood controlled) graph grammars
             belong to the most powerful graph rewriting systems with
             single-node graphs on the left-hand side of the productions.
             From an algorithmic point of view, confluent and boundary eNCE
             graph grammars are the most interesting subclasses of eNCE
             graph grammars. In confluent eNCE graph grammars the order in
             which the nonterminal nodes are substituted is irrelevant for
             the resulting graph. In boundary eNCE graph grammars
             nonterminal nodes are never adjacent. In this paper, we show
             that given a confluent or boundary eNCE graphh grammar ${\cal
             G}$, the problem whether the language $L({\cal G})$ is empty,
             is DEXPTIME-complete.},
  crossref = {WG1994},
  annote = {Summary:
                 The paper considers complexity problems for eNCE
		  graph languages. It had been known before, that the
		  emptiness problem for boundary eNCE graph grammars
		  can be solved in exponential time. The authors proof
		  this problem to be DEXPTIME-hard, and generalize both
		  results to confluent eNCE graph grammars. For this
		  most of the effort had to be put into constructing a
		  nonblocking confluent eNCE grammar from an arbitrary
		  one in exponential time.

                  The paper is a very technical piece of solid work.
		  Practical relevance has not been indicated.},
  WKloc = {A-0232}
}

@Article{Skornyakov-1987,
  author = {L. A. Skornyakov},
  title = {Matrix Relation Algebras},
  journal = MATZAMETKI,
  volume = 41,
  year = 1987,
  pages = {129--137, 285},
  bibliographies = {RelMiCS}
}

@Book{Sleep-Plasmeijer-vanEekelen-1993,
  editor = {M.R. Sleep and M.J. Plasmeijer and van Eekelen, M.C.J.D.},
  booktitle = {Term Graph Rewriting: Theory and Practice},
  title = {Term Graph Rewriting: Theory and Practice},
  publisher = Wiley,
  year = 1993,
  WKloc = {owned},
  abstract = {TERM GRAPH REWRITING refers to techniques and
		  theories for representing terms and term rewrite
		  rules as graphs and graph rewrite rules. Many modern
		  programming paradigms - most notably the functional
		  and logic paradigms - have term rewriting at their
		  heart, although the control regimes and semantics
		  are very different. Practical implementations of
		  these languages share subterms using pointers, and
		  the machine code for such programs manipulates
		  shared structures (called in this volume {\em term graphs}).

                  The operational semantics of the resulting programs
		  may be represented as a set of term graph rewriting
		  rules. Reasoning about the correctness and
		  efficiency of such representations  relies on our
		  ability to reason about term graph rewriting
		  systems, and to relate them to other semantic
		  models. Used in this way, term graph rewriting
		  offers a model of computation which is closer to
		  real implementations than pure term rewriting, but
		  which avoids unnecessary machine detail.

                  Graph rewriting was the focus of a number of
		  European ESPRIT Basic Research Actions most notably
		  SEMAGRAPH and COMPUGRAPH. Both projects led to
		  theoretical and practical advances for term graph
		  rewriting reflecting the state of the art, and these
		  form the contents of this book.},
  PRICE = {DM 89.-, US\$ 47.95},
  contents = {An Introduction to Term Graph Rewriting
J.R.~Kennaway, J.W.~Klop, M.R.~Sleep, and F.J.~de~Vries

Partial Type Assignment in Left Linear Applicative Term Rewriting Systems
S.~van~Bakel, S.~Smetsers, and S.~Brock

How to Get Confluence for Explicit Substitutions
T.~Hardin

An Infinitary Church-Rosser Property for Non-collapsing Orthogonal
Term Rewriting Systems
J.R.~Kennaway, J.W.~Klop, M.R.~Sleep, and F.J.~de~Vries

The Functional Strategy and Transitive Term Rewriting Systems
Y.~Toyama, S.~Smetsers, M.~van~Eekelen and R.~Plasmeijer

Graph Rewriting Systems for Efficient Compilation
Z.M.~Ariola and Arvind


A Fibration Semantics for Externded Term Graph Rewriting
R.~Banach

A New Term Graph Rewriting Formalism: Hyperedge Replacement Jungle
Rewriting
A.~Corradini and F.~Rossi

Abstract Reduction: Towards a Theory via Abstract Interpretation
M.~van~Eekelen, E.~Goubault, C.~Hankin and E.~N\"ocker

A Lattice for the Abstract Interpretation of Term Graph Rewriting Systems
E.~Goubault and C.~Hankin

Event Structures and Orthogonal Term Graph Rewriting
J.R.~Kennaway, J.W.~Klop, M.R.~Sleep, and F.J.~de~Vries

The Adequacy of Term Graph Rewriting for Simulating Term Rewriting
J.R.~Kennaway, J.W.~Klop, M.R.~Sleep, and F.J.~de~Vries

Translations into the Graph Grammar Machine
H-J.~Kreowski

An Algebraic Framework for the Transformation of Attributed Graphs
M.~Lowe, M.~Korff and A.~Wagner

Hypergraph Rewriting: Critical Pairs and Undecidability of Confluence
D.~Plump

A Quick Look at Tree Transductions
J.C.~Raoult

Paragon Specifications and Their Implementations
P.~Anderson, D.~Bolton and P.~Kelly

MONSTR: Term Graph Rewriting for Parallel Machines
R.~Banach

A Graph Rewriting Model Enhanced with Sharing for OR-parallel Execution
of Logic Programs
W.~Damm, F.~Lui and T.~Peikenkamp

A New Process Model for Functions
J.~Glauert, L.~Leth and B.~Thomsen

Parallel Execution of Concurrent Clean on ZAPP
R.~Goldsmith, D.L.~McBurney and M.R.~Sleep

Graph-based Operational Semantics of a Lazy Functional Language
Kristoffer H.~Rose

Graph Rewritng Using the Annotated Functional Strategy
P.~Koopman, S.~Smetsers,  M.~van~Eekelen and R.~Plasmeijer

Implementing Logical Variables and Disjunctions in Graph Rewrite Systems
P.~McBrien

Process Annotations and Process Types
R.~Plasmeijer and M.~van~Eekelen

JALPA: A Functional Modular Programming Language Based on Extended
Graphical Term Rewriting
H.~Yamanaka}
}

@InProceedings{Slind-1994,
  author = {Konrad Slind},
  title = {A Parameterized Proof Manager},
  booktitle = {Proceedings of the 1994 HOL Users Group Meeting, Malta},
  volume = 859,
  series = {LNCS},
  year = 1994,
  publisher = {Springer-Verlag},
  directory = {~kahl/doc/pap},
  URL = {ftp://ftp.informatik.tu-muenchen.de/local/lehrstuhl/nipkow/slind/papers/pman.html}
}

@InProceedings{Slind-Gordon-Boulton-Bundy-1998,
  author = {Konrad Slind and Mike Gordon and Richard Boulton and Alan Bundy},
  title = {System Description: An Interface Between {CLAM} and {HOL}},
  crossref = {CADE1998},
  pages = {134--138},
  OPTabstract = {},
  WKloc = {A-0608}
}

@InProceedings{SloanRH-Stasi-Turan-2012_Hydra_number,
   author = {Sloan, Robert H. and Stasi, Despina and Tur{\'a}n, Gy{\"o}rgy},
   affiliation = {University of Illinois at Chicago, USA},
   title = {Hydras: Directed Hypergraphs and Horn Formulas},
   LNCSbooktitle = {WG 2012},
   booktitle = {Graph-Theoretic Concepts in Computer Science},
   series = LNCS,
   editor = {Golumbic, Martin and Stern, Michal and Levy, Avivit and Morgenstern, Gila},
   publisher = Springer,
   isbn = {978-3-642-34610-1},
   pages = {237--248},
   volume = {7551},
   DOIURL = {http://dx.doi.org/10.1007/978-3-642-34611-8_25},
   DOI = {10.1007/978-3-642-34611-8_25},
   year = {2012},
   WKloc = {doc/pap/BIB},
   abstract = {We consider a graph parameter, the \emph{hydra number},
                  arising from an optimization problem for Horn
                  formulas in propositional logic. The hydra number of
                  a graph $G = (V,E)$ is the minimal number of hyperarcs
                  of the form $u, v \rightarrow w$ required in a directed
                  hypergraph $H = (V,F)$, such that for every pair $(u,v)$,
                  the set of vertices reachable in $H$ from $\{u, v\}$
                  is the entire vertex set $V$ if $(u, v) \in E$, and it is
                  $\{u, v\}$ otherwise. Here reachability is defined by
                  the standard forward chaining or marking algorithm.
                  Various bounds are given for the hydra number. We
                  show that the hydra number of a graph can be upper
                  bounded by the number of edges plus the path cover
                  number of its line graph, and this is a sharp bound
                  for some graphs. On the other hand, we construct
                  graphs with hydra number equal to the number of
                  edges, but having arbitrarily large path cover
                  number. Furthermore we characterize trees with low
                  hydra number, give bounds for the hydra number of
                  complete binary trees, discuss a related
                  optimization problem and formulate several open
                  problems.}
}

@Misc{Smetsers-199X,
  author = {Sjaak Smetsers},
  title = {Term Graph Rewriting and Strong Sequentiality},
  year = {199?},
  WKloc = {A-0428}
}

@InProceedings{Smetsers-Barendsen-vanEekelen-Plasmeijer-1993,
  author = {Sjaak Smetsers and Erik Barendsen and van Eekelen, Marko and
		  Rinus Plasmeijer},
  title = {Guaranteeing Safe Destructive Updates Through a Type
		  System with Uniqueness Information for Graphs},
  crossref = {GTCS93},
  pages = {358--379},
  abstract = {In this paper we present a type system for graph
		  rewrite systems: {\em uniqueness typing}. It employs
		  usage information to deduce whether an object is
		  `unique' at a certain moment, i.e.\null{} is only
		  locally acessible. In a type of a function it can be
		  specified that the function rewuires a unique
		  argument object. The correctness of type assignment
		  guarantees that no external access on the original
		  object will take place in the future. The presented
		  type system is proven to be correct. We illustrate
		  the power of the system by defining an elegent
		  quicksort algorithm that performs the sorting {\em
		  in situ} on the data structure.},
  WKloc = {A-0298}
}

@InProceedings{Smith-1994,
  author = {D. A. Smith},
  title = {Why Multi-{SLD} Beats {SLD} (Even on a Uniprocessor)},
  crossref = {PLILP1994},
  pages = {40--56}
}

@Book{Smith-1994a,
  author = {Carl H. Smith},
  title = {A Recursive Introduction to the Theory of Computation},
  publisher = {Springer-Verlag},
  year = 1994,
  series = {Graduate Texts in Computer Science},
  address = {New York},
  UniBwM = {INF001/YA3036},
  keywords = {computability, Turing machine},
  contents = {1. Models of Computation\\
                  2. Basic Recursive Function Theory\\
                  3. Abstract Complexity Theory\\
                  4. Complete Problems},
  annote = {TEXTBOOK}
}

@InCollection{Smith-1998,
  author = {Douglas R. Smith},
  title = {Mechanizing the Development of Software},
  crossref = {Marktoberdorf-1998},
  pages = {??},
  WKloc = {A-1046, doc/pap/BIB}
}

@InProceedings{Smith-David-Morrisett-2000,
  author = {Frederick Smith and David Walker and Greg Morrisett},
  title = {Alias Types},
  crossref = {ESOP2000},
  pages = {366--381},
  authorsAddress = {Cornell University },
  WKloc = {doc/pap/BIB},
  abstract = {Linear type systems allow destructive operations such as
              object deallocation and imperative updates of
              functional data structures. These operations and others,
              such as the ability to reuse memory at different types,
              are essential in low-level typed languages. However,
              traditional linear type systems are too restrictive
              for use in low-level code where it is necessary to
              exploit pointer aliasing. We present a new typed language
              that allows functions to specify the shape of the store
              that they expect and to track the flow of pointers
              through a computation. Our type system is expressive enough
              to represent pointer aliasing
              and yet safely permit destructive operations.}
}

@Misc{SmithDA-Kay-Raabe-Reed-2003,
  author =	 {Smith, D. A. and A. Kay and A. Raab and D. P. Reed },
  title =	 {Croquet: A Collaboration System Architecture},
  howpublished = {Croquet whitepaper, available from
 \textsf{http://croquetproject.org/About_Croquet/whitepapers.html}},
  year =	 2003,
  WKloc = {doc/pap/BIB},
  URL = 	 {http://croquetproject.org/About_Croquet/whitepapers.html},
  abstract = {Croquet is a computer software architecture built
     from the ground up with a focus on deep collaboration between
     teams of users. It is a totally open, totally free, highly
     portable extension to the Squeak programming system. Croquet is a
     complete development and delivery platform for doing real
     collaborative work. There is no distinction between the user
     environment and the development environment. Croquet is focused
     on interactions inside of a 3D shared space that is used for
     context based collaboration, where each user can see all of the
     others and what their current focus is. This allows for an
     extremely compelling shared experience. A new collaboration
     architecture/protocol called TeaTime has been developed to enable
     this functionality. The rendering architecture is built on top of
     OpenGL.}
}

@PhDThesis{SmithP-1997,
  author = {Peter W. Smith},
  title = {The Possibilities and Limitations of Heterogeneous Process Migration},
  school = {University of British Columbia},
  year = 1997,
  URL = {http://www.cs.ubc.ca/spider/psmith/tui.html},
  WKloc = {doc/pap/BIB},
  bibliographies = {ProcMig}
}

@Article{Smolensky-1990,
  author = {Smolensky, P.},
  title = {Tensor product variable binding and the representation of
                   symbolic structures in connectionist systems},
  journal = {Artificial intelligence},
  year = 1990,
  volume = 46,
  number = {1 / 2},
  pages = {159--216},
  month = NOV,
  annote = {knowledge representation, connectionism;
                  inspected 8 June 1995 at LMU Maths Library: not interesting}
}

@TechReport{Smolka-1988,
  author = {Gert Smolka},
  title = {A Feature Logic with Subsorts},
  year = 1988,
  type = {LILOG Report},
  number = 33,
  month = MAY,
  institution = IWBS,
  address = {Postfach 80 08 80, 7000 Stuttgart 80, Germany},
  filename = {LR-33.dvi},
  bibliographies = {RelMiCS},
  abstract = {This paper presents a set description logic with
		  subsorts, feature selection (the inverse of unary
		  function application), agreement, intersection,
		  union and complement.  We define a model theoretic
		  open world semantics and show that sorted feature
		  structures constitute a canonical model, that is,
		  without loss of generality subsumption and
		  consistency of set descriptions can be considered
		  with respect to feature structures only.  We show
		  that deciding consistency of set descriptions is an
		  NP-complete problem.}
}

@PhDThesis{Smolka-1989,
  abstract = {This thesis presents the foundations for relational
		  logic programming over polymorphically order-sorted
		  data types.  This type discipline combines the
		  notion of parametric polymorphism, which has been
		  developed for higher-order functional programming,
		  with the notion of order-sorted typing, which has
		  been developed for equational first-order
		  specification and programming.  Polymorphically
		  order-sorted types are obtained as canonical models
		  of a class of specifications in a suitable logic
		  accommodating sort functions.  Algorithms for
		  constraint solving, type checking and type inference
		  are given and proven correct.},
  year = 1989,
  title = {Logic Programming over Polymorphically Order-Sorted Types},
  school = KAIS,
  month = MAY,
  author = {Gert Smolka},
  address = {Kaiserslautern, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Smolka-1989a,
  filename = {IWBS-93.dvi},
  abstract = {This paper studies feature description languages that
		  have been developed for use in unification grammars,
		  logic programming and knowledge representation.  The
		  distinctive notational primitive of these languages
		  are features that can be understood as unary partial
		  functions on a domain of abstract objects.  We show
		  that feature description languages can be captured
		  naturally as sublanguages of first-order predicate
		  logic with equality and show the equivalence of a
		  loose Tarski semantics with a fixed feature graph
		  semantics for quantifier-free constraints.  For
		  quantifier-free constraints we give a constraint
		  solving method and show the NP-completeness of
		  satisfiability checking.  For general feature
		  constraints with quantifiers satisfiability is shown
		  to be undecidable.  Moreover, we investigate an
		  extension of the logic with sort predicates and
		  set-denoting expressions called feature terms.},
  year = 1989,
  type = {IWBS Report},
  title = {Feature Constraint Logics for Unification Grammars},
  number = 93,
  note = {Published in Journal of Logic Programming 12, 51--87, 1992},
  month = NOV,
  institution = IWBS,
  author = {Gert Smolka},
  address = {Postfach 80 08 80, 7000 Stuttgart 80, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Smolka-1991,
  abstract = {A major difficulty with logic programming is
		  combinatorial explosion: since goals are solved with
		  possibly indeterminate (i.e., branching) reductions,
		  the resulting search trees may grow wildly.
		  Constraint logic programming systems try to avoid
		  combinatorial explosion by building in strong
		  determinate (i.e., non-branching) reduction in the
		  form of constraint simplification.  In this paper we
		  present two concepts, residuation and guarded rules,
		  for further strengthening determinate reduction.
		  Both concepts apply to constraint logic programming
		  in general and yield an operational semantics that
		  coincides with the declarative semantics.
		  Residuation is a control strategy giving priority to
		  determinate reductions. Guarded rules are logical
		  consequences of programs adding otherwise
		  unavailable determinate reductions.},
  year = 1991,
  type = {Research Report},
  title = {Residuation and Guarded Rules for Constraint Logic Programming},
  number = {RR-91-13},
  note = {Also available as PRL Research Report 12, Digital,
                 85 avenue Victor Hugo, 92563 Rueil-Malmaison Cedex, France},
  month = MAY,
  institution = DFKI,
  author = {Gert Smolka},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  bibliographies = {RelMiCS}
}

@InProceedings{Smolka-1994,
  author = {Gert Smolka},
  title = {A Foundation for Higher-Order Concurrent Constraint Programming},
  crossref = {CCL94},
  pages = {50--72},
  note = {invited lecture},
  keywords = {$pi$-calculus, Oz},
  URL = {ftp://ps-ftp.dfki.uni-sb.de/, http://ps-www.dfki.uni-sb.de/},
  abstract = {We present the $\gamma$-calculus, a computational
		  calculus for higher-order concurrent
		  programming. The calculus can elegantly express
		  higher-order functions (both eager and lazy) and
		  concurrent objects with encapsulated state and
		  multiple inheritance. The primitives of the
		  $gamma$-calculus are logic variables, names,
		  procedural abstractiojn, and cells. $\ldots$}
}

@TechReport{Smolka-Henz-Wuertz-1993,
  abstract = {Oz is  an experimental  higher-order concurrent
		  constraint programming system under development  at
		  DFKI.  It  combines ideas from  logic and concurrent
		  programming in   a simple yet   expressive
		  language.  From logic   programming  Oz  inherits
		  logic   variables and  logic  data structures,
		  which   provide for a    programming style  where
		  partial information about the values of  variables
		  is imposed concurrently and incrementally.  A  novel
		  feature  of  Oz is   that   it accommodates
		  higher-order programming  without  sacrificing
		  that  denotation  and equality of variables are
		  captured by  first-order logic.  Another new feature
		  of Oz is constraint  communication, a new form of
		  asynchronous communication  exploiting  logic
		  variables.   Constraint communication avoids the
		  problems   of  stream   communication,  the
		  conventional communication   mechanism employed  in
		  concurrent logic programming. Constraint
		  communication can be seen  as  providing a minimal
		  form  of state fully compatible with logic data structures.

                  Based  on constraint communication   and
		  higher-order programming,  Oz readily supports    a
		  variety  of object-oriented   programming styles
		  including multiple inheritance.},
  year = 1993,
  type = {Research Report},
  title = {Object-Oriented Concurrent Constraint
		  Programming in Oz},
  number = {RR-93-16},
  month = APR,
  institution = DFKI,
  author = {Gert Smolka and Martin Henz and J{\"o}rg W{\"u}rtz},
  address = {Saarbr{\"u}cken, Germany},
  bibliographies = {RelMiCS}
}

@TechReport{Smolka-Treinen-1992,
  abstract = {CFT is a new constraint system providing records
		  as logical data structure for constraint (logic)
		  programming.  It can be seen as a generalization of
		  the rational tree system employed in Prolog~II,
		  where finer-grained constraints are used, and where
		  subtrees are identified by keywords rather than by position.

                  CFT is defined by a first-order structure consisting
		  of so-called feature trees.  Feature trees
		  generalize the ordinary trees corresponding to
		  first-order terms by having their edges labeled with
		  field names called features.  The mathematical
		  semantics given by the feature tree structure is
		  complemented with a logical semantics given by five
		  axiom schemes, which we conjecture to comprise a
		  complete axiomatization of the feature tree structure.

                  We present a decision method for CFT, which decides
		  entailment and disentailment between possibly
		  existentially quantified constraints. Since CFT
		  satisfies the independence property, our decision
		  method can also be employed for checking the
		  satisfiability of conjunctions of positive and
		  negative constraints.  This includes quantified
		  negative constraints such as
                  $\forall y\forall z(x\neq f(y,z))$.

                  The paper also presents an idealized abstract
		  machine processing negative and positive constraints
		  incrementally.  We argue that an optimized version
		  of the machine can decide satisfiability and
		  entailment in quasi-linear time.},
  year = 1992,
  type = {Research Report},
  title = {Records for Logic Programming},
  number = {RR-92-23},
  month = {August},
  institution = {German Research Center for Artificial
		 Intelligence (DFKI)},
  author = {Gert Smolka and Ralf Treinen},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany}
}

@InProceedings{Smolka-Treinen-1992a,
  abstract = {CFT is a new constraint system providing records as
		  logical data structure for constraint (logic)
		  programming.  It can be seen as a generalization of
		  the rational tree system employed in Prolog II,
		  where finer-grained constraints are used, and where
		  subtrees are identified by keywords rather than by position.

                  CFT is defined by a first-order structure consisting
		  of so-called feature trees.  Feature trees
		  generalize the ordinary trees corresponding to
		  first-order terms by having their edges labeled with
		  field names called features.  The mathematical
		  semantics given by the feature tree structure is
		  complemented with a logical semantics given by five
		  axiom schemes, which we conjecture to comprise a
		  complete axiomatization of the feature tree structure.

                  We present a decision method for CFT, which decides
		  entailment/disentailment between possibly
		  existentially quantified constraints. Since CFT
		  satisfies the independence property, our decision
		  method can also be employed for checking the
		  satisfiability of conjunctions of positive and
		  negative constraints.  This includes quantified
		  negative constraints such as
                  ``$\forall y\forall z(x\neq f(y,z))$''.},
  year = 1992,
  title = {Records for Logic Programming},
  publisher = {The MIT Press},
  pages = {240--254},
  month = {November},
  booktitle = {Proceedings of the 1992 Joint International Conference and
                       Symposium on Logic Programming},
  author = {Gert Smolka and Ralf Treinen},
  address = {Washington, DC}
}

@Book{Smyth-2003,
  author = {Bill Smyth},
  title = {Computing Patterns in Strings},
  publisher = {Addison Wesley},
  year = 2003,
  ISBN = {0 201 39839 7},
  WKloc = {owned}
}

@Book{Snir-etal-1996,
  author =	 {Marc Snir and Steve W. Otto and Steven Huss-Lederman and David Walker and Jack Dongarra},
  title = 	 {{MPI} --- The Complete Reference},
  publisher = 	 {MIT Press},
  year = 	 1996,
  series =	 {Scientific and Engineering Computation Series},
  McMaster = {QA 76.642 .M65 1996}
}

@Book{Snow-1992,
  author = {C. R. Snow},
  title = {Concurrent Programing},
  publisher = CUP,
  year = 1992,
  volume = 26,
  series = {Cambridge Computer Science Texts},
  bibliographies = {SE3B},
  McMaster = {QA 76.642 .S66 1992},
  keywords = {CSP, OCCAM, 68000}
}

@Article{Snyder-Gallier-1989,
  author = {W. Snyder and J. Gallier},
  title = {Higher-Order Unification Revisited: Complete Sets of
		  Transformations},
  journal = {Journal of Symbolic Computation},
  year = 1989,
  OPTkey = {},
  volume = 8,
  OPTnumber = {},
  OPTmonth = {},
  pages = {101--140},
  OPTnote = {},
  OPTannote = {}
}

@TechReport{Sobocinski-,
  author =       {Pawe{\l} Soboci{\'n}ski},
  title =        {Synthesising Labelled Transition Systems},
  institution =  {BRICS, University of Aarhus},
  year =         {\unfinished},
  type =      {BRICS Progress Report},
  OPTnumber =    {},
  WKloc =   {doc/pap/BIB},
  URL = {http://www.cl.cam.ac.uk/~ps423/papers/pr.pdf},
  abstract =    {This progress report is about categorical machinery
                  which allows one to synthesise labelled transition
                  systems (LTS) for a wide range of `reactive
                  systems'.  The derivation uses G-relative-pushouts
                  (GRPO) which are a 2-categorical generalisation of
                  relative-pushouts (RPO). We develop their basic
                  properties and show that bisimulation on the LTS
                  derived via GRPOs is a congruence, provided that
                  sufficiently many GRPOs exist. The theory is applied
                  to a simple subset of CCS and the resulting LTS is
                  compared to one derived using a procedure proposed
                  by Sewell. We also preview a result which states
                  that previous theory which relied on complicated and
                  non-standard category theory can be recast with the
                  use of GRPOs.}
}

@Article{Soendergaard-Sestoft-1990,
  author = {H. S{\o}ndergaard and P. Sestoft},
  title = {Referential Transparency, Definiteness and Unfoldability},
  journal = {Acta Informatica},
  year = 1990,
  volume = 27,
  pages = {505-517},
  note = {}
}

@InProceedings{Soerensen-1994,
  author = {Morten Heine S{\oe}rensen},
  title = {A Grammar-Based Data-Flow Analysis to Stop Deforestation},
  crossref = {CAAP94},
  pages = {335--351},
  authorsAddress = {DIKU},
  WKloc = {A-0347},
  keywords = {annotation of programs},
  abstract = {$\ldots$}
}

@InProceedings{Soerensen-Glueck-Jones-1994,
  author = {M.H. S{\oe}rensen and R. Gl\"uck and N.D. Jones},
  title = {Towards Unifying Partial Evaluation, Deforestation,
		  Supercompilation, and {GPC}},
  crossref = {ESOP1994},
  pages = {485--500},
  authorsAddress = {DIKU},
  WKloc = {A-0326},
  abstract = {$\ldots$}
}

@Article{Solitro-1989,
  WKloc = {A-0052},
  year = 1989,
  volume = 68,
  title = {A Typed Calculus Based on a Fragment of Linear Logic},
  pages = {333-342},
  journal = TCS,
  author = {Ugo Solitro},
  bibliographies = {RelMiCS}
}

@InProceedings{Solomon-1978,
  author = {Marvin Solomon},
  title = {Type Definitions with Parameters},
  crossref = {POPL1978},
  pages = {31--38},
  note = {Extended abstract}
}

@Article{Solomon-1989,
  author = {Graham Solomon},
  title = {What Became of {Russell's} ``Relation-arithmetic''?},
  journal = RUSSELL,
  volume = 9,
  number = 2,
  year = 1989,
  pages = {168--173},
  bibliographies = {RelMiCS}
}

@PhDThesis{Soltysiak-1980,
  title = {Die Projektion affiner Strukturen \"uber Fastk\"orpern mit
		  Hilfe relationentheoretischer Methoden},
  author = {Roland Soltysiak},
  year = 1980,
  school = {Univ.\null{} Duisburg, Germany},
  bibliographies = {RelMiCS}
}

@InCollection{Somenzi-1998,
  author = {Fabio Somenzi},
  title = {Binary Decision Diagrams},
  pages = {303--366},
  editor = {Manfred Broy and Ralf Steinbr\"uggen},
  booktitle = {Calculational System Design},
  publisher = {IOS Press in cooperation with NATO Scientific Affairs Division},
  series = {Series F: Computer and Systems Sciences},
  volume = 173,
  note = {ISSN 1387-6694},
  year = 1998,
  bibliographies = {SeminarWT2000},
  keywords = {BDD, model checking}
}

@Manual{Somenzi-2005cudd,
  title = 	 {{CUDD}: {CU} Decision Diagram Package, Release 2.4.1},
  OPTkey = 	 {},
  OPTauthor = 	 {Fabio Somenzi},
  OPTorganization = {},
  OPTaddress = 	 {},
  OPTedition = 	 {},
  OPTmonth = 	 {May 17},
  OPTyear = 	 {2005},
  OPTnote = 	 {},
  WKloc = 	 {A-1643, doc/pap/BIB}
}

@Misc{Sommerville-1995a,
  author = {Ian Sommerville},
  title = {Software Engineering (slides)},
  year = 1995,
  WKloc = {B-0120 (chapters 4--11)}
}

@Misc{Somogyi-Henderson-Conway-199X,
  author = {Zoltan Somogyi and Fergus J. Henderson and Thomas C. Conway},
  title = {Mercury, an Efficient Purely Declarative Logic Programming Language},
  year = {199X},
  WKloc = {A-0487, B-0117}
}

@Misc{Somogyi-Henderson-Conway-199Y,
  author = {Zoltan Somogyi and Fergus J. Henderson and Thomas C. Conway},
  title = {The Execution Algorithm of {Mercury}, an Efficient Purely Declarative Logic Programming Language},
  year = {199?},
  WKloc = {A-0731, B-0117}
}

@Misc{Somogyi-Henderson-Conway-199Z,
  author = {Zoltan Somogyi and Fergus J. Henderson and Thomas C. Conway},
  title = {Determinism Analysis in the {Mercury} Compiler},
  year = {199?},
  WKloc = {A-0733}
}

@Misc{Somogyi-Henderson-Conway-Bromage-199X,
  author = {Zoltan Somogyi and Fergus J. Henderson and Thomas C. Conway
            and Andrew Bromage and others},
  title = {Status of the {Mercury} system},
  year = {199?},
  WKloc = {A-0732}
}

@Article{Sonnenschein-1987,
  author = {Michael Sonnenschein},
  title = {Graph translation schemes to generate compiler parts },
  journal = TOPLAS,
  year = 1987,
  volume = 9,
  number = 4,
  pages = {473-490 },
  URL = {http://www.acm.org/pubs/citations/journals/toplas/1987-9-4/p473-sonnenschein/},
  WKloc = {doc/pap/BIB},
  abstract = {Graph translation schemes (GTSs) are a generalization of attribute grammars and of
       some ideas in Koster's language CDL2 They are specially designed to support a compiler
       writer in defining parts of the back-end of his compiler, but they can also be useful for the
       specification of the analysis pass of a compiler. GTSs combine elements of functional and
       of algorithmic specification techniques to allow iterative attribute evaluation and attributing
       of program graphs. GTSs consist of only a few syntactical elements. We present
       operational semantics and discuss improvements in the efficiency of the proposed
       implementation of GTSs.}
}

@InProceedings{Sorensen-Glueck-JonesND-1994,
  author	= {S{\o}rensen, Morten Heine B. and Gl{\"u}ck, Robert and Jones, Neil D.},
  title		= {Towards Unifying Deforestation, Supercompilation,
                   Partial Evaluation, and Generalized Partial Computation},
  pages		= {485--500},
  crossref	= "ESOP1994",
  url		= "ftp://ftp.diku.dk/diku/semantics/papers/D-190.ps.gz",
  WKloc = {doc/pap/BIB}
}

@TechReport{Sorensen-Urzyczyn-1998,
  author = {S{\o}rensen, Morten Heine B. and Urzyczyn, P.},
  year = 1998,
  title = {Lectures on the {Curry-Howard} Isomorphism},
  type = {DIKU Rapport 98/14},
  URL = {http://www.diku.dk/research-groups/topps/bibliography/1998.html#D-368},
  institution = {Department of Computer Science, University of Copenhagen},
  keywords = {The Curry-Howard isomorphism, Logic, Lambda-Calculus},
  WKloc = {A-0986}
}

@Book{Sowa-1984,
  author = {John F. Sowa},
  title = {Conceptual Structures: Information Processing in
		  Mind and Machine},
  publisher = {Addison-Wesley},
  year = 1984,
  bibliographies = {RelMICS},
  UniBwM = {MAG/N7024}
}

@Misc{Sowa-1998,
  author = {John F. Sowa},
  title = {CG Overview},
  year = 1998,
  WKloc = {A-0416}
}

@Book{Sowa-2000,
  author = {John F. Sowa},
  title = {Knowledge Representation: Logical, Philosophical, and Computational Foundations},
  publisher = {Brooks/Cole Publishing Co.},
  year = 2000,
  address = {Pacific Grove, CA},
  bibliographies = {RelMICS},
  URL = {http://www.jfsowa.com/krbook/index.htm}
}

@Misc{Sowa-2002a,
  author = {John F. Sowa},
  title = {Architectures for Intelligent Systems},
  howpublished = {http://www.jfsowa.com/pubs/arch.htm},
  year = 2002,
  WKloc = {A-1280, doc/pap/BIB},
  URL = {http://www.jfsowa.com/pubs/arch.htm}
}

@Misc{Sowa-2006a,
  author =	 {John F. Sowa},
  title =	 {Concept Mapping},
  howpublished = {slides presented in the track on
    Technology, Instruction, Cognition and Learning (TICL)
    at the AREA Conference, San Francisco},
  month =	 {10~} # APR,
  year =	 2006,
  WKloc = 	 {A-1659}
}

@InCollection{Sowa-2008,
  author =       {John F. Sowa},
  title =        {Conceptual Graphs},
  booktitle =    {Handbook of Knowledge Representation},
  URL =  {http://www.jfsowa.com/cg/cg_hbook.pdf},
  pages =     {213-237},
  publisher = {Elsevier},
  year =      2008,
  WKloc = {doc/pap/BIB},
  editor =    {F. van Harmelen, V. Lifschitz, and B. Porter}
}

@InProceedings{Sozeau-2007,
 author = {Sozeau, Matthieu},
 title = {Programming finger trees in {Coq}},
 booktitle = {Proceedings of the 12th ACM SIGPLAN international conference on Functional programming},
 series = {ICFP '07},
 year = {2007},
 isbn = {978-1-59593-815-2},
 location = {Freiburg, Germany},
 pages = {13--24},
 numpages = {12},
 DOIURL = {http://doi.acm.org/10.1145/1291151.1291156},
 DOI = {10.1145/1291151.1291156},
 acmid = {1291156},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Coq, certification, dependent types, finger trees},
}

@InProceedings{Spartalis-Vougiouklis-1988,
  author = {Stefanos P. Spartalis and Thomas N. Vougiouklis},
  title = {P-cyclic Hypergroups with Three Characteristic Elements},
  pages = {421--426},
  crossref = {IGCS1988},
  bibliographies = {RelMiCS}
}

@Misc{Sparud-1994,
  author = {Jan Sparud},
  title = {An Embryo Debugger for Haskell},
  howpublished = {ftp://ftp.cs.chalmers.se/pub/papers?},
  year = 1994,
  month = JAN,
  WKloc = {A-0379},
  abstract = {We describe a debugger for the lazy functional
		  language Haskell. The idea is to transform a program
		  with a source-to-source transformation to a program
		  that creates an evaluation history when executed. If
		  an error is detected while running the program, the
		  programmer can navigate through the evaluation
		  history, by pointing and clicking in a graphical
		  user interface, to try to find the erroneous
		  definition. The method is compared to other ways of
		  debugging lazy functional programs. Some ideas of
		  improvements are also presented.}
}

@Misc{Speirs-Somogyi-Sondergard-199X,
  author = {Chris Speirs and Zoltan Somogyi and Harald Sondergard},
  title = {Termination Analysis for {Mercury}},
  year = {199?},
  WKloc = {A-0730}
}

@Book{Spenk-1993,
  author = {Michael Spenk},
  title = {{PERPLEX}: ein graphisch interaktives System zur
		  Unterst\"utzung der logischen Programmierung},
  publisher = {Oldenbourg},
  year = 1993,
  volume = 207,
  series = {GMD Berichte},
  address = {M\"unchen},
  note = {zugl. Dissertation Universit\"at Stuttgart 1992}
}

@Book{Spivey-1988b,
  author = {J. M. Spivey},
  title = {Understanding {Z}: A Specification Language and its Formal
                  Semantics},
  publisher = {Cambridge University Press},
  series = {Cambridge Tracts in Theoretical Computer Science},
  volume = 3,
  length = 131,
  month = jan,
  year = 1988,
  ISBN = {0-521-33429-2},
  annote = {Published version of 1985 DPhil thesis.},
  bibliographies = {SpecTech}
}

@Book{Spivey-1989,
  UniBwM = {INF560/T7792},
  year = 1989,
  title = {The {Z} Notation: A Reference Manual},
  series = {Prentice Hall International Series in Computer Science},
  publisher = Prentice,
  author = {J. M. Spivey},
  bibliographies = {SpecTech, RelMiCS},
  note = {Out of print; available via \textsf{URL: http://spivey.oriel.ox.ac.uk/mike/zrm/}},
  URL = {http://spivey.oriel.ox.ac.uk/mike/zrm/}
}

@Article{Spivey-1990,
  UniBwM = {Z6198-14},
  WKloc = {A-0029},
  authorsAddress = {PRG},
  abstract = {Exceptions are a feature often provided by
		  programming languages to deal with computations
		  which may fail. This paper argues that lazy
		  functional programming not only makes a built-in
		  exception mechanism unnecessary, but provides a
		  powerful tool for developing and transforming
		  programs that use exceptions. The basic idea is the
		  simple one of augmenting each type with a
		  distinguished error value; this idea is made
		  practical for writing programs and reasoning about
		  them through the use of higher-order functions. An
		  advantage is that simple equational arguments can be
		  used to reason about the programs.

		  Throughout the paper, the problem of simplifying
		  algebraic expressions using rewriting rules is used
		  as a source of motivation and examples.},
  year = 1990,
  volume = 14,
  title = {A Functional Theory of Exceptions},
  pages = {25--42},
  journal = SCICOP,
  author = {Mike Spivey},
  annote = {--- HOPSnotes ---
		  transformations with error elements},
  bibliographies = {RelMiCS}
}

@Misc{Spivey-1990a,
  author = {Mike Spivey},
  title = {A guide to the {\tt zed} style option},
  year = 1990,
  WKloc = {A-0658},
  bibliographies = {SpecTech}
}

@Book{Spivey-1992,
  year = 1992,
  title = {The {Z} Notation: A Reference Manual},
  series = PrenticeCS,
  edition = {Second},
  publisher = Prentice,
  author = {J. M. Spivey},
  bibliographies = {SpecTech, RelMiCS},
  note = {Out of print; available at \textsf{http://spivey.oriel.ox.ac.uk/\~{}mike/zrm/}},
  URL = {http://spivey.oriel.ox.ac.uk/~mike/zrm/}
}

@Manual{Spivey-2000,
  author = {Mike Spivey},
  title = {The {{\large\it f\kern0.1em}{\normalsize\sc uzz}} Manual},
  year = 2000,
  organization = {The Spivey Partnership},
  address =	 {Oxford},
  edition =	 {Second edition}
}

@Manual{Spivey-2001a,
  title = {Proceduaral Programming Laboratory Manual},
  author = {Mike Spivey},
  organization = {Oxford University Computing Laboratory},
  month = JAN,
  year = 2001,
  keywords = {Oberon},
  WKloc = {A-1427}
}

@Manual{Spivey-2001b,
  title = {Oxford Oberon-2 Compiler: User's Manual},
  author = {Mike Spivey},
  organization = {Oxford University Computing Laboratory},
  month = JAN,
  year = 2001,
  WKloc = {A-1430}
}

@Manual{Spivey-2008fuzz,
  title =  {The \fuzz{} type-checker for {Z}, Version 3.4.1,
    {\rm{and}} {The \fuzz{} Manual, Second Edition}},
  author =    {Mike Spivey},
  year =      2008,
  note =      {Available from \textsf{http://spivey.oriel.ox.ac.uk/mike/fuzz/}
                  (last accessed 2011-06-17)}
}

@InProceedings{Spivey-Seres-1999,
  author = {Mike J. Spivey and Silvija Seres},
  title = {The Algebra of Searching},
  booktitle = {Proceedings of the Symposium in honour of work of of {C.A.R. Hoare}},
  year = 1999,
  month = SEP,
  URL = {http://users.comlab.ox.ac.uk/silvija.seres/Papers/carh99.html}
}

@InProceedings{Springintveld-1995,
  author = {Jan Springintveld},
  title = {Third-Order Matching in the Presence of Type Constructors},
  crossref = {TLCA95},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0562}
}

@InProceedings{Srinivas-1992,
  author = {Yellamraju V. Srinivas},
  title = {Derivation of a parallel Matching Algorithm},
  crossref = {MPC1992},
  pages = {323--343},
  WKloc = {A-0242},
  abstract = {We present a derivation of a parallel version of the
		  Knuth-Morris-Pratt algorithm for finding occurrences
		  of a pattern string in a target string. We show that
		  the failure function, the source of efficiency of
		  the sequential algorithm, is a form of search in an
		  ordered domain. This view enables the generalization
		  of the algorithm both beyond sequential execution
		  and the string data structure. Our derivation
		  systematically uses a divide-and-conpuer strategy.
		  The computation tree so generated can be mapped onto
		  time, yielding a naive sequential algorithm, onto a
		  processor tree, yielding a parallel algorithm, or
		  onto a data structure, yielding the failure function.}
}

@InProceedings{Srinivas-1994,
  author = {Y.V. Srinivas},
  title = {Augmenting Algebraic Specifications with Structured
		  Sorts and Structural Subsorting},
  crossref = {PROCOMET94},
  pages = {521--540},
  keywords = {Requirements/Specifications; Languages Constructs
		  and Features; Object-oriented Programming; Types in
		  Specifications}
}

@InProceedings{Srinivas-Juellig-1995,
  author = {Yellaraju V. Srinivas and Richard J{\"u}llig},
  title = {{\sc Specware}:$^{\sc tm}$ Formal Support for Composing Software},
  crossref = {MPC1995},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0706}
}

@InProceedings{Staerk-1994,
  title = {The Declarative Semantics of the {Prolog} Selection Rule},
  author = {Robert F. St{\"a}rk},
  pages = {252--261},
  crossref = {LICS9},
  abstract = {We axiomatize the Prolog selection rule which always selects
      the leftmost literal in a goal. We introduce a new completion of a
      logic program which we call the $\ell$-completion of the program. The
      $\ell$-completion is formulated as a first-order theory in a language
      extended by new predicate symbols which express success, failure and
      left-termination of queries. The main results of the paper are the
      following. If a query succeeds, fails or is left-terminating under
      the Prolog selection rule, then the corresponding formula in the
      extended language is provable from the $\ell$-completion. Conversely,
      if a logic program and a query are correct with respect to some mode
      assignment and if one can prove in the $\ell$-completion that the
      query succeeds and is left-terminating, then the goal is successful
      and Prolog, using its depth first search, will compute an answer
      substitution for the goal. This result can even be extended to so
      called non-floundering queries.}
}

@Article{StallmannMF-2012,
 author = {Stallmann, Matthias F.},
 title = {A heuristic for bottleneck crossing minimization and its performance on general crossing minimization: Hypothesis and experimental study},
 journal = {J. Exp. Algorithmics},
 issue_date = {July 2012},
 volume = {17},
 number = {1},
 month = jul,
 year = {2012},
 issn = {1084-6654},
 pages = {1.3:1.1--1.3:1.30},
 articleno = {1.3},
 numpages = {1.2},
 DOIURL = {http://doi.acm.org/10.1145/2133803.2212314},
 doi = {10.1145/2133803.2212314},
 acmid = {2212314},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {Barycenter heuristic, crossing minimization, sifting heuristic},
 bibiliographies = {GraTraVis}
}

@Book{Stamm-1995,
  author = {Marcelo R. Stamm},
  title = {{Die Reorganisation der Philosophie aus einem Prinzip}},
  publisher = {Klett-Cotta},
  year = 1995,
  address = {Stuttgart}
}

@InProceedings{Staples-1978,
  WKloc = {A-0058},
  title = {A Graph-Like Lambda Calculus for which Leftmost-Outermost Reduction is Optimal},
  pages = {440--455},
  crossref = {GG1978},
  author = {John Staples}
}

@Article{Staples-1980a,
  WKloc = {B-0007},
  authorsAddress = {Department of Mathematics and Computer Science,
             Queensland Institute of Technology, Brisbane 4001, Australia},
  abstract = {This paper begins a three-part study of efficient evaluations
             of expressions, which shows that for a wide class of
             expressions there are theoretical benefits in allowing
             graph-like expressions in evaluation algorithms. In this
             first part, a theory of graph-like expressions is developed
             as far as the basic theoretical result which underlies the
             remainder of the study. It asserts a modified commutativity
             property for computational steps in suitable systems of
             graph-like expressions.},
  year = 1980,
  volume = 10,
  title = {Computation on Graph-Like Expressions},
  pages = {171--185},
  journal = {Theoretical Computer Science},
  author = {John Staples}
}

@Article{Staples-1980b,
  WKloc = {B-0007},
  abstract = {This paper is the second part in a three-part study of
      efficient evaluations of expressions. Here a general evaluation
      algorithm of the outermost type is given, and proved to be optimal
      for a wide class of systems of graph-like expressions. Several
      examples of the application of the algorithm are also given.},
  year = 1980,
  volume = 10,
  title = {Optimal Evaluations of Graph-Like Expressions},
  pages = {297--316},
  journal = {Theoretical Computer Science},
  author = {John Staples}
}

@Article{Staples-1980c,
  WKloc = {B-0007},
  abstract = {This paper is the last in a three-part study of efficient
      evaluations of expressions. Here the optimal evaluation algorithms
      for systems of graph-like expressions studied earlier are compared
      with the best possible performance of evaluation algorithms for
      classical systems of expressions. For a wide class of systems it is
      shown that a graph-like version can speed up the classical system
      while maintaining correctness.},
  year = 1980,
  volume = 11,
  title = {Speeding up Subtree Replacement Systems},
  pages = {39--47},
  journal = {Theoretical Computer Science},
  author = {John Staples}
}

@Article{Staples-1981,
  author = {John Staples},
  title = {Efficient Combinatory Reduction},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Zeitschrift f\"ur Mathematische Logik und Grundlagen
		  der Mathematik},
  year = 1981,
  volume = 27,
  OPTnumber = {},
  pages = {391--402},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  keywords = {functional},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Staples-1982,
  WKloc = {A-0057},
  abstract = {This paper proposes a solution to the problem of implementing
	an optimal evaluation strategy for the lambda calculus. The solution
	uses in a substantial way the notion of composition rewrite rules,
	first studied by Ehrig and others, in the `algebraic' approach to
	graph grammars, under the name `concurrent productions'.},
  title = {Two-Level Expression Representation for Faster Evaluation},
  pages = {392--404},
  crossref = {GG1982},
  author = {John Staples}
}

@Article{Staples-1988,
  author = {Staples, John},
  title = {Delaying Unification Algorithms for Lambda Calculi},
  journal = {Theoretical Computer Science},
  year = 1988,
  volume = 56,
  number = 3,
  pages = {277--288},
  abstract = {Huet proposed delaying some unifications of typed
		  lambda terms, as part of a higher order resolution
		  method. This paper abstracts that idea from the
		  resolution context. Unification delays are
		  formalised independently of any context, by defining
		  a lambda calculus where a unification constraint
		  forms an integral part of each term. This calculus
		  is shown to support an unusually simple unification
		  theory, where most general unifiers trivially always
		  exist.  Attention shifts from the existence of
		  unifiers to the simplification of expressions for
		  most general unifiers. The approach is convenient
		  for discussing the unification of untyped lambda
		  terms and has promise for discussing schematic
		  unification of term schemes. It may also be a
		  convenient format for unification algorithms in
		  rewriting systems which use lazy evaluation to
		  compute with notations for infinite structures.},
  annote = {also as TR No. 75, Department of Computer Science,
		  University of Queensland, St. Lucia, 1986}
}

@InProceedings{Staples-1991a,
  author = {John Staples},
  title = {A New Technique for Analysing Parameter Passing
		  Applied to the Lambda Calculus},
  OPTcrossref = {},
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTpages = {},
  booktitle = {Proceedings of the 4th Computer Science Conference,
		  St. Lucia, Australia},
  year = 1981,
  OPTorganization = {},
  OPTpublisher = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {functional},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Staples-Robinson-1987,
  author = {Staples, John and Robinson, Peter J.},
  title = {Unification of Quantified Terms},
  pages = {426--450},
  crossref = {GR86},
  WKloc = {A-0096},
  bibliographies = {RelMiCS},
  abstract = {Unification algorithms for quantified terms are
		  needed for the implementation of extended functional
		  and logic programming languages, and also for the
		  implementation of other symbolic computation systems
		  such as theorem provers and proof editors. This
		  paper describes and proves correct such a
		  unification algorithm. Although discussed here in a
		  theoretically convenient way, the algorithm is
		  suitable for enhancement of conventional unification
		  algorithms for free variable terms, such as are
		  found in, for example, Prolog interpreters. The
		  algorithm has been demonstrated by modifying a
		  conventional Prolog interpreter so as to interpret
		  formulas which include previously declared quantifiers.}
}

@Article{Staples-Robinson-1988,
  author = {Staples, John and Robinson, Peter J.},
  title = {Efficient Unification of Quantified Terms},
  journal = JLOG,
  year = 1988,
  volume = 5,
  number = 2,
  pages = {133--150},
  month = JUN,
  UniBwM = {Z6732},
  WKloc = {A-0095},
  bibliographies = {RelMiCS},
  abstract = {Conventional logic-programming languages rely
		  fundamentally on symbolic computation with
		  quantifier-free terms. Much theoretical logic uses
		  the richer vocabulary of quantified terms, however.
		  In this paper we sketch some first steps in a
		  program of research for developing data structures
		  and algorithms to support efficient computation
		  directly on quantified terms. We describe a simple
		  concept of quantified term, and efficient
		  unification algorithms for both structure-sharing
		  and non-structure-sharing representations of those
		  terms. The efficiency of the approach results from
		  the techniques used to represent terms, which enable
		  naive substitution to implement correct substitution
		  for quantified terms. The non-structure-sharing
		  unification algorithm described here has been
		  prototyped by modification of a conventional
		  logic-programming interpreter.}
}

@InProceedings{Stark-1989,
  author = {Eugene Stark},
  title = 	 {Compositional relational semantics for indeterminate dataflow networks },
  crossref =  {CTCS1989},
  pages = 	 {52--74},
  DOI = 	 {10.1007/BFb0018344},
  URL = {http://www.springerlink.com/content/539h816r1m4v6621/?p=4081553b75c24853b9eb0b7f9673ebac&pi=3},
  PDF = {http://www.springerlink.com/content/539h816r1m4v6621/fulltext.pdf},
  WKloc = {doc/pap/BIB},
  abstract = 	 {Given suitable categories $T, C$ and functor $F : T \rightarrow C$,
      if $X,Y$ are objects of $T$, then we define an $(X,Y)$-relation in $C$
      to be a triple
      $$
         \left( {R,\underset{\raise0.3em\hbox{$\smash{\scriptscriptstyle-}$}}{r} ,\bar r}
         \right)
         \enskip,
      $$
      where $R$ is an object of $C$ and $r : R \rightarrow FX$ and
      $$\bar r:R \to FY
      $$
      are morphisms of $C$.
      We define an algebra of relations in $C$,
      including operations of ``relabeling'', ``sequential composition'',
      ``parallel composition'', and ``feedback'',
      which correspond intuitively to ways
      in which processes can be composed into networks.
      Each of these operations is defined in terms of composition and limits in $C$,
      and we observe that any operations defined in this way
      are preserved under the mapping from relations in $C$
      to relations in $C'$ induced by a continuous functor $G : C \rightarrow C'$.

      To apply the theory,
      we defined a category $Auto$ of concurrent automata,
      and we give an operational semantics
      of dataflow-like networks of processes with indeterminate behaviors,
      in which a network is modeled as a relation in $Auto$.
      We then define a category $EvDom$ of ``event domains'',
      a (non-full) subcategory
      of the category of Scott domains and continuous maps,
      and we obtain a coreflection between $Auto$ and $EvDom$.
      It follows, by the limit-preserving properties of coreflectors,
      that the denotational semantics
      in which dataflow networks are represented by relations in $EvDom$,
      is ``compositional'' in the sense
      that the mapping from operational to denotational semantics
      preserves the operations on relations.
      Our results are in contrast to examples of Brock and Ackerman,
      which imply that no compositional semantics is possible
      in terms of set-theoretic relations.}
}

@TechReport{Stark-1991,
  author = {Eugene Stark},
  title = {Dataflow Networks are Fibrations},
  institution = {State University of New York Stony Brook},
  year = 1991,
  URL = {ftp://sbcs.sunysb.edu/pub/TechReports/stark/fibrations.dvi.Z},
  authorsAddress = {stark\@cs.sunysb.edu},
  WKloc = {A-0228},
  abstract = {Dataflow networks are a paradigm for concurrent
		  computation in which a collection of concurrently
		  and asynchronously executing processes communicate
		  by sending messages over FIFO message channels. In a
		  previous paper, we showed that dataflow networks
		  could be represented as certain spans in a category
		  of automata, or more abstractly, in a category of
		  domains, and we identified some universal properties
		  of various operations for building networks from
		  components. Not all spans corresponded to dataflow
		  processes, and we raised the question of what might
		  be an appropriate categorical characterization of
		  those spans that are ``dataflow-like.'' In this
		  paper, we answer this question by obtaining a
		  characterization of the dataflow-like spans as {\em
		  split right fibrations}, either in a 2-category of
		  automata or a 2-category of domains. This
		  characterization makes use of the theory of
		  fibrations in a 2-category developed by Street. In
		  that theory, the split right fibrations are the
		  algebras of a certain doctrine (or 2-monad) $R$ on a
		  category of spans. For the 2-categories we consider,
		  $R$ has a simple interpretation as an ``input
		  buffering'' construction.}
}

@TechReport{Stark-1991a,
  author = {Eugene W. Stark},
  title = {Compositional Relational Semantics for Indeterminate
		  Dataflow Networks},
  institution = {State University of New York Stony Brook CS},
  year = 1991,
  note = {A version of this paper appeared as: E. W. Stark,
		  Compositional Relational Semantics for Indeterminate
		  Dataflow Networks Category Theory and Computer
		  Science, Manchester, England pp. 52-74 Volume 389 of
		  Lecture Notes in Computer Science Springer-Verlag,
		  1989},
  WKloc = {A-0280},
  abstract = {{\def\ubar#1{\underline{#1}}\def\obar#1{\overline{#1}}
                  \def\AUTO{{\bf Auto}}\def\EVDOM{{\bf EvDom}}
                  \def\C{{\bf C}}\def\T{{\bf T}}
                  Given suitable categories $\T, \C$ and functor
		  $F: \T\rightarrow\C$, if $X, Y$ are objects of $\T$,
		  then we define an $(X, Y)$-{\em relation in} $\C$ to
		  be a triple $(R, \ubar{r}, \obar{r})$, where $R$ is
		  an object of $\C$ and $\ubar{r}: R\rightarrow FX$
		  and $\obar{r}: R\rightarrow FY$ are morphisms of
		  $\C$. We define an algebra of relations in $\C$,
		  including operations of ``relabeling,'' ``sequential
		  composition,'' ``parallel composition,'' and
		  ``feedback,'' which correspond intuitively to ways
		  in which processes can be composed into
		  networks. Each of these operations is defined in
		  terms of composition and limits in $\C$, and we
		  observe that any operations defined in this way are
		  preserved under the mapping from relations in $\C$
		  to relations in $\C'$ induced by a continuous
		  functor $G: \C\rightarrow \C'$.

                  To apply the theory, we define a category $\AUTO$ of
		  concurrent automata, and we give an operational
		  semantics of dataflow-like networks of processes
		  with indeterminate behaviors, in which a network is
		  modeled as a relation in $\AUTO$. We then define a
		  category $\EVDOM$ of ``event domains,'' a (non-full)
		  subcategory of the category of Scott domains and
		  continuous maps, and we obtain a coreflection
		  between $\AUTO$ and $\EVDOM$. It follows, by the
		  limit-preserving properties of coreflectors, that
		  the denotational semantics in which dataflow
		  networks are represented by relations in $\EVDOM$,
		  is ``compositional'' in the sense that the mapping
		  from operational to denotational semantics preserves
		  the operations on relations. Our results are in
		  contrast to examples of Brock and Ackerman, which
		  imply that no compositional semantics is possible in
		  terms of set-theoretic relations.}},
  bibliographies = {RelMiCS}
}

@PhDThesis{Stark-1994,
  author = {Ian David Bede Stark},
  title = {Names and Hogher-Order Functions},
  school = {University of Cambridge},
  year = 1994,
  month = DEC,
  WKloc = {A-0967},
  bibliographies = {LogRel}
}

@Manual{Starr-Starr-1988,
  title = 	 {Practical Piano Skills},
  author =	 {William Starr and Constance Starr},
  edition =	 {Fourth edition},
  year =	 1988,
  McMaster = 	 {MT 225 .S83 1988},
  bibliographies = {Cynthia}
}

@InCollection{Statman-1980,
  author = {Richard Statman},
  title = {On the Existence of Closed Terms in Typed $\lambda$-Calculus I},
  crossref = {Seldin-Hindley-1980},
  pages = {511--534}
}

@Article{Statman-1985,
  author = {Richard Statman},
  title = {Logical Relations and the Typed $\lambda$-Calculus},
  journal = {Information and Control},
  year = 1985,
  volume = 65,
  pages = {85--97},
  bibliographies = {RelMiCS, LogRel},
  WKloc = {A-0956}
}

@TechReport{Stebletsova-Venema-1997,
  author = {Vera Stebletsova and Yde Venema},
  title = {$Q$-algebras},
  institution = {Department of Philosophy -- Utrecht University},
  year = 1997,
  type = {Logic Group Preprint Series},
  number = 177,
  WKloc = {C-0012}
}

@PhDThesis{Steckler-1994,
  author = {Paul Steckler},
  title = {Correct Higher-Order Program Transformations},
  school = {Northeastern University, College of Computer Science},
  year = 1994,
  month = JUL,
  type = {D.~Phil.~thesis},
  URL = {ftp://ftp.ccs.neu.edu//pub/people/steck/thesis.ps},
  authorsAddress = {http://www.ccs.neu.edu/home/steck},
  WKloc = {B-0026},
  OPTabstract = {We present a method for proving the correctness of
		  compiler optimizations for higher-order programming
		  languages.  Programs are annotated with propositions
		  derived as the solutions to dataflow constraints
		  formulated as local consistency conditions for each
		  parse tree node.  We can prove that any such
		  solution yields sound annotations, that is, the
		  propositions are true.  Each compiler optimization
		  is presented as a source-to-source transformation.
		  Using the annotations and additional constraint
		  information, we can prove that the optimization is
		  correct with respect to some criterion.  The
		  correctness criterion is similar for each
		  transformation.  The particular optimizations we
		  exhibit are {\em selective and lightweight closure
		  conversion} (constructing source-level closures for
		  procedures, with two variations), {\em \ultrabeta}
		  (a generalization of copy propagation to
		  higher-order languages), and {\em selective
		  thunkification} (transforming call-by-name programs
		  into call-by-value equivalents).}
}

@InProceedings{Steele-1994,
  author = {Steele Jr., Guy L.},
  title = {Building Interpreters by Composing Monads},
  crossref = {POPL1994},
  pages = {472--492},
  authorsAddress = {Thinking Machines},
  WKloc = {A-0400},
  keywords = {pseudomonads}
}

@InProceedings{Steele-1998,
  author = {Steele Jr., Guy L.},
  title = {Growing a Language},
  crossref = {},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0715},
  keywords = {Java}
}

@Unpublished{Stefanescu-1994,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {Flownomials: regular expressions for distributed computation},
  note = {obtained at Dagstuhl seminar 9403},
  WKloc = {A-0218},
  OPTabstract = {We present an extension of the calculus of regular
		  expressions to the case of atoms having many input
		  and/or output connecting ports, called the {\em
		  calculus of flownomials}. The strings of ports model
		  `local states' and they make the extension more
		  expressive and able to be used for distributed computation.}
}

@TechReport{Stefanescu-1994a,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {Algebra of Flownomials --- {Part 1: Binary} Flownomials; Basic Theory},
  year = 1994,
  month = SEP,
  institution = {Technische Universit\"at M\"unchen, Institut f\"ur Informatik},
  OPTtype = {},
  number = {TUM-I9437},
  URL = {http://www.comp.nus.edu.sg/~gheorghe/papers/algflowAbs.html},
  WKloc = {A-0890, doc/pap/BIB}
}

@InProceedings{Stefanescu-1996b,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {A Short Tour on {FEST}},
  booktitle = {Proc. of International Congress on Actual Trends in Cybernetics and Phliosophy of Science},
  month = OCT,
  year = 1996,
  publisher = {???},
  pages = {???},
  annote = {IMAR-96-38},
  filename = {IMAR-96-38.ps}
}

@Article{Stefanescu-1998,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {Reaction and Control {I}. Mixing Additive and Multiplicative Network Algebras},
  pages = {349--368},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  abstract = {This paper is included in a series aiming to contribute to
      the algebraic theory of distributed computation. The key problem in
      understanding Multi-Agent Systems is to find a theory which
      integrates the {\em reactive} part and the {\em control} part of such
      systems.

      To this end we use the {\em calculus of flownomials}. It is a
      polynomial-like calculus for representing flowgraphs and their
      behaviours. An `additive' interpretation of the calculus was
      intensively developed to study control flowcharts and finite
      automata. For instance, regular algebra and iteration theories are
      included in a unified presentation. On the other hand, a
      `multiplicative' interpretation of the calculus of flownomials was
      developed to study dataflow networks.
      %Such networks consist of a collection of
      %concurrent asynchronous processes which communicate by
      %sending data over FIFO channels.

      The claim of this series of papers is that the mixture of the
      additive and multiplicative network algebras will contribute to the
      understanding of distributed computation.

      The r\^ole of this first paper is to present a few motivating
      examples.},
  URL = {http://www.oup.co.uk/igpl/Volume_06/Issue_02/},
  annote = {IMAR-96-33},
  WKloc = {A-0891},
  filename = {IMAR-96-33.ps},
  bibliographies = {RelMiCS}
}

@Misc{Stefanescu-1999a,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {Axiomatizing Mixed Relations (preliminary results)},
  year = 1999,
  month = DEC,
  OPTannote = {},
  WKloc = {A-0892},
  filename = {IMAR-96-37.ps}
}

@Book{Stefanescu-2000,
  author = {{Gh}eorghe {\c{S}}tef{\u{a}}nescu},
  title = {Network Algebra},
  publisher = Springer,
  year = 2000,
  URL = {http://funinf.cs.unibuc.ro/~ghstef/na/myAdv.html},
  address = {London},
  month = APR,
  ISBN = {1-85233-195-X},
  pages = {xvi+400},
  bibliographies = {RelMiCS},
  abstract = {Network Algebra considers the algebraic study of networks
   and their behaviour. It contains general results on the
   algebraic theory of networks, recent results on the
   algebraic theory of models for parallel programs, as well as
   results on the algebraic theory of classical control
   structures. The results are presented in a unified framework
   of the calculus of flownomials, leading to a sound
   understanding of the algebraic fundamentals of the network
   theory. Network Algebra will be of interest to anyone
   interested in network theory or its applications and
   provides them with the results needed to put their work on a
   firm basis. Graduate students will also find the material
   within this book useful for their studies.},
  contents = {An Introduction to Network Algebra:
	   Short Overview on the key results.
	   (http://funinf.cs.unibuc.ro/~ghstef/na/naIntr.ps.gz)
	   Network Algebra and its applications.
   Relations, Flownomials and Abstract Networks:
	   Networks modulo graph isomorphism.
	   Algebraic models for branching constants.
	   Network behaviour.
	   Elgot theories.
	   Kleene theories.
   Algebraic Theory of Special Networks:
	   Flowchart schemes.
	   Automata.
	   Process Algebra.
	   Dataflow Networks.
	   Petri Nets.
   Towards an Algebraic Theory for Software Components:
	   Mixed Network Algebra.
	   Related Calculi, Closing Remarks.
   Appendices.
   Bibliography.
   (http://funinf.cs.unibuc.ro/~ghstef/na/naRef.ps.gz)
   List of Tables.
   List of Figures.
   Index.}
}

@InProceedings{Stefanescu-Zhou-1994,
  author = {Dan {\c{S}}tef{\u{a}}nescu and Yuli Zhou},
  title = {An equational framework for the
                   flow analysis of higher order
                   functional programs},
  pages = {318--327},
  OPTabstract = {},
  OPTurl = {},
  OPTwkloc = {A-09},
  year = 1994,
  month = {},
  crossref = {},
  booktitle = {Proceedings of the 1994 ACM
                   conference on LISP and
                   functional programming},
  editor = {},
  publisher = {ACM},
  series = {},
  volume = {}
}

@InProceedings{Stefansson-1994,
  title = {Systems of Set Constraints with Negative Constraints are
      {NEXPTIME}-Complete},
  author = {Kjartan Stef{\'a}nsson},
  pages = {137--141},
  crossref = {LICS9},
  abstract = {Systems of Set Constraints with Negative Constraints are
      {\em NEXPTIME\/}-Complete \par A system of set constraints is a
      system of expressions $E\subseteq F$ where $E$ and $F$ describe sets
      of ground terms over a ranked alphabet. Aiken {\em et al.\/}\ (Aiken,
      Kozen, Vardi and Wimmers, 1993) classified the complexity of such
      systems. In (Aiken, Kozen and Wimmers, 1993) it was shown that if
      negative constraints $E\not\subseteq F$ were allowed, then the
      problem is decidable. This was done by reduction to a Diophantine
      problem, the Nonlinear Reachability Problem, which was shown to be
      decidable. \par We show that nonlinear reachability is {\em
      NP\/}-complete. By bounding the reduction of (Aiken, Kozen and
      Wimmers, 1993) we conclude that systems of set constraints, allowing
      negative constraints, are {\em NEXPTIME\/}-complete.}
}

@InProceedings{Steffen-1991,
  title = {Data Flow Analysis as Model Checking},
  author = {Bernhard Steffen},
  pages = {346--364},
  crossref = {TACS1991},
  abstract = {The paper develops a framework that is based on the idea
		  that modal logic provides an appropriate framework
		  for the specification of data flow analysis (DFA)
		  algorithms as soon as programs are represented as
		  models of the logic. This can be exploited to
		  construct a DFA-{\em generator\/} that generates
		  efficient implementations of DFA-algorithms from
		  modal specifications by partially evaluating a
		  specific model checker with respect to the
		  specifying modal formula.  Moreover, the use of a
		  modal logic as specification language for
		  DFA-algorithms supports the compositional
		  development of specifications and structured proofs
		  of properties of DFA-algorithms.  -- The framework
		  is illustrated by means of a real life example: the
		  problem of determining optimal computation points
		  within flow graphs.}
}

@InProceedings{Steffen-Knoop-Ruething,
  author = {Bernhard Steffen and Jens Knoop and Oliver R\"uthing},
  title = {The Value Flow Graph: A Program Representation for
		  Optimal Program Transformations},
  crossref = {ESOP1990},
  pages = {389--405}
}

@InProceedings{Steffen-Pierce-1994,
  author = {M. Steffen and B. Pierce},
  title = {Higher-order Subtyping},
  crossref = {PROCOMET94},
  pages = {501--520},
  keywords = {Object-oriented Programming; Formal Definitions and
		  Theory; Semantics of Programming Languages; Static
		  Type Systems; Lambda-Calculus; Polymorphism;
		  Subtyping}
}

@article{Stehr-2005,
 author = {Stehr, Mark-Oliver},
 title = {The Open Calculus of Constructions (Part II): An Equational Type Theory with Dependent Types for Programming, Specification, and Interactive Theorem Proving},
 journal = FUNDI,
 volume = {68},
 number = {3},
 year = {2005},
 issn = {0169-2968},
 pages = {249--288},
 ACM = {http://portal.acm.org/citation.cfm?id=1227238},
 publisher = {IOS Press},
 address = {Amsterdam, The Netherlands, The Netherlands},
 abstract = {The open calculus of constructions integrates key features
    of Martin-Löf's type theory, the calculus of constructions,
    membership equational logic, and rewriting logic
    into a single uniform language.
    The two key ingredients are dependent function types
    and conditional rewriting modulo equational theories.
    We explore the open calculus of constructions
    as a uniform framework for programming,
    specification and interactive verification
    in an equational higher-order style.
    By having equational logic and rewriting logic as executable sublogics
    we preserve the advantages of a first-order semantic and logical framework
    and we provide a foundation for a broad spectrum of applications
    ranging from what could be called executable mathematics,
    involving symbolic computations and logical proofs,
    to software and system engineering applications,
    involving symbolic execution
    and analysis of nondeterministic and concurrent systems.}
}

@InCollection{Steinbrueggen-1980,
  author = {R. Steinbr{\"u}ggen},
  title = {The use of nested scheme parameters in the system {CIP}},
  booktitle = {{GI --- 10.\null{} Jahrestagung, Saarbr\"ucken}},
  editor = {R. Wilhelm},
  series = {Informatik Fachberichte},
  volume = 33,
  publisher = {Springer-Verlag},
  year = 1980,
  pages = {106},
  DOI = {10.1007/978-3-642-67838-7_9},
  SpringerURL = {http://link.springer.com/chapter/10.1007/978-3-642-67838-7_9},
  note = {extended abstract.},
  annote = {\cite{Partsch-Steinbrueggen-1983} cites this
    for introducing ``context parameters'' (second-order rule variables)},
  abstract = {CIP --- a system for interactive program development ---
    operates on arbitrary context-free languages.
    At the moment it is running on the wide spectrum language CIP-L
    and on the language of relational algebra.
    The efficiency depends on the transformation catalogues.
    In the system CIP, complex transformations can be defined which
    (1) cover wider classes of special cases and
    (2) summarize frequently used chains of transformations.}
}

@TechReport{Steinbrueggen-1980a,
  author = {R. Steinbr{\"u}ggen},
  title =        {Pre-algorithmic specifications of the system {CIP}. Part I},
  institution =  {Institut f\"ur Informatik der TU M\"unchen},
  year =         {1980},
  OPTkey =       {},
  OPTtype =      {},
  number =    {TUM-I8016},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTnote =      {},
  OPTannote =    {}
}

@TechReport{Stell-1994,
  author = {John G. Stell},
  title = {Modelling Term-Rewriting Systems by Sesqui-Categories},
  institution = {Department of Computer Science, Keele University},
  year = 1994,
  URL = {ftp://ftp.cs.keele.ac.uk/pub/techreports/1994/tr94-02.ps},
  number = {TR94-02},
  address = {Keele, Staffordshire, UK},
  month = JAN,
  file = {~kahl/doc/pap/tr94-02.ps.gz},
  WKloc = {A-0259},
  abstract = {It is well-known that a term rewriting system gives
		  rise to a 2-category in which the objects are finite
		  sets of variables, the morphisms are  substitutions,
		  and the 2-cells are rewrites. This paper
		  demonstrates that a generalization of a 2-category,
		  called a sesqui-category, is a more appropriate
		  categorical model for a term rewriting system.  This
		  is principally because, unlike the case of a
		  2-category, the sesqui-category associated to a term
		  rewriting system supports a notion of length on its
		  2-cells.  This notion of length is used in
		  developing definitions of local confluence and
		  critical pair in the context of an arbitrary
		  sesqui-category with length and certain additional
		  structure. The relationship between these abstract
		  definitions and their usual term rewriting
		  counterparts is established. This provides some
		  evidence that sesqui-categories may be an
		  appropriate vehicle for an abstract theory of
		  rewriting capable of handling critical pairs.}
}

@MastersThesis{Stender-1992,
  keywords = {SHOPS},
  year = 1992,
  title = {Erweiterung des {DAG}-orientierten {Programmiersystems}
		  {Smalltalk-HOPS} um {Modulverwaltung} und
		  {Transformationssystem}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  type = {Diplomarbeit},
  number = {ID 42/92},
  note = {ID 42/92},
  author = {J\"org Stender},
  WKloc = {C-0013}
}

@Book{Stenlund-1972,
  author = {S{\"o}ren Stenlund},
  title = {Combinators, $\lambda$-Terms and Proof Theory},
  publisher = {D.~Reidel},
  year = 1972,
  series = {Synthese Library},
  address = {Dordrecht, Holland},
  UniBwM = {MAT006/D13852}
}

@InProceedings{Stepney-Cooper-Woodcock-1998,
  author = {Susan Stepney and David Cooper and Jim Woodcock},
  title = {More Powerful {Z} Data Refinement: Pushing the State of the Art in Industrial Refinement},
  crossref = {ZUM1998},
  pages = {284--307},
  WKloc = {A-1326},
  bibliographies = {RelMiS}
}

@InProceedings{Stevenson-1993,
  author = {D. E. Stevenson},
  title = {Science, Computational Science and Computer Science: At a
          Crossroads},
  abstract = {We describe computational science as interdisciplinary
             approach to doing science on computers. Our purpose is to
             introduce computational science as a legitimate interest of
             computer scientists.

             We present a possible foundation for
             computational science. These foundations show that there is a
             need to consider computational aspects of science at the
             scientific level. We next present some obstacles to computer
             scientists' participation in computational science. We see a
             cultural bias in computer science that inhibits participation.
             Finally, we indicate areas of mutual interests between
             computational science and computer science.},
  crossref = {ACM1993}
}

@InProceedings{Stewart-1993,
  author = {I.A. Stewart},
  title = {Incorporating Generalised Quantifiers and The Least
		  Fixed Point Operator},
  crossref = {CSL93},
  pages = {318--333},
  keywords = {complexity}
}

@InProceedings{StewartDon-Sjanssen-2007,
 author = {Don Stewart and Spencer Sjanssen},
 title = {Xmonad},
 booktitle = {Haskell '07: Proceedings of the ACM SIGPLAN workshop on Haskell workshop},
 year = {2007},
 isbn = {978-1-59593-674-5},
 pages = {119--119},
 location = {Freiburg, Germany},
 doi = {http://doi.acm.org/10.1145/1291201.1291218},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@PhDThesis{StewartRob-2013,
  author  = {Robert Stewart},
  title   = {Reliable Massively Parallel Symbolic Computing: Fault Tolerance for a Distributed {Haskell}},
  school  = {Mathematical and Computer Sciences, Heriot-Watt University},
  address = {Edinburgh, Scotland},
  year    = {2013},
  month   = NOV,
  WKloc   = {doc/pap/BIB},
  ORIGURL = {http://www.macs.hw.ac.uk/~rs46/phd-thesis.html},
  abstract = {As the number of cores in manycore systems grows exponentially,
    the number of failures is also predicted to grow exponentially.
    Hence massively parallel computations must be able to tolerate faults.
    Moreover new approaches to language design and system architecture are needed
    to address the resilience of massively parallel heterogeneous architectures.

    Symbolic computation has underpinned key advances in Mathematics and Computer Science,
    for example in number theory, cryptography, and coding theory.
    Computer algebra software systems facilitate symbolic mathematics.
    Developing these at scale has its own distinctive set of challenges,
    as symbolic algorithms tend to employ complex irregular data and control structures.
    SymGridParII is a middleware for parallel symbolic computing
    on massively parallel High Performance Computing platforms.
    A key element of SymGridParII is a domain specific language (DSL)
    called Haskell Distributed Parallel Haskell (HdpH).
    It is explicitly designed for scalable distributed-memory parallelism,
    and employs work stealing to load balance dynamically generated irregular task sizes.

    To investigate providing scalable fault tolerant symbolic computatio
   n we design, implement and evaluate a reliable version of HdpH, HdpH-RS.
    Its reliable scheduler detects and handles faults,
    using task replication as a key recovery strategy.
    The scheduler supports load balancing with a fault tolerant work stealing protocol.
    The reliable scheduler is invoked with two fault tolerance primitives
    for implicit and explicit work placement,
    and 10 fault tolerant parallel skeletons
    that encapsulate common parallel programming patterns.
    The user is oblivious to many failures, they are instead handled by the scheduler.

    An operational semantics describes small-step reductions on states.
    A simple abstract machine for scheduling transitions and task evaluation is presented.
    It defines the semantics of supervised futures,
    and the transition rules for recovering tasks in the presence of failure.
    The transition rules are demonstrated with a fault-free execution,
    and three executions that recover from faults.

    The fault tolerant work stealing has been abstracted in to a Promela model.
    The SPIN model checker is used to exhaustively search
    the intersection of states in this automaton
    to validate a key resiliency property of the protocol.
    It asserts that an initially empty supervised future on the supervisor node
    will eventually be full in the presence of all possible combinations of failures.

    The performance of HdpH-RS is measured using five benchmarks.
    Supervised scheduling achieves a speedup of 757 with explicit task placement
    and 340 with lazy work stealing
    when executing Summatory Liouville up to 1400 cores of a HPC architecture.
    Moreover, supervision overheads are consistently low scaling up to 1400 cores.
    Low recovery overheads are observed in the presence of frequent failure
    when lazy on-demand work stealing is used.
    A Chaos Monkey mechanism has been developed
    for stress testing resiliency with random failure combinations.
    All unit tests pass in the presence of random failure, terminating with the expected results.}
}

@Article{Steyaert-Flajolet-1983,
  nutshell = {Linear time expected complexity of trivial tree pattern matching.},
  year = 1983,
  volume = 58,
  title = {Patterns and Pattern-Matching in Trees: An Analysis},
  pages = {19-58},
  journal = {Information and Control},
  author = {J.-M. Steyaert and P. Flajolet}
}

@Article{Stoller-CohenE-2006,
  author =       {Scott D. Stoller and Ernie CohenE},
  title =        {Optimistic Synchronization-Based State-Space Reduction},
  journal =      {Formal Methods in System Design},
  year =         2006,
  volume =    28,
  number =    3,
  pages =     {263--289},
  month =     MAY,
  DOI =      {10.1007/s10703-006-0003-4},
  URL =    {http://www.springerlink.com/content/4041241177322mn6/},
  WKloc = {doc/pap/BIB},
  keywords = {partial-order methods, reduction, model checking, omega algebra},
  abstract = {Reductions that aggregate
     fine-grained transitions into coarser transitions
     can significantly reduce the cost of automated verification,
     by reducing the size of the state space.
     We propose a reduction that can exploit common synchronization disciplines,
     such as the use of mutual exclusion for accesses to shared data structures.
     Exploiting them using traditional reduction theorems
     requires checking that the discipline is followed
     in the original (i.e., unreduced) system.
     That check can be prohibitively expensive.
     This paper presents a reduction that instead requires
     checking whether the discipline is followed in the reduced system.
     This check may be much cheaper,
     because the reachable state space is smaller.}
}

@Article{Stomp-2003,
  author = {Frank Stomp},
  title = {Correctness of Substring-Preprocessing in {Boyer-Moore}'s Pattern Matching Algorithm},
  journal = TCS,
  year = 2003,
  volume = 290,
  pages = {59--78},
  WKloc = {A-1463, doc/pap/BIB},
  annote = {reviewed for CR, July 2003

     \textbf{Idea:} derive the algorithm from its specification!}
}

@Article{Stone-1934,
  author = {Marshall H. Stone},
  title = {Boolean Algebras and their Application to Topology},
  journal = PRONAS,
  volume = 20,
  year = 1934,
  pages = {197--202},
  bibliographies = {RelMiCS}
}

@Article{Stone-1936,
  author = {Marshall H. Stone},
  title = {The Theory of Representations for Boolean Algebras},
  journal = TRAMS,
  volume = 40,
  year = 1936,
  pages = {37--111},
  bibliographies = {RelMiCS}
}

@Article{Stone-1937,
  author = {Marshall H. Stone},
  title = {Applications of the Theory of Boolean Rings to General Topology},
  journal = TRAMS,
  volume = 41,
  year = 1937,
  pages = {375--481},
  bibliographies = {RelMiCS}
}

@InProceedings{Stotzer-Leiss-2009,
  author = 	 {Eric J. Stotzer and Ernst L. Leiss},
  title = 	 {Modulo Scheduling without Overlapped Lifetimes},
  crossref =	 {LCTES2009},
  pages =	 {1--10},
  DOIURL = 	 {http://doi.acm.org/10.1145/1543136.1542454},
  bibliographies = {Coconut},
  abstract = {This paper describes complementary software- and hardware-based
     approaches for handling overlapping register lifetimes
     that occur in modulo scheduled loops.
     Modulo scheduling takes the N-instructions in a loop body
     and constructs an M-stage software pipeline.
     The length of each stage in the software pipeline
     is the Initiation Interval (II),
     which is the rate at which new loop iterations are started.
     An overlapped lifetime has a live range longer than the II,
     and as a consequence,
     the current iteration writes a new value to a register
     before a previous loop iteration has fin-ished using the old value.
     Hardware and software solutions for dealing with overlapped lifetimes
     have been proposed by re-searchers
     and also implemented in commercial products.
     These solutions include rotating register files, register queues,
     modulo variable expansion, and post-scheduling live range splitting.
     Each of these approaches has drawbacks for embedded systems
     such as an increase in silicon area, power consumption, and code size.

     Our approach, which is an improvement to the current solutions,
     prevents overlapped lifetimes
     through a combination of hardware and software techniques.
     The hardware element of our approach
     implements a register assignment latency that allows
     multiple in-flight writes to be pending to the same register.
     The software element of our approach uses dependence analysis and a
     constrained modulo scheduling algorithm to prevent overlapped lifetimes.
     We describe how to use these hardware and software techniques
     during modulo scheduling.
     Finally, we present the results
     of using our approach to compile embedded application code
     and present results
     in terms of modulo schedule quality and application performance.}
}

@Article{Stoutemyer-2011,
  author =       {David R. Stoutemyer},
  title =        {Ten Commandments for Good Default Expression Simplification},
  note =    {Special Issue in Honour of Keith Gedders on his 60th Birthday},
  journal =  JSYCO,
  pages =     {859--887},
  year =      2011,
  volume =    46,
  number =    7,
  bibliographies =    {Coconut}
}

@InCollection{Stoy-1994,
  author = {J. E. Stoy},
  title = {The Semantics of {Id}},
  crossref = {Roscoe-1994},
  pages = {379--404},
  chapter = 22,
  OPTnote = {},
  OPTannote = {}
}

@Book{Stoyan-1984,
  author = {Herbert Stoyan},
  title = {{Maschinen-unabh\"angige Code-Erzeugung als
		  semantikerhaltende beweisbare Programmtransformation}},
  publisher = {Springer-Verlag},
  year = 1984,
  volume = 91,
  series = {Informatik-Fachberichte},
  UniBwM = {INF460-N2096}
}

@PhDThesis{Stoye-1984,
  author = {William R. Stoye},
  title = {},
  school = {},
  year = 1984,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTnote = {},
  annote = {Daan Leijen" <daanleijen@xs4all.nl> 8 Feb 2003:

      A possible solution to this could be one-bit reference counting. With
      this technique, every pointer to an item is marked as either a unique
      pointer or a shared pointer. Normal GC techniques are used to garbage
      collect the heap but certain operations can use the uniqueness
      information to update-in-place. This kind of reference counting is
      cheaper since it involves bit-twiddling on the pointers directly
      instead of indirect fields. However, I think that in the end the
      price may be too high for the benefits obtained. It may be applicable
      however to interpreted systems where these register operations come
      relatively cheap.

      [...]

      The remarkable PhD. thesis of William R. Stoye says more about
      one-bit reference counts that he uses in his (hardware) SKI machine.
      Allthough fairly old (1984) it is still a very interesting thesis to
      read. It is especially fun to read how he already describes very
      clearly "why functional programming matters", even dwelling on the
      subject of functional operating systems.}
}

@MastersThesis{Strassburger-2000,
  author =  {Lutz Stra{\ss}burger},
  title =   {Rational Forest Languages and Sequential Forest Transducers},
  school =  {Technische Universit\"at Dresden},
  year =    2000,
  WKloc = {doc/pap/BIB},
  URL = {http://www.ps.uni-sb.de/~lutz/},
  summary = {I showed a Kleene theorem for forest automata, which
     are a generalization of tree automata. The advantage of using
     forests instead of trees is that the alphabet has not to be
     extended when showing that the rational languages are the same as
     the recognizable languages.}
}

@PhdThesis{Strecker-1999,
  author = 	 {Martin Strecker},
  title = 	 {Construction and Deduction in Type Theories},
  school = 	 {Universit{\"a}t Ulm},
  year = 	 1999,
  URL = {http://www.informatik.uni-ulm.de/ki/Strecker/phd.html},
  WKloc = {A-1573, doc/pap/BIB},
  abstract = { This dissertation is concerned with interactive
     proof construction and automated proof search in type theories,
     in particular the Calculus of Constructions and its subsystems.

     Type theories can be conceived as expressive logics which combine
     a functional programming language, strong typing and a
     higher-order logic. They are therefore a suitable formalism for
     specification and verification systems. However, due to their
     expressiveness, it is difficult to provide appropriate deductive
     support for type theories. This dissertation first examines
     general methods for proof construction in type theories and then
     explores how these methods can be refined to yield proof search
     procedures for specialized fragments of the language.

     Proof development in type theories usually requires the
     construction of a term having a given type in a given
     context. For the term to be constructed, a metavariable is
     introduced which is successively instantiated in the course of
     the proof. A naive use of metavariables leads to problems, such
     as non-commutativity of reduction and instantiation and the
     generation of ill-typed terms during reduction. For solving these
     problems, a calculus with explicit substitutions is introduced,
     and it is shown that this calculus preserves properties such as
     strong normalization and decidability of typing.

     In order to obtain a calculus appropriate for proof search, the
     usual natural deduction presentation of type theories is replaced
     by a sequent style presentation. It is shown that the calculus
     thus obtained is correct with respect to the original
     calculus. Completeness (proved with a cut-elimination argument)
     is shown for all predicative fragments of the lambda cube.

     The dissertation concludes with a discussion of some techniques
     that make proof search practically applicable, such as
     unification and pruning of the proof search space by exploiting
     impermutabilities of the sequent calculus.}
}

@InCollection{Strecker-Luther-vonHenke-1998,
  author = 	 {M. Strecker and M. Luther and F. von Henke},
  title = 	 {{Interactive and automated proof construction in type theory}},
  crossref =	 {Deduction1998},
  volume =	 {I: Foundations},
  chapter =	 {3: Interactive Theorem Proving}
}

@Misc{Street-1995,
  author = {Ross Howard Street},
  title = {Descent Theory},
  howpublished = {Oberwolfach, 17--23 September 1995},
  year = 1995,
  WKloc = {A-1257},
  abstract = {Higher descent theory, non-abelian cohomology, and
      higher-order category theory are all one subject which might be
      called \emph{post-modern algebra} (or even ``post-modern
      mathematics'' since geometry and algebra are handled equally well by
      higher categories).},
  annote = {Section 7 exists as separate paper \emph{Fusion operators and cocycloids in monoidal categories}.}
}

@InCollection{Street-1996,
  author = {Ross Howard Street},
  title = {Categorical Structures},
  booktitle = {Handbook of Algebra},
  volume = 1,
  editor = {M. Hazewinkel},
  publisher = {Elsevier Science},
  address = {Amsterdam},
  year = 1996,
  pages = {529--577}
}

@Article{Street-Day-1995,
  author = {Ross Howard Street and B. Day},
  title = {Kan Extensions along Promonoidal Functors},
  journal = {Theory and Applications of Categories},
  volume = 1,
  number = 4,
  year = 1995,
  pages = {72--78},
  dvi-url = {ftp://ftp.tac.mta.ca/pub/tac/volumes/1995/n4/v1n4.dvi},
  ps-url = {ftp://ftp.tac.mta.ca/pub/tac/volumes/1995/n4/v1n4.ps}
}

@Unpublished{Street-Day-19XX,
  author = {Ross Howard Street and B. J. Day},
  title = {Monoidal Bicategories and {H}opf Algebroids},
  note = {in preparation}
}

@Article{Street-Gordon-Power-1995,
  author = {Ross Howard Street and R. Gordon and A. J. Power},
  title = {Coherence for Tricategories},
  journal = {Memoirs of the American Math Society},
  volume = 117,
  year = 1995,
  number = 558
}

@Unpublished{Street-Joyal-1988,
  author = {Ross Howard Street and A. Joyal},
  title = {Planar Diagrams and Tensor Algebra},
  year = 1988,
  note = {???}
}

@Article{Street-Joyal-Verity-1996,
  author = {Ross Howard Street and A. Joyal and D. Verity},
  title = {Traced Monoidal Categories},
  journal = {Mathematical Proceedings of the Cambridge
                 Philosophical Society},
  volume = 119,
  number = 3,
  year = 1996,
  pages = {425--446},
  DOI = {10.1017/S0305004100074338},
  URL = {http://journals.cambridge.org/article_S0305004100074338},
  abstract = {Traced monoidal categories are introduced, a structure theorem is proved for them, and an example is provided where the structure theorem has application.}
}

@Article{Street-Walters-1973,
  author = {R. H. Street and R.F.C. Walters},
  title = {Comprehensive factorization of a functor},
  journal = BUAMS,
  year = 1973,
  pages = {936--941},
  volume = 79
}

@Article{Street-Walters-1978,
  author = {R.H. Street and R.F.C. Walters},
  title = {Yoneda structures on 2-categories},
  journal = JALG,
  year = 1978,
  pages = {350--379},
  volume = 50
}

@PhdThesis{Streicher-1993,
  author = 	 {Thomas Streicher},
  title = 	 {Investigations into intensional type theory},
  school = 	 {Ludwig Maximilian Universit{\"a}t},
  year = 	 1993,
  type =	 {Habilitation Thesis}
}

@InProceedings{Strobl-Minas-2009,
  title = {Implementing an Animated Lambda-Calculus},
  author = {Strobl, Torsten and Minas, Mark},
  url = {http://CEUR-WS.org/Vol-510/paper6.pdf},
  added-at = {2009-12-22T10:44:27.000+0100},
  bdsk-file-1 = {YnBsaXN0MDDUAQIDBAUGCQpYJHZlcnNpb25UJHRvcFkkYXJjaGl2ZXJYJG9iamVjdHMSAAGGoNEHCFRyb290gAFfEA9OU0tleWVkQXJjaGl2ZXKoCwwXGBkdJCVVJG51bGzTDQ4PEBEUViRjbGFzc1dOUy5rZXlzWk5TLm9iamVjdHOAB6ISE4ACgAOiFRaABIAGWWFsaWFzRGF0YVxyZWxhdGl2ZVBhdGjSDRobHFdOUy5kYXRhgAVPEQG8AAAAAAG8AAIAAAxNYWNpbnRvc2ggSEQAAAAAAAAAAAAAAAAAAADD8WucSCsAAACSOoMQVkxMMDkgKENFVVIpLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAJkcM8au4JMAAAAAAAAAAAACAAUAAAkgAAAAAAAAAAAAAAAAAAAABDIwMDkAEAAIAADD8V2MAAAAEQAIAADGrsRzAAAAAQAcAJI6gwAL+dsAC+tSAAvlIwAK1uUACqT4AAB62wACAEtNYWNpbnRvc2ggSEQ6VXNlcnM6bWluYXM6RWlnZW5lIERhdGVpZW46VGV4dDpQYXBlcjpQUzoyMDA5OlZMTDA5IChDRVVSKS5wZGYAAA4AIgAQAFYATABMADAAOQAgACgAQwBFAFUAUgApAC4AcABkAGYADwAaAAwATQBhAGMAaQBuAHQAbwBzAGgAIABIAEQAEgA+VXNlcnMvbWluYXMvRWlnZW5lIERhdGVpZW4vVGV4dC9QYXBlci9QUy8yMDA5L1ZMTDA5IChDRVVSKS5wZGYAEwABLwAAFQACAAz//wAA0h4fICFYJGNsYXNzZXNaJGNsYXNzbmFtZaMhIiNdTlNNdXRhYmxlRGF0YVZOU0RhdGFYTlNPYmplY3RfECkuLi8uLi9UZXh0L1BhcGVyL1BTLzIwMDkvVkxMMDkgKENFVVIpLnBkZtIeHyYnoicjXE5TRGljdGlvbmFyeQAIABEAGgAfACkAMgA3ADoAPwBBAFMAXABiAGkAcAB4AIMAhQCIAIoAjACPAJEAkwCdAKoArwC3ALkCeQJ+AocCkgKWAqQCqwK0AuAC5QLoAAAAAAAAAgEAAAAAAAAAKAAAAAAAAAAAAAAAAAAAAvU=},
  bdsk-url-1 = {http://www.youtube.com/watch?v=K-p3ddhwVrg},
  bdsk-url-2 = {http://sunsite.informatik.rwth-aachen.de/Publications/CEUR-WS/},
  biburl = {http://www.bibsonomy.org/bibtex/2c2c31ea735550fbd8d320773cba15715/minas},
  booktitle = {Workshop on Visual Languages and Logic, satellite of 2009 IEEE Symposium on Visual Languages and Human-Centric Computing, Corvallis, OR, USA, September 20, 2009},
  date-added = {2009-09-10 12:58:16 +0200},
  date-modified = {2009-12-22 10:43:14 +0100},
  interhash = {d287caa2d8c028d4a72bae366edc04c7},
  intrahash = {c2c31ea735550fbd8d320773cba15715},
  keywords = {2009 Animation ConferencePaper DiaGen GraGra myown},
  series = {CEUR Workshop Proceedings},
  timestamp = {2009-12-22T10:44:27.000+0100},
  volume = 510,
  year = 2009
}

@PhDThesis{Stroehlein-1970,
  author = {Thomas Str{\"o}hlein},
  title = {{Untersuchungen \"uber kombinatorische Spiele}},
  school = {Technische Univ.\null{} M\"unchen},
  year = 1970,
  type = Doct,
  bibliographies = {RelMiCS}
}

@Article{Strom-Yemini-1986,
  author = {R. E. Strom and S. Yemini},
  title = {Typestate: {A} Programming Language Concept for
                 Enhancing Software Reliability},
  journal = {IEEE Transactions on Software Engineering},
  year = 1986,
  volume = {SE-12},
  number = 1,
  month = jan,
  pages = {157--171},
  annote = {Typestates are interfaces that an object may assume,
                 depending on it's current state. 18 references},
  keywords = {Program analysis; program verification; security;
                 software reliability; type checking; typestate},
  abstract = {We introduce a new programming language concept called
                 typestate, which is a refinement of the concept of
                 type. Whereas the type of a data object determines the
                 set of operations ever permitted on the object,
                 typestate determines the subset of these operations
                 which is permitted in a particular context. Typestate
                 tracking is a program analysis technique which enhances
                 program reliability by detecting at compile-time
                 syntactically legal but semantically undefined
                 execution sequences. These include, for example,
                 reading a variable before it has been initialized,
                 dereferencing a pointer after the dynamic object has
                 been deallocated, etc. Typestate tracking detects
                 errors that cannot be detected by type checking or by
                 conventional static scope rules. Additionally,
                 typestate tracking makes it possible for compilers to
                 insert appropriate finalization of data at exception
                 points and on program termination, eliminating the need
                 to support finalization by means of either garbage
                 collection or unsafe deallocation operations such as
                 Pascal's dispose operation. By enforcing typestate
                 invariants at compile-time, it becomes practical to
                 implement a ``secure language''- that is, on in which
                 all successfully compiled program modules have fully
                 defined execution-time effects, and the only effects of
                 program errors are incorrect output values. This paper
                 defines typestate, gives examples of its application,
                 and shows how typestate checking may be embedded into a
                 compiler. We discuss the consequences of typestate
                 checking for software reliability and software
                 structure, and conclude with a discussion of our
                 experience using a high-level language incorporating
                 typestate checking.},
  bibdate = {Fri Aug 21 14:26:38 1987},
  owner = {manning}
}

@InProceedings{Struth-2001,
  author = 	 {Georg Struth},
  title = 	 {Calculating Church-Rosser Proofs in {Kleene} Algebra},
  crossref =  {RelMiCS2001},
  pages =	 {276--290},
  WKloc = 	 {doc/pap/BIB},
  abstract = {We present simple calculational proofs of
     Church-Rosser theorems for equational theories, quasiorderings
     and non-symmetric transitive relations in Kleene algebra. We also
     calculate the abstract part of two standard proofs of
     Church-Rosser theorems in the $\lambda$-calculus and further
     central statements of rewriting. Since proofs avoid deduction, in
     particular induction, and large parts are amenable to automata,
     the approach is suited for mechanization. Since proofs
     algebraically reconstruct precisely the usual diagrams, they are
     also very natural for a human. In all considerations, Kleene
     algebra is an excellent means of abstraction.},
  bibliographies = {RelMiCS}
}

@InProceedings{Struth-2003,
  author = {Georg Struth},
  title = {A Calculus for Set-Based Program Development},
  OPTcrossref = {},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = {A-1480, doc/pap/BIB/Struth-2003_latom.pdf (draft?)},
  abstract = {We propose an algebraic calculus for set-based program
     development. First, we reconstruct a fragment of set theory via atomic
     distributive lattices (ADL). Semantically, ADL extends boolean
     reasoning about sets by element-wise reasoning; it avoids presupposing
     a universal set. Operationally, ADL yields abstract, concise, elegant
     proofs from few elementary principles. Second, we develop a focused
     automated proof-search procedure for ADL with simple deduction and
     powerful reduction and simplification rules. Proof-search is guided by
     rewriting techniques.  The procedure decides several subclasses. Main
     application is the proof-support for formal methods like B or Z.}
}

@Article{Stubbe-2005,
  author =       {Isar Stubbe},
  title =        {Categorical Structures Enriched in a Quantaloid: Categories, Distributors and Fuctors},
  journal =      TAC,
  year =         {2005},
  volume =    14,
  number =    1,
  pages =     {1--45},
  DirectURL = {http://tac.mta.ca/tac/volumes/14/1/14-01abs.html},
  WKloc =        {doc/pap/BIB},
  annote =    {contains definitions for lax cocones, lax colimits.}
}

@InProceedings{Stuckey-Sulzmann-Wazny-2006,
  author =       {Peter J. Stuckey and Martin Sulzmann and Jeremy Wazny},
  title =        {Type Processing by Constraint Reasoning},
  crossref =  {APLAS2006},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = {doc/pap/BIB},
  OPTnote =      {},
  annote =    {Chameleon}
}

@InProceedings{Subramanian-1993,
  author = {S. Subramanian},
  title = {A Fully Dynamic Data Structure for Reachability in
		  Planar Digraphs},
  crossref = {ESA93},
  pages = {372--383}
}

@InProceedings{Suedholt-1994,
  author = {M. S{\"u}dholt},
  title = {Data Distribution Algebras - {A} Formal Basis for
		  Programming Using Skeletons},
  crossref = {PROCOMET94},
  pages = {38--57},
  keywords = {Functional Parallel Programming; Data Distribution
		  Algebras; Skeleton-Based Programming Methodology;
		  Mathematics of Program Derivation}
}

@InCollection{Sufrin-deMoor-1999,
  author = {Bernard Sufrin and de Moor, Oege},
  title = {Modeless structure editing},
  editor = {A. W. Roscoe and J.C.P. Woodcock},
  booktitle = {Proceedings of the {Oxford-Microsoft} symposium in
      Celebration of the work of {Tony Hoare}, {September} 13--15, 1999},
  OPTcrossref = {},
  OPTkey = {},
  OPTpages = {},
  OPTpublisher = {},
  year = 1999,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTtype = {},
  OPTchapter = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  WKloc = {A-0983},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/papers/edit.ps.gz},
  bibliographies = {EdComb}
}

@InCollection{Sufrin-deMoor-Sage-2000,
  author = {Bernard Sufrin and de Moor, Oege and Meurig Sage},
  title = {An extensible structure editor},
  year = 2000,
  note = {Available via URL: \url{http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/}}
}

@InProceedings{Sulzmann-2002,
  author =       {Martin Sulzmann},
  title =        {An Overview of the Chameleon System},
  crossref =  {APLAS2002},
  pages =     {16--30}
}

@TechReport{Sun-1992,
  author = {Yong Sun},
  title = {A Framework for Binding Operators},
  year = 1992,
  number = {92-91},
  institution = {Edinburgh University, Department of Computer Science},
  annote = {tubibmue, {--- PLGnotes ---}}
}

@Article{Sun-Walters-1993,
  author = {Sun, Shu-Hao and R.F.C. Walters},
  title = {Representaions of modules and cauchy completeness},
  journal = CTGD,
  year = 1993,
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  month = MAY,
  note = {to appear}
}

@Article{SunR-1992,
  author = {Sun, Ron},
  title = {On Variable Binding in Connectionist Networks},
  OPTcrossref = {},
  OPTkey = {},
  journal = {Connection science},
  year = 1992,
  volume = 4,
  number = 2,
  pages = 93,
  OPTmonth = {},
  OPTnote = {},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTwkloc = {},
  OPTabstract = {},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@InCollection{Suppes-1973a,
  author = {Patrick Suppes},
  title = {Semantics of context-free fragments of natural languages},
  booktitle = {Approaches to Natural Languages},
  year = 1973,
  editor = {Jaakko Hintikka and Julius M. E. Moravcsik and Patrick Suppes},
  publisher = Reidel,
  address = {Dordrecht},
  pages = {370--394},
  bibliographies = {RelMiCS}
}

@InCollection{Suppes-1973b,
  author = {Patrick Suppes},
  title = {Facts and Fantasies of Education},
  booktitle = {Changing Education: Alternatives from Educational Research},
  publisher = Prentice,
  year = 1973,
  editor = {M. C. Wittrock},
  address = {Englewood Cliffs, N.J.},
  pages = {6--45},
  bibliographies = {RelMiCS}
}

@Article{Suppes-1976,
  author = {Patrick Suppes},
  title = {Elimination of Quantifiers in the Semantics of Natural
		Languages by the use of Extended Relation Algebras},
  journal = REVPHIL,
  volume = 30,
  year = 1976,
  pages = {243--259},
  bibliographies = {RelMiCS}
}

@InCollection{Suppes-1979a,
  author = {Patrick Suppes},
  title = {Variable-free semantics for negations with prosodic variation},
  booktitle = {Essays in Honor of Jaakko Hintikka},
  year = 1979,
  editor = {Risto Hilpinen and I. Niiniluoto and M. P. Hintikka},
  publisher = Reidel,
  address = {Dordrecht},
  pages = {49--59},
  bibliographies = {RelMiCS}
}

@Article{Suppes-1979b,
  year = 1979,
  volume = 38,
  title = {Logical inference in English: A preliminary analysis},
  pages = {375--391},
  journal = STUDLOG,
  author = {Patrick Suppes},
  bibliographies = {RelMiCS}
}

@InCollection{Suppes-1982,
  author = {Patrick Suppes},
  title = {Variable-free semantics with remarks on procedural extensions},
  booktitle = {Language, Mind and Brain},
  year = 1982,
  editor = {T. W. Simon and R. J. Scholes},
  volume = {},
  publisher = {Lawrence Erlbaum},
  address = {Hillsdale NJ},
  pages = {21--34},
  bibliographies = {RelMiCS}
}

@Book{Suppes-1991,
  author = {Patrick Suppes},
  year = 1991,
  title = {Language for Humans and Robots},
  address = {Oxford},
  publisher = Blackwell,
  bibliographies = {RelMiCS}
}

@InCollection{Suppes-Macken-1978,
  author = {Patrick Suppes and Elizabeth Macken},
  title = {Steps toward a variable-free semantics of attributive
      adjectives, possessives, and intensifying adverbs},
  booktitle = {Children's Language},
  publisher = {Gardner Press},
  year = 1978,
  editor = {K. E. Nelson},
  volume = 1,
  address = {New York},
  pages = {81--115},
  bibliographies = {RelMiCS}
}

@Article{Suppes-Zanotti-1977,
  year = 1977,
  volume = 36,
  title = {On using random relations to generate upper and lower probabilities},
  pages = {427--440},
  journal = Synthese,
  author = {Patrick Suppes and Mario Zanotti},
  bibliographies = {RelMiCS}
}

@Article{Sutherland-Ebergen-2002,
  author = {Ivan E. Sutherland and Jo Ebergen},
  title = {Computers without Clocks},
  journal = {Scientific American},
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  month = JUL # 15,
  WKloc = {A-1369},
  Obibliographies = {SE3B}
}

@TechReport{Suzuki-Katayama-Schlichting-1996,
  author = {Masato Suzuki and Takuya Katayama and Richard
                 Schlichting},
  institution = {Arizona University, Computer Science},
  title = {{FTAG}: {A} Functional and Attribute Based Model for
                 Writing Fault-Tolerant Software},
  year = 1996,
  number = {TR96-05},
  URL = {http://www.cs.arizona.edu/research/reports.html},
  WKloc = {B-0051}
}

@Misc{Swadi-Appel-2001,
  author = 	 {Kedar N. Swadi and Andrew W. Appel},
  title = 	 {Typed Machine Language and its Semantics},
  year =  {2001},
  month = JUL,
  URL = {http://www.cs.princeton.edu/},
  WKloc = {A-1596, doc/pap/BIB},
  bibliographies = {OPG, Coconut}
}

@PhDThesis{Swarup-1992,
  author = {Swarup, Vipin},
  title = {Type theoretic properties of assignments},
  school = {University  of  Illinois at Urbana-Champaign},
  year = 1992,
  OPTcrossref = {},
  OPTkey = {},
  OPTaddress = {Urbana, Illinois},
  OPTmonth = {},
  OPTtype = {},
  OPTnote = {puplished as Department of Computer Science
		  Technical  Report Nr. 1777},
  OPTauthorsaddress = {},
  OPTunibwm = {},
  OPTfile = {~kahl/doc/pap/uiuc/UIUCDCS-R-92-1777.ps.gz},
  OPTwkloc = {B-0019},
  OPTabstract = {This  thesis  is  concerned  with   extending   the
                    correspondence   between  intuitionistic  logic  and
                    functional programming to  include  assignments  and
                    dynamic  data.   We  propose a theoretical framework
                    for adding these imperative features  to  functional
                    languages    without    violating   their   semantic
                    properties.   We  also   describe   a   constructive
                    programming  logic  that embodies the principles for
                    reasoning about the extended language.

                    We  present  an  abstract  formal  language,  called
                    Imperative  Lambda  Calculus (ILC), that extends the
                    typed lambda calculus  with  imperative  programming
                    features,  namely  references  and  assignments. The
                    language shares with typed lambda calculus important
                    properties  such  as  the Church-Rosser property and
                    strong normalization.  Thus,  programs  produce  the
                    same  arrays,  linked lists, trees, and graphs to be
                    constructed and used.  Shared values may be  updated
                    destructively  rather than by copying.  This permits
                    pure  functional   languages   to   have   efficient
                    implementations  of  problems  such  as  topological
                    sort, graph reduction, and unification.

                    We describe the logical symmetries that underlie ILC
                    by   exhibiting   a   constructive   logic,   called
                    Observation Type Theory (OTT), for which  ILC  forms
                    the  language  of  constructions.   Central  to this
                    formulation is the view that references play a  role
                    similar  to  that  of  variables.  References can be
                    used to range over values and can be instantiated to
                    specific  values.   Thus,  we  obtain  a new form of
                    universal  quantification   that   uses   references
                    instead  of  variables.   The  term forms of ILC are
                    then  obtained  as   the   constructions   for   the
                    introduction  and  elimination  of  this quantifier.
                    While references duplicate the  role  of  variables,
                    they  also  have  important differences.  References
                    are {\em semantic} values whereas variables are syntactic
                    entities;  further, references are {\em reusable}.  These
                    differences allow us to use  references  in  a  more
                    flexible   fashion,   leading   to   efficiency   in
                    constructions and algorithms.  Finally, we  describe
                    a  higher-order  type  theory, namely the Nuprl type
                    theory, and illustrate how its inductive  types  can
                    be used to define well-founded orderings.  These can
                    then be used to construct recursive programs.},
  OPTkeywords = {},
  OPTcontents = {},
  OPTannote = {}
}

@Misc{Swierstra-199Xa,
  author = {S. D. Swierstra},
  title = {Micro Manual for {UU\_AG}},
  year = {199X},
  WKloc = {A-0513}
}

@Misc{Swierstra-Azero-1996,
  author = {S. Doaitse Swierstra and Pablo R. Azero},
  title = {Attribute Grammars in the Functional Style},
  year = 1998,
  WKloc = {A-0515},
  annote = {partially garbled PostScript layout}
}

@InProceedings{Swierstra-Azero-1999,
  author = {Swierstra, S.D. and Azero Alcocer, P.R.},
  title = {Fast, Error Correcting Parser Combinators: A Short Tutorial},
  booktitle = {{SOFSEM'99,  Theory and Practice of Informatics, 26th Seminar on Current Trends in Theory and Practice of Informatics, 1999,
     November}},
  editor = {Pavelka, Jan and Tel, Gerard and Bartosek, Miroslav},
  AuthorURL = {http://www.staff.science.uu.nl/~swier101/Papers/1999/SofSem99.pdf},
  pages = {111--129},
  series = LNCS,
  volume = 1725,
  year = 1999,
  WKloc = {A-1011}
}

@InProceedings{Swierstra-Azero-Saraiva-1999,
  author = {Swierstra, S. Doaitse and Azero Alcocer, Pablo R. and Jo{\~a}o Saraiva},
  title = {Designing and Implementing Combinator Languages},
  DOI = {10.1007/10704973_4},
  DOIURL = {http://dx.doi.org/10.1007/10704973_4},
  SpringerURL = {http://link.springer.com/chapter/10.1007%2F10704973_4},
  pages = {150--206},
  year = 1998,
  WKloc = {A-1223, doc/pap/BIB},
  bibliographies = {FP},
  abstract = {Ever since the Computer Science community has discovered
    the concept of a programming language there has been a continuous quest
    for the ideal, all-encompassing programming language;
    despite this we have been overwhelmed by an everlasting flow
    of all kinds of special purpose programming languages.
    Attempts to bridge this gap between a single language and infinitely many
    caused research into so-called extensible programming languages.},
  OPTcrossref = {AFP1998_Braga},
  isbn={978-3-540-66241-9},
  booktitle = {Advanced Functional Programming},
  LNCSbooktitle = {AFP 1998},
  volume = 1608,
  series = LNCS,
  editor = {Swierstra, S. Doaitse and Oliveira, Jos{\'e} N. and Henriques, Pedro R.},
  publisher = Springer,
}

@InProceedings{Swierstra-Duponcheel-1996,
  author = {S. Doaitse Swierstra and Luc Duponcheel},
  title = {Deterministic, Error-Correcting Combinator Parsers},
  pages = {184--207},
  crossref = {AFP1996},
  WKloc = {A-0692},
  bibliographies = {FP, EdComb}
}

@InProceedings{SwierstraW-Altenkirch-2007,
  author =	 {Wouter Swierstra and Thorsten Altenkirch},
  title =	 {Beauty in the Beast},
  crossref =  {Haskell2007},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Synder-Lynch-1990,
  abstract = {In this extended abstract of a full paper we present
		  an inference system for Horn clause logic with
		  equality which is complete not only refutationally
		  but also with respect to the answer substitutions
		  returned, fithout using functional reflexitivity
		  axioms or paramodulating at variable positions. It
		  is goal directed in the sense that---as in SLD
		  resolution---inference rules are only applied to the
		  goal statement. We present the inference system
		  (which subsumes those used for SLD-resolution and
		  for general E-unification), prove its soundness and
		  completeness, and then suggest how this approach
		  formas the appropriate foundation for the study of
		  inference systems (like logic programming
		  interpreters) which return answer substitutions.},
  title = {An Inference System for Horn Clause Logic with Equality},
  pages = {454--461},
  crossref = {CTRS1990},
  author = {Wayne Synder and Christopher Lynch}
}

@Book{Syropoulos-Tsolomitis-Sofroniou-2003,
  author =	 {Syropoulos, Apostolos and Tsolomitis, Antonis and Sofroniou, Nick},
  title = 	 {Digital Typography Using {\LaTeX}},
  publisher = 	 Springer,
  year = 	 2003,
  ISBN = 	 {0-387-95217-9},
  WKloc = 	 {owned, \lent{Wu Jun}}
}

@Article{Szigeti-1983,
  author =       {Jen{\"o} Szigeti},
  title =        {On limits and colimits in the {Kleisli} category},
  journal =      CTGD,
  year =         1983,
  volume =    24,
  number =    4,
  pages =     {381--391},
  URL = {http://www.numdam.org/item?id=CTGDC_1983__24_4_381_0}
}

@Misc{Taentzer-199X,
  author = {Gabriele Taentzer},
  title = {Towards Synchronous and Asynchronous Graph Transformations},
  year = {199?},
  WKloc = {A-0663}
}

@Misc{Taentzer-199Y,
  author = {Gabriele Taentzer},
  title = {Distributed Graphs and Graph Transformation},
  year = {199?},
  WKloc = {A-0664}
}

@InProceedings{Taentzer-2002,
  author = 	 {Gabriele Taentzer},
  title = 	 {A Visual Modeling Framework for Distributed Object Computing},
  booktitle =	 {Formal methods for open object-based distributed systems},
  year =	 2002,
  editor =	 {V, B. Jacobs and A. Rensink},
  publisher =	 {Kluwer Academic Publishers},
  abstract = {Distributed object computing is a computing paradigm
     that allows objects to be distributed over a heterogeneous
     network. Infrastructures help to develop distributed object
     applications by offering necessary services for distributed
     computing. Having a comprehensive infrastructure to hand, the
     development of complex distributed object systems is feasible in
     principle. Flexibly evolving architectures as well as highly
     dynamic distributed object structures are key requirements for
     nowadays distributed solutions. They can hardly be well designed
     on this level of programming, due to their complexity. A visual
     modeling framework is presented which offers a more abstract and
     intuitive approach to the relevant aspects of a distributed
     object system. In this framework, network and object structures
     as well as their evolution are visualized in a diagrammatic
     style, e.g. in UML notation. Semantically, this approach relies
     on graphs and their transformation, i.e. it has a precise
     background useful for further reasoning.}
}

@InProceedings{Taentzer-Beyer-1993,
  author = {Gabriele Taentzer and Martin Beyer},
  title = {Amalgamated Graph Transformations and Their Use for
		  Specifying {AGG} --- an Algebraic Graph Grammar System},
  crossref = {GTCS93},
  pages = {380--394},
  abstract = {The {\sc Agg}-system is a prototype implementation
		  of the algebraic approach to graph
		  transformation. It consists of a flexible graph
		  editor and a transformation component. The editor
		  allows the graphical representation of production
		  rules, occurrences and transformation results. The
		  transformation component performs direct
		  transformation steps for user-selected production
		  rules and occurrences.

                  First steps toward a graph specification of an
		  abstract version of the {\sc Agg}-system are
		  possible by using amalgamated graph
		  transformations. {\sc Agg}-states are modelled by
		  graphs whereas {\sc Agg}-operations are described by
		  amalgamated graph transformations combining parallel
		  and sequential rewrite of graphs.},
  WKloc = {A-0299}
}

@InCollection{Taentzer-Ermel-Rudolf-1999,
  author = {Claudia Ermel and Michael Rudolf and Gabriele Taentzer},
  title = {The {AGG} Approach: Language and Environment},
  crossref = {HBGraTraII},
  pages = {551--603},
  chapter = 14,
  abstract = {The Attributed Graph Grammar system AGG provides
    a visual programming environment which is based on graph transformation
    and Java. The idea is to program the topmost object structures of an
    application as well as its dynamics visually. For this purpose,
    these structures are designed as graphs which may be attributed by
    Java objects and further arbitrary data. The application's behavior
    is described by graph rules using an if-then-programming style.

    Application of a graph rule changes the structure graph.
    Control flow is not programmed at all, but implicitly handled
    by the dependency of rule applications. Basic data types as well as
    object classes already available in class libraries may be used to
    program the application "in the small". Moreover, new classes may be
    included. This part of the application is done textually using the
    modern object-oriented programming language Java. The visually and the
    textually programmed application parts are combined by attributing the
    structure graphs by attribute values. These may be data as well as
    object references.

    The graph rules may be attributed by Java expressions which are
    evaluated during rule applications. It's up to the programmer's
    decision how much of the application is realized visually and how much
    textually. Combining visual and textual programming in this way,
    programming should become easier because of visual concepts.
    On the other side, the wheel is not reinvented again, since
    well elaborated programming concepts, tools and class libraries
    mirroring the state of the art of programming can be used.
    The tool environment provides graphical editors for graphs
    and rules as well as a textual editor for Java expressions
    integrated into the visual editors. Moreover, visual interpretation
    and debugging of AGG programs is supported.}
}

@Book{Taha-1975,
  author =	 {Hamdy A. Taha},
  title = 	 {Integer Programming: Theory, Applications, and Computations},
  publisher = 	 {Academic Press},
  year = 	 1975,
  series = 	 {Operations Research and Industrial Engineering},
  address = 	 {New York},
  McMaster = 	 {T 57.7 .T33 (Thode and Innis)}
}

@Misc{Takahashi-Akama-Hirokawa-199X,
  author = {Masako Takahashi and Yohji Akama and Sachio Hirokawa},
  title = {Normal Proofs and Their Grammar},
  year = {199?},
  WKloc = {A-0728}
}

@Misc{Takahashi-Hagiya-1998,
  author = {Koichi Takahashi and Masami Hagiya},
  title = {Proving as Editing {HOL} Tactics},
  year = 1998,
  note = {UITP98},
  URL = {http://www.etl.go.jp/~takahasi/papers/uitp98.ps},
  WKloc = {A-0926},
  bibliographies = {ETL}
}

@InProceedings{Takai-2004,
  author = 	 {Toshinori Takai},
  title = 	 {A Verification Technique Using Term Rewriting Systems and Abstract Interpretation},
  OPTcrossref =  {RTA2004?},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1551}
}

@InProceedings{Takai-Ohsaki-,
  author = 	 {Toshinori Takai and Hitoshi Ohsaki},
  title = 	 {{\textsf{ACTAS}}: Associative and Commutative Tree Automata Simulator},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1595},
  OPTannote = 	 {}
}

@Article{Takamizawa-Nishizeki-Saito-1982,
  author = {K. Takamizawa and T. Nishizeki and N. Saito},
  title = {Linear-Time Computability Problems on Series-Parallel Graphs},
  journal = JACM,
  volume = 29,
  number = 3,
  pages = {623--641},
  year = 1982
}

@Article{Takano-Hu-Takeichi-1998,
  author = {Akihiko Takano and Zhenjiang Hu and Masato Takeichi},
  title = {Program Transformation in Calculational Form},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 7},
  WKloc = {A-0902, 27--30},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Talcott-1990,
  author = {Carolyn Talcott},
  title = {A Theory for Program and Data Type Specification},
  crossref = {DISCO90},
  pages = {91--100},
  annote = {see also \cite{Talcott-1992}}
}

@Article{Talcott-1992,
  author = {Carolyn Talcott},
  title = {A Theory for Program and Data Type Specification},
  journal = {Theoretical Computer Science},
  volume = 104,
  year = 1992
}

@Article{Talcott-1993,
  author = {Talcott, Carolyn L.},
  title = {A Theory of Binding Structures and  Applications to
		  Rewriting},
  journal = {Theoretical Computer Science},
  year = 1993,
  volume = 112,
  pages = {68--81},
  WKloc = {B-0020},
  filename = {91amast-tcs.dvi},
  DIRECTORY = {~/doc/pap/stanford/MT},
  abstract = {?},
  note = {Short version in: Second International Conference on Algebraic
      Methodology and Software Technology, AMAST91}
}

@Article{Tamassia-1996,
 author = {Tamassia, Roberto},
 title = {Data structures},
 journal = {ACM Comput.\null{} Surv.},
 issue_date = {March 1996},
 volume = {28},
 number = {1},
 month = mar,
 year = {1996},
 issn = {0360-0300},
 pages = {23--26},
 numpages = {4},
 DOIURL = {http://doi.acm.org/10.1145/234313.234323},
 DOI = {10.1145/234313.234323},
 acmid = {234323},
 publisher = {ACM},
 address = {New York, NY, USA},
 WKloc = {doc/pap/BIB}
}

@Misc{Tammet-1993,
  author = {Tanel Tammet},
  title = {Proof Strategies in Linear Logic},
  howpublished = {ftp://ftp.cs.chalmers.se/pub/papers?},
  year = 1993,
  note = {accepted in 1993 by the ``Journal of Automated Reasoning''},
  WKloc = {A-0381}
}

@InProceedings{TanGang-Appel-2001,
  author = 	 {Gang Tan and Andrew W. Appel},
  title = 	 {Semantics of Machine Instructions at Multiple Levels of Abstraction},
  crossref =  {LICS16},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {Short paper},
  URL = 	 {http://www.cs.princeton.edu/~gtan/},
  WKloc = 	 {A-1597, doc/pap/BIB},
  bibliographies = {OPG}
}

@TechReport{TanGang-Appel-2005,
  author = 	 {Gang Tan
  URL = 	 {http://www.cs.princeton.edu/~gtan/}, and Andrew Appel},
  title = 	 {A Compositional Logic for Control Flow (Extended Version)},
  institution =  {Princeton University},
  year = 	 {2005},
  OPTkey = 	 {},
  month = 	 JAN,
  OPTnote = 	 {},
  OPTannote = 	 {},
  URL = 	 {http://www.cs.princeton.edu/~gtan/},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {OPG}
}

@InProceedings{TanGang-Appel-Swadi-WuDinghao-2004,
  author = {Gang Tan and Andrew Appel and Kedar Swadi and Dinghao Wu},
  title = {Construction of a Semantic Model for a Typed Assembly Language},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {Fifth International Conference on Verification, Model Checking and Abstract Interpretation {(VMCAI'04), Venice, Italy}},
  OPTpages = 	 {},
  year = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  URL = 	 {http://www.cs.princeton.edu/~gtan/},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {OPG}
}

@PhDThesis{Tarditi-1996,
  author = {David Tarditi},
  title = {Design and Implementation of Code Optimizations for a Type-Directed Compiler for {Standard-ML}},
  school = {Carnegie Mellon University},
  year = 1996,
  OPTkey = {},
  OPTtype = {},
  OPTaddress = {},
  OPTmonth = DEC,
  OPTnote = {},
  WKloc = {B-0085}
}

@Article{Tarski-1938,
  year = 1938,
  volume = {?},
  number = {?},
  title = {?},
  pages = {?},
  journal = {?},
  author = {Alfred Tarski},
  note = {algebraic and topological framework for Intuitionistic Logic and Modal Systems},
  annote = {Heyting lattice, closure algebra, derivative algebra,
             see Leo Esakia, doc/pap/BIB/Tarski-centenary},
  OPTbibliographies = {RelMiCS}
}

@Article{Tarski-1941,
  year = 1941,
  volume = 6,
  number = 3,
  title = {On the Calculus of Relations},
  pages = {73--89},
  journal = JSYLO,
  author = {Alfred Tarski},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1952,
  author = {Alfred Tarski},
  title = {On Representable Relation Algebras},
  journal = BUAMS,
  volume = 58,
  year = 1952,
  pages = 172,
  note = {Abstract 145},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1953,
  author = {Alfred Tarski},
  title = {Some Metalogical Results Concerning the Calculus of Relations},
  journal = JSYLO,
  volume = 18,
  year = 1953,
  pages = {188--189},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1953a,
  author = {Alfred Tarski},
  title = {A Formalization of Set Theory without Variables},
  journal = JSYLO,
  volume = 18,
  year = 1953,
  pages = 189,
  bibliographies = {RelMiCS}
}

@Article{Tarski-1953b,
  author = {Alfred Tarski},
  title = {An Undecidable System of Sentential Calculus},
  journal = JSYLO,
  volume = 18,
  year = 1953,
  pages = 189,
  bibliographies = {RelMiCS}
}

@Article{Tarski-1954,
  author = {Alfred Tarski},
  title = {A General Theorem Concerning the Reduction of Primitive Notions},
  journal = JSYLO,
  volume = 19,
  year = 1954,
  bibliographies = {RelMiCS}
}

@Article{Tarski-1954a,
  author = {Alfred Tarski},
  title = {On Equationally Complete Rings and Relation Algebras},
  journal = BUAMS,
  volume = 60,
  year = 1954,
  note = {Abstract 202},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1954b,
  author = {Alfred Tarski},
  title = {On the Reduction of the Number of Generators in Relation Rings},
  journal = JSYLO,
  volume = 19,
  year = 1954,
  pages = {158--159},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1955,
  author = {Alfred Tarski},
  title = {Contributions to the Theory of Models, III},
  journal = INDAG,
  volume = 17,
  year = 1955,
  pages = {56--64},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1956,
  author = {Alfred Tarski},
  title = {Equationally Complete Rings and Relation Algebras},
  journal = INDAG,
  volume = 18,
  year = 1956,
  pages = {39--46},
  bibliographies = {RelMiCS}
}

@Book{Tarski-1956a,
  author = {Alfred Tarski},
  title = {Ordinal Algebras},
  note = {with appendices {\it Some additional theorems on ordinal
      algebras} by Chen-Chung Chang and {\it A unique decomposition theorm
      for relational addition} by Bjarni J\'onsson},
  year = 1956,
  pages = 133,
  publisher = NoHo,
  address = {Amsterdam},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1965,
  author = {Alfred Tarski},
  title = {A Simplified Formalization of Predicate Logic with Identity},
  journal = ARMAT,
  volume = 7,
  year = 1965,
  pages = {61--79},
  bibliographies = {RelMiCS}
}

@Article{Tarski-1966,
  author = {Alfred Tarski},
  title = {On Direct Products of Boolean Algebras
		with Additional Operations},
  journal = NOTIC,
  volume = 13,
  year = 1966,
  pages = {728--729},
  note = {Abstract 66T-457},
  bibliographies = {RelMiCS}
}

@Book{Tarski-Givant-1987,
  author = {Alfred Tarski and Steven Givant},
  title = {A Formalization of Set Theory without Variables},
  series = Colloq,
  volume = 41,
  publisher = AMS,
  address = {Providence},
  year = 1987,
  pages = {xxi+318},
  bibliographies = {RelMiCS},
  UniBwM = {Mag R14970: HB Schmidt}
}

@InProceedings{Tatsuta-1991,
  title = {Monotone Recursive Definition of Predicates and Its
		  Realizability Interpretation},
  author = {Makoto Tatsuta},
  pages = {38--52},
  crossref = {TACS1991},
  abstract = {The main aim of the paper is to construct a logic by which
		  we can formalize properties of programs.  Inductive
		  definition or recursive definition plays a very
		  important role for this purpose. Inductive
		  definition has been studied for untyped theories,
		  predicative typed theories and impredicative typed
		  theories.  Monotone recursive definition in an
		  untyped theory is studied in this paper. The main
		  point is realizability interpretation of monotone
		  recursive definition.

                  Untyped predicative theory ${\bf TID_0}$ and ${\bf
		  TID_1}$ are presented, which have monotone recursive
		  definition of predicates. ${\bf TID_1}$ has full
		  monotone recursive definition and ${\bf TID_0}$ has
		  only restricted monotone recursive definition. {\bf
		  q}-realizability interpretation of ${\bf TID_0}$ and
		  ${\bf TID_1}$ is defined.  It is proved that the
		  realizability interpretation of ${\bf TID_0}$ is
		  sound and that the realizability interpretation of
		  ${\bf TID_1}$ is not sound, though ${\bf TID_1}$ and
		  its interpretation seem very natural.}
}

@Article{Taylor-1998,
  author = {Charles S. Taylor},
  title = {{Holzwege} and {Feldwege} in Cyberwald: The Multimedia Philosophy Lecture},
  journal = {EJournal},
  year = 1998,
  volume = 8,
  number = 1,
  month = APR,
  WKloc = {A-0652}
}

@PhDThesis{Tchier-1996,
  author = {Fairouz Tchier},
  title = {S\'emantiques relationelles d\'emoniaques et
                  v\'erification de boucles non d\'eterministes},
  year = 1996,
  month = AUG,
  school = {D\'epartment de math\'ematiques et de statistique,
                  Facult\'e des sciences et de g\'enie, Universit\'e
                  Laval, Qu\'ebec},
  WKloc = {B-0042},
  annote = {Jules Desharnais},
  bibliographies = {RelMiCS}
}

@InProceedings{Tchier-2000,
  author = {Fairouz Tchier},
  title = {Demonic Relational Semantics of Compound Diagrams},
  crossref = {RelMiCS2000-M},
  pages = {117--140},
  bibliographies = {RelMiCS, RelMiCS5M},
  abstract = {We consider that nondeterministic programs behave
    as badly as they can and loop forever whenever they have the
    possibility to do so. This is the demonic approach to the
    semantics of nondeterministic programs. In order to obtain
    definitions which are flexible enough to allow simple algebraic
    manipulations, a unified description of flow control and single steps
    of a program is given. This is achieved by using the notion of
    relational diagram. We show how the notion of relational diagram,
    introduced by Schmidt, can be used to give a single demonic
    definition for a wide range of programming constructs. Our main result
    is Theorem 18, where we show that the input-output relation of a
    compound diagram is equal to that of the diagram in which each
    sub-diagram has been replaced by its input-output relation. This
    process is repeated until we obtain elementary diagrams to which
    we apply the results given in previous work.}
}

@InProceedings{Tchier-Desharnais-1995,
  author = {Fairouz Tchier and Jules Desharnais},
  title = {Generalization of a Theorem of {Mills}},
  booktitle = {$10^{th}$ Internat.\null{} Sympos.\null{} on
               Computer and Information Sciences (ISCIS X)},
  year = 1995,
  editor = {A. E. Harmanci and E. Gelenbe and B. \"Orencik},
  pages = {27--34},
  address = {Ku\c{s}adasi, Turkey},
  month = oct,
  organization = {{Istanbul} Technical Univ.},
  bibliographies = {RelMiCS}
}

@Article{Tembrowski-1984,
  author = {B. Tembrowski},
  title = {On some Class of Boolean Algebras with an Additional
		Binary Relation},
  journal = DEMONST,
  volume = 15,
  year = 1984,
  pages = {189--206},
  bibliographies = {RelMiCS}
}

@Book{Tennent-1991,
  author =	 {R. D. Tennent},
  title = 	 {Semantics of Programming Languages},
  publisher = 	 Prentice,
  year = 	 1991,
  McMaster = 	 {QA 76.7 .T473 1991},
  series =	 PrenticeCS,
  bibliographies = {SE3E}
}

@InCollection{Tennent-1994,
  author = {R. D. Tennent},
  title = {Correctness of Data Representations in {Algol}-like Languages},
  crossref = {Roscoe-1994},
  pages = {405--417},
  chapter = 23,
  OPTnote = {},
  OPTannote = {},
  bibliographies = {LogRel}
}

@Book{Teschl-Teschl-2008,
  author = 	 {Gerald Teschl and Susanne Teschl},
  title = 	 {{Mathematik für Informatiker ---
                  Band 1: Diskrete Mathematik und Lineare Algebra}},
  publisher = 	 Springer,
  year = 	 2008,
  series = 	 {eXamen.press},
  ISBN = {978-3-540-77431-0 (Print) 978-3-540-77432-7 (Online)},
  DOI = 	 {10.1007/978-3-540-77432-7},
  edition = 	 {3. Auflage},
  URL = 	 {http://www.springerlink.com/content/978-3-540-77431-0},
  bibliographies = {CS1FC}
}

@InProceedings{Tesson-HashimotoHideki-HuZhenjiang-Loulergue-TakeichiMasato-2010,
   author = {Tesson, Julien and Hashimoto, Hideki and Hu, Zhenjiang and Loulergue, Frédéric and Takeichi, Masato},
   affiliation = {LIFO, Université d’Orléans, France},
   title = {Program Calculation in Coq},
   crossref = {AMAST2010},
   pages = {163-179},
   DOIURL = {http://dx.doi.org/10.1007/978-3-642-17796-5_10},
   DOI = {10.1007/978-3-642-17796-5_10},
   abstract = {Program calculation, being a programming technique that derives programs from specification by means of formula manipulation, is a challenging activity. It requires human insights and creativity, and needs systems to help human to focus on clever parts of the derivation by automating tedious ones and verifying correctness of transformations. Different from many existing systems, we show in this paper that Coq, a popular theorem prover, provides a cheap way to implement a powerful system to support program calculation, which has not been recognized so far. We design and implement a set of tactics for the Coq proof assistant to help the user to derive programs by program calculation and to write proofs in calculational form. The use of these tactics is demonstrated through program calculations in Coq based on the theory of lists.}
}

@MastersThesis{Thaller-2006,
  Author = {Wolfgang Thaller},
  School = {McMaster University, Department of Computing and Software},
  Title = {Explicitly Staged Software Pipelining},
  Url = {http://www.cas.mcmaster.ca/~anand/papers/ThallerMScExSSP.pdf},
  note = {\newline\textsf{http://www.cas.mcmaster.ca/\~{}anand/papers/ThallerMScExSSP.pdf}},
  Year = {2006},
  WKloc = {A-1668},
  bibliographies = {Coconut}
}

@InProceedings{Thiagarajan-1994,
  title = {A Trace Based Extension of Linear Time Temporal Logic},
  author = {P. S. Thiagarajan},
  pages = {438--447},
  crossref = {LICS9},
  abstract = {The Propositional Temporal logic of Linear time (PTL) is
      interpreted over linear orders of order type~$(\omega, \leq)$. In
      applications, these linear orders consist of interleaved descriptions
      of the infinite runs of a concurrent program. Recent research on
      partial order based verification methods suggests that it might be
      fruitful to represent such runs as partial orders called infinite
      traces. We design here a natural extension of PTL called TrPTL to be
      interpreted directly over infinite traces. Using automata-theoretic
      techniques we show that the satisfiability problem for TrPTL is
      decidable. The automata that arise in this context turn out to be an
      attractive model of finite state concurrent programs. As a result, we
      also solve the model checking problem for TrPTL with respect to
      finite state concurrent programs.}
}

@InProceedings{Thiemann-1994,
  author = {Peter J. Thiemann},
  title = {Safe Sequencing of Assignments in Purely Functional
		  Programming Languages},
  crossref = {Honnef94},
  pages = {121--124},
  keywords = {monads, state transformsers}
}

@InProceedings{Thiemann-2000,
  author = {Peter J. Thiemann},
  title = {Modelling {HTML} in {Haskell}},
  pages = {263--277},
  crossref = {PADL2000},
  OPTabstract = {},
  URL = {http://www.informatik.uni-freiburg.de/~thiemann/papers/padl00.ps.gz},
  WKloc = {A-0975},
  bibliographies = {FP, EdComb}
}

@InProceedings{Thies-Vivien-Sheldon-Amarasinghe-2001,
  author = {William Thies and Fr{\'e}d{\'e}ric Vivien and Jeffrey Sheldon and Saman Amarasinghe},
  title = {A unified framework for schedule and storage optimization},
  booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
  year = 2001,
  ISBN = {1-58113-414-2},
  pages = {232--242},
  location = {Snowbird, Utah, United States},
  doi = {http://doi.acm.org/10.1145/378795.378852},
  publisher = {ACM Press},
  WKloc = {A-1479},
  bibliographies = {Anand}
}

@InProceedings{Thomanek-Dickmanns-1992,
  year = 1992,
  title = {Obstacle detection, tracking and
             estimation for autonomous road vehicle
             guidance},
  organization = {IEEE/RSJ},
  month = {July},
  booktitle = {Proc. Int. Conf. on Intelligent
             Robots and Systems, Raleigh NC},
  author = {Thomanek, Frank and Dickmanns, Dirk}
}

@InProceedings{Thomas-1995,
  author = {Wolfgang Thomas},
  title = {On the Synthesis of Strategies in Ifinite Games},
  crossref = {STACS1995},
  pages = {1--13},
  note = {invited talk},
  WKloc = {A-0407},
  abstract = {Infinite two-person games are a natural framework
		  for the study of reactive nonterminating
		  programs. The effective construction of winning
		  strategies in such games is an approach to the
		  synthesis of reactive programs. We describe the
		  automata theoretic setting of infinite games (given
		  by ``gam graphs''), outline a new construction of
		  winning strategies in finite-state games, and
		  formulate some questions which arise for games over
		  effectively presented infinite graphes.}
}

@PhDThesis{Thompson-1979,
  author = {Richard J. Thompson},
  title = {Transformational Structure of Algebraic Logics},
  note = Doct,
  school = BERKELEY,
  address = {Berkeley},
  year = 1979,
  bibliographies = {RelMiCS}
}

@Booklet{Thompson-1987,
  author = {Richard J. Thompson},
  title = {A Finitely Presented Semigroup Satisfying Sain's
		Conjecture for Finitizable Algebraic Logic. {D}etailed Proof},
  note = {Preprint, Math.\null{} Inst.\null{} Hungar.\null{} Acad.\null{} Sci.},
  year = 1987,
  bibliographies = {RelMiCS}
}

@Article{Thompson-1987a,
  author = {Richard J. Thompson},
  title = {Semigroup for Algebraic Logic},
  journal = ABAMS,
  volume = 8,
  year = 1987,
  bibliographies = {RelMiCS}
}

@Article{Thompson-1988,
  author = {Richard J. Thompson},
  title = {Noncommutative Cylindric Algebras and Relativizations of
		Cylindric Algebras},
  journal = POLINST,
  volume = 17,
  year = 1988,
  pages = {75--81},
  bibliographies = {RelMiCS}
}

@Article{Thompson-1989,
  author = {Simon Thompson},
  title = {Lawful Functions and Program Verification in Miranda},
  journal = SCICOP,
  year = 1989,
  volume = 13,
  pages = {181--218},
  keywords = {functional, programming, language, laws, views}
}

@Book{Thompson-1991,
  author = {Simon Thompson},
  title = {Type Theory and Functional Programming},
  publisher = {Addison-Wesley},
  year = 1991,
  ISBN = {0-201-41667-0},
  WKloc = {A-1348 (--195) A-1349 (196--), doc/pap/BIB},
  note = {out of print. Available via \textsf{http://www.cs.ukc.ac.uk/people/staff/sjt/TTFP/}},
  abstract = {Constructive Type theory has been a topic of research
      interest to computer scientists, mathematicians, logicians and
      philosophers for a number of years. For computer scientists it
      provides a framework which brings together logic and programming
      languages in a most elegant and fertile way: program development and
      verification can proceed within a single system. Viewed in a
      different way, type theory is a functional programming language with
      some novel features, such as the totality of all its functions, its
      expressive type system allowing functions whose result type depends
      upon the value of its input, and sophisticated modules and abstract
      types whose interfaces can contain logical assertions as well as
      signature information. A third point of view emphasizes that programs
      (or functions) can be extracted from proofs in the logic.



      Up until now most of the material on type theory has only appeared in
      proceedings of conferences and in research papers, so it seems
      appropriate to try to set down the current state of development in a
      form accessible to interested final-year undergraduates, graduate
      students, research workers and teachers in computer science and
      related fields - hence this book.



      The book can be thought of as giving both a first and a second course
      in type theory. We begin with introductory material on logic and
      functional programming, and follow this by presenting the system of
      type theory itself, together with many examples. As well as this we
      go further, looking at the system from a mathematical perspective,
      thus elucidating a number of its important properties. Then we take a
      critical look at the profusion of suggestions in the literature about
      why and how type theory could be augmented. In doing this we are
      aiming at a moving target; it must be the case that further
      developments will have been made before the book reaches the press.
      Nonetheless, such an survey can give the reader a much more developed
      sense of the potential of type theory, as well as giving the
      background of what is to come.



      Outline

      It seems in order to give an overview of the book. Each chapter
      begins with a more detailed introduction, so we shall be brief here.
      We follow this with a guide on how the book might be approached.



      The first three chapters survey the three fields upon which type
      theory depends: logic, the lambda-calculus and functional programming
      and constructive mathematics. The surveys are short, establishing
      terminology, notation and a general context for the discussion;
      pointers to the relevant literature and in particular to more
      detailed introductions are provided. In the second chapter we discuss
      some issues in the lambda-calculus and functional programming which
      suggest analogous questions in type theory.



      The fourth chapter forms the focus of the book. We give the formal
      system for type theory, developing examples of both programs and
      proofs as we go along. These tend to be short, illustrating the
      construct just introduced - Chapter 6 contains many more examples.



      The system of type theory is complex, and in chapter which follows we
      explore a number of different aspects of the theory. We prove certain
      results about it (rather than using it) including the important facts
      that programs are terminating and that evaluation is deterministic.
      Other topics examined include the variety of equality relations in
      the system, the addition of types (or `universes') of types and some
      more technical points.



      Much of our understanding of a complex formal system must derive from
      out using it. Chapter six covers a variety of examples and larger
      case studies. From the functional programming point of view, we
      choose to stress the differences between the system and more
      traditional languages. After a lengthy discussion of recursion, we
      look at the impact of the quantified types, especially in the light
      of the universes added above. We also take the opportunity to
      demonstrate how programs can be extracted from constructive proofs,
      and one way that imperative programs can be seen as arising. We
      conclude with a survey of examples in the relevant literature.



      As an aside it is worth saying that for any formal system, we can
      really only understand its precise details after attempting to
      implement it. The combination of symbolic and natural language used
      by mathematicians is surprisingly suggestive, yet ambiguous, and it
      is only the discipline of having to implement a system which makes us
      look at some aspects of it. In the case of TT, it was only through
      writing an implementation in the functional programming language
      Miranda [Miranda is a trade mark of Research Software Limited] that
      the author came to understand the distinctive role of assumptions in
      TT, for instance.



      The system is expressive, as witnessed by the previous chapter, but
      are programs given in their most natural or efficient form? There is
      a host of proposals of how to augment the system, and we look at
      these in Chapter 7. Crucial to them is the incorporation of a class
      of subset types, in which the witnessing information contained in an
      existential type is suppressed. As well as describing the subset
      type, we lay out the arguments for its addition to type theory, and
      conclude that it is not as necessary as has been thought. Other
      proposals include quotient (or equivalence class) types, and ways in
      which general recursion can be added to the system without its losing
      its properties like termination. A particularly elegant proposal for
      the addition of co-inductive types, such as infinite streams, without
      losing these properties, is examined.



      Chapter eight examines the foundations of the system: how it compares
      with other systems for constructive mathematics, how models of it are
      formed and used and how certain of the rules, the closure rules, may
      be seen as being generated from the introduction rules, which state
      what are the canonical members of each type. We end the book with a
      survey of related systems, implemented or not, and some concluding
      remarks.



      Bibliographic information is collected at the end of the book,
      together with a table of the rules of the various systems.

      We have used standard terminology whenever were able, but when a
      subject is of current research interest this is not always possible.

      Using the book

      In the hope of making this a self-contained introduction, we have
      included chapters one and two, which discuss natural deduction logic
      and the lambda-calculus - these chapters survey the fields and
      provide an introduction to the notation and terminology we shall use
      later. The core of the text is Chapter four, which is the
      introduction to type theory.



      Readers who are familiar with natural deduction logic and the
      lambda-calculus could begin with the brief introduction to
      constructive mathematics provided by Chapter three, and then turn to
      Chapter four. This is the core of the book, where we lay out type
      theory as both a logic and an functional programming system, giving
      small examples as we go. The chapters which follow are more or less
      loosely coupled.



      Someone keen to see applications of type theory can turn to Chapter
      six, which contains examples and larger case studies; only
      occasionally will readers need to need to refer back to topics in
      Chapter five.



      Another option on concluding chapter four is to move straight on to
      Chapter five, where the system is examined from various mathematical
      perspectives, and an number of important results on the consistency,
      expressibility and determinacy are proved. Chapter eight should be
      seen as a continuation of this, as it explores topics of a
      foundational nature.



      Chapter seven is perhaps best read after the examples of Chapter six,
      and digesting the deliberations of Chapter five.

      In each chapter exercises are included. These range from the routine
      to the challenging. Not many programming projects are included as it
      is expected that readers will to be able to think of suitable
      projects for themselves - the world is full of potential
      applications, after all.}
}

@InProceedings{Thompson-1997,
  author = {Simon Thompson},
  title = {{\em Where do I begin?} {A} problem solving approach to teaching functional programming},
  pages = {323--334},
  crossref = {PLILP1997},
  URL = {http://www.cs.ukc.ac.uk/pubs/1997/208},
  keywords = {functional programming Haskell palindrome Polya problem solving},
  bibliographies = {FP, DM},
  WKloc = {A-1169}
}

@Book{Thompson-1999,
  author = {Simon Thompson},
  title = {{Haskell}: The Craft of Functional Programming},
  publisher = {Addison-Wesley},
  year = 1999,
  edition = {Second Edition},
  pages = 507,
  ISBN = {0-201-34275-8},
  URL = {http://www.cs.ukc.ac.uk/people/staff/sjt/craft2e/},
  WKloc = {owned, \lent{Millie}},
  abstract = {The second edition of Haskell: The Craft of Functional
      Programming is essential reading for beginners to functional
      programming and newcomers to the Haskell programming language. The
      emphasis is on the process of crafting programs and the text contains
      many examples and running case studies, as well as advice an program
      design, testing, problem solving and how to avoid common pitfalls.

      Building on the strengths of the first edition, the book includes
      many new and improved features: \begin{itemize}

      \item Complete coverage of Haskell 98, the standard version of
      Haskell which will be stable and supported by implementations for
      years to come.

      \item An emphasis on software engineering principles, encouraging a
      disciplined approach to building reusable libraries of software
      components.

      \item Detailed coverage of the Hugs interpreter with an appendix
      covering other implementations.

      \item A running case study of pictures emphasizes the built-in
      functions which appear in the standard prelude and libraries. It is
      also used to give an early preview of some of the more complex
      language features, such as high-order functions.

      \item List comprehensions and the standard functions over lists are
      covered before recursion.

      \item Early coverage of polymorphism supporting the "toolkit"
      approach and encouraging the resuse of built-in functions and types.

      \item Extensive reference material containing details of further
      reading in books, journals and on the World Wide Web.

      \item Accompanying Web Site supporting the book, containing all the
      program code, further teaching materials and other useful resources.
      \end{itemize}

      Synopsis This books introduces Haskell at a level appropriate for
      those with little or no prior experience of functional programming.
      The emphasis is on the process of crafting programs, solving
      problems, and avoiding common errors.}
}

@Misc{Thompson-1999a,
  author = {Simon Thompson},
  title = {Verifying {Fran} Programs},
  month = APR,
  year = 1999,
  WKloc = {A-1151}
}

@Misc{Thompson-199X,
  author = {Simon Thompson},
  title = {A Functional Reactive Animation of a Lift Using {Fran}},
  year = {199?},
  WKloc = {A-1152}
}

@Misc{Thompson-2000,
  author = {Simon Thompson},
  title = {Logic and Dependent Types in the {Aldor Computer Algebra System}},
  year = 2000,
  WKloc = {A-1418}
}

@Misc{Thompson-Reinke-2001,
  author = {Simon Thompson and Claus Reinke},
  title = {A Catalogue of Functional Refactorings},
  month = AUG,
  year = 2001,
  WKloc = {A-1424}
}

@Article{Tilson-1987,
  author = 	 {Bret Tilson},
  title = 	 {Categories as algebra: an essential ingredient in the theory of monoids},
  journal = 	 JPAA,
  year = 	 1987,
  volume =	 48,
  pages =	 {83--198},
  bibliographies = {RelMiCS},
  DOI = {http://dx.doi.org/10.1016/0022-4049(87)90108-3},
  bibliographies = {RelMiCS}
}

@TechReport{Tip-1994,
  author = {Frank Tip},
  title = {Generic Techniques for Source-Level Debugging and Dynamic Program Slicing},
  institution = {Centrum voor Wiskunde en Informatica (CWI)},
  number = {CS-R9453},
  address = {ISSN 0169-118X},
  month = sep # { 30},
  year = 1994,
  WKloc = {A-0531}
}

@TechReport{Tip-1997,
  type = {Technical Report},
  number = {CS-R9438},
  institution = {CWI - Centrum voor Wiskunde en Informatica},
  title = {A Survey of Program Slicing Techniques.},
  month = jul # { 31,},
  year = 1994,
  bibdate = {August 13, 1997},
  URL = {ftp://ftp.cwi.nl/pub/CWIreports/AP/CS-R9438.ps.Z},
  author = {Frank Tip},
  WKloc = {B-0092}
}

@InProceedings{Tiuryn-1990,
  WKloc = {A-0036},
  year = 1990,
  title = {Type Inference Problems: A Survey},
  series = LNCS,
  publisher = Springer,
  pages = {105--120},
  number = 452,
  booktitle = {Proc..\null{} MFCS 90},
  author = {Jerzy Tiuryn},
  address = {Bansk\`a Bystrica},
  bibliographies = {RelMiCS}
}

@Book{Toelle-1993,
  author = {Yasner T{\"o}lle},
  title = {Study and Research Guide in Computer Science:
		  Profiles of Universities in the {USA}},
  publisher = {Springer},
  year = 1993,
  address = {Berlin}
}

@InProceedings{Tofte-1992,
  author = {Mads Tofte},
  title = {Principal Signatures for Higher-Order Program Modules},
  crossref = {POPL1992},
  pages = {189--199},
  abstract = {Under the Damas-Milner type discipline for
		  functional languages, every expression has a
		  principal type, if it elaborates at all. In the type
		  discipline for ML Modules, a signature expression
		  has a principal signature, if it elaborates at all.
		  However, while functions can be higher-order in ML,
		  parameterised modules in ML are first-order only. We
		  present a type discipline for a skeletal
		  higher-order module language which has principal
		  signatures. Sharing and multiple views of structures
		  are handled in a manner which is compatible with the
		  semantics of the first-order ML modules.},
  WKloc = {A-0170}
}

@InProceedings{Tofte-Talpin-1994,
  author = {Mads Tofte and Jean-Pierre Talpin},
  title = {Implementation of the Typed Call-by-Value
		  $\lambda$-Calculus using a Stack of Regions},
  crossref = {POPL1994},
  pages = {188--201}
}

@Misc{Tokuda-1999s,
  author = {Takehiro Tokuda},
  title = {Slides on Constraint Solving Methods},
  year = 1999,
  WKloc = {A-0850}
}

@Article{Tokuda-Watanabe-1994,
  author = {Takehiro Tokuda and Yoshimichi Watanabe},
  title = {An Attribute Evaluation of Context-Free Languages},
  journal = {Information Processing Letters},
  year = 1994,
  volume = 57,
  OPTnumber = {},
  OPTmonth = {},
  pages = {91--98},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0851},
  abstract = {We present a method of evaluating attributes of input strings
               defined by attribute grammars having
               general context-free underlying grammars
               and relatively general semantic rules.

               Our attribute evaluation method
               is based on Earley's parsing method [1,2].
               Hence we can evaluate attributes
               without building corresponding derivation trees
               or attribute dependency graphs.
               Also we can compute all possible attribute values
               for unambiguous or ambiguous context-free underlying grammars.},
  OPTcontents = {},
  annote = {Derive this via deforestation!
                  Relate with Swierstra's parsing combinators!}
}

@Unpublished{Tolmach-2001,
  author = {Andrew Tolmach},
  title = {{DRAFT: An External Representation for the GHC Core Language}},
  month = JUN,
  year = 2001,
  WKloc = {A-1108}
}

@Article{Tolmach-Oliva-1998,
  title = {From {ML} to {Ada}: Strongly-typed language
                 interoperability via source translation},
  author = {Andrew Tolmach and Dino P. Oliva},
  pages = {367--412},
  journal = {Journal of Functional Programming},
  month = jul,
  year = 1998,
  volume = 8,
  number = 4,
  WKloc = {B-0087}
}

@InProceedings{Tompkins-Hoos-2004,
  author = 	 {Dave A. D. Tompkins and Holger H. Hoos},
  title = 	 {{UBCSAT}: An Implementation and Experimentation Environment for {SLS} Algorithms for {SAT} and {MAX-SAT}},
  OPTcrossref =  {},
  pages = 	 {37--46},
  booktitle = {Proceedings of the Seventh International Conference on Theory and Applications of Satisfiability Testing {(SAT 2004)}},
  year = 	 {2004},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  URL = 	 {http://www.satlib.org/ubcsat/},
  WKloc = {A-1728, doc/pap/BIB},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@PhDThesis{Tourlas-2000,
  author = {Konstantinos Tourlas},
  title = {Diagrammatic Representations in Domain-Specific Languages},
  school = {University of Edinburgh},
  year = 2000,
  WKloc = {A-1064}
}

@Misc{Tourlas-2000a,
  author = {Konstantinos Tourlas},
  title = {Towards the Principled Design of Diagrams in Computing and Software Engineering},
  howpublished = {slides from a talk given in Birmingham on 27th October 2000},
  month = OCT,
  year = 2000,
  note = {URL: http://www.dcs.ed.ac.uk/home/kxt/birmingham\_4up.ps.gz}
}

@TechReport{TOY2006,
  OPTeditor = 	 {Rafael Caballero and Jaime S\'{a}nchez},
  author = 	 {Rafael Caballero and Jaime S\'{a}nchez and others},
  title = 	 {$\mathcal{TOY}$: A Multiparadigm Declarative Language, Version 2.2.3},
  institution =  {Universidad Complutense Madrid},
  year = 	 2006,
  month =	 JUL,
  note =	 {\textsf{http://toy.sourceforge.net}}
}

@InProceedings{Toyama-1987,
  abstract = {Commutativity is very useful in showing the Church-Rosser
	property for the union of term rewriting systems. This paper studies
	the critical pair technique for proving commutativity of term
	rewriting systems. Extending the concept of critical pairs between
	two term rewriting systems, a sufficient condition for commutativity
	is proposed. Using this condition, a new sufficient condition is
	offered for the Church-Rosser property of left-linear term rewriting
	systems.},
  title = {Comutativity of Term Rewriting Systems},
  pages = {393-407},
  crossref = {ProgFutGenCompII},
  author = {Yoshihito Toyama},
  bibliographies = {RelMiCS}
}

@InProceedings{Toyama-2005,
  author =       {Yoshihito Toyama},
  title =        {Confluent Term Rewriting Systems},
  crossref =  {RTA2005},
  pages =     1,
  note =      {(invited talk)}
}

@InProceedings{Toyama-Oyamaguchi-1994,
  author = {Y. Toyama and M. Oyamaguchi},
  title = {Church-Rosser Property and Unique Normal Form Property of
          Non-Duplication Term Rewriting Systems},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Mie University, Japan}
}

@Misc{Toyama-Smetsers-vanEekelen-Plasmeijer-1992,
  author = {Yoshihito Toyama and Sjaak Smetsers and van Eekelen, Marko and Rinus Plasmeijer},
  title = {The Functional Strategy and Transitive Term Rewriting Systems},
  year = 1992,
  WKloc = {A-0425},
  submission = {LICS'92 submission},
  note = {URL: \url{ftp://ftp.cs.kun.nl/pub/Clean/papers/1993/toyy93-functional.ps.gz}}
}

@InCollection{Toyama-Smetsers-vanEekelen-Plasmeijer-1993,
  author = {Yoshihito Toyama and Sjaak Smetsers and van Eekelen, Marko and Rinus Plasmeijer},
  title = {The Functional Strategy and Transitive Term Rewriting Systems},
  crossref = {Sleep-Plasmeijer-vanEekelen-1993},
  pages = {61--76},
  annote = {special class of strongly sequential systems \cite{Huet-Levy-1979}}
}

@InProceedings{Toyn-1998,
  author = {Ian Toyn},
  title = {Innovations in the Notation of {Standard Z}},
  crossref = {ZUM1998},
  pages = {193-213},
  bibliographies = {RelMiCS, RelMiS},
  WKloc = {A-1329}
}

@Book{Toyn-1999,
  editor = {Ian Toyn},
  title = {{Z} Notation, Final Committee Draft, CD 13568.2},
  publisher = {Developed by members of the Z Standards Panel,
    BSI Panel IST/5/-/19/2 (Z Notation),
    ISO Panel JTC1/SC22/WG19 (Rapporteur Group for Z),
    Project number JTC1.22.45},
  year = 1999,
  month = AUG,
  URL = {http://web.comlab.ox.ac.uk/oucl/research/groups/zstandards/},
  note = {Current version: Consensus Working Draft 2.7, October 12, 2001.
   \textsf{URL: http://web.comlab.ox.ac.uk/oucl/research/groups/zstandards/index.html}.},
  bibliographies = {RelMiCS, RelMiS}
}

@Misc{Toyn-2000,
  OPTkey = {},
  editor = {Ian Toyn},
  title = {Formal Specification --- {Z} Notation --- Syntax, Type and Semantics, Consensus Working Draft 2.6},
  OPThowpublished = {},
  month = AUG,
  year = 2000,
  URL = {http://web.comlab.ox.ac.uk/oucl/research/groups/zstandards/index.html},
  bibliographies = {RelMiCS, SpecTech},
  WKloc = {B-0124},
  keywords = {ZStandard}
}

@InProceedings{Toyn-Dix-Runciman-1987,
  author = {Ian Toyn and Alan Dix and Colin Runciman},
  title = {Performance Polymorphism},
  crossref = {FPCA-1987},
  pages = {325--346},
  keywords = {programming environment ``glide'' for exploratory
		  programming in a purely functional lazy language},
  abstract = {In an interactive functional programming environment
		  with a Milner-style polymorphic type system (Milner
		  1978), a modification to one definition may imply
		  changes in the types of other definitions. A
		  polymorphic typechecker must carry out some {\em
		  re-typechecking} to determine all of these changes.
		  This paper presents a new typechecking algorithm
		  which performs fine-grained re-typechecking based on
		  analysis of individual type constraints. The new
		  algorithm is compared with that of Nikhil (Nikhil
		  85), which performs re-typechecking of entire definitions.},
  annote = {--- PLGnotes ---
		  local ``re-typechecking''}
}

@Misc{Toyn-McDermid-199X,
  OPTkey = {},
  author = {Ian Toyn and John A. McDermid},
  title = {{\sf CADi$\mathbb{Z}$}: An Architecture for {Z} Tools and its Implementation},
  OPThowpublished = {},
  OPTmonth = {},
  year = {1995?},
  WKloc = {A-1032},
  annote = {The CADiZ manual cites a paper of this title,
      authored only by Ian Toyn as appeareed in SOFTWARE --- Practice and Experience, 25(3):305-330, March 1995}
}

@InProceedings{Toyn-Valentine-Stepney-King-2000,
  author = {Ian Toyn and Samuel H. Valentine and Susan Stepney and Steve King},
  title = {Typechecking {Z}},
  series = LNCS,
  volume = 1878,
  pages = {264--??},
  year = 2000,
  ISSN = {0302-9743},
  URL = {http://link.springer-ny.com/link/service/series/0558/bibs/1878/18780264.htm},
  pdf = {http://link.springer-ny.com/link/service/series/0558/papers/1878/18780264.pdf}
}

@Book{Traub-1991,
  author = {Kenneth R. Traub},
  title = {Implementation of Non-Strict Functional Programming Languages},
  publisher = {Pitman},
  year = 1991,
  ISBN = {0-273-08827-0},
  WKloc = {B-0030},
  McMaster = {QA 76.62 .T73 1991},
  keywords = {compilers, parallel, lenience, lenient}
}

@TechReport{Treinen-1990,
  filename = {A-09-90.dvi},
  abstract = {We claim that the reduction of Post's Correspondence
		  Problem to the decision problem of a theory provides
		  a useful tool for proving undecidability of first
		  order theories given by an interpretation.  The goal
		  of this paper is to propose a framework for such
		  reduction proofs.  The method proposed is
		  illustrated by proving the undecidability of the
		  theory of a term algebra modulo AC and the theory of
		  a partial lexicographic path ordering.},
  year = 1990,
  type = {Internal Report},
  title = {A New Method for Undecidability Proofs of First Order Theories},
  number = {A09/90},
  month = MAY,
  institution = {Universit\"at des Saarlandes},
  author = {Ralf Treinen},
  address = {Fachbereich 14: Informatik, 6600 Saarbr\"ucken 11},
  bibliographies = {RelMiCS}
}

@PhDThesis{Treinen-1991,
  year = 1991,
  title = {{Modulare Datentypdefinitionen und Ihre Beziehungen
			zur Logik erster Stufe}},
  school = {Universit\"at des Saarlandes},
  note = {In german},
  month = DEC,
  author = {Ralf Treinen},
  bibliographies = {RelMiCS}
}

@TechReport{Treinen-1991a,
  filename = {A-01-90.dvi},
  abstract = {This paper concerns the relation between parameterized
		  first order data types and first order logic.
		  Augmenting first order logic by data type
		  definitions yields in general a strictly stronger
		  logic than first order logic.  We consider some
		  properties of the new logic for fixed data type
		  definitions. While our new logic always fulfills the
		  downward Skolem-L\"owenheim property, compactness is
		  fulfilled if and only if for the given data type
		  definition the new logic has the same expressive
		  power than first order logic. We show that this last
		  property is undecidable.},
  year = 1991,
  type = {Interner Bericht},
  title = {First Order Data Types and First Order Logic},
  number = {A01/91},
  month = JAN,
  institution = {Universit\"at des Saarlandes},
  author = {Ralf Treinen},
  address = {Fachbereich 14: Informatik, 6600 Saarbr\"ucken 11},
  bibliographies = {RelMiCS}
}

@InProceedings{Treinen-1991b,
  title = {First Order Data Types and First Order Logic},
  author = {Ralf Treinen},
  pages = {594--614},
  crossref = {TACS1991},
  abstract = {This paper concerns the relation between parameterized
		  first order data types and first order
		  logic. Augmenting first order logic by data type
		  definitions yields in general a strictly stronger
		  logic than first order logic.  Some modeltheoretic
		  properties of the new logic are investigated.  While
		  the new logic always fulfills the downward
		  Skolem-L\"owenheim property, compactness is
		  fulfilled if and only if for the given data type
		  definition the new logic has the same expressive
		  power than first order logic.  This last property is
		  shown to be undecidable.},
  bibliographies = {RelMiCS}
}

@InProceedings{Treinen-1993,
  filename = {MFCS93.ps.Z},
  DIRECTORY = {dfki-saarbruecken},
  abstract = {Feature Constraint Systems have been proposed as a
		  logical data structure for constraint (logic)
		  programming. They provide a record-like view to
		  trees by identifying subtrees by keyword rather than
		  by position. Their atomic constraints are finer
		  grained than in the constructor-based approach. The
		  recently proposed {\sl CFT\/}
		  \cite{ST:RecordsLogProg92} in fact generalizes the
		  rational tree system of Prolog II.

                  We propose a new feature constraint system {\sl
		  EF\/} which extends {\sl CFT\/} by considering
		  features as first class values. As a consequence,
		  {\sl EF\/} contains constraints like $x[v]w$ where
		  $v$ is a variable ranging over features, while {\sl
		  CFT\/} restricts $v$ to be a fixed feature symbol.

                  We show that the satisfiability of conjunctions of
		  atomic \EF-constraints is NP-complete.
		  Satisfiability of quantifier-free \EF-constraints is
		  shown to be decidable, while the
		  $\exists^*\forall^*\exists^*$ fragment of the first
		  order theory is undecidable.},
  year = 1993,
  title = {Feature Constraints with First-Class Features},
  month = SEP,
  booktitle = {Mathematical Foundations of Computer Science},
  author = {Ralf Treinen},
  address = {Gda\'nsk, Poland},
  bibliographies = {RelMiCS}
}

@Misc{Trinder-1999,
  author = {Philip W. Trinder},
  title = {Motivation for {Glasgow distributed Haskell}, a non-strict Functional Language},
  year = 1999,
  keywords = {GdH, GpH, concurrent Haskell},
  WKloc = {A-1255, doc/pap/BIB}
}

@Misc{Trinder-BarryJr-Hammond-Junaidu-Klusic-Loidl-PeytonJones-1998,
  author = {Philip W. Trinder and Barry Jr., Ed. and M. Kei Davis and
      Kevin Hammond and Sahalu B. Junaidu and Ulrike Klusic and
      Hans-Wolfgang Loidl and Peyton Jones, Simon L.},
  title = {{\sc GpH}: An Architecture-independent Functional Language},
  year = 1998,
  month = JUL,
  WKloc = {A-0832}
}

@Article{Trinder-Hammond-Loidl-PeytonJones-1998,
  author = {P.W. Trinder and K. Hammond and H-W. Loidl and Peyton Jones, S.L.},
  title = {Algorithm + Strategy = Parallelism},
  journal = JFP,
  year = 1998,
  OPTkey = {},
  OPTvolume = 8,
  OPTnumber = 1,
  OPTpages = {23--60},
  OPTmonth = JAN,
  URL = {http://www.cee.hw.ac.uk/~dsg/gph/papers/ps/strategies.ps.gz},
  WKloc = {A-1253, doc/pap/BIB}
}

@InProceedings{TristanJB-Leroy-2008,
  author = 	 {Jean-Baptiste Tristan and Xavier Leroy},
  title = 	 {Formal Verification of Translation Validators,
    A Case Study on Instruction Scheduling Optimizations},
  crossref =  {POPL2008},
  DOI = {http://doi.acm.org/10.1145/1328438.1328444},
  pages = 	 {17--27},
  URL = 	 {http://pauillac.inria.fr/~xleroy/publi/validation-scheduling-07.pdf},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {Coconut},
  abstract = 	 {Translation validation consists of transforming a
                  program and a posteriori validating it in order to
                  detect a modification of its semantics. This
                  approach can be used in a verified compiler,
                  provided that validation is formally proved to be
                  correct. We present two such validators and their
                  Coq proofs of correctness. The validators are
                  designed for two instruction scheduling
                  optimizations: list scheduling and trace
                  scheduling.}
}

@Book{Troelstra-1992,
  author = {A. S. Troelstra},
  title = {Lecture on Linear Logic},
  publisher = CSLI_P,
  year = 1992,
  volume = 29,
  series = {CSLI Lecture Notes},
  address = {Stanford, CA},
  OPTedition = {},
  OPTnote = {},
  bibliographies = {RelMiCS}
}

@InCollection{Troelstra-1993,
  author = {A. S. Troelstra},
  title = {Tutorial on Linear Logic},
  booktitle = {Substructural Logics},
  publisher = OXFORD_P,
  year = 1993,
  editor = {Kosta Do{\v{s}}en and Peter Schroeder-Heister},
  pages = {327--356},
  bibliographies = {RelMiCS}
}

@Article{Tropashko-2005,
  author    = {Vadim Tropashko},
  title     = {Relational Algebra as non-Distributive Lattice},
  journal   = {CoRR},
  volume    = {abs/cs/0501053},
  year      = {2005},
  ee        = {http://arxiv.org/abs/cs/0501053},
  bibsource = {DBLP, http://dblp.uni-trier.de},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Tsang-1987,
  author = {E. P. K. Tsang},
  title = {{Time Structures for AI}},
  booktitle = {Proc.\null{} of the {$10^{th}$} Internat.\null{} Joint Conf.\null{}
		on Artificial Intelligence},
  address = {Milano, Italy},
  publisher = Kauf,
  year = 1987,
  pages = {456--461},
  bibliographies = {RelMiCS}
}

@InProceedings{Tsang-1987a,
  author = {E. P. K. Tsang},
  title = {The Consistent Labelling Problem in Temporal Reasoning},
  crossref = {AAAI1987},
  pages = {251--255},
  bibliographies = {RelMiCS}
}

@Article{Tse-1987,
  author = {T.H. Tse},
  title = {On the Detection of Unstructuredness in Flowgraphs},
  journal = ACTIN,
  year = 1987,
  volume = 25,
  pages = {189--193},
  WKloc = {A-1315},
  bibliographies = {SQRL}
}

@InProceedings{Tse-Zdancewic-2004,
  author = 	 {Stephen Tse and Seve Zdancewic},
  title = 	 {Translating Dependency into Parametricity},
  OPTcrossref =  {LICS2004},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {submitted},
  WKloc = 	 {A-1547}
}

@InProceedings{Tsiaras-Triantafilou-Tollis-2007,
  author = 	 {Vassilis Tsiaras and Sofia Triantafilou and Ioannis G. Tollis},
  title = 	 { Treemaps for Directed Acyclic Graphs},
  crossref =  {GD2007},
  DOI = 	 {10.1007/978-3-540-77537-9_37},
  pages = 	 {377--388},
  URL =          {http://www.springerlink.com/content/33806l8415804p54},
  abstract = 	 {Gene Ontology information related to the biological
                  role of genes is organized in a hierarchical manner
                  that can be represented by a directed acyclic graph
                  (DAG). Treemaps graphically represent hierarchical
                  information via a two-dimensional rectangular
                  map. They efficiently display large trees in limited
                  screen space. Treemaps have been used to visualize
                  the Gene Ontology by first transforming the DAG into
                  a tree. However this transformation has several
                  undesirable effects such as producing trees with a
                  large number of nodes and scattering the rectangles
                  associated with the duplicates of a node around the
                  screen. In this paper we introduce the problem of
                  visualizing a DAG as a treemap, we present two
                  special cases, and we discuss complexity results.}
}

@InCollection{Tucker-,
  author = {Albert W. Tucker},
  title = {Interview by Stephen B. Maurer},
  crossref = {Albers-Alexanderson-},
  pages = {337--348},
  WKloc = {A-1316},
  annote = {Ridha Khedri}
}

@Book{Tucker-Noonan-2002,
  author = {Allen B. Tucker and Robert E. Noonan},
  title = {Programming Languages: Principles and Paradigms},
  publisher = {McGraw-Hill},
  year = 2002,
  ISBN = {0-07-238111-6 (North America),
                  0-07-112280-X (ISE --- ``International Edition'')},
  URL = {http://www.mhhe.com/tucker/},
  bibliographies = {SE3E}
}

@Book{Tucker-Zucker-1988,
  UniBwM = {INF460/R12092},
  year = 1988,
  volume = 6,
  title = {Program Correctness over Abstract Data Types, with Error-State Semantics},
  series = {CWI Monographs},
  publisher = NoHo,
  author = {J. V. Tucker and J. I. Zucker},
  bibliographies = {RelMiCS}
}

@InProceedings{Tucker-Zucker-1992,
  authorsAddress = {T: University College of Swansea, Wales; Z:
		  McMaster University, Hamilton, Ontario, Canada},
  abstract = {The theory of computable functions on abstract data
		  types is outlined. Methods for extending the theory
		  to establish the scope and limits of computation on
		  streams over abstract data types are described.
		  Applications of these methods to the theory of
		  synchronous concurrent algorithms are discussed},
  title = {Theory of Computation over Stream Algebras, and its
		  Applications},
  pages = {62--80},
  note = {invited lecture},
  crossref = {MFCS1992},
  author = {J. V. Tucker and J. I. Zucker},
  bibliographies = {RelMiCS}
}

@Article{Tucker-Zucker-2005,
 author = {J. V. Tucker and J. I. Zucker},
 title = {Abstract Versus Concrete Computation on Metric Partial Algebras},
 journal = {ACM Trans.~Comput.~Logic},
 volume = {5},
 number = {4},
 year = {2004},
 issn = {1529-3785},
 pages = {611--668},
 doi = {http://doi.acm.org/10.1145/1024922.1024924},
 publisher = {ACM Press},
 address = {New York, NY, USA},
}

@InProceedings{Tullsen-2000,
  author = {Mark Tullsen},
  title = {First Class Patterns},
  crossref = {PADL2000},
  pages = {1--15},
  URL = {http://www.cs.yale.edu/~tullsen/patterns.ps},
  WKloc = {A-0919},
  bibliographies = {PMC},
  abstract = {Pattern matching is a great convenience in programming. However, pattern matching has its problems: it conflicts with data ion; it is complex (at least in Haskell, which has pattern guards, irrefutable patterns, n+k patterns, as patterns, etc.); it is a source of run-time errors; and lastly, one cannot abstract over patterns as they are not a first class language construct. This paper proposes a simplification of pattern matching that makes patterns first class. The key idea is to treat patterns as functions of type \texttt{Maybe b} --- i.e., \texttt{(Nothing|Just b)}; thus, patterns and pattern combinators can be written as functions in the language.}
}

@InProceedings{Tullsen-2000a,
  author = {Mark Tullsen},
  title = {The Zip Calculus},
  crossref = {MPC2000},
  pages = {},
  OPTabstract = {},
  URL = {http://www.cs.yale.edu/~tullsen/zip-mpc.ps},
  URLslides = {http://cs-www.cs.yale.edu/~tullsen/zip-mpc-talk.ps}
}

@TechReport{Tullsen-Hudak-1998,
  author = {Mark Tullsen and Paul Hudak},
  title = {An Intermediate Meta-Language for Program Transformation},
  year = 1998,
  month = JUN,
  institution = {Department of Computer Science, Yale University},
  type = {Research Report},
  number = {YALEU/DCS/RR-1154},
  OPTaddress = {},
  URL = {http://www.cs.yale.edu/~tullsen/imlforpt.ps},
  WKloc = {A-0921}
}

@InProceedings{Tullsen-Hudak-1999,
  author = {Mark Tullsen and Paul Hudak},
  title = {Shifting Expression Procedures into Reverse},
  crossref = {PEPM1999},
  pages = {},
  OPTabstract = {},
  URL = {http://www.cs.yale.edu/~tullsen/shifting.ps},
  WKloc = {A-0918}
}

@Misc{Tunes1995,
  author = {Fran\c{c}ois Ren{\'e} Rideau},
  title = {Welcome to the {Tunes} Project},
  note = {{\sf http://www.tunes.org/}},
  WKloc = {B-0082}
}

@InProceedings{Turbak-1996,
  author = {Franklyn Turbak},
  title = {First-Class Synchronization Barriers},
  crossref = {ICFP1996},
  URL = {http://www-swiss.ai.mit.edu/~lyn/synchron.html},
  WKloc = {A-1026, doc/pap/BIB/Turbak-1996.ps},
  abstract = {Our purpose is to promote a second-class mechanism --- the
      synchronization barrier --- to a first-class value. We introduce the
      synchron , a novel synchronization mechanism that enables the
      coordination of a dynamically varying set of concurrent threads that
      share access to a first-class synchronization token. We demonstrate
      how synchrons can be used to modularly manage resources in cases
      where existing techniques are either inapplicable or non-modular. In
      particular, synchronized lazy aggregates enable the first
      space-efficient aggregate data decomposition of a wide range of
      algorithms. We also introduce explicit-demand graph reduction , a new
      semantic framework that we have developed to describe concurrency and
      explain the meaning of a synchron rendezvous.}
}

@InProceedings{Turbak-Dimock-Muller-Wells-1997,
  author = {Franklyn Turbak and Allyn Dimock and Robert Muller and J. B. Wells},
  title = {Compiling with Polymorphic and Polyvariant Flow Types},
  booktitle = {{Proceedings of the Types in Compilation Workshop, Amsterdam, June 8, 1997}},
  year = 1997,
  URL = {http://www-swiss.ai.mit.edu/~lyn/lyn.html},
  WKloc = {A-1027, doc/pap/BIB/Turbak-Dimock-Muller-Wells-1997.ps}
}

@Article{Turchin-1986,
  author = {Valentin F. Turchin},
  title = {The Concept of a Supercompiler},
  journal = ACM-TOPLAS,
  year = 1986,
  month = JUL,
  volume = 8,
  number = 3,
  pages = {292--325},
  annote = {The general principles and algorithms of supercompilation are
      described and compared with the usual approach to program
      transformation as stepwise application of equivalence
      transformations. Refal, used as the base language for
      supercompilation, is formally defined and compared with Lisp and
      Prolog. Examples are given.},
  WKloc = {A-0968}
}

@Book{Turing,
  UniBwM = {MAT009/T290},
  year = 1989,
  title = {Alan Turing, Enigma},
  publisher = {Kammerer \& Unverzagt},
  author = {Andrew Hodges},
  address = {Berlin}
}

@Article{Turing-Newman-1942,
  author = {Alan M. Turing and M. H. A. Newman},
  title = {A Formal Theorem in Church's Theory of Types},
  year = 1942,
  journal = {J Symbolic Logic},
  volume = 7,
  annote = {Hodges page 215 note 4.36}
}

@InProceedings{Turner-1982,
  author = {David A. Turner},
  title = {Recursion Equations as a Programming Language},
  WKloc = {A-0581},
  pages = {1--27},
  crossref = {Darlington-1982}
}

@Article{Turner-2004,
  author = 	 {D. A. Turner},
  title = 	 {Total Functional Programming},
  journal = 	 JUCS,
  year = 	 2004,
  volume =	 10,
  number =	 7,
  pages =        {751--768},
  DOIURL = 	 {http://dx.doi.org/10.3217/jucs-010-07-0751},
  DOI =          {10.3217/jucs-010-07-0751},
  WKloc = 	 {doc/pap/BIB},
  abstract = 	 {The driving idea of functional programming is
     to make programming more closely related to mathematics.
     A program in a functional language such as Haskell or Miranda
     consists of equations which are both computation rules
     and a basis for simple algebraic reasoning about the functions
     and data structures they define.
     The existing model of functional programming,
     although elegant and powerful,
     is compromised to a greater extent than is commonly recognised
     by the presence of partial functions.
     We consider a simple discipline of total functional programming
     designed to exclude the possibility of non-termination.
     Among other things this requires a type distinction between
     data, which is finite, and codata, which is potentially infinite.}
}

@Book{Turski-Maibaum-1987,
  author = {Wladyslaw M. Turski and Thomas S. E. Maibaum},
  title = {The Specification of Computer Programs},
  publisher = {Addison-Wesley},
  year = 1987,
  series = {International Computer Science Series},
  McMaster = {QA 76.76 .D47 T87 1987; ID:39005042014104}
}

@InProceedings{Tveit-2009,
  author =       {Merete Skjelten Tveit},
  title =        {   A Meta-Model-Based Approach for Specification of Graphical Representations},
  crossref =  {GTVMT2009},
  abstract = {Meta-models are widely used for the specification
     of the internal structure of graphical modelling languages,
     and well-established standards (e.g. MOF) exist for this.
     For the graphical representation there is not the same agreement
     and no related standards.
     This paper presents a new meta-language for an \emph{independent}
     specification of graphical representations.
     A diagram from the domain-specific language \emph{Service}
     is used as a running example to show
     how this meta-model-based approach is appropriate
     for specifying the graphical representation in a precise way,
     but still on a high level of abstraction.},
  keywords = {language specification, graphical representations,
     meta-models, mapping descriptions},
  WKloc = {doc/pap/BIB},
  bibliographies = {GraTraVis}
}

@Book{Type1,
  author = {{Adobe Systems Incorporated}},
  title = {Adobe Type 1 Font Format},
  publisher = {Addison-Wesley},
  year = 1990,
  key = {Type 1},
  note = {The ``black book''. (Third printing, February 1993, Version 1.1)}
}

@Misc{UNIF94abstracts,
  key = {UNIF '94},
  title = {UNIF 94: Abstracts of talks},
  howpublished = {PostScript file on WWW},
  filename = {unif94abstracts.ps},
  year = 1994,
  WKloc = {A-0283},
  contents = {\def\A#1#2{{\em #1}: {#2}\\}
                \A{G\'erard Becher and Uwe Petermann}{Rigid
		  E-Unification by Completion and Rigid
		  Paramodulation}
                \A{Rachaid Echahed}{Abstract Domains of Rewriting
		  for Constructor-Based Specifications}
                \A{David Plaisted}{Rigid E-Unification and Theorem
		  Proving: Special Cases}
                \A{Franz Baader and Klaus Schulz}{A Characterization
		  of Combinable Equational Theories}
                \A{Christophe Ringeissen}{Combination of constraint
		  solvers}
                \A{Regis Curien}{Second-order AC-matching}
                \A{Vincent Padovini}{Decidability of 4th order
		  matching}
                \A{Martin Mueller}{A Constraint-based recast of
		  Hindley/Milner Polymorphic Type Inference}
                \A{Laurent Vigneron}{Profs in equational theories}
                \A{Robert Nieuwenhuis}{General E-Unification and
		  Narrowing: the refutational way}
                \A{Alexandre Boudet and Evelyne Contejean}{On the
		  syntacticity of AC unification}
                \A{Detlef Plump}{Graph Unification and Matching}
                \A{H. Ismail and P. Lecouffe}{The multi-unification
		  algorithm}
                \A{Bertrand Delsart}{Completion Algorithms for
		  Topmost General E-Unification}
                \A{Gibert Boyrau and Francoise
		  Clerin-Debart}{Associative and Unitary Unification:
		  Example of implementation}
                \A{Philippe Balbiani}{A geometrical approach to the
		  mechanization of reasoning in algebraic theories}
                \A{Leszek Pacholski}{Solving word equations}
                \A{Patricia Johann}{A Combinatory Logic approach to
		  order-sorted higher-order unification}
                \A{Gilles Dowek and Th\'er\`ese Hardin and Claude
		  Kirchner}{Higher-order unification via explicit
		  substitutions}
                \A{Denis Lugiez}{Higher-order Disunification}}
}

@Misc{UPennPLC-2004,
  author =	 {{University of Pennsylvania PL Club}},
  title =	 {Dinner with Ambiants},
  howpublished = {ICFP Programming Contest 2004 Task Description},
  month =	 JUN,
  year =	 2004,
  WKloc = 	 {A-1538}
}

@Misc{URL-PBS,
  key  = {PBS},
  title = {{PBS}: The Portable Bookshelf},
  howpublished = {URL: \url{http://swag.uwaterloo.ca/pbs/}},
  bibliographies = {OPG}
}

@Misc{URL-Rigi,
  key  = {Rigi},
  title = {Rigi},
  howpublished = {URL: \url{http://www.rigi.csc.uvic.ca/index.html}},
  bibliographies = {OPG}
}

@Misc{USPatent7100164,
 title     = "Method and apparatus for converting a concurrent control flow graph into a sequential control flow graph",
 howpublished  = {US Patent Nr.~7100164},
 author    = "Edwards, Stephen A. (San Francisco, CA, US)",
 year      = "2006",
 month     = "August",
 url       = "http://www.freepatentsonline.com/7100164.html"
}

@InProceedings{Udink-Herman-Kok-1994,
  author = {R.T. Udink and T. Herman and J.N. Kok},
  title = {Progress for Local Variables in {UNITY}},
  crossref = {PROCOMET94},
  pages = {124--143},
  keywords = {Specifying and Verifying and Reasoning about
		  Programs; Concurrent Programming}
}

@Book{Ullman-1982,
  author = {J. D. Ullman},
  title = {Principles of Database Systems},
  publisher = CompuSci,
  year = 1982,
  note = {{$2^{nd}$} Edition},
  bibliographies = {RelMiCS}
}

@Book{Ullman-1988,
  author = {J. D. Ullman},
  title = {Principles of Database and Knowledge-Base Systems},
  publisher = CompuSci,
  year = 1988,
  bibliographies = {RelMiCS}
}

@Misc{UniForM-1996,
  key = {UniForM},
  title = {{UniForM --- Universelle Entwicklungsumgebung f\"ur formale Methoden}},
  howpublished = {old WWW pages},
  year = 1996,
  WKloc = {B-0073}
}

@TechReport{Urban-Cheney-Berghofer-2008a,
  author = 	 {Christian Urban and James Cheney and Stefan Berghofer},
  title = 	 {Mechanizing the Metatheory of {LF}},
  institution =  {arXiv.org},
  year = 	 2008,
  number = 	 {arXiv:0804.1667v1 [cs.LO]},
  month = 	 APR,
  note = {Expanded technical report for LICS 2008 conference paper},
  URL = 	 {http://arxiv.org/abs/0804.1667v1},
  WKloc = {doc/pap/BIB},
  bibliographies = {HHOL},
  abstract = 	 {LF is a dependent type theory in which many other
                  formal systems can be conveniently
                  embedded. However, correct use of LF relies on
                  nontrivial metatheoretic developments such as proofs
                  of correctness of decision procedures for LF's
                  judgments. Although detailed informal proofs of
                  these properties have been published, they have not
                  been formally verified in a theorem prover. We have
                  formalized these properties within Isabelle/HOL
                  using the Nominal Datatype Package, closely
                  following a recent article by Harper and
                  Pfenning. In the process, we identified and resolved
                  a gap in one of the proofs and a small number of
                  minor lacunae in others. Besides its intrinsic
                  interest, our formalization provides a foundation
                  for studying the adequacy of LF encodings, the
                  correctness of Twelf-style metatheoretic reasoning,
                  and the metatheory of extensions to LF.}
}

@InProceedings{Urzyczyn-1994,
  title = {The Emptiness Problem for Intersection Types},
  author = {Pawe{\l} Urzyczyn},
  pages = {300--309},
  crossref = {LICS9},
  abstract = {We prove that it is undecidable whether a given intersection
      type is non-empty, i.e., whether there exists a closed term of this
      type.}
}

@InProceedings{Uselton-Smolka-1994,
  author = {A.C. Uselton and S.A. Smolka},
  title = {A Process Algebraic Semantics for Statecharts via
		  State Refinement},
  crossref = {PROCOMET94},
  pages = {262--281},
  keywords = {Refinement of Concurrent Systems}
}

@Misc{Utting-2000,
  OPTkey = {},
  author = {Mark Utting},
  title = {Data Structures for {Z} Testing Tools},
  OPThowpublished = {},
  OPTmonth = {},
  year = 2000,
  OPTnote = {},
  WKloc = {A-1116},
  keywords = {Jaza, Haskell}
}

@InProceedings{Utting-Robinson-1992,
  author = {Mark Utting and Ken Robinson},
  title = {Modular Reasoning in an Object-Oriented Refinement Calculus},
  crossref = {MPC1992},
  pages = {344--367},
  OPTabstract = {Object-oriented languages typically use late binding
		  for procedure calls on objects. This raises a
		  potential problem for prgrammers who wish to reason
		  abouttheir programs, because the effects of a
		  procedure call cannot always be determined
		  statically. In this paper we develop a simple model
		  of procedure invocation for object-oriented
		  languages based on the refinement calculus [Morgan
		  and Robinson 87] and define the minimum requirements
		  for a system to support {\em modular reasoning}. In
		  such systems, reasoning about procedure calls is
		  easier, because the behaviour of a procedure call
		  with arguments of type $T$ can be used as an
		  approximation to its behaviour on more specialised
		  arguments.}
}

@Article{Utumi-1949,
  author = {Y. Utumi},
  title = {On Hypergroups of Group Right Cosets},
  journal = OSAKA,
  volume = 1,
  year = 1949,
  pages = {73--80},
  bibliographies = {RelMiCS}
}

@InCollection{Uustalu-Vene-2002,
  author = {Tarmo Uustalu and Varmo Vene},
  title = {The Dual of Substitution is Redecoration},
  booktitle = {},
  OPTcrossref = {},
  OPTkey = {},
  pages = {99--110},
  OPTpublisher = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTtype = {},
  chapter = 9,
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  WKloc = {A-1421},
  abstract = {It is well known that type constructors of \emph{incomplete
      trees} (trees with variables) carry the structure of a monad with
      \emph{substitution} as the extension operation. Less known are the
      facts that the same is true of type constructors of \emph{incomplete
      cotrees} (=non-wellfounded trees) and that the corresponding monads
      exhibit a special structure. We wish to draw attentian to the dual
      facts which are as meaningful for functional programming: type
      constructors of \emph{decorated cotrees} carry the structure of a
      comonad with \emph{redecoration} as the coextension operation, and so
      do---even more interestingly---type constructors of \emph{decorated
      trees}.}
}

@Article{Vaeaenaenen-2001_SecondOrderLogic,
  author =       {J. V{\"a\"a}n{\"a}nen},
  title =        {Second-Order Logic and Foundations of Mathematics},
  journal =      {Bulletin of Symbolic Logic},
  year =         2001,
  volume =    7,
  number =    4,
  pages =     {504--520},
  DOI =     {10.2307%2F2687796},
  DOIURL =      {http://dx.doi.org/10.2307%2F2687796},
  JSTOR =    2687796,
  JSTORURL =    {http://www.jstor.org/stable/2687796}
}

@Article{Vagner-1967,
  author = 	 {V. V. Vagner},
  title = 	 {Diagrammatizable semigroupoids and generalized groupoids},
  journal = 	 {Izv. Vys{\v{x}}. U{\v{c}}ebn. Zaved. Matematika},
  year = 	 {1967},
  OPTkey = 	 {},
  OPTvolume = 	 {65},
  OPTnumber = 	 {10},
  OPTpages = 	 {11--23},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {first definition of semigroupoid?}
}

@InCollection{Vakarelov-1987,
  author = {Vakarelov, D.},
  title = {Abstract characterization of some knowledge
                  representation systems and the logic {NIL} of
                  nondeterministic information},
  booktitle = {Artificial Intelligence {II},
                  Methodology, Systems, Applications},
  publisher = NoHo,
  year = 1987,
  editor = {Ph. Jorrand and V. Sgurev},
  address = {Amsterdam},
  pages = {255--260},
  bibliographies = {RelMiCS}
}

@InProceedings{Vakarelov-1989,
  author = {Dimiter Vakarelov},
  title = {Modal Logics for Knowledge Representation Systems},
  pages = {257--277},
  editor = {A. R. Meyer and M. A. Taitslin},
  booktitle = {Proc.\null{} of the Sympos.\null{} on Logical Foundations of
                 Computer Science},
  conflocation = {Pereslavl-Zalessky, USSR},
  month = jul,
  year = 1989,
  series = LNCS,
  volume = 363,
  publisher = Springer,
  address = {Berlin},
  publication-year = 1989,
  bibliographies = {RelMiCS}
}

@InProceedings{Vakarelov-1991a,
  author = {D. Vakarelov},
  title = {Logical Analysis of Positive and Negative Similarity
                 Relations in Property Systems},
  pages = {491--500},
  ISBN = {2-87892-002-3},
  editor = {Michel DeGlas and Dov Gabbay},
  booktitle = {Proc.\null{} of the {{$1^{st}$} World Conf.\null{} on the
                 Fundamentals of Artificial Intelligence}},
  address = {Paris, France},
  month = jul,
  year = 1991,
  publisher = {Angkor},
  bibliographies = {RelMiCS}
}

@Article{Vakarelov-1991b,
  author = {Vakarelov, D.},
  title = {A modal logic for similarity relations in {Pawlak}
		  knowledge representation systems},
  journal = FUNDI,
  year = 1991,
  volume = 15,
  pages = {61--79},
  bibliographies = {RelMiCS}
}

@Book{Valdes-Perez-1986,
  author = {Ra\'ul E. Vald\'es-P\'erez},
  title = {Spatio-temporal Reasoning and Linear Inequalities},
  note = {A.\null{} I.\null{} Memo 875},
  publisher = {MIT Artificial Intelligence Laboratory},
  year = 1986,
  pages = {1--33},
  bibliographies = {RelMiCS}
}

@InProceedings{Valdes-Perez-1987,
  author = {Ra\'ul E. Vald\'es-P\'erez},
  title = {The Satisfiability of Temporal Constraint Networks},
  pages = {256--260},
  crossref = {AAAI1987},
  bibliographies = {RelMiCS}
}

@Article{Valdes-Tarjan-Lawler-1982,
  author = {J. Valdes and R.E. Tarjan and E.L. Lawler},
  title = {The recognition of series-parallel digraphs},
  journal = SIAMCOMP,
  year = 1982,
  volume = 11,
  number = 2,
  month = MAY,
  pages = {298--313},
  WKloc = {A-1213},
  abstract = {We present a linear-time algorithm to recognize the class of
    vertex series-parallel (VSP) digraphs. Our method is based on the
    relationship between VSP digraphs and the class of edge series-parallel
    multidigraphs. As a byproduct of our analysis, we obtain efficient
    methods to compute the transitive closure and transitive reduction
    of VSP digraphs, and to test isomorphism of minimal VSP digraphs.}
}

@InProceedings{Valentine-1998,
  author = {Samuel H. Valentine},
  title = {Inconsistency and Undefinedness in {Z} --- A Practical Guide},
  crossref = {ZUM1998},
  pages = {233--249},
  bibliographies = {RelMiS},
  WKloc = {A-1328},
  abstract = {Consistency is essential for a Z specification
         to have any useful meaning. We give some sufficient conditions
         and stylistic guidelines for achieving it and for proving
         that this has been done. We also describe the Z interpretation
         of ``undefined'' expressions, and relate this to the rules of proof.
         The paper is mainly tutorial; experience has shown
         that these issues have caused confusion.}
}

@Article{VanDenBussche-2001,
  author = {van den Bussche, Jan},
  title = {Simulation of the Nested Relational Algebra
           by the Flat Relational Algebra,
           with an Application to the Complexity of
           Evaluating Powerset Algebra Expressions},
  journal = TCS,
  volume = 254,
  number = {1--2},
  pages = {363--377},
  year = 2001,
  URL = {http://alpha.luc.ac.be/~lucp1080/},
  WKloc = {A-1103},
  bibliographies = {RelMiCS}
}

@InProceedings{VanDenBussche-VanGucht-Vansummeren-2007,
  author = {Van den Bussche, Jan and Van Gucht, Dirk and Vansummeren, Stijn},
  title = {A Crash Course on Database Queries},
  booktitle = {Proceedings of the Twenty-sixth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems},
  series = {PODS '07},
  year = {2007},
  isbn = {978-1-59593-685-1},
  location = {Beijing, China},
  pages = {143--154},
  numpages = {12},
  url = {http://doi.acm.org.libaccess.lib.mcmaster.ca/10.1145/1265530.1265551},
  doi = {10.1145/1265530.1265551},
  acmid = {1265551},
  publisher = {ACM},
  address = {New York, NY, USA},
  keywords = {XQuery, nested relational calculus, reflection, relational algebra, runtime error, typability, type inference, type system, well-definedness},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {Complex database queries, like programs in general, can "crash", i.e., can raise runtime errors. We want to avoid crashes without losing expressive power, or we want to correctly predict the absence of crashes. We show how concepts and techniques from programming language theory, notably type systems and reflection, can be adaptedto this end. Of course, the specific nature of database queries (asopposed to general programs), also requires some new methods, andraises new questions.}
}

@Book{VanRoy-Haridi-2004,
  author = {Van Roy, Peter and Seif Haridi},
  title = {Concepts, Techniques, and Models of Computer Programming},
  publisher = {Universit\'e catholique de Louvain,
                  D\'epartement d'Ing\'enierie Informatique},
  year = 2004,
  pages = {900pp+xxix},
  ISBN = {0-262-22069-5},
  publisher = {MIT Press},
  URL = {http://www.info.ucl.ac.be/people/PVR/book.html},
  abstract = {This textbook brings the computer science student a
      comprehensive and up-to-date presentation of all major programming
      concepts, techniques, and paradigms. It is designed for second-year
      to graduate courses in computer programming. It has the following
      notable features:

      \begin{itemize}

      \item Concurrency: the broadest presentation of practical concurrent
      programming available anywhere. All important paradigms are
      presented, including the three most practical ones: declarative
      concurrency, message-passing concurrency, and shared-state
      concurrency.

      \item Practicality: all examples can be run on the accompanying
      software development platform, the Mozart Programming System.

      \item Programming paradigms: the most complete integration of
      programming paradigms available anywhere.

      \item Formal semantics: a complete and simple formal semantics that
      lets practicing programmers predict behavior, execution time, and
      memory usage.

      \end{itemize}

      The book is organized around programming concepts. It starts with a
      small language containing just a few concepts. It shows how to
      design, write programs, and reason in this language. It then adds
      concepts one by one to the language to overcome limitations in
      expressiveness. In this way, it situates most well-known programming
      paradigms in a uniform framework. More than twenty paradigms are
      given, all represented as subsets of the multiparadigm language Oz.

      We list a few highlights of the book. A transaction system using
      strict two-phase locking with deadlock avoidance. A deep discussion
      of the uses and limits of declarative programming. A presentation of
      declarative concurrency, a little-known yet extremely useful and
      simple way to do concurrent programming. All the different ways to do
      encapsulation for abstract data types (most object-oriented languages
      only do a few). A mixed declarative/imperative approach to user
      interface design that is well-suited to context-sensitive interfaces.
      An efficient approach to network-transparent distributed programming.}
}

@Article{Vardi-1998,
  author = {Moshe Y. Vardi},
  title = {Computational Model Theory},
  journal = {Logic Journal of the IGPL},
  year = 1998,
  volume = 6,
  number = 4,
  OPTmonth = {},
  pages = {601--623},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0566},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Veloso-1977,
  author = {Paulo A.S. Veloso},
  title = {The History of an Error in the Theory of Representations
		of Relation Algebras},
  journal = JSYLO,
  volume = 42,
  year = 1977,
  bibliographies = {RelMiCS}
}

@Article{Veloso-1984,
  year = 1984,
  volume = 21,
  title = {Outline of a Mathematical Theory of General Problems},
  pages = {354--365},
  journal = {Philosophia Naturalis},
  author = {Paulo A. S. Veloso},
  bibliographies = {RelMiCS}
}

@InProceedings{Veloso-Haeberer-1991a,
  title = {A New Algebra of First-Order Logic},
  pages = {??--??},
  crossref = {LMPS1991},
  author = {Paulo A. S. Veloso and Armando M. Haeberer},
  bibliographies = {RelMiCS}
}

@Article{Veloso-Haeberer-1991b,
  author = {Paulo A. S. Veloso and Armando M. Haeberer},
  title = {A Finitary Relational Algebra for Classical
		  First-Order Logic},
  journal = BUPOLLOG,
  year = 1991,
  pages = {52--62},
  number = 2,
  volume = 20,
  WKloc = {A-0217},
  bibliographies = {RelMiCS}
}

@TechReport{Veloso-Haeberer-Baum-1992,
  year = 1992,
  type = {Res. Rept.},
  title = {Formal Program Construction Within an Extended
		  Calculus of Binary Relations},
  number = {MCC 19},
  note = {Submitted to an special issue on Automatic
		  Programming of the } # JSYCO,
  institution = {Pontif\'{i}cia Universidade Cat\'{o}lica do Rio
		  de Janeiro},
  author = {Paulo A. S. Veloso and Armando M. Haeberer and Gabriel A. Baum},
  bibliographies = {RelMiCS}
}

@Article{Veloso-Haeberer-Frias-1995,
  author = {Paulo A.S. Veloso and Haeberer, Armando Mart\'{\i}n and Frias, Marcelo F.},
  title = {Fork Algebras as Algebras of Logic},
  journal = Bulletin_of_Symbolic_Logic,
  year = 1995,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  month = JUN,
  pages = {265--266},
  OPTnote = {},
  OPTannote = {},
  bibliographies = {RelMiCS}
}

@InProceedings{Veloso-VelosoS-2012,
  author    = " Veloso, Paulo A. S. and Veloso, Sheila R. M. ",
  year      = "2012",
  title     = "On Graph Refutation for Relational Inclusions",
  editor    = "Ronchi della Rocca, Simona and Pimentel, Elaine",
  booktitle = "{\rm Proceedings 6th Workshop on}
               Logical and Semantic Frameworks with Applications,
               {\rm Belo Horizonte, Brazil, 27 August 2011}",
  series    = EPTCS,
  volume    = "81",
  publisher = "Open Publishing Association",
  pages     = "47--62",
  doi       = "10.4204/EPTCS.81.4",
  bibliographies = {RelMiCS},
  DOIURL = {http://dx.doi.org/10.4204/EPTCS.81.4},
  abstract = {We introduce a graphical refutation calculus for relational inclusions:
    it reduces establishing a relational inclusion to establishing
    that a graph constructed from it has empty extension.
    This sound and complete calculus is conceptually simpler and easier to use
    than the usual ones.}
}


@Article{Veltman,
  author = {Veltman, F.},
  journal = JPHIL,
  title = {Defaults in Update Semantics},
  year = {},
  note = {to appear},
  bibliographies = {RelMiCS}
}

@Booklet{Venema-1988,
  author = {Yde Venema},
  title = {Expressiveness and Completeness of an Interval Tense Logic},
  note = {Preprint, Inst.\null{} for Language, Logic, and Information, 88-02,
		Univ.\null{} Amsterdam, pp.\null{} 56},
  bibliographies = {RelMiCS}
}

@PhDThesis{Venema-1991,
  author = {Yde Venema},
  title = {Many-Dimensional Modal Logic},
  school = {Faculteit Wiskunde en Informatica, Amsterdam Univ.},
  year = 1991,
  bibliographies = {RelMiCS}
}

@InProceedings{Venema-1994,
  author = {Yde Venema},
  address = {Berlin},
  booktitle = {Temporal Logic, {{$1^{st}$} Internat.\null{} Conf., ICTL'94}},
  editor = {D.M. Gabbay and Hans J\"urgen Ohlbach},
  volume = 827,
  pages = {149--164},
  publisher = Springer,
  series = LNCS,
  title = {Completeness through flatness},
  year = 1994,
  bibliographies = {RelMiCS}
}

@InCollection{Venema-1995,
  author = {Yde Venema},
  address = {Stanford},
  booktitle = {Arrow Logic and Multi-Modal Logic},
  editor = {Marx, M. and Polos, L.},
  publisher = CSLI_P,
  series = {Studies in Logic, Language and Information},
  title = {A crash course in arrow logic},
  year = 1995,
  bibliographies = {RelMiCS}
}

@InProceedings{Verhoef-1994,
  author = {C. Verhoef},
  title = {A General Conservative Extension Theorem in Process Algebra},
  crossref = {PROCOMET94},
  pages = {144--163},
  keywords = {Concurrent Programming; Operational Semantics;
		  Algebraic Language Theory; Process Algebra}
}

@InProceedings{Verhoeven-Backhouse-1999,
  author = {Richard Verhoeven and Roland Backhouse},
  title = {Towards Tool Support for Program Verification \emph{and} Construction},
  crossref = {FM1999},
  pages = {1128--1146},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html},
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {MathSpad is a document preparation system designed and
     developed by the authors and oriented towards the calculational
     construction of programs. PVS (Prototype Verification System) is a theorem
     checker developed at SRI that has been extensively used for verifying
     software, in particular in safety-critical applications. This paper
     describes how these two systems have been combined into one. We discuss
     the potential benefits of the combination seen from the viewpoint of
     someone wanting to use formal methods for the construction of computer
     programs, and we discuss the architecture of the combined system for the
     benefit of anyone wanting to investigate combining the MathSpad system
     with other programming tools.}
}

@Article{Verma-1992,
  year = 1992,
  volume = 41,
  title = {Strings, trees, and patterns},
  pages = {157-161},
  month = MAR,
  journal = IPLET,
  author = {R. M. Verma},
  bibliographies = {RelMiCS}
}

@InProceedings{Verma-Ramakrishnan-1989,
  author = {Verma, R. M. and Ramakrishnan},
  year = 1989,
  publisher = {Springer-Verlag},
  editor = {B. Monien and R. Cori},
  booktitle = {STACS89 - 6th Annual Symposium on Theoretical Aspects of
               Computer Science}
}

@Article{Verma-Reyner-1989,
  abstract = {It is shown that the proof of the main result in
		  Reyner's paper, similarly titled, is incorrect.
		  Interestingly, by combining a simple modification of
		  the algorithm with a tighter analysis, one can
		  obtain the original result with a minor improvement.},
  year = 1989,
  volume = 18,
  title = {An Analysis of a Good Algorithm for the Subtree Problem,
           Corrected},
  pages = {906-908},
  number = 5,
  month = OCT,
  journal = {SIAM Journal on Computing},
  author = {Verma, R. M. and Reyner, S. W.}
}

@TechReport{Veroff-McCune-2000,
  author = {Robert Veroff and William McCune},
  title = {A Short {Sheffer} Axiom for Boolean Algebra},
  institution = {Argonne National Laboratory, mathematics and Computer Science Division},
  year = 2000,
  type = {Technical Memorandum},
  number = 244,
  address = {9700 South Cass Avenue, Argonne, IL 60439},
  month = JUN,
  URL = {http://www.mcs.anl.gov/\~{}mccune/ba-sheffer-15/},
  WKloc = {A-1030},
  bibliographies = {RelMiCS}
}

@InProceedings{Vestergaard-Brotherston-2001,
  author = {Ren{\'e} Vestergaard and James Brotherston},
  title = {A Formalised First-Order Confluence Proof for the $\lambda$-Calculus
           using One-Sorted Variable Names
           ({Barendregt} Was Right after all $\ldots$ almost)},
  crossref = {RTA2001},
  pages = {306--321},
  WKloc = {A-1401, doc/pap/BIB},
  abstract = {We present the titular proof development which has been
   implemented in Isabelle/HOL. As a first, the proof is conducted
   exclusively by the primitive induction principles of the standard
   syntax and the considered reduction relations: the naive way, so to
   speak. Curiously, the Barendregt Variable Convention takes on a
   central technical role in the proof. We also show (i) that our
   presentation coincides with Curry's and Hindley's when terms are
   considered equal up-to $\alpha$ and (ii) that the confluence
   properties of all considered calculi are equivalent.}
}

@Article{Vestergaard-Brotherston-2002,
  author = {Ren{\'e} Vestergaard and James Brotherston},
  title = {A Formalised First-Order Confluence Proof for the $\lambda$-Calculus using One-Sorted Variable Names},
  journal = IANDC,
  year = {2002?},
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  WKloc = {A-1378, doc/pap/BIB},
  URL = {http://www.jaist.ac.jp/~vester/Writings/vestergaard-brotherston-IandC-rta01-abstr.txt},
  note = {to appear},
  OPTannote = {}
}

@Book{Vickers-1989,
  author = {Steven Vickers},
  title = {Topology via Logic},
  publisher = CambridgeUP,
  year = 1989,
  volume = 5,
  series = Cambridge,
  UniBwM = {MAT660/5774}
}

@InProceedings{Viera-Swierstra-Lempsink-2008,
  author = {Marcos Viera and S. Doaitse Swierstra and Eelco Lempsink},
  title = {{Haskell}, do you read me?: constructing and composing efficient top-down parsers at runtime},
  crossref = {Haskell2008},
  pages = {63--74},
  DOIURL = {http://doi.acm.org/10.1145/1411286.1411296},
  URL = {http://www.cs.uu.nl/wiki/bin/view/Center/TTTAS},
  WKloc = {doc/pap/BIB}
}

@InProceedings{Vilain-Kautz-1987,
  author = {M. Vilain and H. Kautz},
  title = {Constraint Propagation Algorithms for Temporal Reasoning},
  pages = {377--382},
  crossref = {AAAI1987},
  bibliographies = {RelMiCS}
}

@InCollection{Vilain-Kautz-vanBeek-1989,
  author = {M. Vilain and H. Krautz and P. G. van Beek},
  title = {Constraint propagation algorithms for temporal reasoning},
  booktitle = {Readings in Qualitative Reasoning About Physical Systems},
  editor = {Weld and de Kleer},
  publisher = {Morgan Kaufmann},
  note = {Revised version of \cite{Vilain-Kautz-1988}},
  year = 1989
}

@TechReport{Viry-1995,
  author = {Patrick Viry},
  title = {Rewriting modulo a rewrite system},
  year = 1995,
  month = DEC,
  institution = {Dipartimento di Informatica, Universit\`a di Pisa},
  number = {TR-95-20},
  OPTaddress = {},
  WKloc = {A-0639},
  annote = {rewriting logic}
}

@TechReport{Viry-1996,
  author = {Patrick Viry},
  title = {A Rewriting Implementation of pi-calculus},
  year = 1996,
  month = MAR,
  institution = {Dipartimento di Informatica, Universit\`a di Pisa},
  number = {TR-96-30},
  OPTaddress = {},
  WKloc = {A-0640},
  annote = {rewriting logic}
}

@InProceedings{Visser-1999,
  author = {Eelco Visser},
  title = {Strategic Pattern Matching},
  crossref = {RTA1999},
  pages = {30--44},
  OPTabstract = {},
  WKloc = {A-0884}
}

@TechReport{Visser-2000,
  author = {Eelco Visser},
  title = {Language Independent Traversals for Program
                  Transformation},
  institution = {Department of Computer Science, Universiteit
                  Utrecht},
  year = 2000,
  address = {Utrecht, The Netherlands}
}

@article{Visser-2005,
  title = "A survey of strategies in rule-based program transformation systems",
  journal = JSYCO,
  volume = "40",
  number = "1",
  pages = "831--873",
  year = "2005",
  note = "Reduction Strategies in Rewriting and Programming special issue",
  issn = "0747-7171",
  DOI = "10.1016/j.jsc.2004.12.011",
  DOIURL = "http://dx.doi.org/10.1016/j.jsc.2004.12.011",
  url = "http://www.sciencedirect.com/science/article/pii/S0747717105000349",
  author = "Eelco Visser",
  keywords = "Program transformation",
  keywords = "Transformation rule",
  keywords = "Transformation strategy",
  keywords = "Program representation",
  keywords = "Term rewriting",
  keywords = "Pattern matching",
  keywords = "Extensions of term rewriting",
  keywords = "Strategy annotations",
  keywords = "Tree parsing",
  keywords = "Attribute grammars",
  keywords = "Strategy combinators",
  keywords = "Tree traversal",
  keywords = "Congruence operators",
  keywords = "Generic traversal strategies",
  keywords = "Context-sensitive rules ",
  abstract = "Program transformation is the mechanical manipulation of
                  a program in order to improve it relative to some
                  cost function and is understood broadly as the
                  domain of computation where programs are the
                  data. The natural basic building blocks of the
                  domain of program transformation are transformation
                  rules expressing a ‘one-step’ transformation on a
                  fragment of a program. The ultimate perspective of
                  research in this area is a high-level, language
                  parametric, rule-based program transformation
                  system, which supports a wide range of
                  transformations, admitting efficient implementations
                  that scale to large programs. This situation has not
                  yet been reached, as trade-offs between different
                  goals need to be made. This survey gives an overview
                  of issues in rule-based program transformation
                  systems, focusing on the expressivity of rule-based
                  program transformation systems and in particular on
                  transformation strategies available in various
                  approaches. The survey covers term rewriting,
                  extensions of basic term rewriting, tree parsing
                  strategies, systems with programmable strategies,
                  traversal strategies, and context-sensitive rules. "
}


@Article{Visser-Benaissa-1998,
  author = {Eelco Visser and Zine-el-Abidine Benaissa},
  title = {A Core Language for Rewriting},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = 1998,
  volume = 15,
  pages = 20,
  note = {URL: \urlsize{http://www.elsevier.nl/locate/entcs/volume15.html}},
  WKloc = {A-0882},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Visser-Benaissa-Tolmach-1998,
  author = {Eelco Visser and Zine-el-Abidine Benaissa and Andrew Tolmach},
  title = {Building Program Optimizers with Rewriting Strategies},
  crossref = {ICFP1998},
  pages = {13--26},
  OPTabstract = {},
  WKloc = {A-0883},
  annote = {ACM SIGPLAN Notices, 34(1):13-26, January 1999}
}

@TechReport{Visser-Vermeulen-1995,
  author = {Visser, A. and Vermeulen, K.},
  institution = {Dept.\null{} of Philosophy, Utrecht Univ.\null{}},
  title = {Dynamic Bracketing and Discourse Representation},
  year = 1995,
  bibliographies = {RelMiCS}
}

@InProceedings{Voelker-1999,
  author = 	 {Norbert V{\"o}lker},
  title = 	 {Disjoint Sums over Type Classes in {HOL}},
  crossref =	 {TPHOL1999},
  pages =	 {5--18},
  bibliographies = {HHOL},
  WKloc = 	 {doc/pap/BIB},
  abstract = {The standard versions of HOL only support disjoint
     sums over nite families of types. This paper introduces disjoint
     sums over type classes containing possibly a countably in nite
     number of monomorphic types. The result is a monomorphic sum type
     together with an overloaded function which represents the family
     of injections. Model-theoretic reasoning shows the soundness of
     the construction.

     In order to axiomatize the disjoint sums in
     HOL, datatypes are intro- duced which mirror the syntactic
     structure of type classes. The association of a type with its
     image in the sum type is represented by a HOL function carrier.
     This allows a translation of the set-theoretic axiomatization
     of disjoint sums to HOL.

     As an application, a sum type $U$ is
     presented which contains isomorphic copies of many familiar HOL
     types. Finally, a Z universe is constructed which can server as
     the basis of a HOL model of the Z schema calculus.}
}

@PhDThesis{Voermans-1999,
  author = {Ed Voermans},
  title = {Inductive Datatypes with Laws and Subtyping --- A Relational Model},
  school = {Eindhoven University of Technology},
  year = 1999,
  month = JAN,
  abstract = {Inductive datatypes, datatypes where elements of the type
      occur as ``subcomponents'' of other elements of the type, are an
      essential feature of all modern programming languages. Commonly used
      examples of such types are for example binary trees where, a tree can
      have other binary trees as subtrees, or cons-lists, where the tail of
      a cons-list is another cons-list. A standard mathematical method for
      reasoning about such datatypes and programs operating with these
      types was developed by Malcolm. He constructed an elegant generic
      theory of free inductive datatypes using category theory based on the
      concepts of functors and initial algebras. By generic we mean
      parameterised by the shape of the datatype. A limitation of this
      theory is that it only deals with free datatypes, types without rules
      specifying equality of elements or restrictions on the construction
      of elements. In practice there are many common datatypes that are not
      free. For example, join-lists have associativity laws on the join
      operation, and height-balanced trees can not be constructed using
      arbitrary subtrees. Fokkinga extended Malcolm's theory to datatypes
      with laws, but was not able to handle restrictions on the
      construction of elements (subtyping). Other, set-theoretical,
      theories about inductive datatypes can handle both laws and subtyping
      but have as disadvantage that they treat laws and subtyping as dual
      concepts. This complicates reasoning about datatypes that combine
      both laws and subtyping. An example of a type combining both concepts
      is the AVL-tree, where different trees can be used to represent the
      same set of values (law), but where it is not allowed to join two
      arbitrary AVL-trees to construct a new valid AVL tree (restriction).
      The goal of this thesis is to develop a theory about inductive
      datatypes that can handle laws and subtyping in a uniform way. The
      theory should predict when (recursively defined) operations are
      well-defined and when they are uniquely defined. The theory should
      also provide a sound basis for the construction and verification of
      generic programs. The theory of inductive datatypes presented in this
      thesis was inspired by the category-theoretical approach but uses a
      point-free relational calculus to model both datatypes and programs.
      One of the main advantages of using the relational calculus is that
      it opens up the possibility of working with lattices where extreme
      solutions to equations are uniquely defined. Category theory always
      gives solutions ``up to isomorphism'' that are often less suitable
      for direct manipulation. The extreme solutions of lattice equations
      provide unique, canonical representations of the concepts that are
      being modelled. Datatypes and programs are usually specified as
      solutions to equations Another advantage of the lattice structures
      that are available when working with relations is the abundant
      possibility for using Galois connections. Identifying Galois
      connections and using their calculational properties is a recurring
      theme throughout the thesis. We prefer a calculational style for
      constructing and presenting proofs and Galois connections are a great
      tool for this purpose. We identify a special class of relations that
      can be used as representatives for datatypes. These datatypes are the
      elements of a complete lattice where the ordering represents (the
      combination of) subtyping and quotient formation. Combining these
      aspects in a single ordering allows us to find solutions for
      specifications involving both restrictions (subtyping) and laws
      (quotients). Combining these features is often difficult in other
      formalisms for datatypes. This ordering is a vital tool for achieving
      our goal of a uniform treatment of laws and subtyping. Our datatype
      construction methods are inspired by categorical datatype theories
      and we will construct a category where objects and arrows are
      relations. Categorical notions like functors, natural transformations
      and F-algebras lead to relational constructions that are useful for
      the construction of datatypes and programs. A variant of F-algebras
      is used for the introduction of inductive datatypes and structural
      recursion. An important aspect of datatype construction is
      simulation, implementing one datatype using another datatype. The
      notion of simulation can easily be formulated in our theory.
      Inductive types that simulate each other form equivalence classes. We
      prove the remarkable result that every equivalence class contains one
      special representative.The special representatives form a complete
      lattice, using our special ordering of datatypes. The elements of the
      lattice represent all inductively defined datatypes for a given
      induction structure. Using this lattice, we can describe inductive
      datatypes with both laws and restrictions as an extreme fixpoint. We
      will give an equivalent characterization of the extreme fixpoint
      using a Galois connection. This Galois connection, which defines a
      closure operation, turns out to be very convenient for proving
      properties of inductive datatypes. Laws and restrictions can be
      specified with equations, which can be combined to a single
      specification of the datatype. Not only are datatypes described as
      solutions of equations, but recursively defined operations on these
      inductive datatypes are also specified as solutions of equations. We
      will show that a large class of ``recursion structure'' equations for
      operations on inductive datatypes have at most one solution, so they
      are suitable as a specification. Another subject investigated in this
      thesis is conditions under which parameterisation of inductive
      datatypes with laws and restrictions is possible. Here we demonstrate
      that, if the law and restriction equations satisfy certain naturality
      (``polymorphy'') criteria, parameterisation is possible.},
  URL = {http://www.cs.nott.ac.uk/~rcb/papers/abstract.html},
  bibliographies = {RelMiCS}
}

@Unpublished{Voermans-vanderWoude-1993a,
  author = {Voermans, Ed and van der Woude, Jaap},
  title = {A relational perspective on types with laws},
  crossref = {CRCS93},
  WKloc = {A-0138},
  bibliographies = {RelMiCS},
  abstract = {With relational transformational programming in mind,
		  an extension of a ``lawless'' relational theory of
		  datatypes is proposed in order to study and
		  manipulate quotient types within a Tarski-like
		  calculus of relations. The extended notion of type,
		  pertype (from partial equivalence relation), is
		  shown to admit a complete lattice structure by
		  constructing the order via a Galois connection. A
		  pertyping of relations is developed and inductive
		  pertypes generated by equations are discussed.
		  Pertypes do occur in model theory for
		  $\lambda$-calculus but we are unaware of
		  manipulations with inductive ``lawful'' types based
		  on a simple relational calculus.}
}

@Article{Vogt-Swierstra-Kuiper-1989,
  author = {H. H. Vogt and S. D. Swierstra and M. F. Kuiper},
  title = {Higher order attribute grammars},
  journal = SIGPLAN,
  volume = 24,
  number = 7,
  pages = {131--145},
  month = JUL,
  year = 1989,
  ISSN = {0362-1340},
  URL = {http://www.acm.org:80/pubs/citations/proceedings/pldi/73141/p131-vogt/},
  abstract = {A new kind of attribute grammars, called higher order
                 attribute grammars, is defined. In higher order
                 attribute grammars the structure tree can be expanded
                 as a result of attribute computation. A structure tree
                 may be stored in an attribute. The term higher order is
                 used because of the analogy with higher order
                 functions, where a function can be the result or
                 parameter of another function. A relatively simple
                 method, using OAGs, is described to derive an
                 evaluation order on the defining attribute occurrences
                 which comprises all possible direct and indirect
                 attribute dependencies. As in OAGs, visit-sequences are
                 computed from which an efficient algorithm for
                 attribute evaluation can be derived.},
  affiliationaddress = {Utrecht, Neth},
  annote = {Published as part of the Proceedings of PLDI'89.},
  classification = {721; 723},
  conference = {Proceedings of the SIGPLAN '89 Conference on
                 Programming Language Design and Implementation},
  journalabr = {SIGPLAN Not},
  keywords = {Attribute Grammars; Automata Theory; Compiler
                 Construction; Computer Operating Systems--Program
                 Compilers; Computer Programming Languages--Design;
                 design; Grammars; languages; theory},
  meetingaddress = {Portland, OR, USA},
  meetingdate = {Jun 21--23 1989},
  meetingdate2 = {06/21--23/89},
  sponsor = {ACM, Special Interest Group on Programming Languages,
                 New York; SS NY, USA},
  subject = {{\bf F.4.2} Theory of Computation, MATHEMATICAL LOGIC
                 AND FORMAL LANGUAGES, Grammars and Other Rewriting
                 Systems. {\bf F.4.3} Theory of Computation,
                 MATHEMATICAL LOGIC AND FORMAL LANGUAGES, Formal
                 Languages. {\bf D.3.4} Software, PROGRAMMING LANGUAGES,
                 Processors, Compilers.},
  WKloc = {doc/pap/BIB}
}

@InCollection{Volkov-1986,
  author = {N. D. Volkov},
  title = {The Transition from a Relation Algebra to a Halmos Algebra},
  booktitle = {Algebra and Discrete Mathematics: Theoretical
		Foundations of Software},
  note = {(Russian)},
  publisher = LatvGos,
  address = {Riga},
  year = 1986,
  bibliographies = {RelMiCS}
}

@InProceedings{Volpano-1994,
  author = {D.M. Volpano},
  title = {Haskell-style Overloading is {NP}-hard},
  crossref = {ICCL94},
  pages = {88--94},
  WKloc = {A-0391},
  abstract = {$\ldots$}
}

@InProceedings{Volpe-1994,
  author = {Paolo Volpe},
  title = {Concurrent Logic Programming as Uniform Linear Proofs},
  crossref = {ALP1994},
  pages = {133--149},
  keywords = {Linear Logic, Uniform Proofs, Concurrency, Phase
		  Semantics, Chemical Abstract Machine},
  WKloc = {A-0316},
  abstract = {\def\LL{$\cal LL$}We describe \LL, a formalism based
		  on the proof theory of linear logic, whose aim is to
		  specify concurrent computations and whose language
		  restriction (as compared to other linear logic
		  languages) provides a simpler operational model that
		  can lead a more practical langage core. The \LL{}
		  fragment is proved to be an abstract logic
		  programming langage, that is any sequent can be
		  derived by uniform proofs. The resulting class of
		  computations can be viewed in terms of multiset
		  rewriting and is reminiscent of the computations
		  arising in the Chemical Abstract Machine and in the
		  Gamma model.

                  The fragment makes it possible to give a logic
		  foundation to existing extensions of Horn clause
		  logic, such as Generalized Clauses, whose
		  declarative semantics was based on an ad hoc
		  construciton. Programs and goals in \LL{} can
		  declaratively be characterizes by a suitable
		  instance of the phase semantics of linear
		  logic. Such a model gives a full characterization of
		  the program computations and can be obtained through
		  a fixpoint construction.}
}

@Book{Vorlaender-2008,
  author =	 {Michael Vorl{\"a}nder},
  title = 	 {Auralization: Fundamentals of Acoustics, Modelling, Simulation, Algorithms and Acoustic Virtual Reality},
  publisher = 	 Springer,
  year = 	 208,
  series =	 {RWTHedition},
  ISBN = 	 {978-3-540-48829-3},
  pages = 	 {xvi + 335},
  URL = 	 {http://www.springer.com/west/home/computer/mathematics?SGWID=4-151-22-173700525-detailsPage=ppmmedia|aboutThisBook},
  bibliographies = {Vati}
}

@InProceedings{Voronkov-1990,
  author = {A. A. Voronkov},
  title = {Towards the Theory of Programming in Constructive Logic},
  crossref = {ESOP1990},
  pages = {420--435},
  OPTabstract = {We develop an approach to the theory of extracting
		  programs from proofs based on constructive semantics
		  of the first order formulas called {\em constructive
		  truth}. The underlying ideas are discussed. Using
		  this notion of truth we define an appropriate notion
		  of {\em constructive calculus}. Some results on
		  relations between our theory and well known notions
		  of constructive logics and the theory of enumerated
		  modes are proved.},
  keywords = {Curry-Howard}
}

@PhDThesis{Vos-1999,
  author = {Tanja E.J. Vos},
  title = {Diversity in {UNITY}},
  school = {Universiteit Utrecht, Department of Computer Science},
  URL = {http://www.cs.ruu.nl/people/tanja/THESIS.ps},
  year = 1999,
  WKloc = {}
}

@TechReport{Vulanovic-vonMohrenschildt-1997,
  author = 	 {Vulanovic, Igor and von Mohrenschildt, Martin},
  title = 	 {A Grand Table Interface Specification/Developer's/User's Guide},
  institution =  {Communications Research Laboratory, McMaster University},
  year = 	 1997,
  type =	 {CRL Report},
  number =	 352,
  month =	 AUG,
  abstract = {The use of tables to express information in gerneral
     is not at all a new concept. However, tables are being used
     increasingly for the representation of functions. The type of
     functions that can be expressed in tabular form vary quite
     significantly; as a result, so do the shapes of the tables
     used. For this reason, it is necessary to define a set of table
     shapes which span the entire range of useable tables. We call
     this the table space. Once given this table space, it is possible
     to develop a tool that can create and manipulate any useable
     table.

     This manual describes the design and implementation of the Grand
     Table Interface (GTI). The GTI is a tool that provides a friendly
     environment where a user can construct and modify a wide range of
     tables. }
}

@InProceedings{Vougiouklis-1988,
  author = {Thomas~N. Vougiouklis},
  title = {Groups in Hypergroups},
  pages = {459--468},
  crossref = {IGCS1988},
  bibliographies = {RelMiCS}
}

@Article{Vougiouklis-1992,
  author = {Thomas~N. Vougiouklis},
  title = {Representations of Hypergroups by Generalized Permutations},
  journal = ALGU,
  pages = {172--183},
  volume = 29,
  year = 1992,
  bibliographies = {RelMiCS}
}

@InProceedings{Vullinghs-1994,
  author = {Ton Vullinghs},
  title = {Transformational Program Development Using {CIP-S}},
  crossref = {KielTool94},
  pages = {172--186}
}

@InProceedings{Vytiniotis-Weirich-PeytonJones-2005,
  author = 	 {Dimitrios Vytiniotis and Stephanie Weirich and Peyton Jones, Simon},
  title = 	 {Boxy types: Inference for Higher-Rank Types and Impredicativity},
  crossref =	 {PLDI2006?},
  WLloc = 	 {A-1629, doc/pap/BIB},
  abstract = { Languages with rich type systems are beginning to
     employ a blend of type inference and type checking, so that the
     type inference engine is guided by programmer-supplied type
     annotations. In this paper we show, for the first time, how to
     combine the virtues of two well-established ideas:
     unification-based inference, and bidirectional propagation of
     type annotations. The result is a type system that conservatively
     extends Hindley-Milner, and yet supports both higher-rank types
     and impredicativity.}
}

@Article{Waddell-Dybvig-1998,
  author = {Oscar Waddell and R. Kent Dybvig},
  title = {Visualizing Partial Evaluation},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 24},
  WKloc = {A-0902, 91--94},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@TechReport{Wadge-1975,
  author = {William Wadge},
  title = {A Complete Natural Deduction System for the Relational
                   Calculus},
  type = {Theory of Computation Report},
  number = 5,
  institution = {Univ.\null{} of Warwick},
  year = 1975,
  bibliographies = {RelMiCS}
}

@InProceedings{Wadler-1984,
  author = {Phil Wadler},
  title = {Listlessness is Better Than Laziness: Lazy Evaluation and
		  Garbage Collection at Compile Time},
  booktitle = {Proc. 1984 ACM Symposium on LISP and Functional Programming},
  pages = {45--52},
  year = 1984
}

@Misc{Wadler-1985,
  author = {Philip Wadler},
  title = {An Introduction to Orwell (Draft)},
  year = 1985,
  WKloc = {A-0585},
  bibliographies = {FP}
}

@InProceedings{Wadler-1985a,
  author = {Philip Wadler},
  title = {How to Replace Failure by a List of Successes --- {A}
                 Method for Exception Handling, Backtracking, and
                 Pattern Matching in Lazy Functional Languages},
  crossref = {FPCA1985},
  DOI = {10.1007/3-540-15975-4_33},
  DOIURL = {http://dx.doi.org/10.1007/3-540-15975-4_33},
  pages = {113--128},
  WKloc = {A-1630},
  abstract = {Should special features for exception handling,
      backtracking, or pattern matching be included in a programming
      language? This paper presents a method whereby some programs
      that use these features can be rewritten in a functional
      language with lazy evaluation, without the use of any special
      features. This method may be of use for practicing functional
      programmers; in addition, it provides further evidence of the
      power of lazy evaluation. The method itself is straightforward:
      each term that may raise an exception or backtrack is replaced
      by a term that returns a list of values. In the special case of
      pattern matching without backtracking, the method can be applied
      even if lazy evaluation is not present. The method should be
      suitable for applications such as theorem proving using
      tacticals, as in ML/LCF.}
}

@InProceedings{Wadler-1985a,
  author = {Philip Wadler},
  title = 	 {Listlessness is better than laziness {II}: Composing listless functions},
  OPTcrossref =  {},
  OPTkey = 	 {},
  booktitle = {Proceedings of the Workshop on Programs as Data Objects},
  OPTpages = 	 {},
  year = 	 {1985},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  Citeseer = {http://citeseerx.ist.psu.edu/showciting?cid=1727615},
  OPTpublisher = {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}

@InProceedings{Wadler-1987,
  author = {Philip Wadler},
  title = {Views: A Way for Pattern Matching to Cohabit with Data Abstraction},
  crossref = {POPL1987},
  pages = {307--313},
  annote = {Pattern matching requires the underlying type to be
             visible, data abstraction requires it to be invisible.
             \begin{quotation}
                Traditionally, abstraction is achieved by refusing to
                export the representation. With views, abstraction can be
                achieved by exporting as many representations as required.
             \end{quotation}
             The two are important concepts; this paper presents
             a mechanism ---an isomorphism
             between the representation and the abstraction of a data type---
             allowing both at the same time.
             Eventually discarded in Haskell, since it may introduce
             lots of hidden computation.}
}

@InBook{Wadler-1987a,
  author = {Philip Wadler},
  title = {Efficient Compilation of Pattern-Matching},
  chapter = 5,
  pages = {78--103},
  crossref = {PeytonJones-1987},
  annote = {GFA}
}

@article{Wadler-1987_calc,
 author = {Wadler, P},
 title = {A Critique of {Abelson and Sussman} or Why Calculating is Better Than Scheming},
 journal = {SIGPLAN Not.},
 issue_date = {March 1987},
 volume = {22},
 number = {3},
 month = MAR,
 year = {1987},
 issn = {0362-1340},
 pages = {83--94},
 numpages = {12},
 DOIURL = {http://doi.acm.org/10.1145/24697.24706},
 DOI = {10.1145/24697.24706},
 acmid = {24706},
 publisher = {ACM},
 address = {New York, NY, USA},
}

@InProceedings{Wadler-1989,
  author = {Phil Wadler},
  title = {Theorems for free!},
  pages = {347--359},
  year = 1989,
  booktitle = {{Proc.\null{} 1989 ACM Conf. on Lisp and Functional Programming}},
  OPTcrossref = {FPCA-1989},
  WKloc = {A-0965},
  bibliographies = {RelMiCS, LogRel}
}

@Article{Wadler-1990-deforestation,
  author = {Philip Wadler},
  title = {Deforestation: Transforming Programs to Eliminate Trees},
  journal = TCS,
  year = 1990,
  volume = 73,
  pages = {231--248},
  WKloc = {A-0654}
}

@InProceedings{Wadler-1990-linear,
  title = {Linear Types Can Change the World!},
  pages = {561--581},
  crossref = {IFIP1990},
  author = {Philip Wadler},
  WKloc = {Q-011},
  bibliographies = {RelMiCS}
}

@InProceedings{Wadler-1990-monads,
  year = 1990,
  title = {Comprehending Monads},
  booktitle = {Proc. 1990 ACM Conference on Lisp and Functional Programming},
  publisher = {ACM},
  author = {Phil Wadler},
  WKloc = {A-0585},
  bibliographies = {FP}
}

@InProceedings{Wadler-1992,
  title = {The Essence of Functional Programming},
  pages = {1--14},
  note = {invited talk},
  crossref = {POPL1992},
  author = {Philip Wadler},
  WKloc = {A-0108, draft: A-0108a},
  authorsAddress = {wadler\@dcs.glasgow.ac.uk},
  abstract = {This paper explores the use of monads to structure
		  functional programs. No prior knowledge of monads or
		  category theory is required.

                  Monads increase the ease with wich programs may be
		  modified. They can mimic the effect of impure
		  features such as exceptions, state, and
		  continuations; and also provide effects not easily
		  achieved with such features. The types of a program
		  reflect which effects occur.

                  The first section is an extended example of the use
		  of monads. A simple interpreter is modified to
		  support various extra features: error messages,
		  state, output, and non-deterministic choice. The
		  second section describes the relation between monads
		  and continuation-passing style. The third section
		  sketches how monads are used in a compiler for
		  Haskell that is written in Haskell.},
  bibliographies = {FP}
}

@InProceedings{Wadler-1995,
  author = {Philip Wadler},
  title = {How to Declare an Imperative},
  WKloc = {A-0703},
  pages = {},
  note = {invited talk},
  year = 1995,
  booktitle = {ILPS'95},
  bibliographies = {FP}
}

@InProceedings{Wadler-1995a,
  author = {Philip Wadler},
  title = {Monads for Functional Programming},
  crossref = {AFP1995},
  WKloc = {A-1592},
  OPTpages = {},
  bibliographies = {FP}
}

@Misc{Wadler-1998,
  author = {Philip Wadler},
  title = {A Prettier Printer},
  year = 1998,
  WKloc = {A-0497},
  AuthorURL = {http://homepages.inf.ed.ac.uk/wadler/papers/prettier/prettier.pdf},
  bibliographies = {FP, EdComb}
}

@Misc{Wadler-199X,
  author = {Philip Wadler},
  title = {The Marriage of Effects and Monads},
  year = {199?},
  WKloc = {A-0791},
  abstract = {Gifford and others proposed an {\em effect} typing
      discipline to delimit the scope of computational effects within a
      program, while Moggi and others proposed {\em monads} for much the
      same purpose. Here we marry effects to monads, uniting two previosly
      separate lines of research. In particular, we show that the type,
      region, and effect system of Talpin and Jouvelot carries over
      directly to an analogous system for monads, including a type and
      effect reconstruction algorithm. The same technique should allow one
      to transpose any effect systems into a corresponding monad system.}
}

@Misc{Wadler-2000,
  OPTkey = {},
  OPTauthor = {Philip Wadler},
  OPTtitle = {Proofs are Programs: 19th Century Logic and 21st Century Computing},
  OPThowpublished = {},
  OPTmonth = NOV,
  OPTyear = 2001,
  OPTnote = {},
  WKloc = {A-1110}
}

@Article{Wadler-2007,
  author = 	 {Philip Wadler},
  title = 	 {The Girard-Reynolds isomorphism (second edition)},
  journal = 	 TCS,
  year = 	 2007,
  WKloc = 	 {doc/pap/BIB},
  volume = 	 375,
  number = 	 {1--3},
  pages = 	 {201--206},
  month = 	 MAY,
  note = 	 {http://dx.doi.org/10.1016/j.tcs.2006.12.042},
  abstract = 	 {Jean-Yves Girard and John Reynolds independently
                  discovered the second-order polymorphic lambda
                  calculus, F2. Girard additionally proved a
                  Representation Theorem: every function on natural
                  numbers that can be proved total in second-order
                  intuitionistic predicate logic, P2, can be
                  represented in F2. Reynolds additionally proved an
                  Abstraction Theorem: every term in F2 satisfies a
                  suitable notion of logical relation; and formulated
                  a notion of parametricity satisfied by well-behaved
                  models.

                  We observe that the essence of Girard's
                  result is a projection from P2 into F2, and that the
                  essence of Reynolds's result is an embedding of F2
                  into P2, and that the Reynolds embedding followed by
                  the Girard projection is the identity. We show that
                  the inductive naturals are exactly those values of
                  type natural that satisfy Reynolds's notion of
                  parametricity, and as a consequence characterize
                  situations in which the Girard projection followed
                  by the Reynolds embedding is also the identity.

                  An earlier version of this paper used a logic over
                  untyped terms. This version uses a logic over typed
                  term, similar to ones considered by Abadi and
                  Plotkin and by Takeuti, which better clarifies the
                  relationship between F2 and P2.

                  This paper uses
                  colour to enhance its presentation. If the link
                  below is not blue, follow it for the colour version.
                  \url{http://homepages.inf.ed.ac.uk/wadler}}
}

@InProceedings{Wadler-Blott-1989,
  author = {Philip Wadler and Stephen Blott},
  title = {How to make {\em ad-hoc} polymorphism less {\em ad hoc}},
  crossref = {POPL1989},
  pages = {60--76},
  WKloc = {A-0212},
  DOIURL = {http://doi.acm.org/10.1145/75277.75283},
  DOI = {10.1145/75277.75283},
  abstract = {This paper presents \emph{type classes}, a new
		  approach to {\em ad-hoc} polymorphism. Type classes
		  permit overloading of arithmetic operators such as
		  multiplication, and generalise the ``eqtype
		  variables'' of Standard ML. Type classes extend the
		  Hindley/Milner polymorphic type system, and provide
		  a new approach to issues that arise in
		  object-oriented programming, bounded type
		  quantification, and abstract data types. This paper
		  provides an informal introduction to type classes,
		  and defines them formally by means of type inference rules.}
}

@InProceedings{Wadler-Hughes-1987,
  author = {Philip Wadler and R.J.M. Hughes},
  title = {Projections for Strictness Analysis},
  crossref = {FPCA-1987},
  pages = {385--407},
  abstract = {Contexts have been proposed as a means of performing
		  strictness analysis on non-flat domains. Roughly
		  speaking, a {\em context} describes how much a
		  sub-expression will be evaluated by the surrounding
		  program. This paper shows how contexts can be
		  represented using the notion of {\em projection}
		  from domain theory. This is clearer than the
		  previous explanation of contexts over the non-flat
		  list domain. This means that recursive context
		  equations can be solved using standard fixpoint
		  techniques, instead of the algebraic manipulation
		  previously used.}
}

@PhDThesis{Wadsworth-1971,
  WKloc = {B-0002 (only second part)},
  keywords = {normal forms, termination, consistency, graph reduction,
		  call by need, laziness},
  year = 1971,
  type = {{D.Phil.} thesis},
  title = {Semantics and Pragmatics of the Lambda Calculus},
  school = {Oxford University},
  month = SEP,
  author = {Christopher Peter Wadsworth},
  bibliographies = {FP}
}

@InProceedings{Wagner-1987,
  author = {Eric G. Wagner},
  title = {Semantics of Block Structured Languages with Pointers},
  pages = {57--84},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  bibliographies = {RelMiCS},
  WKloc = {A-0073},
  abstract = {This paper presents an algebraic and categorical
		  approachto the mathematical modelling of imparative
		  programming languages. In particular we model
		  languages with block structure, records and
		  variants, user definable recursive types, and
		  pointers, etc., and with ``control constructs'' such
		  as primitive  recursion (generalized to recursive
		  types), while-do, if-then-else, and assignment. In
		  our earlier papers on this subject ([4,5,6]) we
		  showed how data types and operations can be defined
		  in an algebraic framework. In this paper we present
		  a more mathematically sophisticated version of that
		  framework, and we show how it can be used to provide
		  a new approach to languages that have block
		  structure together with objects, such as pointers,
		  which are dynamically declared and may persist
		  outside the block in which they are declared. The
		  main new mathematical concept, and the key to the
		  development, is the concept of an EDHT-category
		  which is an extension of the DHT-symmetric
		  categories introduced by Hoehnke [13] as a
		  categorical framework for partial algebras.}
}

@InProceedings{Wagner-1992,
  author = {E.G. Wagner},
  title = {Overloading and Inheritance},
  crossref = {SADT92},
  pages = {79--97},
  note = {invited paper},
  keywords = {catamorphisms}
}

@Article{Wagner-Gogolla-1996,
  author = {Annika Wagner and Martin Gogolla},
  title = {Defining Operational Behaviour of Object Specifications by
		  Attributed Graph Transformations},
  journal = {Fundamenta Informaticae},
  volume = 26,
  pages = {407--431},
  year = 1996,
  publisher = {IOS Press},
  WKloc = {doc/pap/BIB}
}

@InCollection{Wagner-Gogolla-1999,
  author = {Annika Wagner and Martin Gogolla},
  title = {Semantics of Object-Oriented Languages},
  crossref = {HBGraTraII},
  pages = {181--211},
  chapter = 4,
  keywords = {TROLL light, UML}
}

@Misc{Wainer-1995a,
  OPTkey = {},
  OPTauthor = {Stan S. Wainer},
  OPTtitle = {Basic Proof Theory and Applications to Computation},
  OPThowpublished = {International Summer School Marktoberdorf,
		  Working Material},
  OPTyear = 1995,
  OPTmonth = JUL,
  OPTnote = {},
  OPTwkloc = {A-0411},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Wakeling-1993,
  author = {David Wakeling},
  title = {The Dynamic Compilation of Lazy Functional Programs},
  journal = {J. Functional Programming},
  year = 1993,
  volume = 1,
  number = 1,
  month = JAN,
  pages = {1--21},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0519},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Wakeling-1995,
  author = {David Wakeling},
  title = {A Throw-Away Compiler for a Lazy Functional Language},
  crossref = {FujiInWorkshow1995},
  pages = {287--300},
  OPTabstract = {},
  WKloc = {A-0520}
}

@Misc{Wakeman-Jeffrey-Graves-Owen-1998,
  author = {Ian Wakeman and Alan Jeffrey and Rory Graves and Tim Owen},
  title = {Designing a Programming Language for Active Networks},
  year = 1998,
  WKloc = {A-0753}
}

@TechReport{Walicki-2005a,
  author = 	 {Micha\l Walicki},
  title = 	 {Universal Multialgebra},
  institution =  {Department of Informatics, University of Bergen},
  year = 	 2005,
  type =	 {Reports in Informatics},
  number =	 292,
  month =	 FEB,
  URL = 	 {http://www.ii.uib.no/~michal/marcin/UniMulti.pdf},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@TechReport{Walicki-Bialasik-1997,
  author = {Micha\l Walicki and Marcin Bia{\l}asik},
  title = {Relations, Multialgebras and Homomorphisms},
  institution = {Institute of Computer Science, Polish Academy of Sciences},
  year = 1997,
  number = 838,
  URL = {http://www.ii.uib.no/~michal/marcin/09waw/techrep.ps},
  bibliographies = {RelMiCS},
  WKloc = {A-1337, doc/pap/BIB},
  note = {short version as ``Categories of Relational Structures'' in LNCS 1376 (\cite{WADT1997})},
  abstract = {In the study of universal algebra, the central place
      occupies the pair of ``dual'' notions of congruence and homomorphism:
      every congruence on an algebra induces a homomorphism into a quotient
      and every homomorphism induces a congruence on the source algebra.
      Categroical approach attempts to express \emph{all} (internal)
      properies of algebras in (external) terms of homomorphisms. When
      passign to relational structures, however, the close correspondence
      of these internal and external aspects seems to get lost.}
}

@InProceedings{Walicki-Bialasik-1997wadt,
  author = {Micha\l Walicki and Marcin Bia{\l}asik},
  title = {Categories of Relational Structures},
  crossref = {WADT1997},
  URL = {http://www.ii.uib.no/~michal/marcin/09waw/wadt.ps},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  annote = {short version of \cite{Walicki-Bialasik-1997}},
  abstract = {In the study of universal algebra, the central place
      occupies the pair of ``dual'' notions of congruence and homomorphism:
      every congruence on an algebra induces a homomorphism into a quotient
      and every homomorphism induces a congruence on the source algebra.
      Categroical approach attempts to express \emph{all} (internal)
      properies of algebras in (external) terms of homomorphisms. When
      passign to relational structures, however, the close correspondence
      of these internal and external aspects seems to get lost.}
}

@Article{Walicki-Meldal-1995,
  author = {Michal Walicki and Sigurd Meldal},
  title = {A Complete Calculus for the Multialgebraic and Functional Semantics of Nondeterminism},
  journal = {{ACM} Transactions on Programming languages and Systems},
  year = 1995,
  volume = 17,
  number = 2,
  month = MAR,
  pages = {366--393},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0854},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{Walker-1991,
  title = {$\pi$-Calculus Semantics of Object-Oriented Programming Languages},
  author = {David Walker},
  pages = {532--547},
  crossref = {TACS1991},
  abstract = {The $\pi$-calculus provides a foundation for the study of
		  computational systems with evolving communication
		  structure.  A system is viewed as a collection of
		  agents which may share named communication links.
		  Agents interact by passing to one another along
		  shared links the names of other links.  Semantics
		  for a pair of parallel object-oriented programming
		  languages are presented by translation into the
		  $\pi$-calculus.  The semantics are compared briefly
		  with existing semantics of related languages.}
}

@InProceedings{Walker-1994,
  author = {D. Walker},
  title = {Algebraic Proofs of Properties of Objects},
  crossref = {ESOP1994},
  pages = {501--516}
}

@InProceedings{Walker-Morriset-2000,
  author = {David Walker and Greg Morriset},
  title = {Alias Types for Recursive Data Structures},
  crossref = {TIC2000},
  pages = {177--206}
}

@InProceedings{Wallace-Runciman-1999,
  author = {Malcolm Wallace and Colin Runciman},
  title = {{Haskell} and {XML}: Combinators for Generic Document Processing},
  WKloc = {A-0644},
  crossref = {ICFP1999}
}

@InProceedings{Walshaw-2000,
  author = {C. Walshaw},
  title = {A Multilevel Algorithm for Force-Directed Graph Drawing},
  crossref = {GD2000},
  pages = {171--182},
  annote = {short version of \cite{Walshaw-2000a}},
  URL = {http://www.gre.ac.uk/~c.walshaw/papers/node1.html#WalshawISGD00}
}

@TechReport{Walshaw-2000a,
  author = {Chris Walshaw},
  title = {A Multilevel Algorithm for Force-Directed Graph Drawing},
  institution = {School of Comp uting and Mathematical Sciences, University of Greenwich},
  year = 2000,
  type = {Mathematics Research Report},
  number = {00/IM/60},
  address = {London},
  month = APR,
  WKloc = {A-1211, doc/pap/BIB},
  annote = {long version of \cite{Walshaw-2000}},
  abstract = {We describe a heuristic method for drawing graphs which uses
      a multilevel technique combined with a force directed placement
      algorithm. The multilevel process groups vertices to form clusters,
      uses the clusters to define a new graph and is repeated until the
      graph size falls below some threshold. The coarsest graph is then
      given an initial layout and the layout is successively optimised on
      all the graphs starting with the coarsest and ending with the
      original. In this way the multilevel algorithm both accelerates and
      gives a more global quality to the force directed placement. The
      algorithm can compute both 2 & 3 dimensional layouts and we
      demonstrate it on a number of examples ranging from 500 to 100,000
      vertices. It is also very fast and can compute a layout in around 30
      seconds for a 10,000 vertex graph to around 10-20 minutes for the
      larger graphs.},
  URL = {http://www.gre.ac.uk/~c.walshaw/papers/node1.html#WalshawISGD00}
}

@Article{Walters-1968,
  author = {R.F.C. Walters},
  title = {Alternative derivation of some regular continued fractions},
  journal = JAuMS,
  year = 1968,
  pages = {205-212},
  volume = 8
}

@Article{Walters-1971,
  author = {R.F.C. Walters},
  title = {A categorical approach to universal algebra},
  journal = LNM,
  year = 1971,
  OPTpages = {},
  volume = 106
}

@Article{Walters-1981,
  author = {R.F.C. Walters},
  title = {Sheaves and {Cauchy}-Complete Categories},
  journal = CTGD,
  volume = 22,
  year = 1981,
  pages = {282--286}
}

@Article{Walters-1982,
  author = {R.F.C. Walters},
  title = {Sheaves on Sites as {Cauchy}-Complete Categories},
  journal = JPAA,
  volume = 24,
  year = 1982,
  pages = {95--102}
}

@Book{Walters-1987,
  author = {R.F.C. Walters},
  title = {Number Theory: an Introduction},
  year = 1987,
  publisher = {Carslaw Publications}
}

@Article{Walters-1989,
  author = {R.F.C. Walters},
  title = {Data Types in a Distributive Category},
  journal = BuAuMS,
  year = 1989,
  month = AUG,
  pages = {79--82},
  volume = 40,
  number = 1
}

@Article{Walters-1989a,
  author = {R.F.C. Walters},
  title = {A note on context free languages},
  journal = JPAA,
  year = 1989,
  pages = {199--203},
  volume = 62
}

@Article{Walters-1989b,
  author = {R.F.C. Walters},
  title = {The free category with products on a multigraph},
  journal = JPAA,
  year = 1989,
  pages = {205--210},
  volume = 62
}

@Article{Walters-1989c,
  author = {R.F.C. Walters},
  title = {What is functional programming?},
  journal = {Pure Mathematics Research Reports, University of Sydney},
  year = 1989,
  OPTpages = {},
  volume = 89,
  number = 1
}

@Book{Walters-1992,
  author = {R. F. C. Walters},
  email = {walters_b@maths.su.oz.au},
  title = {Categories and Computer Science},
  series = {Cambridge Computer Science Texts},
  volume = {28},
  publisher = {Cambridge University Press},
  month = aug,
  year = 1991,
  errata = {file://maths.su.oz.au/sydcat/books/ccs.corrections},
  McMaster = {QA 169 .W35 1991},
  sjb = {Easier to get to grips with than any other category
		  theory book I've looked at.  The hardback edition is
		  around 30 pounds, whereas the paperback is only
		  around 12.  Previously published by Carslaw
		  Publications in Australia with the ISBN 1 875399 01
		  1.  Note the corrections file is currently empty.},
  abstract = {This book  has its origins in an undergraduate course on
		  Categories  and  Computer Science  given  by  Bob
		  Walters over the past  three  years to
		  approximately 200 students at the  University of
		  Sydney. The course is a  gentle  introduction  to
		  category theory, with motivating examples form
		  computer science.

                  Some  computer science topics dealt with are:
		  grammar and  languages, data types, Boolean algebra,
		  circuit theory, flow charts, imperative programming,
		  function specification, and categorical algorithms.},
  contents = {Algebra of Functions
              Products and Sums
              Distributive Categories
              Data Types
              Categories of Functors
              More about Products
              Computational Category Theory},
  annote = {p. 55: ``Remark: It is never precise enough in a category
                     just to say that $A$ is isomorphic to $B$.
                     It is important to know the specific isomorphism.''}
}

@Article{Walters-1992a,
  title = {An imperative language based on distributive categories},
  author = {R. F. C. Walters},
  pages = {249--256},
  journal = MSCS,
  month = SEP,
  year = 1992,
  volume = 2,
  number = 3,
  WKloc = {A-0879}
}

@Book{WalukiewiczS-1991,
  author =	 {Stanis{\l}aw Walukiewicz},
  title = 	 {Integer programming},
  publisher = 	 {Kluwer Academic Publishers},
  year = 	 1991,
  volume =	 46,
  series =	 {Mathematics and its Applications (East European series)},
  McMaster = 	 {T 57.74 .W37 1991}
}

@InProceedings{Walukiewicz-1995,
  author = {I. Walukiewicz},
  title = {Completeness of {Kozens} axiomatization of the propositional
      $\mu$-calculus},
  year = 1995,
  booktitle = {Annual Sympos.\null{}  on Logic in Computer Science},
  publisher = IEEE,
  bibliographies = {RelMiCS}
}

@TechReport{Walz-1989,
  bibdate = {October 14, 1993},
  type = {Technical Report},
  number = {TR89-968},
  title = {Extending Attribute Grammar and Type Inference
                 Algorithms},
  year = 1989,
  month = feb,
  institution = {Cornell University, Computer Science Department},
  copyright = {Janet Ann Walz 1988 - All Rights Reserved},
  pages = 120,
  author = {Janet Ann Walz},
  WKloc = {B-0094},
  abstract = {Gated attribute grammars and error-tolerant
                 unification expand upon the usual views of attribute
                 grammars and unification. Normally, attribute grammars
                 are constrained to be noncircular; gated attribute
                 grammars allow fairly general circularities. Most
                 unification algorithms do not behave well when given
                 inconsistent input; the new unification paradigm
                 proposed here not only tolerates inconsistencies but
                 extracts information from them. The expanded views
                 prove to be useful in interactive language-based
                 programming environments. Generalized unification
                 allows the environment to help the user find the
                 sources of type errors in a program, while gated
                 attribute grammars allow the environment to provide an
                 interpreter for incremental reevaluation of programs
                 after small changes to the code. The defining feature
                 of gated attribute grammars is the appearance of a gate
                 attribute (indicating where cycle evaluation should
                 begin and end) within every cycle. Attributes are
                 ordered by collapsing strongly connected components in
                 the dependency graph and topologically sorting the
                 result. The smaller dependency graph for each component
                 (ignoring edges leading to the gate) can be recursively
                 collapsed to provide further ordering. use of the
                 evaluation order defined in this manner allows gated
                 attribute grammars to do without the restrictions on
                 functions within a component needed by the other
                 varieties of circular attribute grammars. Initial and
                 incremental evaluation algorithms are given, as well as
                 a sample grammar allowing an editor for a small
                 language to become an incremental interpreter. Counting
                 unification defines unique solutions to sets of input
                 equations that contain conflicting type information.
                 These solutions are derived from the potential variable
                 constraints implied by the input equations. For each
                 type variable, each branch (a portion of a constraint)
                 is assigned a weight indicating the number of times the
                 input set implied such a constraint. When the input
                 equations are derived from the static analysis of a
                 program, the relative branch weights for a conflicting
                 variable give the overall pattern of uses of that
                 variable and can direct attention to parts of the
                 program that disagree with the majority of uses. A
                 number of error-tolerant unification algorithms are
                 presented.},
  language = {English}
}

@InProceedings{Walz-Johnson-1986,
  author = {Janet Ann Walz and Gregory F. Johnson},
  title = {A Maximum Flow Approach to Anomaly Isolation in
                 Unification-Based Incremental Type Inference},
  booktitle = {Conference Record of the Thirteenth Annual {ACM}
                 Symposium on Principles of Programming Languages},
  organization = {ACM},
  publisher = {ACM},
  month = jan,
  year = 1986,
  pages = {44--57},
  refs = 12,
  checked = 19940410,
  sjb = {Describes a language-based editor for a variant of ML
                 that used a novel approach to the isolation of likely
                 causes of errors. As the user edits their program,
                 unification is incrementally applied to determine type
                 correctness. If an error is detected, a maximum flow
                 technique is applied to the set of type equations to
                 determine the most likely cause of the error. Includes
                 pseudo-code of the various algorithms involved.}
}

@TechReport{Wand-1983,
   author = "Mitchell Wand",
   title = "A Semantic Algebra for Logic Programming",
   institution = "Indiana University",
   number = "TR-148",
   month = AUG,
   year = "1983",
   bibliographies = {PMC}
}

@InProceedings{Wand-1986,
  author = {Mitchell Wand},
  title = {Finding the Source of Type Errors},
  crossref = {POPL1986},
  pages = {38--43},
  sjb = {Includes Schemeish code for the main algorithms.},
  abstract = {It is a truism that most bugs are detected only at a
                 great distance from their source. Although polymorphic
                 type-checking systems like those in ML help greatly by
                 detecting potential run-time type errors at
                 compile-time, such systems are still not very helpful
                 for locating the source of a type error. Typically, an
                 error is reported only when the type-checker can
                 proceed no further, even though the programmer's actual
                 error may have occurred much earlier in the text. We
                 describe an algorithm which appears to be quite helpful
                 in isolating and explaining the source of type errors.
                 The algorithm works by keeping track of the {\em
                 reasons} the checker makes deductions about the type of
                 variables.},
  keywords = {FP, functional programming, type, types, error,
                 errors, message}
}

@InProceedings{Wand-1993,
  author = {Mitchell Wand},
  title = {Specifying the Correctness of Binding-Time Analysis},
  pages = {137--143},
  abstract = {Mogensen has exhibited a very compact partial evaluator for
             the pure lambda calculus, using binding-time analysis followed
             by specialization. We give a correctness criterion for this
             partial evaluator and prove its correctness relative to this
             specification. We show that the conventional properties of
             partial evaluators, such as the Futamura projections, are
             consequences of this specification. By considering both a flow
             analysis and the transformation it justifies together, this
             proof suggests a framework for the incorporating flow analyses
             into verified compilers.},
  crossref = {POPL1993},
  WKloc = {A-0191}
}

@Book{Wang-1974,
  author = {Hao Wang},
  title = {From Mathematics to Philosophy},
  publisher = {Routledge},
  year = 1974
}

@Misc{Wang-1998,
  author = {Daniel C. Wang},
  title = {{\tt asdlGen} Reference Manual},
  year = 1998,
  WKloc = {A-0480}
}

@Misc{Wang-Appel-Korn-Serra-199X,
  author = {Daniel C. Wang and Andrew W. Appel and Jeff L. Korn and Christopher S. Serra},
  title = {The {Zephyr} Abstract Syntax Description Language},
  year = {199X},
  WKloc = {A-0479}
}

@TechReport{Wang-Jeong-Zhang-Shasha-1991,
  year = 1991,
  type = {Technical Report},
  title = {Reference Manual for {ATBE} -- A Tool for Approximate Tree
           Pattern Matching},
  number = 551,
  note = {Alpha Version 1.0},
  month = {March},
  institution = {New York University, Dept. of Comp. Science,
                 Courant Institute of Mathematical Sciences},
  author = {J. T.-L. Wang and K. Jeong and K. Zhang and D. Shasha}
}

@InProceedings{WangQian-Gupta-2003,
  author = {Qian Wang and Gopal Gupta},
  title = {Provably Correct Code Generation for High Assurance Systems via Partial Evaluation: A Case Study},
  crossref = {LOPSTR2003},
  OPTpages = {},
  bibliographies = {Anand},
  abstract = {Provably correct compilation is an important aspect in
     development of high assurance software systems. Traditional
     approaches to proving correctness have either relied on generating
     the compiler automatically from the denotational semantics of a
     programming language or treating the compiler as any other computer
     program and manually proving the correctness of this program using
     traditional verification techniques. In this paper we present an
     approach based on {\it Horn logical semantics} of programming
     languages and partial evaluation. The Horn Logical semantics of a
     programming language ${\cal L}$ {\it automatically} yields an
     interpreter for ${\cal L}$. This interpreter coupled with partial
     evaluation is then used to {\it automatically} generate target
     code. This code-generation process is provably correct since it
     relies on the source language's formal semantics. The advantage of
     our approach is its simplicity, due to its basis in Horn logic. We
     also introduce Definite Clause Semantics that are more suited for
     partial evaluation. We illustrate our approach by developing a
     complete semantics for the SCR (Software Cost Reduction)
     specification language, and using it to (automatically) generate
     target code in a provably correct manner.}
}

@Article{WangXueping-2001,
  author = 	 {Xue-ping Wang},
  title = 	 {Method of solution to fuzzy relation equations in a complete
                   Brouwerian lattice},
  journal = 	 {Fuzzy Sets and Systems },
  year = 	 2001,
  volume =	 120,
  pages =	 {409­-414},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = { Up to now, how to solve a fuzzy relation equation in
     a complete Brouwerian lattice is still an open problem as Di Nola
     et al. point out. To this problem, the key problem is whether
     there exists a minimal element in the solution set when a fuzzy
     relation equation is solvable. In this paper, we first show that
     there is a minimal element in the solution set of a fuzzy
     relation equation $A \odot X = b$
     (where $A = (a_1, a_2, \ldots , a_n )$ and $b$ are known,
     and $X = (x_1, x_2, \ldots, x_n )^{T}$ is unknown)
     when its solution set is nonempty, and $b$ has an
     irredundant finite join-decomposition. Further, we give the method
     to solve $A \odot X = b$ in a complete Brouwerian lattice under
     the same conditions. Finally, a method to solve a more general
     fuzzy relation equation in a complete Brouwerian lattice when its
     solution set is nonempty is also given under similar
     conditions.},

  keywords = {Fuzzy relation; Fuzzy relation equation;
    Fuzzy elementary equation;
    Brouwerian lattice; Minimal solution; Solution set}
}

@Misc{Wansbrough-1998,
  author = {Keith Wansbrough},
  title = {Macros and Preprocessing in Haskell},
  year = 1999,
  URL = {http://www.cl.cam.ac.uk/users/kw217/research/misc/hspp-hw99.ps.gz},
  WKloc = {A-0864}
}

@Misc{Wansbrough-1998a,
  author = {Keith Wansbrough},
  title = {Instance Declarations are Universal},
  year = 1998,
  month = JUL,
  note = {Short WWW note that presents a construction that encodes a Turing machine directly in a series of instance declarations.},
  WKloc = {A-0547}
}

@InProceedings{Wansbrough-PeytonJones-2000,
  author = {Keith Wansbrough and Peyton Jones, Simon},
  title = {Simple Usage Polymorphism},
  booktitle = {ICFP 2000 ?},
  OPTcrossref = {},
  OPTpages = {},
  year = 2000,
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = {},
  WKloc = {A-1176}
}

@Book{Wansing-1993,
  author = {Heinrich Wansing},
  title = {The Logic of Information Structures},
  publisher = {Springer},
  year = 1993,
  volume = 681,
  series = {LNAI},
  address = {Berlin},
  note = {revised dissertation FU Berlin}
}

@InProceedings{Wansing1992????,
  PRICE = {DM 142},
  ISBN = {3-540-57182-5, 0-387-57182-5},
  organization = {Gesellschaft f\"ur Informatik (GI), Fachgruppe
      2.5.1/9.4.3 Information Retrieval; Univ. Regensburg, Fachgebiet
      Linguistische Informationswissenschaft (LIR)},
  author = {Heinrich T. Wansing},
  address = {M\"unchen, Wien}
}

@Article{Ward-Dilworth-1939,
  author = {Ward, M. and Dilworth, R.P.},
  title = {Residuated lattices.},
  journal = TRAMS,
  year = 1939,
  volume = 45,
  pages = {335--354},
  bibliographies = {RelMiCS}
}

@Misc{Wassyng-2001a,
  author = {Alan Wassyng},
  title = {McMaster University Software Engineering 3E03: Design and Selection of Sequential Programming Languages},
  howpublished = {Lecture slides},
  month = DEC,
  year = 2001,
  WKloc = {A-1436}
}

@InProceedings{Wassyng-Janicki-2003,
  author =       {Alan Wassyng and Ryszard Janicki},
  title =        {Using Tabular Expressions},
  booktitle = {Proceedings of International Conference on Software and Systems Engineering and their Applications, {ICSSEA '03, Paris}},
  pages =     {1--17},
  year =      2003,
  volume =    4,
  month =     DEC,
  bibliographies = {Tables},
  annote =    {Appears as ``Tabular Expressions in Software Engineering'' (16 pages) on Ryszard's SE3RA3 page.},
  WKloc = {A-1722, doc/pap/BIB},
  abstract = {Tabular expressions (tables) have been used
    in the software development process for more than twenty years.
    In addition, research has been ongoing to develop
    semantics for tabular expressions.
    At this stage in the history of tabular expressions
    we see a slight split
    between those working to improve the semantic understanding of tables,
    and those using tables to document real industrial software projects.
    Those conducting research in the semantics of tables
    have their sights set on expressive mathematical models of tables
    that are general enough to cope with all known forms of tabular expressions,
    and that can be used as the basis for software tool support of tables.
    The other group uses tables in real-world projects
    under the normal constraints of schedule, budget
    and the skill set of current software professionals.
    Thus, champions of tabular expressions for use in industry
    are focussed on developing notations and approaches
    that will be accepted and useful in an industrial setting.
    To bridge both worlds,
    we present here a brief history of tabular expressions,
    motivation for the development of semantics for tables,
    a description of a semantic definition of tables,
    and a discussion on notation and use of tables in practice
    based on many years of experience
    of using tables in industrial software development projects.}
}

@InProceedings{Wassyng-Lawford-2003,
  author = {Alan Wassyng and Mark Lawford},
  title = {Lessons Learned from a Successful Implementation of Formal Methods in an Industrial Project},
  crossref =  {FM2003},
  pages = 	 {133--153},
  WKloc = 	 {doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {This paper describes the lessons we learned over a
     thirteen year period while helping to develop the shutdown
     systems for the nuclear generating station at Darlington,
     Ontario, Canada. We begin with a brief description of the project
     and then show how we modified processes and notations developed
     in the academic community so that they are acceptable for use in
     industry. We highlight some of the topics that proved to be
     particularly challenging and that would benefit from more
     in-depth study without the pressure of project deadlines.}
}

@InProceedings{Wassyng-Lawford-HuXiayong2005,
  author = {Alan Wassyng and Mark Lawford and Hu Xiayong},
  title = {Timing Tolerances in Safety-Critical Software},
  crossref =  {FM2005},
  OPTpages = 	 {},
  WKloc = 	 {A-1594, doc/Colleagues/Wassyng},
  bibliographies = {RelMiCS, OPG},
  abstract = {Many safety-critical software applications are hard
     real-time systems. They have stringent timing requirements that
     have to be met.  We present a description of timing behaviour
     that includes precise defi- nitions as well as analysis of how
     functional timing requirements interact with performance timing
     requirements, and how these concepts can be used by software
     designers. The definitions and analysis presented ex- plicitly
     deal with tolerances in all timing durations. Preliminary work
     indicates that some requirements may be met at significantly
     reduced CPU bandwidth through reduced variation in cycle time.},
}

@Article{Watson-Zwaan-1996,
  author = {B.W. Watson and G. Zwaan},
  title = {A Taxonomy of Sublinear Multiple Keyword Pattern Matching Algorithms},
  journal = SCICOP,
  year = 1996,
  volume = 26,
  WKloc = {A-1465 (Eindhoven Computer Science Report 95/13)},
  pages = {85--118}
}

@InProceedings{Watt-1987,
  WKloc = {A-0069},
  abstract = {Action semantics is a form of denotational semantics
		  that is based on abstract semantic algebras rather
		  than Scott domains and $\lambda$-notation. It allows
		  formal descriptions of programming languages to be
		  written that are unusually readable and modular.
		  This paper presents an action-semantic description
		  of Standard ML, as evidence for the claimed merits
		  of action semantics. Milner's structural operational
		  semantics of the same language is used as a basis
		  for comparison.},
  title = {An Action Semantics of Standard ML},
  pages = {572--598},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {David A. Watt}
}

@Book{Watt-1990,
  author = {David A. Watt},
  title = {Programming Language Concepts and Paradigms},
  publisher = {Prentice Hall},
  year = 1990,
  series = PrenticeCS,
  McMaster = {QA 76.7 .W39 1990},
  bibliographies = {SE3E}
}

@Book{Watt-1991,
  author = {David A. Watt},
  title = {Programming Language Syntax and Semantics},
  publisher = {Prentice Hall},
  year = 1991,
  series = PrenticeCS,
  McMaster = {QA 76.7 .W4 1991},
  bibliographies = {SE3E}
}

@InProceedings{Webber-1997,
  author = {Adam Brooks Webber},
  title = {Program Analysis Using Binary Relations},
  crossref = {PLDI1997},
  pages = {249--260},
  URL = {http://doi.acm.org/10.1145/258915.258938},
  WKloc = {A-1371, doc/pap/BIB},
  abstract = {This paper presents a method called \emph{relational constraint}
   for finding binary relations among the variables and constants of a program.
   The method constructs a table of binary relations
   and treats the program as a
   collection of constraints on tuples of relations in the table.
   An experimental optimizer called Thinner uses this method
   to analyze programs of size $n$ in $O(n^2)$ time.},
  bibliographies = {RelMiCS}
}

@InProceedings{Webber-2001,
  author = {Adam Brooks Webber},
  title = {What is a class invariant?},
  pages = {86--89},
  crossref = {PASTE2001},
  URL = {http://doi.acm.org/10.1145/379605.379685},
  WKloc = {A-1356, doc/pap/BIB},
  abstract = {This paper is a progress report on our research
     into the problem of automatically identifying and using
     class invariants in object-oriented programs.
     We introduce an example of a class invariant in Java,
     and review applications for class invariants
     in software engineering tools and in compiler optimization.
     We then focus on an elementary problem of definition:
     what is a class invariant?
     This question gives an interesting perspective
     on programming language design.
     We conclude that there are many reasonable categories of class invariants,
     and that the problem of finding good definitions
     cannot be solved by a thought experiment.
     The only way to choose good categories
     is to experiment with the analysis of real programs,
     to see which categories of class invariant are actually useful.
     Our current research focuses on these experiments.},
  bibliographies = {RelMiCS}
}

@InProceedings{Weber-1990,
  title = {Formalization of the {Bird-Meertens} Algorithmic Calculus in the {Deva} Meta-Calculus},
  pages = {201--231},
  crossref = {IFIP1990},
  author = {Matthias Weber},
  WKloc = {Q-011},
  bibliographies = {RelMiCS, RelMiS}
}

@PhDThesis{Weber-1993,
  author = {Franz Weber},
  title = {{Softwareentwicklung mit Logik h\"oherer Stufe}},
  school = {Universit\"at Karlsruhe},
  year = 1993,
  type = {Doktorarbeit},
  directory = {~kahl/doc/pap/fzi/ho-unification},
  filename = {Diss.F.Weber.ps},
  WKloc = {B-0015},
  keywords = {HOL, E-unification, higher order logic, software
		  engineering, theory reduction, non clause reduction}
}

@Article{Weber-Amjad-2009,
  author  = {Tjark Weber and Hasan Amjad},
  title   = {Efficiently Checking Propositional Refutations in {HOL} Theorem Provers},
  journal = {Journal of Applied Logic},
  volume  = {7},
  number  = {1},
  pages   = {26--40},
  month   = mar,
  year    = {2009},
  WKloc = {doc/pap/BIB},
  DOI     = {10.1016/j.jal.2007.07.003},
  DOIURL     = {http://dx.doi.org/10.1016/j.jal.2007.07.003},
  abstract = {This paper describes the integration of zChaff and MiniSat,
    currently two leading SAT solvers,
    with Higher Order Logic (HOL) theorem provers.
    Both SAT solvers generate resolution-style proofs
    for (instances of) propositional tautologies.
    These proofs are verified by the theorem provers.
    The presented approach significantly improves
    the provers' performance on propositional problems,
    and exhibits counterexamples for unprovable conjectures.
    It is also shown that LCF-style theorem provers can serve
    as viable proof checkers even for large SAT problems.
    An efficient representation of the propositional problem
    in the theorem prover turns out to be crucial;
    several possible solutions are discussed.}
}

@InProceedings{Weber-Bloom-Brown-1992,
  author = {Sam Weber and Bard Bloom and Geoffrey Brown},
  title = {Compiling {Joy} into Silicon: An Exercise in Applied
		  Structural Operational Semantics},
  crossref = {REX92},
  pages = {639--659},
  authorsAddress = {Cornell University},
  abstract = {We present the highlights of an algorithm and
		  correctness proof for compiling programs written in
		  Joy, a simple parallel language, into
		  delay-insensitive circuits. The proof relies heavily
		  on techniques of structural operational semantics,
		  many of which should generalize to similar settings.}
}

@Article{Weck-1997,
  author = {Wolfgang Weck},
  title = {Document-Centered Computing: Compound Document Editors as User Interfaces},
  journal = {J. Symbolic Computation},
  year = 1997,
  OPTkey = {},
  volume = 11,
  OPTnumber = {},
  OPTmonth = {},
  pages = {1--24},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0509},
  OPTabstract = {},
  OPTcontents = {},
  annote = {HOPS LitProg}
}

@InCollection{Weck-1997,
  author = {Wolfgang Weck},
  title = {An Abstract Data Type for Freezable Lists and {DAG}s},
  year={1997},
  isbn={978-3-540-62599-5},
  booktitle={Modular Programming Languages},
  volume={1204},
  series=LNCS,
  editor={Mössenböck, Hanspeter},
  doi={10.1007/3-540-62599-2_34},
  url={http://dx.doi.org/10.1007/3-540-62599-2_34},
  publisher={Springer Berlin Heidelberg},
  pages={112--124},
  WKloc = {A-0508, doc/pap/BIB},
  annote = {HOPS implementation},
  abstract = {We propose an abstract data type for freezable data
                  structures. A frozen data structure is immutable. In
                  contrast to general immutable data structures,
                  freezable data structures can be mutated efficiently
                  until they are frozen. Our abstract data type relies
                  on the Carrier-Rider Pattern and information hiding
                  in a module.}
}

@Article{Wedderburn-1934,
  author = {J. H. M. Wedderburn},
  title = {Boolean Linear Associative Algebras},
  journal = ANMA,
  volume = 35,
  year = 1934,
  pages = {185--194},
  bibliographies = {RelMiCS}
}

@Unpublished{Wegman-,
  author = {Wegman},
  title = {???},
  year = {???},
  url = "http://researchweb.watson.ibm.com/people/w/wegman/ompdf/vision.pdf",
  note = {\textsf{http://researchweb.watson.ibm.com/people/w/wegman/ompdf/vision.pdf}},
  bibliographies = {Coconut}
}

@InProceedings{Wegner-Zdonik-1990,
  WKloc = {A-0106},
  abstract = {Incremental modification is a fundamental mechanism not only
      in software systems, but also in physical and mathematical systems.
      inheritance owes its importance in large measure to its flexibility
      as a discrete incremental modification mechanism. Four increasingly
      permissive properties of incremental modification realizable by
      inheritance are examined: behaviour compatibility, signature
      compatibility, name compatibility, and cancellation. inheritance for
      entities with finite sets of attributes is defined and characterized
      as incremental modification with deferred binding of self-reference.
      Types defined as predicates for type checking are contrasted with
      classes defined as templates for object generation. Mathematical,
      operational, and conceptual models of inheritance are then examined
      in detail, leading to a discussion of algebraic models of behavioral
      compatibility, horizontal and vertical signature modification,
      algorithmically defined name modification, additive and subtractive
      exceptions, abstract inheritance networks, and parametric
      polymorphism. Liketypes are defined as a symmetrical general form of
      incremental modification that provide a framework for modelling
      similarity. The combination of safe behaviorally compatible changes
      and less safe radical incremental changes in a single programming
      language is considered.},
  title = {Inheritance as an Incremental Modification Mechanism or What
      Like Is and Isn't Like},
  pages = {55--77},
  crossref = {IFIP1990},
  author = {Peter Wegner and Stanley B. Zdonik},
  bibliographies = {RelMiCS}
}

@Misc{Wehr-1996,
  author = {Martin Wehr},
  title = {Quantum Computing: A New paradigm and its Type Theory},
  year = 1996,
  WKloc = {A-0770}
}

@InProceedings{Wehr-Laemmel-Thiemann-2007,
  author = 	 {Stefan Wehr and Ralf L{\"a}mmel and Peter Thiemann},
  title = 	 {{JavaGI}: Generalized Interfaces for Java},
  crossref =  {ECOOP2007},
  pages = {347--372},
  URL = 	 {http://homepages.cwi.nl/~ralf/JavaGI/},
  DOIURL = {http://dx.doi.org/10.1007/978-3-540-73589-2_17},
  abstract = 	 {JavaGI is an experimental language that extends Java
                  1.5 by generalizing the interface concept to
                  incorporate the essential features of Haskell's type
                  classes. In particular, generalized interfaces cater
                  for retroactive and constrained interface
                  implementations, binary methods, static methods in
                  interfaces, default implementations for interface
                  methods, interfaces over families of types, and
                  existential quantification for interface-bounded
                  types. As a result, many anticipatory uses of design
                  patterns such as Adapter, Factory, and Visitor
                  become obsolete; several extension and integration
                  problems can be solved more easily. JavaGI's
                  interface capabilities interact with subtyping (and
                  subclassing) in interesting ways that go beyond type
                  classes. JavaGI can be translated to Java 1.5. Its
                  formal type system is derived from Featherweight GJ.}
}

@Misc{Wehr-1998,
  author = {Martin Wehr},
  title = {Higher Order Algebraic Data Types},
  year = 1998,
  WKloc = {A-0772}
}

@Misc{Wehr-1999,
  author = {Martin Wehr},
  title = {Higher Dimensional Syntax},
  year = 1999,
  WKloc = {A-0771}
}

@InProceedings{Wehrheim-1994,
  author = {H. Wehrheim},
  title = {Parametric Action Refinement},
  crossref = {PROCOMET94},
  pages = {242--261},
  keywords = {Requirements/Specifications; Semantics of
		  Programming Languages; Refinement of Concurrent
		  Systems}
}

@Article{Weihe-2001,
  author = {Karsten Weihe},
  title = {A Software Engineering Perspective on Algorithmics},
  journal = ACMCS,
  year = 2001,
  OPTkey = {},
  OPTvolume = 33,
  OPTnumber = 1,
  OPTpages = {89--134},
  OPTmonth = MAR,
  OPTnote = {},
  WKloc = {A-1251},
  abstract = {An \emph{algorithm component} is an implementation of an
      algorithm which is not intended to be a stand-alone module, but to
      perform a specific task within a large software package or even
      within several distinct software packages. Therefore, the design of
      algorithm components must also incorporate software-engineering
      aspects. A key design goal is adaptability. This goal is important
      for maintenance throughout a project, prototypical development, and
      reuse in new, unforseen contexts. From a theoretical viewpoint most
      algorithms apply to a range of possible use scenarios. Ideally, each
      algorithm is implemented by one algorithm component, which is easily,
      safely, and efficiently adaptable to all of these contexts.

      Various techniques have been developed for the design and
      implementation of algorithm components. However, a common basis for
      systematic, detailed evaluations and comparisons in view of the
      \emph{real} practical needs is still missing. Basically, this means a
      set of concrete criteria, which specify what sort of adaptability is
      \emph{really} required in practice, and which are well-justified by
      convincing, representative use scenarios.

      This paper is intended to be a first ``milestone'' on the way towards
      such a system of criteria. We will present a set of concrete goals,
      which are general and problem-independent and might appear
      ubiquitously in the algorithmic realm. These goals are illustrated,
      motivated, and justified by an extensive requirements analysis for a
      particular algorithm from a particular algorithmic domain: Dijkstra's
      algorithm for shortest paths in networks.

      Clearly, the field of algorithmics might be too versatile to allow a
      comprehensive, yet concise set of precise, justified criteria. Even a
      domain as restricted as graph and network algorithms includes aspects
      that are not fully understood. The analysis will include a discussion
      of the limits of the case study and the scope of the goals. The case
      study was chosen because it seems to be close to the ``borderline''
      between the aspects that are well understood and the aspects that are
      not. Hence, this example may well serve as an ``acid test'' for
      programming techniques in view of the state of the art.}
}

@Article{ Weil-2002,
  author = "Pascal Weil",
  title = "Profinite methods in semigroup theory",
  journal = {Intl.~J.~Algebra Comput.},
  volume = 12,
  pages = {137--178},
  year = "2002",
  Citeseer = "citeseer.ist.psu.edu/weil00profinite.html",
  WKloc = {doc/pap/BIB},
  bibliographies = {RelMiCS}
}

@Misc{Weis-1995a,
  author = {Pierre Weis},
  title = {Re: c17},
  howpublished = {Message to {\em caml-list@pauillac.inria.fr} in
		  response to a question by Emmanuel Engel},
  year = 1995,
  month = FEB,
  note = {About the new type-checking algorithm for mutable
		  values in the 0.7 release of Caml Light},
  WKloc = {A-0394},
  annote = {Uses ``unknowns'' or ``non-polymorphic type variables''}
}

@InProceedings{Weise-Crew-Ernst-Steensgaard-1994,
  author = {Daniel Weise and Roger F. Crew and Michael Ernst and
		  Bjarne Steensgaard},
  title = {Value Dependence Graphs: Representation without Taxation},
  crossref = {POPL1994},
  pages = {297--310},
  authorsAddress = {Microsoft Research},
  year = 1994,
  URL = {http://www.acm.org:80/pubs/citations/proceedings/plan/174675/p297-weise/},
  abstract = {The {\em value dependence graph\/} (VDG) is a sparse
                 dataflow-like representation that simplifies program
                 analysis and transformation. It is a functional
                 representation that represents control flow as data
                 flow and makes explicit all machine quantities, such as
                 stores and I/O channels. We are developing a compiler
                 that builds a VDG representing a program, analyzes and
                 transforms the VDG, then produces a control flow graph
                 (CFG) [ASU86] from the optimized VDG. This framework
                 simplifies transformations and improves upon several
                 published results. For example, it enables more
                 powerful code motion than [CLZ86, FOW87], eliminates as
                 many redundancies as [AWZ88, RWZ88] (except for
                 redundant loops), and provides important information to
                 the code scheduler [BR91]. We exhibit a fast, one-pass
                 method for elimination of partial redundancies that
                 never performs redundant code motion [KFS92, DS93] and
                 is simpler than the classical [MR79, Dha91] or SSA
                 [RWZ88] methods. These results accrue from eliminating
                 the CFG from the analysis/transformation phases and
                 using {\em demand dependences\/} in preference to
                 control dependences.},
  acknowledgement = ack-nhfb,
  keywords = {theory},
  subject = {{\bf D.3.4} Software, PROGRAMMING LANGUAGES,
                 Processors.},
  WKloc = {A-0953},
  bibliographies = {Coconut}
}

@Booklet{Weisser-1993,
  year = 1993,
  title = {Objektrepr"asentation zur Formsch"atzung
           unter Bewegung},
  note = {UniBwM ID xx/93},
  month = {September},
  howpublished = {Diplomarbeit},
  author = {Wei\ss{}er, Knut},
  address = {Neubiberg}
}

@MastersThesis{Weitz-1993,
  keywords = {SHOPS},
  year = 1993,
  title = {{Eine Schnittstelle zum Erstellen komplexer
		  Transformations-Strategien in
		  {Smalltalk-HOPS}}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  type = {Diplomarbeit},
  number = {ID 28/93},
  note = {ID 28/93},
  author = {Mathias Weitz}
}

@InProceedings{Wells-1994,
  title = {Typability and Type-Checking in the Second-Order
		  $\lambda$-Calculus are Equivalent and Undecidable},
  author = {J. B. Wells},
  pages = {176--185},
  crossref = {LICS9},
  WKloc = {A-0368},
  abstract = {The problems of {\em typability\/} and {\em type
		  checking\/} exist for the Girard/Reynolds
		  second-order polymorphic typed $\lambda$-calculus
		  (also know as ``system~{\bf F}'') when it is
		  considered in the ``Curry style'' (where types are
		  derived for pure $\lambda$-terms).  Until now the
		  decidability of these problems for {\bf F} itself
		  has remained uknown.  We first prove that type
		  checking in~{\bf F} is undecidable by a reduction
		  from {\em semi-unification}. We then prove
		  typability in~{\bf F} is undecidable by a reduction
		  from type checking.  Since the reduction from
		  typability to type checking in~{\bf F} is already
		  know, the two problems in~{\bf F} are equivalent
		  (reducible to each other).  The results hold for
		  both the usual $\lambda K$-calculus and the more
		  restrictive $\lambda I$-calculus.}
}

@InProceedings{Wells-Barr-1987,
  WKloc = {A-0066},
  abstract = {This paper is an exposition of the basic ideas of
		  the mathematical theory of sketches and a detailed
		  description of some of the ways in which this theory
		  can be used in theoretical computer science to
		  specify datatypes. In particular, this theory
		  provides a convenient way of introducing datatypes
		  which have variants, for example in case of errors
		  or nil pointers. The semantics is a generalization
		  of initial algebra semantics which in some cases
		  allows initial algebras depending on a parameter
		  such as a bound for overflow.},
  title = {The Formal Description of Data Types Using Sketches},
  pages = {490--527},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Charles Wells and Michael Barr}
}

@InProceedings{Wells-Haack-2002,
  author = {J. B. Wells and Christian Haack},
  title = {Branching Types},
  crossref = {ESOP2002},
  pages = {115--132},
  DOCUMENTURL = {http://www.church-project.org/reports/Wel+Haa:ESOP-2002.html},
  PDF = {http://www.macs.hw.ac.uk/~jbw/papers/Wells+Haack:Branching-Types:ESOP-2002.pdf},
  SPRINGERURL = {http://link.springer.de/link/service/series/0558/bibs/2305/23050115.htm},
  CHURCHREPORT = {yes},
  annote = {short version of \cite{Wells-Haack-2002a}},
  abstract = {Although systems with intersection types have many
                  unique capabilities, there has never been a fully
                  satisfactory explicitly typed system with
                  intersection types. We introduce $\lambda^{\mathrm{B}}$ with \emph{branching types} and types which are
                  quantified over \emph{type selectors} to provide an
                  explicitly typed system with the same expressiveness
                  as a system with intersection types. Typing
                  derivations in $\lambda^{\mathrm{B}}$ effectively
                  squash together what would be separate parallel
                  derivations in earlier systems with intersection
                  types.}
}

@Unpublished{Wells-Haack-2002a,
  author = {J. B. Wells and Christian Haack},
  title = {Branching Types},
  note = {Long version of \cite{Wells-Haack-2002} with full
                  proofs and extra results},
  month = AUG,
  year = 2002,
  PDF = {http://www.macs.hw.ac.uk/~jbw/papers/Wells+Haack:Branching-Types:2002-08-18.pdf},
  CHURCHREPORT = {yes},
  WKloc = {A-1352, doc/pap/BIB},
  bibliographies = {HOPS, MathScheme},
  abstract = {Although systems with intersection types have many
                  unique capabilities, there has never been a fully
                  satisfactory explicitly typed system with
                  intersection types.  We introduce and prove the basic
                  properties of $\lambda^{\mathrm{B}}$, a typed
                  $\lambda$-calculus with \emph{branching types} and
                  types with quantification over \emph{type selection
                  parameters}.  The new system $\lambda^{\mathrm{B}}$ is an
                  explicitly typed system with the same expressiveness
                  as a system with intersection types.  Typing
                  derivations in $\lambda^{\mathrm{B}}$ use branching types to
                  squash together what would be separate parallel
                  derivations in earlier systems with intersection
                  types.}
}

@InProceedings{Wells-Plump-Kamareddine-2003,
  author = 	 {Joe B. Wells, Detlef Plump and Fairouz Kamareddine},
  title = 	 {Diagrams for Meaning Preservation},
  crossref =	 {RTA2003},
  pages = 	 {88--106},
  WKloc = 	 {A-1528, doc/pap/BIB},
  bibliographies = {RelMiCS},
  abstract = {This paper presents an abstract framework and
     multiple diagram-based methods for proving meaning preservation,
     i.e., that all rewrite steps of a rewriting system preserve the
     meaning given by anoperational semantics based on a rewriting
     strategy. While previous rewriting-based methods have generally
     needed the treated rewritingsystem as a whole to have such
     properties as, e.g., confluence, standardization, and/or
     termination or boundedness of developments, our methods can work
     when all of these conditions fail, and thus can handle more
     rewriting systems. We isolate the new lift/project with termination
     diagram as the key proof idea and show that previous
     rewriting-based methods (Plotkin's method based on confluence and
     standardization and Machkasova and Turbak's method based on
     distinct lift andproject properties) implicitly use this
     diagram. Furthermore, our framework and proof methods help
     reduce the proof burden substantially by,e.g., supporting
     separate treatment of partitions of the rewrite steps, needing
     only elementary diagrams for rewrite step interactions,
     excluding many rewrite step interactions from consideration,
     needing weaker termination properties, and providing generic
     support for using developments in combination with any method.}
}

@InProceedings{Wells-Vestergaard-2000,
  author = {J. B. Wells and Rene{\'e} Vestergaard},
  title = {Equational Reasoning for Linking with First-Class Primitive Modules},
  crossref = {ESOP2000},
  pages = {412--428},
  note = {long version available at \textsf{http://www.cee.hw.ac.uk/\~{}jbw/papers/}},
  abstract = {Modules and linking are usually formalized by encodings
      which use the $\lambda$-calculus, records (possibly dependent), and
      possibly some construct for recursion. In contrast, we introduce the
      m-calculus, a calculus where the primitive constructs are modules,
      linking, and the selection and hiding of module components. The
      m-calculus supports smooth encodings of software structuring tools
      such as functions ($\lambda$-calculus), records, objects
      ($\zeta$-calculus), and mutually recursive definitions. The
      m-calculus can also express widely varying kinds of module systems as
      used in languages like C, Haskell, and ML. We prove the m-calculus is
      confluent, thereby showing that equational reasoning via the
      m-calculus is sensible and well behaved.},
  URL = {http://www.link.springer.de/link/service/series/0558/bibs/1782/17820412.htm},
  WKloc = {A-1144}
}

@InCollection{Welsh-1994,
  author = {Jim Welsh},
  title = {Software is History!},
  crossref = {Roscoe-1994},
  pages = {419--429},
  chapter = 24,
  OPTnote = {},
  OPTannote = {}
}

@Book{Wendt-2008,
  author = 	 {Wendt, Siegfried},
  title = 	 {Was Sokrates nicht wissen konnte.
Eine Bildungsreise zu den Grundlagen unserer technischen Zivilisation},
  publisher = 	 Springer,
  year = 	 2008,
  pages = 	 630,
  ISBN = 	 {978-3-8274-1953-8},
  URL = 	 {http://www.springer.com/spektrum+akademischer+verlag/spektrum-sachb%C3%BCcher/book/978-3-8274-1953-8},
  bibliographies = {Cynthia}
}

@InProceedings{Wenzel-1997,
  author = 	 {Markus Wenzel},
  title = 	 {Type classes and overloading in higher-order logic},
  crossref =	 {TPHOL1997},
  pages =	 {307--322},
  bibliographies = {HHOL}
}

@PhDThesis{Wenzel-2002,
  author = {Markus M. Wenzel},
  title = {{Isabelle/Isar} --- A Versatile Environment for Human-Readable Formal Proof Documents},
  school = {Technische Universit\"at M\"unchen, Fakult\"at f\"ur Informatik},
  year = 2002,
  month = FEB,
  keywords = {mechanized theorem proving, readable formal proofs, high-level languages, document preparation},
  WKloc = {A-1400; adapted for Isabelle-2002: A-1383, doc/pap/BIB},
  bibliographies = {MathScheme, HHOL},
  abstract = {The basic motivation of this work is to make formal theory
      developments with machine-checked proofs accessible to a broader
      audience. Our particular approach is centered around the Isar formal
      proof language that is intended to support adequate composition of
      proof documents that are suitable for human consumption. Such primary
      proofs written in Isar may be both checked by the machine and read by
      human-beings; final presentation merely involves trivial pretty
      printing of the sources. Sound logical foundations of Isar are
      achieved by interpretation within the generic Natural Deduction
      framework of Isabelle, reducing all high-level reasoning steps to
      primitive inferences.

      The resulting Isabelle/Isar system is generic with respect to
      object-logics and proof tools, just as pure Isabelle itself. The full
      Isar language emerges from a small core by means of several derived
      elements, which may be combined freely with existing ones. This
      results in a very rich space of expressions of formal reasoning,
      supporting many viable proof techniques. The general paradigms of
      Natural Deduction and Calculational Reasoning are both covered
      particularly well. Concrete examples from logic, mathematics, and
      computer-science demonstrate that the Isar concepts are indeed
      sufficiently versatile to cover a broad range of applications.}
}

@Manual{Wenzel-2002a,
  title = {The {Isabelle/Isar} Reference Manual},
  author = {Markus Wenzel},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1384},
  bibliographies = {HHOL}
}

@Manual{Wenzel-2002b,
  title = {Miscellaneous {Isabelle/Isar} examples for {Higher-Order Logic}},
  author = {Markus Wenzel},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1389},
  bibliographies = {HHOL}
}

@Manual{Wenzel-2002c,
  title = {Lattices and Orders in {Isabelle/HOL}},
  author = {Markus Wenzel},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1390},
  bibliographies = {HHOL}
}

@Manual{Wenzel-Breghofer-2002a,
  title = {The {Isabelle} System Manual},
  author = {Markus Wenzel and Stefan Berghofer},
  organization = {TU M\"unchen},
  month = MAR,
  year = 2002,
  WKloc = {A-1386}
}

@InProceedings{Wenzel-Wiedjik-2002,
  title = {A Comparison of the Mathematical Proof Languages {Mizar} and {Isar}},
  author = {Markus Wenzel and Freek Wiedjik},
  crossref = {???},
  year = 2002,
  WKloc = {A-1391, doc/pap/BIB},
  abstract = {The mathematical proof checker Mizar by Andrzej Trybulec
       uses a proof input language that is much more readable
       than the input languages of most other proof assistants.
       This system also differs in many other respects from most
       current systems. John Harrison has shown that one can have a
       \emph{Mizar mode} on top of a tactical prover,
       allowing one to combine a mathematical proof language
       with other styles of proof checking.
       Currently the only fully developed Mizar mode in this style
       is the Isar proof language for the Isabelle theorem prover.
       In fact the Isar language has become the official input language
       to the Isabelle system, even though many users still use its
       low-level tactical part only.

       In this paper we compare Mizar and Isar. A small example,
       Euclid's proof of the existence of infinitely many primes,
       is shown in both systems.
       We also include slightly higher-level views of formal proof sketches.
       Moreover a list of differences between Mizar and Isar is presented,
       highlighting the strengths of both systems
       from the perspective of end-users.
       Finally, we point out some key differences of the internal mechanisms
       of structured proof processing in either system.}
}

@InProceedings{Wermelinger-Lopes-1994,
  author = {Michel Wermelinger and Jos\'e Gabriel Lopes},
  title = {Basic Conceptual Structures Theory},
  crossref = {ICCS94},
  pages = {144--159},
  abstract = {Although the theory of Conceptual Structures is over
		  10 years old, basic notions (like canonical graphs)
		  are far from settled and are subject to constant
		  extensions and reformulations. However, most of
		  these are done in an informal way, which doesn't
		  help in clarifying the issues involved. It is our
		  hope that this paper will provide a first step
		  towards the complete and {\em rigorous} account of
		  Conceptual Structures (CS) Theory, which is needed
		  for ongoing standardization and implementation
		  efforts. $\ldots$}
}

@MastersThesis{West-2008,
  author = 	 {Scott West},
  title = 	 {A Graph Transformation and Visualization Framework in {Haskell}},
  type =         {{M.Sc.\null{}} thesis},
  school = 	 {Department of Computing and Software, McMaster University},
  year = 	 2008,
  month =        DEC
}

@InProceedings{West-Kahl-2009,
  author =       {Scott West and Wolfram Kahl},
  title =        {A Generic Graph Transformation, Visualisation, and Editing Framework in {Haskell}},
  crossref =  {GTVMT2009},
  pages =     {12.1--12.18},
  bibliographies = {WK, GraTraVis},
  abstract = {Graph transformation, visualisation, and editing
     are useful in many contexts, and require domain-specific customisation.
     However, many general-purpose graph solutions
     lack customisability in at least one area.

     We present a framework that aims to allow
     polished customisation in all three areas,
     using the powerful abstraction capabilities
     of the pure functional programming language Haskell.
     The design of our framework integrates and adapts
     time-tested object-oriented designs
     into a purely functional framework,
     and uses current user-interface libraries (GTK+ and Cairo)
     to achieve polished presentation.

     Our framework provides both
     a low-level programmed approach to graph transformation,
     and, on top of this, high-level approaches including SPO and DPO,
     which are implemented using categorical abstractions
     in an intuitive and flexible way.},
  keywords = {Programmed graph transformation, Algebraic graph transformation,
     Pure functional programming, Generic graph editor}
}

@InProceedings{West-Kahl-2009a,
  author =       {Scott West and Wolfram Kahl},
  title =        {A Generic Graph Transformation, Visualisation, and Editing Framework in {Haskell}},
  crossref =  {GTVMT2009PP},
  pages =     {97--109},
  abstract = {Graph transformation, visualisation, and editing
     are useful in many contexts, and require domain-specific customisation.
     However, many general-purpose graph solutions
     lack customisability in at least one area.

     We present a framework that aims to allow
     polished customisation in all three areas,
     using the powerful abstraction capabilities
     of the pure functional programming language Haskell.
     The design of our framework integrates and adapts
     time-tested object-oriented designs
     into a purely functional framework,
     and uses current user-interface libraries (GTK+ and Cairo)
     to achieve polished presentation.

     Our framework provides both
     a low-level programmed approach to graph transformation,
     and, on top of this, high-level approaches including SPO and DPO,
     which are implemented using categorical abstractions
     in an intuitive and flexible way.},
  keywords = {Programmed graph transformation, Algebraic graph transformation,
     Pure functional programming, Generic graph editor}
}

@Article{Weyhrauch-1980,
  author = 	 {R. W. Weyrauch},
  title = 	 {Prolegomena to a Theory of Mechanized Formal Reasoning},
  journal = 	 {Artificial Intelligence},
  year = 	 1980,
  volume =	 13,
  number =	 1,
  pages =	 {133--170}
}

@Misc{Wheeler-2012_HAFLOSS,
  author =    {David A. Wheeler},
  title =     {{High Assurance (for Security or Safety)
     and Free-Libre / Open Source Software (FLOSS)\ldots{}
     with Lots on Formal Methods / Software Verification}},
  howpublished = {URL: \url{http://www.dwheeler.com/essays/high-assurance-floss.html}},
  month =     NOV,
  year =      2012,
  note =      {Updated from 2006 original. (Last accessed 2013-01-06.)}
}

@Book{Whiddett-1987,
  author = {Dick Whiddett},
  title = {Concurrent Programming for Software Engineers},
  publisher = {Ellis Horwood},
  year = 1987,
  McMaster = {QA 76.5 .W486 1987},
  bibliographies = {SE3B}
}

@Misc{Whitehead-1998,
  author = {E. James Whitehead},
  title = {Collaborative Authoring on the Web: Introducing {WebDAV}},
  howpublished = {asis Feature},
  OPTmonth = {},
  year = 1998,
  OPTnote = {},
  WKloc = {A-1118}
}

@Misc{Whitehead-Goland-1998,
  OPTkey = {},
  author = {E. James Whitehead and Yaron Y. Goland},
  title = {{WebDAV}, A Network Protocol for Remote Collaborative Authoring on the Web},
  OPThowpublished = {},
  OPTmonth = {},
  year = 1998,
  OPTnote = {},
  WKloc = {A-1119}
}

@Book{Whitehead-Russell-1910,
  author = {Alfred North Whitehead and Bertrand Russell},
  title = {Principia Mathematica, Volume I},
  publisher = CambridgeUP,
  address = {Cambridge, England},
  year = 1910,
  pages = {xv+666},
  bibliographies = {RelMiCS}
}

@Book{Whitehead-Russell-1912,
  author = {Alfred North Whitehead and Bertrand Russell},
  title = {Principia Mathematica, Volume II},
  publisher = CambridgeUP,
  address = {Cambridge, England},
  year = 1912,
  pages = {xxiv+772},
  bibliographies = {RelMiCS}
}

@Book{Whitehead-Russell-1913,
  author = {Alfred North Whitehead and Bertrand Russell},
  title = {Principia Mathematica, Volume III},
  publisher = CambridgeUP,
  address = {Cambridge, England},
  year = 1913,
  pages = {x+419},
  bibliographies = {RelMiCS}
}

@Article{Whitehead-Wiggins-1998,
  author = {E. James Whitehead and Meredith Wiggins},
  title = {{WebDAV}: {IETF} Standard for Collaborative Authoring on the Web},
  journal = {{IEEE} Internet Computing},
  pages = {34--40},
  year = 1998,
  month = {Sept./Oct.},
  URL = {http://computer.org/internet/},
  WKloc = {A-1120}
}

@Book{Whitelock-Kilby-1995,
  author = {Peter Whitelock and Kieran Kilby},
  title = {Linguistic and Computational Techniques in Machine
		  Translation System Design},
  publisher = {UCL Press},
  year = 1995,
  series = {Studies in Computational Linguistics},
  edition = {2nd},
  note = {1st edition 1983},
  UniBwM = {ASL187/YA3888}
}

@Book{Whorf-1956,
  year = 1956,
  title = {Language, Thought \& Reality},
  publisher = {MIT Press},
  author = {Benjamin Lee Whorf},
  annote = {cited by \cite{Budd92} on parallel between different
		  programming paradigms and different natural languages.},
  address = {Cambridge, MA},
  bibliographies = {RelMiCS}
}

@Article{Wickline-Lee-Pfenning-Davies-1998,
  author = {Philip Wickline and Peter Lee and Frank Pfenning and Rowan Davies},
  title = {Modal Types as Staging Specifications for Run-Time Code Generation},
  journal = {{ACM} Computing Surveys},
  year = 1998,
  volume = 30,
  number = {3es},
  note = {Article 8},
  WKloc = {A-0902, 31--36},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@Article{Wiedijk-2007,
  author =       {Freek Wiedijk},
  title =        {The {QED} Manifesto Revisited},
  journal =      {Studies in Logic, Grammar and Rhetoric},
  year =         2007,
  volume =    10,
  number =    23,
  pages =     {121--133},
  PDFURL =     {http://mizar.org/trybulec65/8.pdf},
  WKloc =      {doc/pap/BIB},
  abstract =    {We present an overview of the current state of formalization of
    mathematics, and argue what will be needed to make the vision from the
    QED manifesto come true.}
}

@Booklet{Wiener,
  author = {Norbert Wiener},
  title = {A Comparison Between the Treatment of the Algebra of
		Relatives by {Schr\"oder} and that by {Whitehead and Russell}},
  note = Doct,
  publisher = HarvardUP,
  bibliographies = {RelMiCS}
}

@Misc{Wiener-1993,
  author = {Michael J. Wiener},
  title = {Efficient {DES} Key Search},
  year = 1993,
  WKloc = {A-0776}
}

@InCollection{Wiener1967,
  author = {Norbert Wiener},
  title = {A Simplification of the Logic of Relations},
  booktitle = {{From Frege to G\"odel}},
  note = {J.\null{} van Heijenoort (ed.)},
  pages = {224--227},
  address = {Cambridge, MA},
  publisher = HarvardUP,
  year = 1967,
  bibliographies = {RelMiCS}
}

@Article{Wieringa-1998,
  author = {Roel Wieringa},
  title = {A survey of structured and object-oriented
                  software specification methods and techniques},
  journal = {ACM Computing Surveys},
  year = 1998,
  volume = 30,
  number = 4,
  month = DEC,
  pages = {459--527},
  WKloc = {A-0895},
  bibliographies = {SpecTech}
}

@InProceedings{Wiggins-1994,
  author = {Geraint A. Wiggins},
  title = {Improving the $Whelk$ System: a type-theoretic reconstruction},
  crossref = {LOPSTR-META-94},
  pages = {231--247},
  OPTabstract = {},
  WKloc = {A-0708}
}

@TechReport{Wilde-1994,
  author = {Doran K. Wilde},
  title = {The {ALPHA} Language},
  institution = {IRISA},
  year = 1994,
  URL = {ftp://ftp.irisa.fr/techreports/1994/PI-827.ps.Z},
  number = {PI-827},
  file = {~kahl/doc/pap/irisa/PI-827.ps.gz},
  WKloc = {A-02XX},
  abstract = {This report is a formal description of the Alpha
		  language, as it is currently implemented.  Alpha is
		  a strongly typed, functional language which embodies
		  the formalism of systems of affine recurrence
		  equations.  In this report, Alpha language
		  constructs are described, and denotational and type
		  semantics are given.  The theorems which are the
		  basis for doing transformations on an Alpha program
		  are stated.  And finally, the syntax and semantics
		  of Alpha are given.}
}

@TechReport{Wilder-Tucker-1995,
  author = {Anton J. Wilder and J.V. Tucker},
  title = {System Documentation Using Tables --- {A} short course},
  institution = {McMaster Univ., Communications Research Laboratory, TRIO
      (Telecommunications Research Inst.\null{} of Ontario)},
  year = 1995,
  type = {CRL Report},
  number = 306,
  month = MAY,
  pages = 110,
  note = {Also published as Report CSR 11-95, Computer Science Dept.,
      Univ.\null{} of Wales, Swansea, 1995},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  abstract = {These are lecture notes for a short course (five lectures)
     on tabular methods for system documentation that forms part of a
     second year undergraduate course on system development within
     the Department of Computer Science, University of Wales, Swansea, Wales.
     The topics covered are: tables in the real world;
     tables and the system design life cycle;
     four documentation problems
     (word processor, calculator, polyline and dialogue box);
     general tables and tables of terms over algebras.

     We are keen to receive comments, suggestions and information
     on the material in these notes.}
}

@InProceedings{Wille-2005,
  author = 	 {Rudolf Wille},
  title = 	 {Formal Concept Analysis as Mathematical Theory of Concepts and Concept Hierarchies},
  crossref =  {FCA2005},
  WKloc = 	 {A-1701, doc/pap/BIB},
  pages =	 {1--33}
}

@Article{Williams-1982,
  author = {J. H. Williams},
  title = {On the Development of the Algebra of Functional Programs},
  journal = ACM-TOPLAS,
  volume = 4,
  number = 4,
  year = 1982,
  pages = {733--755},
  bibliographies = {RelMiCS}
}

@Manual{Williams-1992,
  title = {{FunnelWeb} User's Manual},
  author = {Ross N. Williams},
  month = MAY,
  year = 1992,
  note = {Part of the FunnelWeb distribution,
                  available at \url{http://www.ross.net/funnelweb/}}
}

@InProceedings{Wills-1993,
  author = {Linda M. Wills},
  title = {FlexibleCon trol for Program Recognition},
  booktitle = {Working Conference on Reverse Engineering. {Baltimore, MD., May 1993}},
  pages = {134--143},
  year = 1993,
  WKloc = {A-1364, doc/pap/BIB},
  abstract = {Recognizing commonly used data structures and algorithms
    is a key activity in reverse engineering.
    Systems developed to automate this recognition process have been isolated,
    stand-alone systems, usually targeting a special task.
    We are interested in applying recognition to multiple tasks requiring
    reverse engineering, such as inspecting, maintaining, and reusing software.
    This requires a flexible, adaptable recognition architecture,
    since the tasks vary in the amount and accuracy of knowledge available
    about the program, the requirements on recognition power,
    and the resources available.
    We have developed a recognition system based on graph parsing.
    It has a flexible, adaptable control structure
    that can accept advice from external agents.
    Its flexibility arises from using a chart parsing algorithm.
    We are studying this graph parsing approach to determine
    what types of advice can enhance its capabilities, performance, and
    scalability.}
}

@InProceedings{Wilson-1992,
  author = {Paul R. Wilson},
  title = {Uniprocessor Garbage Collection Techniques},
  booktitle = {International Workshop on Memory Management,
   {St. Malo, France, September 1992}},
  series = LNCS,
  volume = 637,
  publisher = Springer,
  year = 1992,
  abstract = {We survey basic garbage collection algorithms, and variations such as incremental and generational collection.
         The basic algorithms include reference counting, mark-sweep, mark-compact, copying, and treadmill
         collection. Incremental techniques can keep garbage collection pause times short, by interleaving small amounts
         of collection work with program execution. Generational schemes improve efficiency and locality by garbage
         collecting a smaller area more often, while exploiting typical lifetime characteristics to avoid undue overhead
         from long-lived objects.},
  URL = {ftp://ftp.cs.utexas.edu/pub/garbage/gcsurvey.ps},
  WKloc = {doc/pap/BIB}
}

@Article{Wilson-1995,
  author = {Paul R. Wilson},
  title = {Uniprocessor Garbage Collection Techniques},
  journal = {ACM Computing Surveys},
  year = {1995?},
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  URL = {ftp://ftp.cs.utexas.edu/pub/garbage/bigsurv.ps},
  WKloc = {A-1227, doc/pap/BIB/Wilson_bigsurv.ps},
  abstract = {We survey basic garbage collection algorithms, and variations such as incremental and generational collection;
         we then discuss low-level implementation considerations and relationships between storage management
         systems, languages, and compilers. Throughout, we attempt to present a unified view based on abstract traversal
         strategies, addressing issues of conservatism, opportunism, and immediacy of reclamation; we also point out a
         variety of implemetation details that are likely to have significant impact on the performance. }
}

@Misc{Wilson-1999,
  author = {Greg Wilson},
  title = {Are {VHLLs} Really High-Level?
           Thoughts on the Surprising Conservatism of Computer Programming},
  URL = {http://www.ora.com/news/vhll_1299.html},
  publisher = {O'Reilly},
  howpublished = {URL: {\sf {http://www.ora.com/news/vhll\_1299.html}}},
  year = 1999,
  month = DEC
}

@InProceedings{Wilson-Johnstone-Neely-Boles-1995,
  author = {Paul R. Wilson and Mark S. Johnstone and Michael Neely and David Boles},
  title = {Dynamic Storage Allocation: A Survey and Critical Review},
  booktitle = {International Workshop on Memory Management, {Kinross, Scotland, UK, September 1995}},
  year = 1995,
  WKloc = {A-1228, doc/pap/BIB},
  abstract = {Dynamic memory allocation has been a fundamental part of most computer systems since roughly 1960, and
         memory allocation is widely considered to be either a solved problem or an insoluble one. In this survey, we
         describe a variety of memory allocator designs and point out issues relevant to their design and evaluation. We
         then chronologically survey most of the literature on allocators between 1961 and 1995. (Scores of papers are
         discussed, in varying detail, and over 150 references are given.)

         We argue that allocator designs have been unduly restricted by an emphasis on mechanism, rather than policy,
         while the latter is more important; higher-level strategic issues are still more important, but have not been
         given much attention.

         Most theoretical analyses and empirical allocator evaluations to date have relied on very strong assumptions of
         randomness and independence, but real program behavior exhibits important regularities that must be exploited
         if allocators are to perform well in practice.}
}

@inproceedings{Widsteiger-2012_Theorema2GUI,
    author = {Wolfgang Windsteiger},
    title = {{Theorema 2.0}: A Graphical User Interface for a Mathematical Assistant System},
    booktitle = {{Proceedings 10th International Workshop On User Interfaces for Theorem Provers, Bremen, Germany, July 11th 2012}},
    language = {english},
    abstract = {Theorema 2.0 stands for a re-design including a complete re-implementation of the Theorema system, which was originally designed, developed, and implemented by Bruno Buchberger and his Theorema group at RISC. In this paper, we present the first prototype of a graphical user interface (GUI) for the new system. It heavily relies on powerful interactive capabilities introduced in recent releases of the underlying Mathematica system, most importantly the possibility of having dynamic objects connected to interface elements like sliders, menus, check-boxes, radio-buttons and the like. All these features are fully integrated into the Mathematica programming environment and allow the implementation of a modern interface comparable to standard Java-based GUIs.},
    series = EPTCS,
    volume = {118},
    pages = {72--82},
    publisher = {Open Publishing Association},
    isbn_issn = {ISSN 2075-2180},
    year = {2012},
    DOI = {10.4204/EPTCS.118.5},
    editor = {Cezary Kaliszyk and Christoph Lueth},
    refereed = {yes},
    length = {8},
    conferencename = {UITP 2012},
    DirectURL = {http://arxiv.org/abs/1307.1945v1}
}

@InProceedings{Winsborough-Waern-1988,
  title = {Transparent And-Parallelism in the Presence of Shared Free Variables},
  pages = {749--764},
  crossref = {ICLP1988},
  author = {Winsborough, Will and Waern, Arnika},
  bibliographies = {RelMiCS}
}

@Book{Winskel-1993,
  author = {Glynn Winskel},
  title = {The Formal Semantics of Programming Languages: An Introduction},
  series = {Foundations of Computing},
  year = 1993,
  publisher = {MIT Press},
  pages = 361,
  ISBN = {0-262-23169-7},
  address = {Cambridge, Mass.},
  McMaster = {QA 76.7 .W555 1993},
  bibliographies = {SE3E}
}

@InProceedings{Winskel-1994,
  author = {Glynn Winskel},
  title = {Stable Bistructure models of PCF},
  crossref = {MFCS94},
  pages = {177--197},
  WKloc = {A-0311},
  abstract = {Stable bistructures are a generalisation of event
		  structures to represent spaces of functions at
		  higher types; the partial order of causal dependency
		  is replaced by two orders, one associated with input
		  and the other output in the behaviour of
		  functions. They represent Berry's bidomains. The
		  representation can proceed in two
		  stages. Bistructures form a categorical model of
		  Girard's linear logic consisting of a linear
		  category together with a comonad. The comonad has a
		  co-Kleisli category which is equivalent to a
		  cartesian-closed full subcategory of Berry's
		  bidomains. A main motivation for bidomains came from
		  the full abstraction problem for Plotkin's language
		  PCF. However, althouh the bidomain model
		  incorporates both the Berry stable order and the
		  Scott pointwise order, its PCF theory (those
		  inequalities on terms which hold in the bidomain
		  model) does not include that of the Scott
		  model. With a simple modification we can obtain a
		  new model of PCF, combining the Berry and Scott
		  orders, which does not have this inadequacy.}
}

@InCollection{Winskel-Nielsen-1995,
  author = {Glynn Winskel and Mogens Nielsen},
  title = {Models for Concurrency},
  crossref = {HBLCS-IV},
  WKloc = {A-0636}
}

@Misc{Winstanley-ODonnell-199X,
  author = {Noel Winstanley and John {O'Donnell}},
  title = {Parallel Disptributed Programming with {Haskell+PVM}},
  year = {199?},
  WKloc = {A-0631}
}

@InProceedings{Winter-1994,
  author = {Michael Winter},
  title = {Program Verification with {RELVIEW}},
  crossref = {KielTool94},
  pages = {62--78}
}

@PhDThesis{Winter-1998,
  author = {Michael Winter},
  title = {{Strukturtheorie heterogener Relationenalgebren
            mit Anwendung auf Nichtdeterminismus in Programmiersprachen}},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 1998,
  month = APR,
  bibliographies = {RelMiCS},
  WKloc = {A-1191, doc/pap/BIB}
}

@Article{Winter-1999,
  author = {Michael Winter},
  title = {A relation algebraic approach to interaction categories},
  journal = {Information Sciences},
  year = 1999,
  volume = 119,
  number = {3--4},
  month = DEC,
  pages = {301--314},
  WKloc = {A-0576},
  abstract = {Samson Abramsky introduced in the concept of Interaction
      Categories. His motivating example of a synchronous Interaction
      Category is the category {\bf SProc} of synchronisation trees between
      concurrent system specifications. In this paper we show that this
      category is a unitary division allegory in the sense of Peter Freyd.
      Furthermore, we want to introduce the notion of time-extended
      allegories, i.e., a theory for relations extended in time. Some
      properties of this kind of allegories and strongly guarded functors
      are proven and the connections to Interaction Categories are
      discussed.},
  URL = {http://www.elsevier.nl/gej-ng/10/23/143/47/27/36/abstract.html},
  bibliographies = {RelMiCS}
}

@Unpublished{Winter-2000a,
  author = {Michael Winter},
  title = {An Algebraic Formalisation of $L$-Fuzzy Relations},
  note = {submission to RelMiCS 5 proceedings},
  month = MAR,
  year = 2000,
  WKloc = {A-0984}
}

@Article{Winter-2001,
  author = {Michael Winter},
  title = {A New Algebraic Approach to $L$-fuzzy Relations Convenient to Study Crispness},
  journal = {Information Sciences},
  year = {2001},
  volume = 139,
  number = {3--4},
  month = DEC,
  pages = {233--252},
  WKloc = {A-1530},
  abstract = {The aim of this paper is to develop a suitable
     calculus of $L$-fuzzy relations. We show that for this purpose
     the theory of Dedekind categories is too weak. Therefore, we
     introduce the notion of a Goguen category as a suitable algebraic
     definition and show several properties of this kind of relational
     category.},
  bibliographies = {RelMiCS, RelMiCS3}
}

@PhDThesis{Winter-2002,
  author = {Michael Winter},
  title = {Goguen Categories???},
  school = {Fakult\"at f\"ur Informatik, Universit\"at der Bundeswehr M\"unchen},
  year = 2002,
  type = {Habil.\null{} thesis},
  bibliographies = {RelMiCS}
}

@Article{Winter-2002a,
  author = 	 {Michael Winter},
  title = 	 {Derived Operations in {Goguen} Categories},
  journal = 	 {Theory and Applications of Categories},
  year = 	 2002,
  volume =	 10,
  number = 11,
  pages =	 {230--247},
  WKloc = 	 {A-1616},
  bibliographies = {RelMiCS}
}

@Article{Winter-2003,
  author = 	 {Michael Winter},
  title = 	 {Representation Theory of {Goguen} Categories},
  journal = 	 {Fuzzy Sets and Systems},
  year = 	 2003,
  volume =	 138,
  pages =	 {85--126},
  WKloc = 	 {A-1531},
  bibliographies = {RelMiCS}
}

@Book{Winter-2007,
  author = 	 {Winter, Michael},
  title = 	 {{Goguen} Categories: A Categorical Approach to $L$-fuzzy Relations},
  publisher = 	 Springer,
  year = 	 2007,
  pages = 	 208,
  volume = 	 25,
  series = 	 {Trends in Logic},
  ISBN = 	 {978-1-4020-6163-9},
  URL = 	 {http://www.springer.com/west/home/philosophy/logic?SGWID=4-40392-22-173734652-details},
  bibliographies = {RelMiCS},
  abstract = {Goguen categories extend the relational calculus and its
                  categorical formalization to the fuzzy
                  world. Starting from the fundamental concepts of
                  sets, binary relations and lattices this book
                  introduces several categorical formulations of an
                  abstract theory of relations such as allegories,
                  Dedekind categories and related structures. It is
                  shown that neither theory is sufficiently rich to
                  describe basic operations on fuzzy relations. The
                  book then introduces Goguen categories and provides
                  a comprehensive study of these structures including
                  their representation theory, and the definability of
                  norm-based operations.

                  The power of the theory is
                  demonstrated by a comprehensive example. A certain
                  Goguen category is used to specify and to develop a
                  fuzzy controller. Based on its abstract description
                  as well as certain desirable properties and their
                  formal proofs, a verified controller is derived
                  without compromising the --- sometimes --- intuitive
                  choice of norm-based operations by fuzzy engineers.}
}

@Article{Winter-2008,
  author = "Michael Winter",
  title = "Products in categories of relations",
  journal = JLAP,
  volume = 76,
  number = 1,
  pages = "145--159",
  year = 2008,
  OPTnote = "Relations and Kleene Algebras in Computer Science",
  issn = "1567-8326",
  DOI = "10.1016/j.jlap.2007.10.003",
  DOIURL = "http://dx.doi.org/10.1016/j.jlap.2007.10.003",
  DIRECTURL = "http://www.sciencedirect.com/science/article/pii/S1567832607000859",
  abstract = "The relational product construction is often consider as
                  an abstract version of cartesian products. The
                  existence of those products is strongly connected
                  with the representability of that category. In this
                  paper we investigate a canonical weakening of the
                  notion of a relational product. Unlike the strong
                  version, any (small) category of relations can be
                  embedded into a suitable category providing all weak
                  relational products. Furthermore, we provide several
                  examples, and we study the categorical properties of
                  the new construction."
}


@InProceedings{Winter-Kempf-2003,
  author = 	 {Michael Winter and Peter Kempf},
  title = 	 {Processes as Relations},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  pages = 	 {59--73},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  OPTnote = 	 {},
  WKloc = 	 {A-1532}
}

@InProceedings{WinterA-2001,
  author = 	 {Andreas Winter},
  title = 	 {Exchanging Graphs with {GXL}},
  pages = {485--500},
  crossref =  {GD2001},
  WKloc = 	 {A-1510},
  bibliographies = {GXL}
}

@InProceedings{WinterA-Kullbach-Riediger-2002,
  author = 	 {Andreas Winter and Bernt Kullbach and Volker Riediger},
  title = 	 {An Overview of the {GXL} Graph Exchange Language},
  crossref =	 {SoftVis2002},
  pages =	 {324--336},
  WKloc = 	 {A-1511},
  bibliographies = {GXL}
}

@InCollection{Wirsing-1990,
  author = {Martin Wirsing},
  title = {Algebraic specifications},
  booktitle = {Handbook of Theoretical Computer Science B},
  year = 1990,
  editor = {J. van Leeuwen},
  pages = {675--788},
  publisher = {Elsevier},
  bibliographies = {RelMiCS}
}

@Article{Wirth-1971,
  author = {N. Wirth},
  title = {Program Development by Stepwise Refinement},
  journal = CACM,
  volume = 14,
  number = 4,
  pages = {221--227},
  month = APR,
  year = 1971,
  ISSN = {0001-0782},
  classcodes = {C6110 (Systems analysis and programming)},
  corpsource = {Eidgenossische Tech. Hochschule, Zurich, Switzerland},
  keywords = {development; education; programming; programming
                 techniques; refinement; stepwise}
}

@InProceedings{Wirth-Becker-1994,
  author = {C.-P. Wirth and K. Becker},
  title = {Abstract Notions and Inference Systems for Proofs by Mathematical
          Induction},
  pages = {?},
  abstract = {?},
  crossref = {CTRS1994},
  authorsAddress = {Kaiserslautern}
}

@Book{Wolfe-1996,
  author =	 {Michael Joseph Wolfe},
  title = 	 {High Performance Compilers for Parallel Computing},
  publisher = 	 {Addison Wesley},
  year = 	 1996,
  ISBN = {0805327304},
  bibliographies = {Anand}
}

@Book{Wolferen-1989,
  author = {Karl Wolferen},
  title = {The Enigma of Japanese Power},
  publisher = {Alfred A. Knopf Press},
  year = 1989,
  note = {this book used to be given away when ever you
		  bought a subscription to Fortune Magazine. I'm not
		  sure if it still is.}
}

@Misc{Wolff-Shi-1994,
  author = {Bernhard Wolff and Hui Shi},
  title = {A Calculus of Transformation},
  year = 1994,
  WKloc = {A-0491}
}

@InProceedings{Wolfram-1991,
  author = {D. A. Wolfram},
  title = {Rewriting, and Equational Unification: the
		  Higher-Order Cases},
  crossref = {RTA91},
  pages = {25--36},
  WKloc = {A-0215},
  abstract = {We give here a general definition of term rewriting
		  in the simply typed $\lambda$-calculus, and use it
		  to define higher-order forms of term rewriting
		  systems, and equational unification and their
		  properties. This provides a basis for generalizing
		  the first- and restricted higher-order results for
		  these concepts. As examples, we generalize Plotkin's
		  criteria for building-in equational theories, and
		  show that pure third-order equational matching is
		  undecidable. This approach simplifies computations
		  in applications involving lexical scoping, and
		  equations. We discuss open problems and summarize
		  future research directions.},
  annote = {--- PLGnotes ---
                  Combinatory Reduction Systems without only BVars in MetaVars?}
}

@InProceedings{Wolfram-1991-x,
  author = {D. A. Wolfram},
  title = {Rewriting, and Equational Unification: the
		  Higher-Order Cases},
  pages = {25--36},
  WKloc = {A-0215},
  abstract = {We give here a general definition of term rewriting
		  in teh simply typed $\lambda$-calculus, and use it
		  to define higher-order forms of term rewriting
		  systems, and equational unification and their
		  properties. This provides a basis for generalizing
		  the first- and restricted higher-order results for
		  these concepts. As examples, we generalize Plotkin's
		  criteria for building-in equational theories, and
		  show that pure third-order equational matching is
		  undecidable. This approach simplifies computations
		  in applications involving lexical scoping, and
		  equations. We discuss open problems and summarize
		  future research directions.},
  annote = {--- PLGnotes ---
                  Combinatory Reduction Systems without only BVars in MetaVars?},
  booktitle = {Rewriting Techniques and Applications: 4th
		  international conference, {RTA 4}},
  year = 1991,
  volume = 488,
  series = {LNCS},
  publisher = {Springer},
  UniBwM = {INF700/Z9362-4},
  address = {Berlin}
}

@InProceedings{Wolfram-1993,
  author = {D.A. Wolfram},
  title = {An implementation of higher-order rewriting},
  crossref = {HOA1993-PP},
  WKloc = {A-0112},
  abstract = {We discuss an implementation of a restricted form
		  of higher-order rewriting. It is illustrated with
		  encodings of programs that have been derived using
		  the functional form of the Bird-Meertens Formalism.
		  The implementation allows ``leftmost-outermost'' and
		  ``rightmost-innermost'' rewriting orders to be
		  selected, and conditional higher-order rewrite-rules
		  to be used. The results are encouraging and suggest
		  that systems which use rewrite rules can achieve
		  greater conciseness, applicability, and efficiency.}
}

@Book{Wolfram-1993a,
  author = {D. A. Wolfram},
  title = {The Clausal Theory of Types},
  publisher = {Cambridge University Press},
  year = 1993,
  volume = 21,
  number = {Cambridge Tracts in Theoretical Computer Science},
  UniBwM = {INF400/X4718},
  WKloc = {B-0027},
  contents = {1 Logic Programming: A Case Study
                  2 Simply Typed $\lambda$-Calculus
                  3 Higher-Order Logic
                  4 Higher-Order Equational Unification
                  5 Higher-Order Equational Logic Programming
                  Bibliography
                  Index}
}

@TechReport{Wolfram-1993b,
  author = {D.A. Wolfram},
  title = {An appraisal of {INTERNIST-I}},
  institution = {Oxford University Computing Laboratory},
  year = 1993,
  number = {PRG-TR-13-93},
  note = {Submitted for publication.},
  WKloc = {A-0279},
  abstract = {{INTERNIST-I} was an expert system designed in the early
		  1970's to diagnose multiple diseases in internal
		  medicine by modelling the behaviour of
		  clinicians. Its form and operation are described,
		  and evaluations of the system are surveyed. The
		  major result of the prject was its knowledge base
		  which has been used in successor systems for medical
		  education and clinical use. We also survey the
		  effects of the project through these systems, and
		  conclude that the most successful of them in the
		  near future is likely to be Quick Medical Reference
		  (QMR) when used as an ``electronic textbook'' of medicine.}
}

@TechReport{Wolfram-1993d,
  author = {D.A.\ Wolfram},
  title = {An implementation of higher-order rewriting
             (extended abstract)},
  institution = {Oxford University Computing Laboratory},
  year = 1993,
  number = {PRG-TR-8-93},
  WKloc = {A-0281},
  note = {accepted for HOA '93 as \cite{Wolfram-1993}}
}

@Book{Wolsey-1998,
  author =	 {Laurence A. Wolsey},
  title = 	 {Integer Programming},
  publisher = 	 {Wiley},
  year = 	 1998,
  series =	 {Wiley-Interscience Series in Discrete Mathematics and Optimization},
  McMaster = 	 {T 57.74 .W67 1998}
}

@Article{Wolter-1998,
  author = {Frank Wolter},
  title = {On Logics with Coimplication},
  journal = {Journal of Philosophical Logic},
  year = 1998,
  volume = 27,
  number = 4,
  pages = {353--387},
  topic = {intuitionistic-logic;temporal-logic;},
  WKloc = {A-1072},
  bibliographies = {RelMiCS}
}

@Article{Wolter-Koenig-2015,
  author = 	 {Uwe Wolter and Harald K{\"o}nig},
  title = 	 {Fibred Amalgamation, Descent Data, and Van Kampen Squares in Topoi},
  journal = 	 {Applied Categorical Structures},
  year = 	 {2015},
  OPTkey = 	 {},
  OPTvolume = 	 {23},
  OPTnumber = 	 {3},
  OPTpages = 	 {447--486},
  DOI = 	 {10.1007/s10485-013-9339-2},
  WKloc = 	 {A-1764, doc/pap/BIB},
  abstract = {Reliable semantics for software systems has to follow
    the semantics-as-instance principal (fibred semantics)
    rather than the semantics-as-interpretation principal (indexed semantics).
    While amalgamation of interpretations is simple and nearly always possible,
    amalgamation of instances is very much involved and not possible in many cases.
    A condition when two compatible instances (a span of pullbacks) are amalgamable,
    is presented for presheaves, i.e. functor categories $SET^{\mathcal{S}}$.
    Based on this individual condition we prove further a total condition for amalgamation
    which simultaneously yields a necessary and sufficient condition
    for pushouts to be Van Kampen squares.
    As a necessary and adequate basis to achieve these results
    we provide a full revision and adaption of the theory of descent data in topoi
    for applications in diagrammatic specifications including graph transformations.
    Especially, we characterize Van Kampen squares in arbitrary topoi
    by pullbacks of categories of descent data.}
}

@InProceedings{Wolter-Loewe-1992,
  abstract = {Inspired by the work of S. Kaplan about
		  positive/negative conditional rewriting, we
		  investigate initial semantics for algebraic
		  specifications with Gentzen-formulas. Since the
		  standard initial approach is limited to conditional
		  equations (i.e. positive Horn-formulas), the notion
		  of semi-initial and quasi-initial algebras is
		  introduced and it is shown that all specifications
		  with (positive) Gentzen-formulas admit quasi-initial
		  models.

		  The whole approach is generalized to the parametric
		  case where quasi-initiality generalizes to
		  quasi-freeness. Since quasi-free objects need not be
		  isomorphic, the persistence requirement is added to
		  obtain a {\em unique} semantics for many interesting
		  practical examples. Unique persistent quasi-free
		  semantics can be described as a free construction
		  when the parameter category is restricted to
		  injective homomorphisms.

		  An example which does not admit a correct initial
		  semantics but a correct unique persistent
		  quasi-initial semantics demonstrates that the
		  concepts introduced in this paper might be of some
		  importance w.r.t.\ practical application.},
  title = {Beyond Conditional Equations, Quasi-Initial
		  Semantics for Parametric Algebraic Specifications},
  pages = {342--361},
  crossref = {CAAP92},
  author = {Uwe Wolter and Michael L{\"o}we},
  bibliographies = {RelMiCS}
}

@Article{Wood-2002,
  author = {David R. Wood},
  title = {Optimal Three-Dimensional Orthogonal Graph Drawing in the General Position Model},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  note = {To appear. A preliminary version of this paper was presented at the 6th International Symposium on Graph Drawing (GD '98), Montr\'eal, August 13--15, 1998.},
  WKloc = {doc/pap/BIB},
  bibliographies = {GD},
  abstract = {Let $G$ be a graph with maximum degree at most six. A
      three-dimensional orthogonal drawing of $G$ positions the vertices at
      grid-points in the three-dimensional orthogonal grid, and routes
      edges along grid lines such that edge routes only intersect at common
      end-vertices. In this paper, we consider three-dimensional orthogonal
      drawings in the general position model; here no two vertices are in a
      common grid-plane. Minimising the number of bends in an orthogonal
      drawing is an important aesthetic criterion, and is NP-hard for
      general position drawings. We present an algorithm for producing
      general position drawings with an average of at most $2{2}\over{7}$
      bends per edge. This result is the best known upper bound on the
      number of bends in three-dimensional orthogonal drawings, and is
      optimal for general position drawings of K7. The same algorithm
      produces drawings with two bends per edge for graphs with maximum
      degree at most five; this is the only known non-trivial class of
      graphs admitting two-bend drawings.},
  keywords = {Graph algorithm; Graph drawing; Orthogonal; Three-dimensional}
}

@Book{Woodcock-Davies-1996,
  author = {Jim Woodcock and Jim Davies},
  title = {Using {Z}: Specification, Refinement, and Proof},
  publisher = Prentice,
  year = 1996,
  series = PrenticeCS,
  ISBN = {0-13-948472-8},
  bibliographies = {SpecTech, RelMiCS, RelMiS},
  URL = {http://www.usingz.com/},
  OPTnote = {\textsf{http://softeng.comlab.ox.ac.uk/usingz/}},
  note = {Out of print; available via \textsf{URL: http://www.usingz.com/}},
  WKloc = {owned}
}

@InProceedings{Woodcock-Morgan-1990,
  author = {J.C.P. Woodcock and Carroll Morgan},
  title = {Refinement of State-Based Concurrent Systems},
  crossref = {VDM1990},
  pages = {340--351},
  bibliographies = {RelMiS},
  WKloc = {A-1334},
  abstract = {The traces, failures and divergences of CSP can be expressed
      as weakest precondition formul\ae{} over action systems. We show how
      such systems may be refined up to failure-divergences, by giving two
      proof methods which are sound and jointly complete: forwards and
      backwards simulations. The technical advantage of our weakest
      precondition approach over the usual relational approach is in our
      simple handling of divergence; the practical advantage is in the fact
      that the refinement calculus for sequential programs may be used to
      calculate forwards simulations. Our methods may be adapted to
      state-based development methods such as VDM and Z.}
}

@InCollection{Wook-1995,
  author = {Derick Wook},
  title = {Standard Generalized Markup Language: Mathematical and Philosophical Issues},
  crossref = {LNCS 1000},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0701}
}

@Article{Wooyenaka-1959,
  author = {U. Wooyenaka},
  title = {On Postulate Sets for Relation Algebras},
  journal = NOTIC,
  volume = 6,
  year = 1959,
  pages = {534--535},
  bibliographies = {RelMiCS}
}

@Book{Wordsworth-1993,
  author = {J. B. Wordsworth},
  title = {Software Development with {Z}: A Practical Approach to
                  Formal Methods in Software Engineering},
  publisher = {Addison-Wesley Publishing Company},
  ISBN = {0-201-62757-4},
  URL = {http://www.blackwell.co.uk/cgi-bin/bb_item?0201627574},
  length = 334,
  price = {\pounds21.95},
  year = 1993,
  URL = {http://heg-school.aw.com/cseng/authors/wordsworth/softdev/softdev.html},
  annote = {This book provides a guide to developing software from
                  specification to code, and is based in part on work done at
                  IBM's UK Laboratory that won the UK Queen's Award for
                  Technological Achievement in 1992. \par Contents:
                  Introduction; A simple Z specification; Sets and
                  predicates; Relations and functions; Schemas and
                  specifications; Data design; Algorithm design;
                  Specification of an oil terminal control system.},
  bibliographies = {RelMiCS}
}

@Book{Wordsworth-1996,
  author = {J. B. Wordsworth},
  title = {Software Engineering with {B}},
  publisher = {Addison-Wesley},
  year = 1996,
  WKloc = {owned}
}

@Article{Wostner-1976,
  author = {Ulf Wostner},
  title = {Finite Relation Algebras},
  journal = NOTIC,
  volume = 23,
  year = 1976,
  bibliographies = {RelMiCS}
}

@Article{Wostner-1976a,
  author = {Ulf Wostner},
  title = {On Equationally Definable Classes of Partial Ordering Relations},
  journal = NOTIC,
  volume = 23,
  year = 1976,
  bibliographies = {RelMiCS}
}

@InProceedings{Wright-1992,
  author = {Andrew K. Wright},
  title = {Typing References by Effect Inference},
  crossref = {ESOP1992},
  pages = {473--491},
  authorsAddress = {Rice University, Houston, Texas, wright\@cs.rice.edu},
  abstract = {Hindley/Milner-style polymorphism is a simple,
		  natural, and flexible type discipline for functional
		  languages, but incorporating imperative extensions
		  is difficult. We present a new technique for typing
		  references in the presence of polymorphism by
		  inferring a concise summary of each expression's
		  allocation behaviour---a {\em type effect}. A simple
		  technique for proving soundness with respect to a
		  reduction semantics demonstrates that the type
		  system prevents type errors. By establishing that
		  the system corresponds to an alternate system better
		  suited to implementation, we obtain an algorithm to
		  perform type and effect inference.},
  annote = {perhaps a connection to Odersky,Rabin,Hudak in \cite{Odersky-Rabin-Hudak-1992}},
  bibliographies = {RelMiCS}
}

@Article{Wright-Cartwright-1997,
  author = {Andrew K. Wright and Robert Cartwright},
  title = {A Practical Soft Type System for {Scheme}},
  journal = ACM-TOPLAS,
  volume = 19,
  number = 1,
  pages = {87--152},
  month = jan,
  year = 1997,
  coden = {ATPSDT},
  ISSN = {0164-0925},
  bibdate = {Wed Mar 12 08:06:48 MST 1997},
  URL = {http://www.acm.org/pubs/citations/journals/toplas/1997-19-1/p87-wright/},
  abstract = {A {\em soft type system\/} infers types for the
                 procedures and data structures of dynamically typed
                 programs. Like conventional static types, soft types
                 express program invariants and thereby provide valuable
                 information for program optimization and debugging. A
                 soft type {\em checker\/} uses the types inferred by a
                 soft type system to eliminate run-time checks that are
                 provably unnecessary; any remaining run-time checks are
                 flagged as potential program errors. {\em Soft
                 Scheme\/} is a practical soft type checker for R4RS
                 Scheme. Its underlying type system generalizes
                 conventional Hindley-Milner type inference by
                 incorporating recursive types and a limited form of
                 union type. Soft Scheme accommodates all of R4RS Scheme
                 including uncurried procedures of fixed and variable
                 arity, assignment, and continuations.},
  acknowledgement = ack-nhfb,
  keywords = {performance; reliability},
  subject = {{\bf F.3.3}: Theory of Computation, LOGICS AND
                 MEANINGS OF PROGRAMS, Studies of Program Constructs,
                 Type structure. {\bf D.3.2}: Software, PROGRAMMING
                 LANGUAGES, Language Classifications, Applicative
                 languages. {\bf D.3.4}: Software, PROGRAMMING
                 LANGUAGES, Processors, Optimization.}
}

@Article{WrightAlex-2010,
  author = {Wright, Alex},
  title = {Type Theory Comes of Age},
  journal = CACM,
  volume = {53},
  number = {2},
  year = {2010},
  issn = {0001-0782},
  pages = {16--17},
  doi = {http://doi.acm.org/10.1145/1646353.1646361},
  publisher = {ACM},
  address = {New York, NY, USA},
  bibliographies = {CAS706},
  abstract = {Type systems are moving beyond the realm of data structure and into more complex domains like security and networking.}
}


@MastersThesis{WuJun-2004,
  author = 	 {Jun Wu},
  title = 	 {Formalization of {GXL} in {Z} Notation},
  school = 	 {McMaster University, Department of Computing and Software},
  year = 	 {2004},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  bibliographies = {RelMiCS, GXL}
}

@TechReport{Wuertz-1992,
  abstract = {Two-literal clauses of the form $L\!\leftarrow\!R\/$
		  occur quite frequently in logic programs, deductive
		  databases, and---disguised as an equation---in term
		  rewriting systems. These clauses define a cycle if
		  the atoms $L\/$ and $R\/$ are weakly unifiable,
		  i.e., if $L\/$ unifies with a new variant of $R\/$.
		  The obvious problem with cycles is to control the
		  number of iterations through the cycle. In this
		  paper we consider the  cycle unification problem of
		  unifying two literals $\,G\,$ and $\,F\,$ modulo a
		  cycle. We review the state of the art of cycle
		  unification and give new results for a special type
		  of cycles called unifying cycles, i.e., cycles
		  $\,L\!\leftarrow\!R\,$ for which there exists a
		  substitution $\,\sigma\,$ such that $\,\sigma L =
		  \sigma R\,$. Altogether, these results  show  how
		  the deductive process can be efficiently controlled
		  for special classes of cycles without losing completeness.},
  year = 1992,
  type = {Research Report},
  title = {Unifying Cycles},
  number = {RR-92-22},
  month = MAR,
  institution = DFKI,
  author = {J. W{\"u}rtz},
  address = {Stuhlsatzenhausweg 3, 6600 Saarbr\"ucken 11, Germany},
  bibliographies = {RelMiCS}
}

@InProceedings{Wuertz-1992b,
  abstract = {Two-literal clauses of the form $L\!\leftarrow\!R\/$
		  occur quite frequently in logic programs, deductive
		  databases, and -- disguised as an equation -- in
		  term rewriting systems. These clauses define a cycle
		  if the atoms $L\/$ and $R\/$ are weakly unifiable,
		  i.e.,\ if $L\/$ unifies with a new variant of $R\/$.
		  The obvious problem with cycles is to control the
		  number of iterations through the cycle. In this
		  paper we consider the   problem of unifying two
		  literals $\,G\,$ and $\,F\,$ modulo a cycle. We
		  review the state of the art of cycle unification and
		  give new results for a special type of cycles called
		  unifying cycles, i.e.,\ cycles
		  $\,L\!\leftarrow\!R\,$ for which there exists a
		  substitution $\,\sigma\,$ such that $\,\sigma L =
		  \sigma R\,$. Altogether, these results  show  how
		  the deductive process can be efficiently controlled
		  for special classes of cycles without losing completeness.},
  year = 1992,
  title = {Unifying Cycles},
  publisher = Wiley,
  pages = {60--64},
  month = AUG,
  editor = {B. Neumann},
  booktitle = {Proceedings of the European Conference on Artificial
                 Intelligence},
  author = {J. W{\"u}rtz},
  bibliographies = {RelMiCS}
}

@Misc{Wurth-1995,
  author = {Bernd Wurth},
  title = {Relationenalgebraische Verfahren in der Logiksynthese},
  howpublished = {Slides to a talk given on 26.1.1995 at UniBwM INF2},
  year = 1995,
  month = JAN,
  WKloc = {A-0384}
}

@Misc{Xanadu-1999,
  OPTkey = {},
  OPTauthor = {},
  OPTtitle = {Xanadu Technologies --- An Introduction},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = {},
  abstract = {Udanax.com and Project Xanadu have agreed to put their code in Open Source and their proprietary technical information in the public domain. [$\ldots$]},
  WKloc = {A-1124}
}

@InProceedings{XiHongwei-1997,
  author = {Hongwei Xi},
  title = {Evaluation under lambda-abstraction},
  crossref = {PLILP1997},
  pages = {259--273},
  WKloc = {A-0978}
}

@InProceedings{XiHongwei-Harper-2001,
  title={A dependently typed assembly language},
  author={Xi, Hongwei and Harper, Robert},
  booktitle={ICFP 2001},
  series={ACM SIGPLAN Notices},
  volume={36},
  number={10},
  pages={169--180},
  year={2001},
  DOI = {10.1145/507546.507657},
  DOIURL = {http://dx.doi.org/10.1145/507546.507657},
  organization={ACM}
}

@InProceedings{XiongJianxin-Johnson-Johnson-Padua-2001,
  author = {Jianxin Xiong and Jeremy Johnson and Robert Johnson and David Padua},
  title = {SPL: a language and compiler for DSP algorithms},
  booktitle = {Proceedings of the ACM SIGPLAN 2001 conference on Programming language design and implementation},
  year = 2001,
  ISBN = {1-58113-414-2},
  pages = {298--308},
  location = {Snowbird, Utah, United States},
  doi = {http://doi.acm.org/10.1145/378795.378860},
  publisher = {ACM Press},
  bibliographies = {Anand},
  WKloc = {A-1471}
}

@MastersThesis{XuJian-2003,
  author = 	 {Jian Xu},
  title = 	 {Models of Computation on Abstract Data Types Based on Recursive Schemes},
  school = 	 {Department of Computing and Software, McMaster University},
  year = 	 {2003},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 SEP,
  note = 	 {Tech.\null{} Rep.\null{} No.\null{} CAS 03-09-JZ},
  WKloc = 	 {C-0020}
}

@InProceedings{Yamamoto-Takahashi-Hagiya-Nishizaki-Tamai-,
  author = {M. Yamamoto and K. Takahashi and M. Hagiya and S. Nishizaki and T. Tamai},
  title = {Formalization of Graph Search Algorithms and Its Applications},
  crossref = {TpHol1998},
  pages = {},
  OPTabstract = {},
  URL = {http://www.etl.go.jp/~takahasi/papers/hol98.ps},
  WKloc = {A-0927},
  bibliographies = {ETL, SpecTech}
}

@TechReport{Yamanaka-1993,
  author = {H. Yamanaka},
  title = {Graph Narrowing and its Simulation by Graph Reduction},
  institution = {Fujitsu International Institute for Advanced Study
		  of Social Information Science},
  year = 1993,
  type = {Research Report},
  number = {93-10E},
  tubibmue = {2.93R296/10E}
}

@Book{Yan-2000,
  author = {Song Y. Yan},
  title = {Number Theory for Computing},
  publisher = Springer,
  year = 2000,
  UniBwM = {MAT100/YF3408},
  ISBN = {3-540-65472-0},
  keywords = {cryptography},
  annote = {s.yan@aston.ac.uk}
}

@InProceedings{Yang-1989,
  title = {Solving Simple Substitution Ciphers in {Andorra-I}},
  pages = {113--128},
  crossref = {ICLP1989},
  author = {Yang, R.}
}

@TechReport{Yang-1998a,
  author = {Zhe Yang},
  title = {Encoding Types in {ML}-like Languages},
  year = 1998,
  month = APR,
  institution = {BRICS},
  number = {RS-98-9},
  WKloc = {A-0533}
}

@InProceedings{Yang-1998b,
  author = {Zhe Yang},
  title = {Encoding Types in {ML}-like Languages},
  year = 1998,
  crossref = {ICFP-1998},
  booktitle = {Proc. of the 1998 {ACM SIGPLAN} International Conference on Functional Programming},
  pages = {},
  OPTabstract = {},
  WKloc = {A-0534}
}

@Article{Yang-Chen-Chau-1989,
  author = {Chao-Chih Yang and Jennifer Jau-Yin Chen and
		  H. Lewis Chau},
  title = {Algorithms for Constructing Minimal Deduction Graphs},
  journal = {IEEE Transactions on Software Engineering},
  year = 1989,
  volume = 15,
  number = 6,
  month = JUN,
  pages = {760--770},
  abstract = {Two algorithms for constructing minimal deduction
		  graphs (MDG) for inferring rules and factsin an
		  extended version of the Horn clause logic are
		  developed. A deduction graph (DG) is minimal if the
		  number of arcs in the graph isminimized. Horn
		  clauses (HC) are extended to Horn formulas (HF) such
		  that the head or body of an HF can be a conjunction
		  of positive literals or a disjunction of the bodies
		  of some rule instances, respectively. $\ldots$}
}

@Article{Yetter-1990,
  author = {D. Yetter},
  title = {Quantales and (Noncommutative) Linear Logic},
  journal = JSYLO,
  year = 1990,
  volume = 55,
  pages = {41--64},
  bibliographies = {RelMiCS}
}

@InProceedings{Yoder-Cohn-1994,
  author = {A.G. Yoder and D.L. Cohn},
  title = {Real Spreadsheets for Real Programmers},
  crossref = {ICCL94},
  pages = {20--30}
}

@Article{Yokouchi-1989,
  contents = {Introduction
		1. Preliminary definitions
		2. Properties of norm
		3. Church-Rosser theorem
		4. A reduction strategy
		5. Hierarchy in CCL
		Acknowledgement
		References},
  abstract = {This paper develops the Church-Rosser theorem for
		  the rewriting system CCL$\beta$ on type-free
		  categorical combinators introduced by Curien. The
		  system CCL$\beta$ is not confluent. However we show
		  that there are various sets $D$ of categorical
		  combinator terms such that each $D$ satisfies the
		  following two conditions: (1) $D$ is closed under
		  reduction by CCL$\beta$; (2) CCL$\beta$ is confluent
		  on $D$. Moreover we examine the relation among these sets.},
  year = 1989,
  volume = 65,
  title = {Church-Rosser Theorem for a Rewriting System on
		  Categorical Combinators},
  pages = {271--290},
  journal = TCS,
  author = {Hirofumi Yokouchi},
  bibliographies = {RelMiCS}
}

@Misc{Younger-Luo-Bennet-199X,
  author = {E. J. Younger and Z. Luo and K. H. Bennet},
  title = {Proving Program Transformation for Reverse Engineering},
  year = {199X},
  WKloc = {A-0504}
}

@Misc{Younger-Luo-Bennet-Bull-199X,
  author = {E. J. Younger and Z. Luo and K. H. Bennet and T. M. Bull},
  title = {Reverse Engineering Concurrent Programs Using Formal Modelling and Analysis},
  year = {199X},
  WKloc = {A-0505}
}

@Book{Yourdon-1993,
  author = {Edward Yourdon},
  title = {The Decline and Fall of the American Programmer},
  publisher = {Prentice-Hall},
  year = 1993,
  ISBN = {0-13-309527-4}
}

@InProceedings{Zamfir-1987,
  WKloc = {A-0067},
  abstract = {The purpose of this paper is to show that {\em
		  initial algebra semantics} has an immediate and
		  useful application in the area of communicating
		  computing systems. The major technical feature is a
		  category of continuous many-sorted algebras called
		  {\em parallel-nondeterministic} algebras. In this
		  setting parallel and nondeterministic behaviour of
		  communicating computing systems can be rigorously
		  formulated as sequences of rewritings on abstract
		  objects called {\em parallel-nondeterministic terms}
		  or {\em diamonds}. It is shown that diamonds are
		  free in teh category of continuous {\em
		  parallel-nondeterministic} algebras. (To demonstrate
		  this fact, some results concerning categories of
		  continuous algebras, which can be found in the ork
		  of the ADJ group, are presented in a self-contained
		  form.)

                  Nondeterminism and parallelism are modeled
		  explicitely by introducing a {\em choice} operator
		  and a {\em parallel} operator, respectively.

                  In a companion paper [10] {\em flow nets} are
		  introduced to describe parallel and nondeterministic
		  behaviours of computing systems that communicate
		  with each other, just as conventional flowcharts are
		  used to describe sequential computations. In a
		  continuous parallel-nondeterministic algebra a flow
		  net is represented by its unfoldment --- the
		  solution of a finite system of recursive equations.},
  title = {Initial ALgebra Semantics and Concurrency},
  pages = {528--549},
  crossref = {Main-Melton-Mislove-SchmidtD-1987},
  author = {Maria Zamfir},
  bibliographies = {RelMiCS}
}

@TechReport{Zamulin-1994,
  author = {Alexandre Zamulin},
  title = {The Database Specification Language {Ruslan} (a
		  preliminary communication)},
  institution = {Sibirian Division of the Russian Acadamy of
		  Sciences, Institute of Informatics Systems},
  year = 1994,
  type = {Preprint},
  number = 28,
  address = {Novosibirsk},
  WKloc = {?}
}

@InProceedings{Zandy-Miller-Livny-1999,
  author = {Victor C. Zandy and Barton P. Miller and Miron Livny},
  title = {Process Hijacking},
  booktitle = {The Eighth IEEE International Symposium on High Performance Distributed Computing {(HPDC8), Redondo Beach, California, August 1999}},
  pages = {177--184},
  year = 1999,
  bibliographies = {ProcMig},
  URL = {http://www.cs.wisc.edu/condor/doc/hijack-hpdc8.ps}
}

@Article{Zeman-1989,
  author = {Jay Zeman},
  title = {Peirce on the Algebra of Logic: Some Comments on {Houser}},
  journal = PEIRCE,
  year = 1989,
  volume = 25,
  pages = {51--56},
  bibliographies = {RelMiCS}
}

@InProceedings{Zerny-2009,
  author =       {Ian Zerny},
  title =        {On Graph Rewriting, Reduction and Evaluation},
  crossref =  {TFP2009},
  pages =     {81--96},
  chapter = {6},
  abstract =    {We inter-derive two prototypical styles of graph reduction: redction machines \`a la Turner and graph rewriting systems \`a la Barendregt et al.
    To this end, we adapt Danvy et al.s mecahnical program derivations from the world of terms to the world of graphs.
    We also outline how to inter-derive a third style of graph reduction: a graph evaluator.},
  bibliographies = {ZhaoYuhang}
}

@Article{Zhang-1992a,
  author = {Zhang, Guo-Qiang},
  title = {Some Monoidal Closed Categories of Stable Domains and
  	 Event Structures},
  journal = MSCS,
  year = 1992,
  URL = {http://boole.stanford.edu/pub/monoidal.ps.gz},
  WKloc = {doc/pap/BIB/Zhang-1992_monoidal.ps.gz},
  bibliographies = {RelMiCS},
  abstract = {This paper introduces the following new constructions on
      stable domains and event structures: the tensor product, the linear
      function space, and the exponential. It results in a monoidal closed
      category of dI-domains as well as one of stable event structures
      which can be used to interpret intuitionistic linear logic. Finally,
      the usefulness of the category of stable event structures for
      modeling concurrency and its relation to other models is discussed.}
}

@InProceedings{Zhang-Shasha-Wang-1992,
  author = {Kaizhong Zhang and Dennis Shasha and Jason Tsong-Li Wang},
  title = {Fast Serial and Parallel Algorithms for Approximate
		  Tree Matching with {VLDC's}},
  crossref = {CPM92},
  pages = {151--161},
  note = {(extended abstract)},
  abstract = {Ordered, labeled trees are trees in which each node
		  has a label and the left-to-right order of its
		  children (if it has any) is fixed. Suppose we define
		  the distance between two ordered trees to be the
		  weighted number (the user chooses the weighting) of
		  edit operations (insert, delet, and relabel) to
		  transform one tree to the other. This paper presents
		  algorithms to perform approximate matching for such
		  trees with variable-length don't cares (VLDC's). As
		  far as we know, these are the first algorithms ever
		  to be presented.},
  bibliographies = {RelMiCS}
}

@InProceedings{Zhang-Sokolsky-Smolka-1994,
  title = {On the Parallel Complexity of Model Checking in the Modal
      Mu-Calculus},
  author = {Shipei Zhang and Oleg Sokolsky and Scott A. Smolka},
  pages = {154--163},
  crossref = {LICS9},
  abstract = {The modal mu-calculus is an expressive logic that can be
      used to specify safety and liveness properties of concurrent systems
      represented as labeled transition systems (LTSs). We show that {\em
      Model Checking in the Modal Mu-Calculus\/} (MCMMC)---the problem of
      checking whether an LTS is a model of a formula of the propositional
      modal mu-calculus---is P-hard even for a very restrictive version of
      the problem involving the alternation-free fragment. In particular,
      MCMMC is P-hard even if the formula is fixed and alternation-free,
      and the LTS is deterministic, acyclic, and has fan-in and fan-out
      bounded by~2. The reduction used is from a restricted version of the
      circuit value problem known as {\em Synchronous Alternating Monotone
      Fanout 2 Circuit Value Problem}. \par Our P-hardness result is tight
      in the sense that placing any further non-trivial restrictions on
      either the formula or the LTS results in membership in NC for MCMMC.
      Specifically, we exhibit NC-algorithms for two potentially useful
      versions of the problem, both of which involve alternation-free
      formulas containing a constant number of fixed point operators: 1)
      the LTS is a finite tree with bounded fan-out; and 2) the formula is
      $\wedge$-free and the LTS is deterministic and over an action
      alphabet of bounded size. \par In the course of deriving our
      algorithm for 2), we give a parallel constant-time reduction from the
      alternation-free modal mu-calculus to Datalog. We also provide a
      polynomial-time reduction in the other direction thereby establishing
      an interesting link between the two formalisms.}
}

@InProceedings{ZhangGuoqiag-SmithL-,
  author = {Guo-Qiang Zhang and Leon Smith},
  title = {A Collection of Functional Libraries for Theory of Computation},
  OPTcrossref = {},
  OPTkey = {},
  OPTbooktitle = {},
  OPTpages = {},
  OPTyear = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTmonth = {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = {A-1416},
  abstract = {This paper reports the design and initial implementation of
      an extensive collection of libraries for automata theory and Turing
      machines in the functional programming languages SML and Haskell. The
      implemented package includes Turing machines, deterministic and
      non-deterministic finite stte machines, and standard constructions
      such as union, intersection, concatenation, Kleene closure, and state
      minimization. Such an effort has at least three benefits: one is
      conceptual clarity, in the sense that computational concepts
      andconstruction algorithms match directly and faithfully to types and
      functions in the implementation; the second is the low implementation
      ``cost'', by exploiting the unusual features of these languages, such
      as functors and exception handling in SML, and type classes in
      Haskell. Thirdly, truly describing transition functions as pure
      emph{functions} (and not necessarily as tables) allows flexible
      creation and concise definition of finite state automata, achieving a
      description exponentially smaller in size than even the minimized
      finite automaton in some cases.}
}

@InProceedings{ZhangLintao-Malik-,
  author = 	 {Lintao Zhang and Sharad Malik},
  title = 	 {The Quest for Efficient Boolean Satisfiability Solvers},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1638},
  keywords = 	 {SAT, DPLL}
}

@InCollection{Zhou-Li-1994,
  author = {Zhou, Chaochen and Li, Xiaoshan},
  title = {A Mean Value Calculus of Durations},
  crossref = {Roscoe-1994},
  pages = {431--451},
  chapter = 25,
  OPTnote = {},
  OPTannote = {}
}

@InProceedings{Zhou-Muller-1990,
  author = {Yuli Zhou and Robert Muller},
  title = {Domain Theory for Nonmonotonic Functions},
  crossref = {ALP1990},
  pages = {144--157},
  abstract = {We prove several lattice theoretical fixpoint
		  theorems based on the classical theorem of Knaster
		  and Tarski. These theorems give sufficient
		  conditions for a system of generally nonmonotonic
		  functions on a complete lattice to define a unique
		  minimal fixpoint. The primary objective of this
		  paper is to develop a domain theoretic framework to
		  study the semantics of logic programs as well as
		  various rule-based systems where the rules define
		  generally nonmonotonic functions on lattices.}
}

@Article{Zhu-Loh-Siy-1987,
  author = {M. Zhu and N. K. Loh and P. Siy},
  title = {Towards the Minimum Set of Primitive Relations
		in Temporal Logic},
  journal = IPLET,
  volume = 26,
  year = {1987/88},
  pages = {121--126},
  bibliographies = {RelMiCS}
}

@Book{Zielinski-1991,
  year = 1991,
  title = {Unequal Equities, Power and Risk in Japan's Stock Market},
  publisher = {Kodansha International},
  author = {Zielinski}
}

@MastersThesis{Zierer-1983,
  keywords = {relational semantics, relation algebra, domain
		  theory, domains},
  year = 1983,
  title = {Relationale {Semantik}},
  school = {Techn. Univ. M\"unchen},
  author = {Hans Zierer},
  bibliographies = {RelMiCS}
}

@PhDThesis{Zierer-1988,
  author = {Hans Zierer},
  title = {{Programmierung mit Funktionsobjekten: Konstruktive
		  Erzeugung semantischer Bereiche und Anwendung auf
		  die partielle Auswertung}},
  school = U_TUMI,
  year = 1988,
  type = {Dissertation},
  note = {Report TUM-I8803},
  bibliographies = {RelMiCS},
  contents = {1 Einleitung
                  2 Relationenalgebraische Grundlagen
                  3 Relationenalgebraische Charakterisierung von Bereichen
                  4 Konstruktion von Bereichen
                  5 L\"osen von Bereichsgleichungen
                  6 Eine applikative Sprache
                  7 Totale und partielle Auswertung
                  8 Zusammenfassung und Ausblick}
}

@Article{Zierer-1991,
  WKloc = {A-0020},
  abstract = {Aiming at a constructive approach to domain theory,
		  the definition of domains with deflations is
		  presented. This class of domains is closed with
		  respect to the common domain constructions. Another
		  concern of this paper is to provide a formal
		  calculus for a uniform algebraic treatment of order
		  theoretic and functional aspects of domain theory.
		  The abstract relation algebra turns out to be an
		  appropriate technical means for the characterization
		  and construction of domains. As partial functions
		  present no problem in relation algebra, domains need
		  not contain an additional $\bottom$-element and
		  functions between domains are generally not total.
		  Using symmetric quotients the relation algebraic
		  approach is extended to cope with higher order functions.},
  year = 1991,
  volume = 87,
  title = {Relation-Algebraic Domain Constructions},
  pages = {163--188},
  journal = TCS,
  author = {Hans Zierer},
  bibliographies = {RelMiCS}
}

@InProceedings{Zierer-Schmidt-Berghammer-1986,
  title = {An Interactive Graphical Manipulation System for Higher Objects Based on Relational Algebra},
  pages = {68--81},
  crossref = {WG86},
  author = {Hans Zierer and Gunther Schmidt and Rudolf Berghammer},
  DOI = {10.1007/3-540-17218-1_50},
  DOIURL = {http://dx.doi.org/10.1007/3-540-17218-1_50},
  bibliographies = {RelMiCS},
  WKloc = {A-0580},
  abstract = {A set of bricks is introduced
    to serve as elementary particles in the programming process.
    They are highly generic building blocks in a graphical language using DAGs
    (directed acyclic graphs) and may be manipulated by graphic interaction.
    Semantics of the language as well as that of the basic DAG-operations
    is given in terms of relational algebra.
    An extended example shows the application of the language.}
}

@Book{Zimmer-1991,
  author = {Ralf Martin Zimmer},
  title = {{Zur Pragmatik eines operationalisierten
		  $\lambda$-Kalk\"uls als Basis f\"ur interaktive
		  Reduktionssysteme}},
  publisher = {R.~Oldenbourg Verlag},
  year = 1991,
  volume = 192,
  series = {Berichte der GMD},
  address = {M\"unchen, Wien},
  UniBwM = {MAT010/V10507}
}

@MastersThesis{Zimmermann,
  year = 1991,
  title = {Studie zum {Modulkonzept} in {HOPS-3}},
  school = {Universit\"at der Bundeswehr M\"unchen, Fakult\"at
		  f\"ur Informatik},
  type = {Diplomarbeit},
  number = {ID 38/91},
  note = {ID 17/91},
  month = DEC,
  author = {Thomas Zimmermann}
}

@InProceedings{Zinssmeister-McCreary-1996,
  author = {Gaby Zin{\ss}meister and Carolyn L. McCreary},
  title = {{Drawing Graphs with Attribute Graph Grammars}},
  crossref = {GG1994},
  pages = {443--453}
}

@Article{Zuck-Pnueli-FangY-Goldberg-2004,
  author = 	 {L. Zuck and A. Pnueli and Y. Fang and B. Goldberg},
  title = 	 {{VOC}: A Methodology for Translation Validation of Optimizing Compilers},
  journal = 	 JUCS,
  year = 	 {2004?},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  WKloc = 	 {A-1563, doc/pap/BIB},
  URL = 	 {http://cs.nyu.edu/~zuck/pubs/jucs.ps},
  listURL = {http://www.cs.nyu.edu/validation/pubs.html},
  note = 	 {to appear},
  bibliographies = {Coconut}
}

@Article{Zucker-1996,
  author = {Jeffery I. Zucker},
  title = {Transformations of Normal and Inverted Function Tables},
  journal = FACOMP,
  year = 1996,
  volume = 8,
  pages = {679--705},
  note = {(Also as CRL Report No.\null{} 291, August
                  1994, McMaster University, Communications Research Laboratory and
    Telecommunications Research Inst.\null{} of Ontario.)},
  bibliographies = {RelMiCS},
  WKloc = {doc/pap/BIB},
  abstract = {We develop a theory of function tables,
    similar to and inspired by, that given in the work of D. Parnas.
    We consider, in particular, two classes of function tables:
    normal and inverted.
    We study effective transformations between tables of these two classes,
    as well as transformations which change the dimension of a table.
    We also consider the interrelationship between these three
    types of transformation.}
}

@TechReport{Zucker-ShenH-1998,
  author = {Jeffery I. Zucker and Hong Shen},
  title = {Table transformation: Theory and tools},
  institution = {McMaster University, Dept of Computing and Software},
  year = 1998,
  number = {CAS 98-01},
  WKloc = {doc/pap/BIB},
  note = {also Communications Research Laboratory (CRL) Report 363},
  bibliographies = {RelMiCS},
  abstract = {We work in a theory of function tables, similar to, and
      inspired by, that given in the work of D. Parnas. Table
      transformation algorithms transform one kind of table into another,
      preserving the semantics. We consider, in particular, two kinds of
      function tables: normal and inverted. We study effective
      transformations between tables of these two kinds, as well as
      transformations which change the dimension of a table. We also
      consider the interrelationship between these three types of
      transformation.

      Most of these algorithms have been implemented as part of the Table
      Tool System developed by the Software Engineering Research Group at
      McMaster University. Some of the issues related to implementation are
      discussed.}
}

@inproceedings{Zuendorf-Schoerr-1991,
  author = {Albert Z\"{u}ndorf and Andy Sch\"{u}rr},
  title = {Nondeterministic Control Structures for Graph Rewriting Systems},
  booktitle = {WG '91: Proceedings of the 17th International Workshop},
  year = {1992},
  isbn = {3-540-55121-2},
  pages = {48--62},
  publisher = {Springer-Verlag},
  address = {London, UK}
}

@InProceedings{Zwiers-1990,
  author = {Job Zwiers},
  title = {Refining Data to Processes},
  crossref = {VDM1990},
  pages = {352--369},
  bibliographies = {RelMiS},
  WKloc = {A-1333},
  abstract = {Data reification is generalized, allowing ``abstract'' data
      to be implemented by parallel processes. He Jifeng and Hoare's
      \cite{Hoare-He-1987} approach to integrate theories for ``programs as
      predicates'' and ``programs as predicate transformers'' is
      generalized to parallel processes and is used to formulate syntactic
      verification conditions to check the correctness of reification by
      means of processes.}
}

@Article{deAlfaro-Kapur-2002,
  author = {Luca de Alfaro and Arjun Kapur},
  title = {Hybrid diagrams},
  journal = TCS,
  year = 2002,
  OPTkey = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTpages = {},
  OPTmonth = {},
  bibliographies = {SQRL, MathScheme},
  WKloc = {A-1299, doc/pap/BIB},
  abstract = {Hybrid systems provide a formal model for physical systems
      controlled by discrete-state controllers. To help with the design of
      correct controllers, we present a methodology that enables the
      verification of linear-time temporal logic properties of general,
      non-linear hybrid systems. The methodology is based on the deductive
      transformation and algorithmic checking of hybrid diagrams.

      Hybrid diagrams are graphs whose vertices and edges are labeled with
      first-order assertions; they represent system abstractions, together
      with the progress properties that have been proved about them. The
      verification process begins with the automatic construction of an
      initial diagram, whose behavior coincides with that of the hybrid
      system. The proof of a specification is constructed by applying a
      series of diagram transformations to this initial diagram. The
      transformations preserve behavior containment, and the aim of the
      transformations is to obtain a diagram that can be algorithmically
      shown to satisfy the specification. Whenever the algorithmic check of
      a diagram fails, the check returns guidance for the further
      transformation of the diagram, or indications about possible
      counterexamples to the specification.

      We present four rules for transforming diagrams: each rule enables
      the study of a certain class of temporal logic properties. While some
      rules can be applied unconditionally, others require the proof of
      first-order verification conditions. We prove that the rules lead to
      the first verification methodology for general hybrid systems that is
      complete (relative to first-order reasoning) for proving
      specifications expressed in first-order linear-time temporal logic,
      provided no temporal operator appears in the scope of a quantifier.},
  note = {to appear. A preliminary version of this paper appeared in: L.
      de Alfaro, A. Kapur and Z. Manna. Hybrid diagrams: a
      deductive-algorithmic approach to hybrid system verification, in:
      Proc. 14th Ann. Symp. on Theoretical Aspects of Computer Science
      (STACS 97), Lecture Notes in Computer Science, vol. 1200, Springer,
      Berlin, 1997, pp. 151--164.},
  OPTannote = {}
}

@InCollection{deBakker-1976,
  author = {J. W. de Bakker},
  title = {Semantics and Termination of Nondeterministic Recursive
		Programs},
  booktitle = {Automata, Languages, and Programming},
  editor = {S. Michelson and R. Milner},
  publisher = Edinb,
  year = 1976,
  pages = {436--477},
  bibliographies = {RelMiCS}
}

@InProceedings{deBakker-deRoever-1973,
  author = {de Bakker, J.W. and de Roever, Willem Paul},
  year = 1973,
  title = {A calculus for recursive program schemes},
  booktitle = {Automata, Languages, and Programming, Proc.\null{} of a
		Sympos.\null{}  (IRIA), 3--7 July 1972},
  publisher = NoHo,
  pages = {167--196},
  editor = {Maurice Nivat},
  bibliographies = {RelMiCS}
}

@Article{deBruijn-1972,
  author = {N.G. de Bruijn},
  title = {Lambda Calculus Notation with Nameless Dummies, a Tool for
	  Automatic Formula Manipulation, with Application to the
	  Church-Rosser Theorem},
  journal = {Indagationes Mathematicae},
  volume = 34,
  year = 1972,
  pages = {381--392}
}

@Article{deDiosCastro-LopezFraguas-2007,
  author = 	 {de Dios Castro, J. and L{\'{o}}pez Fraguas, F.J.},
  title = 	 {Extra Variables can be Eliminated from Functional Logic Programs},
  journal = 	 {ENTCS},
  year = 	 2007,
  pages =	 {16 pages},
  note =	 {Special issue with extended selected papers from PROLE 2006},
  WKloc = {doc/pap/BIB},
  bibliographies = {PMC}
}

@Misc{deFalco-2006,
  author =    {de Falco, Marc},
  title =     {Interaction Nets Laboratory},
  ee = {http://inl.sourceforge.net/},
  OPThowpublished = {Available via \url{http://inl.sourceforge.net/}},
  year =      2006,
  OPTnote =      {last accessed 2014-05-15},
  annote =    {http://sourceforge.net/projects/inl/}
}

@MastersThesis{deGuzman-2004,
  author = 	 {de Guzman, Millie Rhoss},
  title = 	 {Relation-Algebraic Proofs in {Isabelle/Isar}},
  school = 	 {McMaster University, Department of Computing and Software},
  year = 	 {2004},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  bibliographies = {RelMiCS}
}

@InProceedings{deGroote-1993,
  author = {Philippe de Groote},
  title = {Defining $\lambda$-Typed $\lambda$-Calculi by Axiomatizing the
          Typing Relation},
  pages = {712--723},
  crossref = {STACS1993},
  WKloc = {A-0157},
  abstract = {We present a uniform framework for defining different
             $\lambda$-typed $\lambda$-calculi in terms of systems to derive
             typing judgements, akin to Barendregt's Pure Type Systems [3].
             We first introduce a calculus called $\lambda^{\lambda}$ and
             study its abstract properties. These are, among others, the
             property of Church-Rosser, the property of subject reduction,
             and the one of strong normalization. Then we show how to extend
             $\lambda^{\lambda}$ to obtain an inferential definition of
             Nederpelt's $\Lambda$ [20]. One may also extend
             $\lambda^{lambda}$ to get inferential definitions of van Daalen
             $\Lambda_{\beta}$ [24], and de Bruijn's $\Lambda \Delta$ [9]
             and we argue that these new inferential definitions are well
             suited for language-theoretic investigations.}
}

@InProceedings{deGroote-1993a,
  author = {Philippe de Groote},
  title = {The Conservation Theorem revisited},
  pages = {163--178},
  abstract = {This paper describes a method of proving strong normalization
             based on an extension of the conservation theorem. We introduce
             a structural notion of reduction that we call $\beta_S$, and we
             prove that any $\lambda$-term that has a $\beta_Ibeta_S$-normal
             form is strongly $\beta$-nornalizable. We show how to use this
             result to prove the strong normalization of different typed
             $\lambda$-calculi.},
  crossref = {TLCA93},
  WKloc = {A-0183}
}

@InProceedings{deGroote-1994,
  author = {Philippe de Groote},
  title = {On the Relation between the $\lambda\mu$-Calculus and
		  the Syntactic Theory of Sequential Control},
  crossref = {LPAR94},
  pages = {31--43},
  authorsAddress = {INRIA-Lorraine, Nancy},
  abstract = {We construct a translation of firrst order
		  $\lambda\mu$-calculus \cite{Parigot-1992} into a subtheory of
		  Felleisen's $\lambda_c$-calculus [5,6]. This
		  translation preserves typing and reduction. Then, by
		  constructing the inverse translation, we show that
		  the two claculi are actualli isomorphic.}
}

@InProceedings{deGroote-1994a,
  author = {de Groote, Philippe},
  title = {A {CPS}-Translation of the $\lambda\mu$-Calculus},
  crossref = {CAAP94},
  pages = {85--99},
  abstract = {We present a translation of Parigot's
		  $\lambda\mu$-calculus into the usual
		  $\lambda$-calculus. This translation, which is based
		  on the so-called {\em continuation-passing style},
		  is correct with respect to equality and with respect
		  to evaluation. At the type level, it induces a
		  logical interpretation of classical logic into
		  intuitionistic one, akin to Kolmogorov's negative
		  translation. As a by-product, we get the
		  normalization of second order typed $\lambda\mu$-calculus.},
  annote = {see also \cite{deGroote-1994}!}
}

@MastersThesis{deHoon-1993,
  author = {Walter de~Hoon},
  title = {Designing a Spreadsheet in a Pure Functional Graph Rewriting
          Language},
  school = {Department of Informatics, University of Nijmegen},
  year = 1993,
  annote = {In Clean. The spreadsheet includes a Clean and a Miranda parser
         and evaluator, and cell entries can be arbitrary functions.},
  URL = {ftp://ftp.cs.kun.nl/pub/Clean/papers/ThesisSpreadsheet.ps.Z,
        ftp://ftp.cs.kun.nl/pub/Clean/FunSheet/}
}

@Manual{deJong-Olivier-2000,
  title = {{ATerm} Library User Manual},
  author = {de Jong, Hayco A. and Pieter A. Olivier},
  organization = {Centrum voor Wiskunde en Informatica (CWI)},
  address = {Kruislaan 413, 1098 SJ Amsterdam, The Netherlands},
  year = 2000,
  WKloc = {A-1045}
}

@Article{deLaraJaramillo-Vangheluwe-AlfonsecaMoreno-2003,
  author    = {Juan de Lara Jaramillo and
               Hans Vangheluwe and
               Manuel Alfonseca Moreno},
  title     = {Using Meta-Modelling and Graph Grammars to Create Modelling
               Environments},
  journal   = {Electr. Notes Theor. Comput. Sci.},
  volume    = {72},
  number    = {3},
  year      = {2003},
  ee        = {http://www1.elsevier.com/gej-ng/31/29/23/127/48/show/Products/notes/index.htt\#005},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@InProceedings{deLiguoro-Piperno-1992,
  abstract = {This paper studies the interplay between functional
		  application and nondeterministic choice in the
		  context of untyped $\lambda$-calculus. We introduce
		  an operational semantics which is based on the idea
		  of {\em must} preorder, coming from the theory of
		  process algebras. To characterize this relation, we
		  build a model using the classical inverse limit
		  construction, and we prove it fully abstract using a
		  generalization of B\"ohm trees.},
  title = {Must Preorder in Non-Deterministic Untyped
		  $\lambda$-Calculus},
  pages = {203--220},
  crossref = {CAAP92},
  author = {de'Liguoro U. and A. Piperno},
  annote = {--- PLGnotes ---
		  call-by-value and call-by-name with nondeterminism}
}

@Article{deLuca-1998,
  author = {de Luca, Aldo},
  title = {A Conjecture on Continued Fractions},
  journal = {Theoretical Computer Science},
  year = 1998,
  volume = 204,
  OPTnumber = {},
  OPTmonth = {},
  pages = {75--86},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0689},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{deMol-vanEekelen-Plasmeijer-2001,
  author = {de Mol, Maarten and van Eekelen, Marko and Rinus Plasmeijer},
  title = {Theorem Proving for Functional Programmers --- \textsc{Sparkle}: A Functional Theorem Prover},
  crossref = {IFL2001},
  pages = {55--71},
  bibliographies = {MathScheme},
  OPTabstract = {}
}

@PhDThesis{deMoor-1990,
  author = {de Moor, Oege},
  title = {Categories, Relations and Dynamic Programming},
  year = 1990,
  school = {Programming Research Group, Oxford University Computing
      Laboratory},
  note = {published as technical report PRG-TR-18-90},
  WKloc = {B-0012},
  bibliographies = {RelMiCS},
  abstract = {Functional programming formalisms have the
		  advantage of a very rich type structure: the
		  presence of higher-order functions allows the
		  expression of algebraic identities with a minimum of
		  bound variables. A category theorist might explain
		  this phenomenon by saying that the category of sets
		  and total functions has a very rich type structure,
		  in that it allows all major categorical
		  constructions like limits, colimits and
		  exponentials. This property is not shared by the
		  category of sets and relations, and as a consequence
		  programming calculi based on relations lack the
		  abundant type structure of their functional
		  counterparts.

                  Motivated by this observation, we examine a
		  categorical construction of relations where
		  relations are defined in terms of total
		  functions. This construction makes it possible to
		  extend the popular higher-order operators of
		  functional programming to relations. It is also
		  possible to tell which algebraic properties of these
		  functional operators are still valid when they are
		  lifted to relations. As an application of the
		  calculus obtained in this manner, we consider the
		  derivation of dynamic programming algorithms.}
}

@Misc{deMoor-1996,
  author = {de Moor, Oege},
  title = {An Exercise in Polytypic Program Derivation: \emph{repmin}},
  month = SEP,
  year = 1996,
  WKloc = {A-1156}
}

@Unpublished{deMoor-1999,
  author = {de Moor, Oege},
  title = {First-class attribute grammars},
  note = {Available via URL:\hfil\strut\penalty-2000 \url{http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/pubs.htm}},
  year = 1999,
  WKloc = {A-1009}
}

@InProceedings{deMoor-Bird-1992-x,
  author = {de Moor, Oege and Richard Bird},
  title = {Solving Optimisation Problems with Catamorphisms},
  pages = {45--66},
  year = 1992,
  booktitle = {2nd Conference on the Mathematics of Program
		  Construction},
  editor = {R. S. Bird and C. C. Morgan and J. C. P. Woodcock},
  series = {LNCS},
  volume = 669,
  WKloc = {A-0102},
  bibliographies = {RelMiCS},
  abstract = {Efficient algorithms for solving optimization
		  problems can often be expressed as homomorphisms on
		  initial data types. Such homomorphisms, which
		  correspond to the familiar {\sl fold} operators in
		  functional programming, are called {\em
		  catamorphisms}. In this paper, we report on an
		  attempt to characterize those optimization problems
		  whose efficient solution can be expressed as a
		  catamorphism. Our results are a natural
		  generalization of earlier work by Jeuring
		  [6],\cite{Jeuring-1990a}, who considered the same
		  problem in a slightly less abstract setting. The
		  main result of this paper is to show how seemingly
		  disparate results about subsequences, permutations,
		  sequence partitions and subtrees can be stated as a
		  single theorem.}
}

@Article{deMoor-Gibbons-1999,
  author = {de Moor, Oege and Jeremy Gibbons},
  title = {Bridging the Algorithm Gap: A Linear-time Functional Program for Paragraph Formatting},
  year = 1999,
  month = SEP,
  journal = SCICOP,
  volume = 35,
  number = 1,
  WKloc = {A-0982},
  bibliographies = {FP, EdComb},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/papers/para.ps.gz}
}

@Misc{deMoor-Gibbons-2000,
  author = {de Moor, Oege and Jeremy Gibbons},
  title = {Pointwise Relational Programming},
  crossref = {AMAST2000},
  pages = {371--390},
  URL = {http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/},
  WKloc = {A-1185},
  abstract = {The point-free relational calculus has been very successful
      as a language for discussing general programming principles. However,
      when it comes to specific applications, the calculus can be rather
      awkward to use: some things are more clearly and simply expressed
      using variables. The combination of variables and relational
      combinators such as converse and choice yields a kind of
      nondeterministic functional programming language. We give a semantics
      for such a language, and illustrate with an example application.},
  bibliographies = {RelMiCS},
  annote = {[From Oege's home page:] Pointwise relational program
      derivation. The point free relational calculus has proved quite
      successful in the derivation of general programming principles. When
      applied to specific examples, however, it can be somewhat painful to
      use, due to the large number of combinators required. In a recent
      exploratory paper, Jeremy Gibbons and myself have started to explore
      the suggestion that for specific examples, it may be preferable to
      conduct relational derivations of concrete programs in a pointwise
      calculus of nondeterministic mappings. So far the exploration has
      been largely informal, focussing on examples, and not on formal
      semantics. Write to me if you know about the semantics of
      nondeterministic lambda calculi, and would like to see your work
      applied to program derivation. The above paper has identified some
      interesting mathemathematical problems.}
}

@InProceedings{deMoor-PeytonJones-VanWyk-199X,
  author = {de Moor, Oege and Peyton Jones, Simon and Van Wyk, Eric},
  title = {Aspect-Oriented Compilers},
  crossref = {POPL?},
  pages = {??},
  OPTabstract = {},
  WKloc = {A-0646}
}

@Unpublished{deMoor-Sittampalam-1999,
  author = {de Moor, Oege and Ganesh Sittampalam},
  title = {Generic Program Transformation},
  note = {Available via URL:\hfil\strut\penalty-2000 \url{http://web.comlab.ox.ac.uk/oucl/work/oege.demoor/pubs.htm}},
  year = 1999,
  WKloc = {A-1010}
}

@Unpublished{deMoor-Sittampalam-1999b,
  author = {de Moor, Oege and Ganesh Sittampalam},
  title = {Higher-Order Matching for Program Transformation},
  note = {Available via URL:\hfil\strut\penalty-2000 \url{http://www.comlab.ox.ac.uk/oucl/work/oege.demoor/papers/match.ps.gz}},
  year = 1999,
  WKloc = {A-1117, doc/pap/BIB/deMoor-Sittampalam-1999b.ps.gz}
}

@Unpublished{deMoor-Swierstra-1992,
  author = {Oege de Moor and Swierstra, D.S. },
  title = {Virtual Data Structures},
  year = 1992,
  note = {Presented at IFIP WG 2.1 state of the art summer school,
      Itacuru\c{c}\'{a} Island, Brazil, Jan. 10-23, 1992. to appear.},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1846,
  author = {De Morgan, Augustus},
  title = {On the Structure of the Syllogism},
  note = {Reprinted in \cite{deMorgan-1966}.},
  journal = TRCAMB,
  year = 1846,
  volume = 8,
  pages = {379--408},
  bibliographies = {RelMiCS}
}

@Book{deMorgan-1847,
  author = {De Morgan, Augustus},
  title = {Formal Logic; or, The Calculus of Inference, Necessary
		and Probable},
  publisher = Taylor,
  address = {28, Upper Gower Street, London},
  year = 1847,
  note = {Reprint by The Open Court Company, London, 1926, ed.\null{} by
      A.\null{} E.\null{} Taylor},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1850,
  author = {De Morgan, Augustus},
  title = {On the Symbols of Logic, the Theory of the Syllogism,
		and in Particular of the Copula,
		and the Application of the Theory of Probabilities to
		some Questions in the Theory of Evidence},
  note = {Reprinted in \cite{deMorgan-1966}.},
  journal = TRCAMB,
  year = 1850,
  volume = 9,
  pages = {79--127},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1858,
  author = {De Morgan, Augustus},
  title = {{On the Syllogism: III; and on Logic in General}},
  note = {Reprinted in \cite{deMorgan-1966}.},
  journal = TRCAMB,
  year = 1858,
  volume = 10,
  pages = {173--230},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1860,
  author = {De Morgan, Augustus},
  title = {{On the Syllogism: IV; and on the Logic of Relations}},
  note = {(dated 12 November 1859) Reprinted in \cite{deMorgan-1966}.},
  journal = TRCAMB,
  year = 1860,
  volume = 10,
  pages = {331--358},
  WKloc = {A-1029},
  bibliographies = {RelMiCS},
  annote = {Proposes $L^v$ as one notation for converse of $L$
             (p. 222 of \cite{deMorgan-1966}).

             Theorem K (p. 224) is Schr\"oder,
             but with a a variant of residuals
             (p. 221, $LM' \defeq \relnot{\relnot{L}\rcmp M}$,
             and $L,M \defeq \relnot{L \rcmp \relnot{M}}$ )
             instead of negated composition ---
             De Morgan uses case-change (from uppercase to lowercase and back)
             to denote negation (p. 222),
             so it is not possible to directly express
             what we write as ``$\relnot{R\rcmp S}$''.}
}

@Article{deMorgan-1862,
  author = {De Morgan, Augustus},
  title = {{On the Syllogism: V; and on Various Points of the Onymatic System}},
  note = {Reprinted in \cite{deMorgan-1966}.},
  journal = TRCAMB,
  year = 1862,
  volume = 10,
  pages = {428--487},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1864,
  author = {De Morgan, Augustus},
  title = {On the Syllogism: {III}, and on Logic in General},
  note = {(read February 8, 1858) Reprinted in \cite{deMorgan1966}.},
  journal = TRCAMB,
  volume = 10,
  year = 1864,
  pages = { 173--230},
  bibliographies = {RelMiCS}
}

@Article{deMorgan-1864a,
  author = {De Morgan, Augustus},
  title = {On the Syllogism: {IV}, and on the Logic of Relations},
  note = {(read April 23, 1860) Reprinted in \cite{deMorgan1966}.},
  journal = TRCAMB,
  volume = 10,
  year = 1864,
  pages = {331--358},
  bibliographies = {RelMiCS}
}

@Book{deMorgan-1966,
  author = {De Morgan, Augustus},
  title = {On the Syllogism, and Other Logical Writings},
  OPTnote = {edited, with an introduction, by Peter Heath, BC135.D399o.},
  publisher = YaleP,
  address = {New Haven},
  year = 1966,
  pages = {xxxi+355},
  bibliographies = {RelMiCS}
}

@InProceedings{deNivelle-1998,
  author = {de Nivelle, Hans},
  title = {A Resolution Decision Procedure for the Guarded Fragment},
  crossref = {CADE1998},
  pages = {191--204},
  OPTabstract = {},
  WKloc = {A-0612 (with PDF faults)}
}

@PhdThesis{deQueiroz-1990,
  author = 	 {de Queiroz, Ruy Jos{\'e} Guerra Barretto},
  title = 	 {Proof Theory and Computer Programming --- The Logical Foundations of Computation},
  school = 	 {Imperial College London, Department of Computing},
  year = 	 {1990},
  OPTkey = 	 {},
  OPTtype = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 JAN,
  OPTnote = 	 {},
  WKloc = 	 {Tom Maibaum}
}

@Article{deQueiroz-Maibaum-1990,
  author = 	 {de Queiroz, Ruy Jos{\'e} Guerra Barretto and Maibaum, Tom},
  title = 	 {Proof Theory and Computer Programming},
  journal = 	 {Zeitschrift f\"ur mathematische Logik und Grundlagen der Mathematik},
  year = 	 1990,
  volume =	 36,
  pages =	 {389--414},
  McMaster = {QA 1.Z38}
}

@Article{deQueiroz-Maibaum-1991,
  author = 	 {de Queiroz, Ruy Jos{\'e} Guerra Barretto and Maibaum, Tom},
  title = 	 {Abstract Data Types and Type Theory: Theories as Types},
  journal = 	 {Zeitschrift f\"ur mathematische Logik und Grundlagen der Mathematik},
  year = 	 1991,
  volume =	 37,
  pages =	 {149--166},
  McMaster = {QA 1.Z38}
}

@Book{deRoever-Coenen-Buth-Engelhardt-Lakhnech-Stomp-1994,
  author = {de Roever, Willem-Paul and Jos Coenen and Karl-Heinz
		  Buth and Kai Engelhardt and Yassine Lakhnech and
		  Frank Stomp},
  title = {State-based Formalisms for Data Refinement: Methods,
		  Theory, Comparison},
  publisher = {{\sl Draft}},
  year = 1994,
  month = JAN,
  WKloc = {Q-007},
  note = {private communication},
  contents = {1 Introduction
                  2 Proof methods for reification
                  3 Representation invariants and Reynolds' reification method
		  4 Reification in VDM
                  5 Notation and semantics for predicates,...
                  6 On Specifications and L-simulation using Hoare's logic
                  7 Motions of simulation and their properties
                  8 The simulation theorem: semantic formulation
                  9 The Simulation Theorem
                 10 Relating Reynolds' method and VDM to L-simulation
                 11 Total Correctness
                 12 Refinement Calculi
                 13 Implementation bias
                  A Introduction to Hoare Logic
                  B Hoare Logic
                  C Proof- and transformation-rules
                  Bibliography}
}

@Book{deRoever-Engelhardt-1998,
  author = {de Roever, Willem-Paul and Kai Engelhardt},
  title = {Data Refinement: Model-Oriented Proof Methods
                  and their Comparison},
  publisher = CambridgeUP,
  OPTseries = Cambridge,
  OPTvolume = 47,
  year = 1998,
  ISBN = {0-521-64170-5},
  bibliographies = {RelMiS, SpecTech}
}

@Book{deRoever-deBoerHanneman-Hooman-Lakhnech-Poel-Zwiers-2001,
  author = {de Roever, Willem-Paul and de Boer, Frank and
    Ulrich Hanneman and Jozef Hooman and Yassine Lakhnech and
    Mannes Poel and Job Zwiers},
  title = {Concurrency Verification: An Introduction to State-based Methods},
  publisher = CambridgeUP,
  year = 2001,
  volume = 54,
  series = Cambridge,
  ISBN = 0521806089,
  URL = {http://uk.cambridge.org/order/WebBook.asp?ISBN=0521806089},
  bibliographies = {RelMiS, SpecTech}
}

@InProceedings{deSalvo-1988,
  author = {Mario de Salvo},
  title = {Commutative Finite A-Hypergroups of Length Two},
  pages = {147--156},
  crossref = {IGCS1988},
  bibliographies = {RelMiCS}
}

@Book{deSwart-1994,
  author = {H. C. M. de Swart},
  title = {Logic (2 Vols)},
  publisher = {},
  year = 1994,
  OPTkey = {},
  OPTeditor = {},
  OPTvolume = {},
  OPTnumber = {},
  OPTseries = {},
  OPTaddress = {},
  OPTedition = {},
  OPTmonth = {},
  OPTnote = {},
  UniBwM = {MAT006/Y2993-1,2},
  OPTannote = {}
}

@InProceedings{delaEncina-Pena-2001,
  author = {de la Encina, Alberto and Ricardo Pe{\~n}a},
  title = {Proving the Correctness of the STG Machine},
  crossref = {IFL2001},
  pages = {88--104},
  OPTnote = {},
  OPTannote = {}
}

@Unpublished{fudgets,
  author = {Magnus Carlsson and Thomas Hallgren},
  title = {{\sc Fudgets}, A Graphical User Interface in a Lazy
		  Functional Language},
  note = {(draft, Jan 1993, contained in the Chalmers
		  University Haskell Distribution)},
  refsfrom = { <(ifip94:2)> }
}

@Misc{japanno,
  title = {{A Japan that Can Say No! (to America)}},
  note = {Note the version of this book sold in stores
        is a phony. 1/2 of the original version is missing (Akio Morita
         removed his part fearing it would hurt SONY's sales in the
         U.S.) and there is a new appendix specifically written for
         American consumption, much of which is blantantly false).},
  howpublished = {US. Department of Defense version},
  author = {Akio Morita and Shintaro Ishihara},
  annote = {monu6.cc.monash.edu.au:pub/nihongo}
}

@Unpublished{leThanh-1986,
  author = {le Thanh, N.},
  title = {Contribution {\`a} l'{\'e}tude de la G{\'e}n{\'e}ralisation
		  et de l'Association dans une Base de Donn{\'e}es
		  Relationnelle: les Isod{\'e}pendances et le
		  Mod{\`e}le b-relationnel},
  year = 1986,
  note = {Diss.\null{} de Doctorat d'Etat des sciences,
		  Univ.\null{} de Nice},
  bibliographies = {RelMiCS}
}

@Manual{lhs2TeX,
  title = {lhs2TeX},
  author = {Andres L{\"o}h},
  year = 2012,
  note = {\url{http://www.andres-loeh.de/lhs2tex/}}
}

@Manual{makeindex,
  title = {makeindex},
  author = {Pehong Chen},
  month = NOV,
  year = 1989,
  note = {manal page},
  WKloc = {M-0056}
}

@InProceedings{vanBakel-1993,
  author = {van Bakel, Steffen},
  title = {Partial Intersection Type Assignment in Applicative Term
          Rewriting Systems},
  pages = {29--59},
  abstract = {This paper intruoduces a notion of partial type assignment on
             applicative term rewriting systems that is based on a
             combination of an essential intersection type assignment
             system, and the assignment system as defined for ML [16], both
             extensions of Curry's type assignment system [11]. Terms and
             rewrite rules will be written as trees, and type assignment
             will consists of assigning intersection types function symbols,
             and specifying the way in which types can be assigned to nodes
             and edges between nodes. The only constraints on this system
             are local: they are imposed by the relation between the type
             assigned to a node and those assigned to its incoming and
             out-going edges. In general, given an arbitrary typeable
             applicative term rewriting system, the subject reduction
             property does not hold. We formulate a sufficient but
             undecidable condition typeable rewrite rules should satisfy in
             order to obtain this property.},
  crossref = {TLCA93},
  WKloc = {A-0178}
}

@Article{vanBakel-1995,
  Author =        {van Bakel, Steffen},
  Title =        {Intersection Type Assignment Systems},
  Journal =        TCS,
  Volume =        151,
  Number =        2,
  Year =        1995,
  Pages =        "385--435",
  annote = {overview}
}

@InProceedings{vanBakel-Fernandez-1993,
  author = {Steffen van Bakel and Maribel Fernandez},
  title = {Strong normalization of typeable rewrite systems},
  crossref = {HOA1993},
  pages = {20--39},
  WKloc = {A-0314},
  bibliographies = {RelMiCS},
  abstract = {This paper studies termination properties of
		  rewrite systems that are typeable using intersection
		  types. It introduces a notion of partial assignment
		  on Currified Term Rewrite Systems, that consists of
		  assigning intersection types to function symbols,
		  and specifying the way in which types can be
		  assigned to nodes and edges between nodes in the
		  tree representation of term. Using a more liberal
		  approach to recursion, a general scheme for
		  recursive definitions is presented, that generalizes
		  primitive recursion, but has full Turing-machine
		  computational power. It will be proved that, for all
		  systems that satisfy this scheme, every typeable
		  term is strongly normailizable.}
}

@InProceedings{vanBakel-Smetsers-Brock-1992,
  WKloc = {A-0025},
  abstract = {This paper introduces a notion of partial type
		  assignment on left linear applicative term rewriting
		  systems that is based on the extension defined by
		  Mycroft of Curry's type assignment system. The left
		  linear applicative TRS we consider are extensions to
		  those suggested by most functional programming
		  languages in that they do not discriminate against
		  the varieties of function symbols that can be used
		  in patterns. As such there is no distinction between
		  function symbols (such as \textsf{append} and \textsf{plus})
                  and constructor symbols (such as \textsf{cons}
		  and \textsf{succ}). Terms and rewrite rules will be
		  written as trees, and type assignment will consist
		  of assigning types to function symbols, nodes and
		  edges between nodes. The only constraints on this
		  system are imposed by the relation between the type
		  assigned to a node and those assigned to its
		  incoming and out-going ewdges. We will show that
		  every typeable term has a principal type, and
		  formulate a needed and sufficient condition typeable
		  rewrite rules should satisfy in order to gain
		  preservance of types under rewriting. As an example
		  we will show that the optimisation function
		  performed after bracket abstraction is typeable.
		  Finally we will present a type check algorithm that
		  checks if rewrite rules are correctly typed, and
		  finds the principal pair for typeable terms.},
  title = {Partial Type Assignment in Left Linear Applicative
		  Term Rewriting Systems},
  pages = {300--321},
  crossref = {CAAP92},
  author = {Steffen van Bakel and Sjaak Smetsers and Simon Brock},
  annote = {--- PLGnotes ---
		  p303: term variables in rewrite rules are considered
		  bound, with a special ``binding occurrence'' (the
		  one on the LHS, limiting considerations to left
		  linear rules).
		  Interesting for typed rules.},
  bibliographies = {RelMiCS}
}

@PhDThesis{vanBenthem-1976,
  author = {Johan F.A.K. van Benthem},
  school = {Mathematisch Inst.\null{} \&\ Inst.\null{} voor
      Grondslagenonderzoek, Univ.\null{} Amsterdam},
  title = {Modal Correspondence Theory},
  year = 1976,
  bibliographies = {RelMiCS}
}

@Book{vanBenthem-1983,
  author = {van Benthem, Johan F.A.K.},
  title = {The Logic of Time},
  publisher = Reidel,
  address = {Dordrecht, NL},
  year = 1983,
  bibliographies = {RelMiCS}
}

@InCollection{vanBenthem-1989,
  author = {Johan F.A.K. van Benthem},
  address = {Amsterdam},
  booktitle = {Logic Colloquium 1988},
  editor = {Garrido, M.},
  publisher = NoHo,
  title = {Semantic Parallels in Natural Language and Computation},
  year = 1989,
  bibliographies = {RelMiCS}
}

@Article{vanBenthem-1991a,
  author = {van Benthem, Johan F.A.K.},
  title = {Language in Action},
  journal = JPHIL,
  volume = 20,
  number = 3,
  year = 1991,
  month = AUG,
  pages = {225--264},
  bibliographies = {RelMiCS}
}

@InProceedings{vanBenthem-1993,
  author = {Johan F.A.K. van Benthem},
  address = {Amsterdam},
  booktitle = {Proc.\null{} {{$9^{th}$} Internat.\null{} Congress of
      Logic, Methodology and Philosophy of Science, Uppsala 1991}},
  editor = {D. Prawitz and B. Skyrms and D. Westerst{\aa}hl},
  pages = {693--724},
  publisher = Elsevier,
  title = {Logic and the flow of information},
  year = 1993,
  bibliographies = {RelMiCS}
}

@InCollection{vanBenthem-1994,
  author = {Johan F.A.K. van Benthem},
  address = {Cambridge, MA},
  booktitle = {Logic and Information Flow},
  editor = {Eijck, J.~van and Visser, A.},
  publisher = MIT_P,
  title = {Dynamic arrow logic},
  year = 1994,
  bibliographies = {RelMiCS}
}

@TechReport{vanBenthem-1994a,
  author = {Johan F.A.K. van Benthem},
  address = {Stanford Univ.},
  institution = CSLI,
  note = {to appear in  Logic Colloquium, 1994, North-Holland},
  number = {93-197},
  title = {Programming operations that are safe for bisimulations},
  type = {CSLI Research Report},
  year = 1993,
  bibliographies = {RelMiCS}
}

@InCollection{vanBenthem-1996,
  author = {Johan F.A.K. van Benthem and Muskens, R. and Visser, A.},
  address = {Amsterdam},
  booktitle = {Handbook of Logic and Language},
  editor = {Johan F.A.K. van Benthem and ter~Meulen, A.},
  publisher = Elsevier,
  title = {Dynamics},
  year = {to appear},
  bibliographies = {RelMiCS}
}

@Book{vanBenthem1991,
  author = {Johan F.A.K. van Benthem},
  address = {Amsterdam},
  publisher = NoHo,
  series = {Studies in Logic},
  title = {Language in Action},
  volume = 130,
  year = 1991,
  bibliographies = {RelMiCS}
}

@InProceedings{vanDeursen-1991,
  author = {van Deursen, Arie},
  title = {An Algebraic Specification for the Static Semantics of Pascal},
  year = 1991,
  WKloc = {A-0529}
}

@InProceedings{vanDeursen-Dinesh-1993,
  author = {van Deursen, Arie and T.B. Dinesh},
  title = {Origin tracking for higher-order rewrite systems},
  crossref = {HOA1993},
  pages = {76--95},
  WKloc = {A-0114},
  bibliographies = {RelMiCS},
  abstract = {{\em Origin Tracking} is a technique which, in the
		  framework of first-order term rewriting systems,
		  establishes relations between each subterm $t$ of a
		  normal form and a set of subterms, the {\em origins
		  of $t$}, in the initial term. Origin tracking is
		  based on the notion of residuals. It has been used
		  successfully for the generation of error handlers
		  and debuggers from algebraic specifications of
		  programming languages. Recent experiments with the
		  use of higher-order algebraic specifications for the
		  definition of programming languages, reveaked a need
		  to extend origin tracking to higher-order term
		  rewriting systems. This extension is discussed,
		  covering a definition and some alternatives, as well
		  as an assessment with respect to existing specifications.}
}

@Misc{vanDeursen-Klint-Tip-199X,
  author = {van Deursen, Arie and P. Klint and F. Tip},
  title = {Origin Tracking},
  year = {199X},
  WKloc = {A-0528}
}

@InProceedings{vanDeursen-Klint-Verhoef-1999,
  author = {van Deursen, Arie and P. Klint and Chris Verhoef},
  title = {Research Issues in the Renovation of Legacy Systems},
  crossref = {FASE1999},
  pages = {1--21},
  bibliographies = {SQRL},
  WKloc = {A-1301, doc/pap/BIB},
  abstract = {The goals of this tutorial are to: (i) give the reader a
      quick introduction to the field of software renovation as a whole;
      (ii) show that many techniques from compiler technology and formal
      methods can be applied; (iii) demonstrate that research should be
      driven by real-life, industrial, case studies; and (iv) indicate that
      many challenging problems are still unsolved. During the presentation
      of this turorial, demonstrations will be given of several of the case
      studies discussed here.}
}

@InProceedings{vanEekelen-Plasmeijer-1987,
  author = {M. van Eekelen and R. Plasmeijer},
  editor = {J. H. Fasel and R. M. Keller},
  title = {Specification of Reduction Strategies in Term
                 Rewriting Systems},
  booktitle = {Graph Reduction: Proceedings of a Workshop at Santa
                 F{\'e}, New Mexico},
  pages = {215--239},
  publisher = Springer,
  address = {New York, NY},
  year = 1987,
  keywords = {functional trs},
  ISBN = {0-387-18420-1},
  abstract = {Three formal methods for specifying reduction
                 strategies in TRS's are presented.},
  series = LNCS,
  volume = 279
}

@InProceedings{vanEekelen-Smetsers-Plasmeijer-1996,
  author = {van Eekelen, Marko and Sjaak Smetsers and Rinus Plasmeijer},
  title = {Graph Rewriting Semantics for Functional Programming Languages},
  crossref = {CSL1996},
  UniBwM = {INF700/Z8122-10},
  pages = {106--128},
  abstract = {The lambda calculus forms without any question {\em the}
      theoretical backbone of functional programming languages. For the
      design and implementation of the lazy functional programming language
      Concurrent Clean we have used a related computational model: Term
      Graph Rewriting Systems (TGRSs). This paper wraps up our main
      conclusions after 10 years of experience with graph rewriting
      semantics for functional programming languages. TGRSs are not a
      direct extension of the lambda calculus, so one sometimes has to
      re-establish known theoretical results`. But TGRSs are that much
      closer to the world of functional programming that its use has been
      proven to be very wothwhile. in TGRSs functions have names, there are
      constants, patern matching and one can choose to either share
      expressions or copy them.

      Graph reduction very accurately models the essential behaviour of
      most implementations of functional languages and therefore it forms a
      good base for reasoning about reduction properties as well as the
      time and space consumption of functional applications.

      With uniqueness typing important information can be derived for
      efficient implementation and for purely functional interfacing with
      imparative programs.},
  WKloc = {A-0795}
}

@Misc{vanEijck-2003,
  author =	 {van Eijck, Jan},
  title =	 {Constraint Tableux for Hybrid Logics},
  month =	 FEB,
  year =	 2003,
  URL = {http://www.cwi.nl/^jve/hylotab},
  WKloc = 	 {A-1508},
  bibliographies = {HHOL}
}

@Book{vanGasteren-1990,
  author = {van Gasteren, A. J. M. (Antonetta J. M.)},
  title = {On the shape of mathematical arguments},
  year = 1990,
  series = LNCS,
  volume = 445,
  publisher = Springer,
  pages = {viii + 180},
  ISBN = {978-3-540-52849-4},
  DOI = {10.1007/BFb0020908},
  URL = {http://www.springerlink.com/content/k97761678620/},
  keywords = {proof theory},
  WKloc = {B-0105}
}

@Book{vanHeijenoort-1967,
  author = {Jean van Heijenoort},
  title = {{From Frege to G\"odel: A Source Book in Mathematical
		Logic, 1879--1931}},
  publisher = HarvardUP,
  address = {Cambridge, MA},
  year = 1967,
  pages = {xi+660},
  bibliographies = {RelMiCS}
}

@Book{vanOosten-1995,
  author = {van Oosten, Jaap},
  title = {Basic Category Theory},
  publisher = {BRICS, Department of Computer Science, University of Aarhus},
  year = 1995,
  number = {LS-95-1},
  series = {BRICS Lecture Series},
  month = jan,
  ISSN = {1395-2048},
  WKloc = {B-0040}
}

@TechReport{vanOostrom-1990,
  author = {van Oostrom, Vincent},
  title = {Lambda Calculus with Patterns},
  institution = {Vrije Universiteit, Amsterdam},
  year = 1990,
  number = {IR 228},
  month = NOV,
  WKloc = {A-1380, doc/pap/BIB},
  annote = {GFA}
}

@PhDThesis{vanOostrom-1994,
  author = {van Oostrom, Vincent},
  title = {Confluence for Abstract and Higher-Order Rewriting},
  school = {Vrije Universiteit te Amsterdam},
  year = 1994,
  WKloc = {A-1414, doc/pap/BIB},
  annote = {confluence by decreasing diagrams, confluence modulo}
}

@InProceedings{vanOostrom-1995,
  author = {van Oostrom, Vincent},
  title = {Development Closed Critical Pairs},
  crossref = {HOA1995},
  OPTpages = {},
  WKloc = {A-1413, doc/pap/BIB},
  abstract = {The class of orthogonal rewriting systems (rewriting systems
      where rewrite steps cannot depend on one another) is the main class
      of not-necessarily-terminating rewriting systems for which confluence
      is known to hold. Huet and Toyama have shown that for left-linear
      first-order term rewriting systems (TRSs) the orthogonality
      restriction can be relaxed somewhat by allowing \emph{critical pairs}
      (arising from maximally general ways of dependence between steps),
      but requiring them to be parallel closed. We extend these results by
      replacing the parallel closed condition by a \emph{development
      closed} condition. This also permits to generalise them to
      higher-order term rewriting, yielding a confluence criterion for
      Klop's combinatory reduction systems (CRSs), Khasidashvili's
      expression reduction systems (ERSs), and Nipkow's higher-order
      pattern rewriting systems (PRSs).}
}

@InProceedings{vanOostrom-1996,
  author = {van Oostrom, Vincent},
  title = {Higher-Order Families},
  crossref = {RTA96},
  pages = {392--407},
  OPTabstract = {},
  WKloc = {A-0558}
}

@Misc{vanOostrom-1997,
  OPTkey = {},
  OPTauthor = {van Oostrom, Vincent},
  OPTtitle = {Finite Family Developments},
  OPThowpublished = {},
  OPTmonth = {},
  OPTyear = 1997,
  WKloc = {A-1412, doc/pap/BIB},
  abstract = {Consider Adam and Eve. Count generations starting from them.
     Supposing that there will always be people, then it's true that
     for any generation $X$, eventually there will be people
     belonging to the next generation $X + 1$.

     In this paper the same result is established for the class of
     higher order pattern rewriting systems.}
}

@InProceedings{vanOostrom-vandeLooij-Zwisterlood-,
  author = 	 {van Oostrom, Vincent and van de Looij, Kees-Jan and Marijn Zwisterlood},
  title = 	 {Lambdascope, Another Optimal Implementation of the Lambda-Calculus},
  OPTcrossref =  {},
  OPTkey = 	 {},
  OPTbooktitle = {},
  OPTpages = 	 {},
  OPTyear = 	 {},
  OPTeditor = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTseries = 	 {},
  OPTaddress = 	 {},
  OPTmonth = 	 {},
  OPTorganization = {},
  OPTpublisher = {},
  WKloc = 	 {A-1649},
  OPTannote = 	 {}
}

@InProceedings{vanOostrom-vanRaamsdonk-1993,
  author = {van Oostrom, Vincent and van Raamsdonk, Femke},
  title = {Comparing combinatory reduction systems and
		  higher-order rewrite systems},
  crossref = {HOA1993},
  pages = {276--304},
  WKloc = {A-0111},
  abstract = {In this paper two formats of higher-order rewriting
		  are compared: Combinatory Reduction Systems
		  introduced by Klop and Higher-order Rewrite Systems
		  defined by Nipkow. Although it always has been
		  obvious that both formats are closely related to
		  each other, up to now the exact relationship between
		  them has not been clear. This was an unsatisfying
		  situation since it meant that proofs for much
		  related frameworks were given twice. We present two
		  translations, one from Combinatory Reduction Systems
		  into Higher-order Rewrite Systems and one vice
		  versa, based on a detailed comparison of both
		  formats. Since the translations are very `neat' in
		  the sense that the rewrite relation is preserved and
		  (almost) reflected, we can conclude that as far as
		  theory is concerned, Combinatory Reduction Systems
		  and Higher-order Rewrite Systems are equivalent, the
		  only difference being that Combinatory Reduction
		  Systems employ a more `lazy' evaluation strategy.
		  Moreover, due to this result it is the case that
		  some syntactic properties derived for one class
		  also hold for the other.},
  ACMcat = {AMS: 68Q50, CR: F4.1.1, F.3.3.3},
  keywords = {lambda-calculus, developments, confluence},
  bibliographies = {RelMiCS}
}

@InProceedings{vanRaamsdonk-1993,
  author = {van Raamsdonk, F.},
  title = {Confluence and Superdevelopments},
  crossref = {RTA93},
  pages = {168--182},
  WKloc = {A-0132},
  abstract = {In this paper a short proof is presentes for confluence of a
             quite general class of reduction systems, containing
             $\lambda$-calculus and term rewrite system: the orthogonal
             combinatory reduction systems. Combinatory reduction system
             (CRSs for short) were introduced by klop generalizing an idea
             of Aczel. In CRSs, the usual first-order term rewriting format
             is extended with binding structures for variables. This permits
             to express besides first order term rewriting also
             $\lambda$-calculus, extensions of $\lambda$-calculus and proof
             normalizations. Confluence will be proved for orthogonal CRSs,
             that is, for those CRSs having left-linear rules and no
             critical pairs. The proof proceeds along the lines of the proof
             of Tait and Martin-L\"of for confluence of $\lambda$-calculus,
             but uses a different notion of `parallel reduction' as employed
             by Aczel. It gives rise to an extended notion of development,
             called `superdevelopment'. A superdevelopment is a reduction
             sequence in which besides redexes that descend from the initial
             term also some redexes that are created during reduction may be
             contracted. For the case of $\lambda$-calculus, all
             superdevelopments are proved to be finite. A link with the
             confluence proof is provided by proving that superdevelopments
             characterize exactly the Aczel's notion of `parallel reduction'
             used in order to obtain confluence.}
}

@PhDThesis{vanRaamsdonk-1996,
  author = {van Raamsdonk, Femke},
  title = {Confluence and Normalisation for Higher-Order Rewriting},
  school = {Vrije Universiteit te Amsterdam},
  year = 1996,
  WKloc = {B-0070}
}

@InCollection{vanRaamsdonk-2003HOR,
  author =	 {van Raamsdonk, Femke},
  title = 	 {Higher-order Rewriting},
  chapter = 	 11,
  crossref = 	 {Terese-2003},
  pages =	 {588--667}
}

@Article{vanRenesse-Birman-Maffeis-19XX,
  author = {van Renesse, Robbert and Kenneth P. Birman and Silvano Maffeis},
  title = {Horus: A Flexible Group Communications System},
  journal = {Communications of the ACM},
  year = {19??},
  volume = {},
  OPTnumber = {},
  OPTmonth = {},
  pages = {},
  OPTnote = {},
  OPTunibwm = {INF/Z},
  WKloc = {A-0781},
  OPTabstract = {},
  OPTcontents = {},
  OPTannote = {}
}

@InProceedings{vandePol-1993,
  author = {van de Pol, Jaco},
  title = {Proving termination of higher-order rewrite systems},
  crossref = {HOA1993},
  pages = {305--325},
  WKloc = {A-0113},
  keywords = {?},
  contents = {?},
  abstract = {This paper deals with termination proofs for
		  Higher-Order Rewrite Systems (HRSs), introduced in
		  [Nip91, Nip93]. This formalism combines the
		  computational aspects of term rewriting and simply
		  typed lambda calculus. The result is a proof
		  technique for the termination of a HRS, similar to
		  the proof technique ``Termination by interpretation
		  in a well-founded partial ordering. The operations
		  must be strictly monotonic in this ordering. This
		  choice generates a model for the HRS. If the choice
		  can be made in such a way that for each rule the
		  interpretation of the left hand side is greater than
		  the interpretation of the right hand side, then the
		  HRS is terminating. At the end of the paper some
		  applications of this technique are given, which show
		  that this technique is natural and can easily be applied.}
}

@Book{vandeSnepscheut-1993,
  title = {What Computing Is All About},
  series = {Texts and monographs in computer science},
  publisher = Springer,
  pages = 478,
  author = {van de Snepscheut, Jan L. A.},
  address = {New-York},
  year = 1993,
  bibliographies = {RelMiCS}
}

@Misc{vandenBrand-199X,
  author = {van den Brand, M. G. J.},
  title = {Prettyprinting without losing comments},
  year = {199X},
  WKloc = {A-0530}
}

@Article{vandenBrand-Klint-2007,
  author = 	 {Brand, Mark G.J. van den, Klint, Paul},
  title = 	 {{ATerms} for manipulation and exchange of structured data: it's all about sharing},
  journal = 	 {Information and Software Technology},
  year = 	 2007,
  volume = 	 49,
  number = 	 1,
  pages = 	 {55--64},
  WKloc = 	 {A-1682, doc/pap/BIB},
  bibliographies = {HHOL, PMC},
  DOI = {http://dx.doi.org/10.1016/j.infsof.2006.08.009},
  URL = {http://venus.tue.nl/ep-cgi/ep_publ_detail.opl?fac_id=92&rn=20060017&taal=NL&volgnr=201703&code_output=01},
  abstract = 	 {Some data types are so simple that they tend to be
                  reimplemented over and over again. This is certainly
                  true for terms, tree-like data structures that can
                  represent prefix formulae, syntax trees,
                  intermediate code, and more. We fist describe the
                  motivation to introduce Annotated Terms
                  (\textsf{ATerm}s): unifying several term formats,
                  optimizing storage requirements by introducing
                  maximal subterm sharing, and providing a
                  language-neutral exchange format. Next, we present a
                  brief overview of the \textsf{ATerm} technology
                  itself and of its wide range of applications. A
                  discussion of competing technologies and the future
                  of \textsf{ATerm}s concludes the paper.}
}

@Article{vandenBrand-Klint-Verhoef-1998,
  author = {J. van den Brand and P. Klint and C. Verhoef},
  title = {Term Rewriting for Sale},
  journal = {Electronic Notes in Theoretical Computer Science},
  year = 1998,
  volume = 15
}

@Misc{vandenBrand-deJong-Klint-Olivier-2000,
  author = {van den Brand, M. G. J. and de Jong, H. A. and P. Klint and P. A. Olivier},
  title = {Efficient Annotated Terms},
  month = SEP,
  year = 2000,
  annote = {describes the ATerm library},
  WKloc = {A-1044}
}

@InProceedings{vandenBroek-1991,
  author = {van den Broek, P.M.},
  title = {Algebraic Graph Rewriting Using a Single Pushout},
  pages = {90--102},
  WKloc = {A-0204},
  abstract = {We show how graph rewriting can be described with a
		  single pushout in a suitable category of graphs, and
		  compare our result with the conventional approach
		  which uses double pushouts.},
  crossref = {CAAP91}
}

@InProceedings{vandenBroek-1991-x,
  author = {van den Broek, P.M.},
  title = {Algebraic Graph Rewriting Using a Single Pushout},
  pages = {90--102},
  WKloc = {A-0204},
  crossref = {TAPSOFT1991},
  abstract = {We show how graph rewriting can be described with a
		  single pushout in a suitable category of graphs, and
		  compare our result with the conventional approach
		  which uses double pushouts.}
}

@InProceedings{vandenEijnde-1992,
  author = {van den Eijnde, J. P. H. W.},
  title = {Conservative Fixpoint Functions on a Graph},
  pages = {80--100},
  abstract = {In this paper we present a derivation of a general solution
             for a class of programming problems. In these problems a
             function over the vertices of a directed graph is to be
             computed, being defined as a least fixed point of some
             monotonic operator. If this operator satisfies a certain
             restriction with respect to its image for a differential change
             in its argument, it is called conservative, and an elegant
             general solution may be derived. It is stipulated that a
             strictly calculational derivation is only possible if the level
             of abstraction is sufficiently high. To that end a modest
             extension to the functional calculus is proposed, including
             partial functions, and a few simple high level programming
             constructs are introduced. The program scheme obtained is
             applied to a particular example, for which so far no
             ferivation, other than informal ones, is known to exist. The
             solutions presented are not new, but the calculational,
             abstract and compact technique of deriving them is meant to
             improve and complement the current techniques [KNU77, REY81,
             REM84]. It is believed to simplify the derivations for a wider
             algorithm class [EIJ92] than the one treated here.},
  crossref = {MPC1992},
  WKloc = {A-0235}
}

@PhDThesis{vanderMeulen-1994,
  author = {van der Meulen, Emma Anna},
  title = {Incremental Rewriting},
  school = {Universiteit van Amsterdam},
  year = 1994,
  WKloc = {B-0071}
}

@InProceedings{vanderMeyden-1994,
  title = {Axioms for Knowledge and Time in Distributed Systems with
      Perfect Recall},
  author = {Ron van der Meyden},
  pages = {448--457},
  crossref = {LICS9},
  abstract = {A distributed system, possibly asynchronous, is said to have
      perfect recall if at all times each processor's state includes a
      record of all its previous states. The completeness of a
      propositional modal logic of knowledge and time with respect to such
      systems is established. The logic includes modal operators for
      knowledge, and the linear time operators ``next'' and ``until.''}
}

@InProceedings{vanderWeegen-Spitters-2010,
  author =       {van der Weegen, Eelis and Bas Spitters},
  title =        {Developing the algebraic hierarchy with type classes in Coq},
  crossref =  {ITP2010},
  OPTkey =       {},
  OPTbooktitle = {},
  OPTpages =     {},
  OPTyear =      {},
  OPTeditor =    {},
  OPTvolume =    {},
  OPTnumber =    {},
  OPTseries =    {},
  OPTaddress =   {},
  OPTmonth =     {},
  WKloc = {doc/pap/BIB},
  OPTnote =      {},
  OPTannote =    {}
}

@InProceedings{vonKarger-1994,
  author = {von Karger, B.},
  title = {Plotkin, {Hoare} and {Smyth} Order: On Observational
		  Models for {CSP}},
  crossref = {PROCOMET94},
  pages = {377--396},
  keywords = {Semantics of Programming Languages; Concurrent
		  Programming; Semantics of Nondeterminism and
		  Concurrency}
}

@Article{vonKarger-Berghammer-1998,
  title = {A Relational Model for Temporal Logic},
  author = {B. von Karger and R. Berghammer},
  pages = {157--173},
  volume = 6,
  number = 2,
  year = 1998,
  journal = {Logic Journal of the IGPL},
  bibliographies = {RelMiCS},
  abstract = {We use Tarski's relational calculus to construct a model of
      linear temporal logic. Both discrete and dense time are covered and
      we obtain denotational domains for a large variety of reactive
      systems.}
}

@TechReport{vonMohrenschildt-1999,
  author = {von Mohrenschildt, Martin},
  title = {Communicating Software Specifications using {XML}: {OpenSpec}},
  institution = {McMaster University, Department of Computing and Software},
  year = 1999,
  type = {CRL REPORT},
  number = 373,
  WKloc = {A-1123},
  bibliographies = {RelMiCS, SpecTech}
}

@InCollection{vonNeumann-1956,
  author = 	 {J. von Neumann},
  title = 	 {Probabilistic Logics and the Synthesis of Reliable Organisms From Unreliable Components},
  crossref =  {Shannon-McCarthy-1956},
  pages =	 {43--98}
}

@InCollection{vonNeumann-1966,
  author = {J. von Neumann},
  title = {Theory of Self-Reproducing Automata},
  booktitle = {Cellular Automata?},
  publisher = {University of Illinois},
  year = 1966,
  editor = {A. W. Burks},
  address = {Urbana Champaign}
}

@Misc{vonOheimb-1995,
  author = {von Oheimb, David},
  title = {Zur Konstruktion eines auf Isabelle gest\"utzten Beweissystems f\"ur die Relationenalgebra},
  howpublished = {TU M\"unchen, Fortgeschrittenenpraktikum bei Tobias Nipkow, betreut von Thomas Gritzner},
  month = FEB,
  year = 1995,
  WKloc = {B-0104},
  keywords = {RALL},
  bibliographies = {RelMiCS}
}

@InProceedings{vonOheimb-Gritzner-1997,
  author = {von Oheimb, David and  Thomas F. Gritzner},
  title = {{RALL}: Machine-supported Proofs for Relation Algebra},
  crossref = {CADE1997},
  URL = {http://www4.informatik.tu-muenchen.de/papers/CADE-14_oheimb_1997_Conference.html},
  abstract = {We present a theorem proving system for abstract relation algebra
        called RALL (= Relation-Algebraic Language and Logic),
        based on the generic theorem prover Isabelle. On the one hand,
        the system is an advanced case study for Isabelle/HOL,
        and on the other hand, a quite mature proof assistant for
        research on the relational calculus.
        RALL is able to deal with the full language of heterogeneous
        relation algebra including higher-order operators and domain
        constructions, and checks the type-correctness of all formulas
        involved. It offers both an interactive proof facility,
        with special support for substitutions and estimations,
        and an experimental automatic prover.
        The automatic proof method exploits an isomorphism between
        relation-algebraic and predicate-logical formulas,
        relying on the classical universal-algebraic concepts of
        atom structures and complex algebras.},
  CRClassification = {F.4.1, G.2.0},
  CRGenTerms = {Languages, Theory, Verification},
  pages = {380--394},
  bibliographies = {RelMiCS},
  WKloc = {A-1041}
}

@ZZZ{zzz-Local-variables,
  locvars = {
% {Matching brace for below
% Local variables:
% folded-file: t
% fold-internal-margins: 0
% eval: (fold-set-marks "@" "}")
% eval: (fold-whole-buffer)
% end:
  .}
}
